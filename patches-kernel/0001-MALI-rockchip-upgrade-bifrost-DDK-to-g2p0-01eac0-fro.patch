From c0720cd95e81ef4b991e9954b3344757d00fdb7c Mon Sep 17 00:00:00 2001
From: Zhen Chen <chenzhen@rock-chips.com>
Date: Thu, 12 Nov 2020 11:41:18 +0800
Subject: [PATCH] MALI: rockchip: upgrade bifrost DDK to g2p0-01eac0, from
 r25p1-01bet0

In addition, rename some configs that have been used in drivers/gpu/arm/midgard.

Change-Id: I5356d6b3f544195eb6bebe88927abe7c8bcb7dd1
Signed-off-by: Zhen Chen <chenzhen@rock-chips.com>
---
 .../devicetree/bindings/arm/mali-bifrost.txt  |    4 +-
 drivers/gpu/arm/bifrost/Kbuild                |   34 +-
 drivers/gpu/arm/bifrost/Mconfig               |    2 +-
 .../arm/bifrost/arbiter/mali_kbase_arbif.c    |    8 +-
 .../bifrost/arbiter/mali_kbase_arbiter_pm.c   |  103 +-
 drivers/gpu/arm/bifrost/backend/gpu/Kbuild    |    6 +-
 .../gpu/mali_kbase_cache_policy_backend.c     |    4 +-
 .../gpu/mali_kbase_clk_rate_trace_mgr.c       |  287 ++
 .../gpu/mali_kbase_clk_rate_trace_mgr.h       |  155 +
 .../gpu/mali_kbase_debug_job_fault_backend.c  |    4 +-
 .../backend/gpu/mali_kbase_device_hw.c        |  387 --
 .../backend/gpu/mali_kbase_device_internal.h  |  127 -
 .../backend/gpu/mali_kbase_gpuprops_backend.c |   20 +-
 .../backend/gpu/mali_kbase_instr_backend.c    |  113 +-
 .../backend/gpu/mali_kbase_instr_defs.h       |    3 +
 .../backend/gpu/mali_kbase_irq_linux.c        |    7 +-
 .../bifrost/backend/gpu/mali_kbase_jm_as.c    |    2 +
 .../bifrost/backend/gpu/mali_kbase_jm_defs.h  |    2 +
 .../bifrost/backend/gpu/mali_kbase_jm_hw.c    |   45 +-
 .../backend/gpu/mali_kbase_jm_internal.h      |    6 +-
 .../bifrost/backend/gpu/mali_kbase_jm_rb.c    |   11 +-
 .../backend/gpu/mali_kbase_js_backend.c       |   20 +-
 .../backend/gpu/mali_kbase_l2_mmu_config.c    |   39 +-
 .../backend/gpu/mali_kbase_pm_backend.c       |  147 +-
 .../bifrost/backend/gpu/mali_kbase_pm_ca.c    |    2 +
 .../bifrost/backend/gpu/mali_kbase_pm_defs.h  |   43 +
 .../backend/gpu/mali_kbase_pm_driver.c        |  375 +-
 .../backend/gpu/mali_kbase_pm_internal.h      |   37 +-
 .../backend/gpu/mali_kbase_pm_mcu_states.h    |   39 +
 .../backend/gpu/mali_kbase_pm_metrics.c       |    6 +
 .../backend/gpu/mali_kbase_pm_policy.c        |   19 +
 .../backend/gpu/mali_kbase_pm_shader_states.h |    2 +
 .../arm/bifrost/backend/gpu/mali_kbase_time.c |   21 +-
 drivers/gpu/arm/bifrost/build.bp              |    3 +
 .../context/backend/mali_kbase_context_csf.c  |  177 +
 .../context/backend/mali_kbase_context_jm.c   |   67 +-
 .../arm/bifrost/context/mali_kbase_context.c  |  137 +-
 drivers/gpu/arm/bifrost/csf/Kbuild            |   40 +
 .../arm/bifrost/csf/mali_base_csf_kernel.h    |  598 +++
 .../csf/mali_gpu_csf_control_registers.h      |   33 +
 .../arm/bifrost/csf/mali_gpu_csf_registers.h  | 1252 +++++
 drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c  | 2547 ++++++++++
 drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h  |  444 ++
 .../bifrost/csf/mali_kbase_csf_csg_debugfs.c  |  460 ++
 .../bifrost/csf/mali_kbase_csf_csg_debugfs.h  |   48 +
 .../gpu/arm/bifrost/csf/mali_kbase_csf_defs.h |  883 ++++
 .../arm/bifrost/csf/mali_kbase_csf_firmware.c | 1993 ++++++++
 .../arm/bifrost/csf/mali_kbase_csf_firmware.h |  663 +++
 .../bifrost/csf/mali_kbase_csf_firmware_cfg.c |  306 ++
 .../bifrost/csf/mali_kbase_csf_firmware_cfg.h |   72 +
 .../csf/mali_kbase_csf_firmware_no_mali.c     | 1012 ++++
 .../csf/mali_kbase_csf_heap_context_alloc.c   |  196 +
 .../csf/mali_kbase_csf_heap_context_alloc.h   |   76 +
 .../arm/bifrost/csf/mali_kbase_csf_ioctl.h    |  379 ++
 .../gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c | 1737 +++++++
 .../gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h |  305 ++
 .../bifrost/csf/mali_kbase_csf_kcpu_debugfs.c |  199 +
 .../bifrost/csf/mali_kbase_csf_kcpu_debugfs.h |   38 +
 .../csf/mali_kbase_csf_protected_memory.c     |  120 +
 .../csf/mali_kbase_csf_protected_memory.h     |   72 +
 .../bifrost/csf/mali_kbase_csf_reset_gpu.c    |  355 ++
 .../bifrost/csf/mali_kbase_csf_scheduler.c    | 4135 +++++++++++++++++
 .../bifrost/csf/mali_kbase_csf_scheduler.h    |  408 ++
 .../bifrost/csf/mali_kbase_csf_tiler_heap.c   |  584 +++
 .../bifrost/csf/mali_kbase_csf_tiler_heap.h   |  113 +
 .../csf/mali_kbase_csf_tiler_heap_debugfs.c   |  107 +
 .../csf/mali_kbase_csf_tiler_heap_debugfs.h   |   38 +
 .../csf/mali_kbase_csf_tiler_heap_def.h       |  112 +
 .../arm/bifrost/csf/mali_kbase_csf_timeout.c  |  169 +
 .../arm/bifrost/csf/mali_kbase_csf_timeout.h  |   69 +
 .../bifrost/csf/mali_kbase_csf_tl_reader.c    |  555 +++
 .../bifrost/csf/mali_kbase_csf_tl_reader.h    |  181 +
 .../bifrost/csf/mali_kbase_csf_trace_buffer.c |  623 +++
 .../bifrost/csf/mali_kbase_csf_trace_buffer.h |  177 +
 .../mali_kbase_debug_ktrace_codes_csf.h       |  116 +
 .../mali_kbase_debug_ktrace_codes_jm.h        |    3 +
 .../backend/mali_kbase_debug_ktrace_csf.c     |  143 +
 .../backend/mali_kbase_debug_ktrace_csf.h     |  148 +
 .../mali_kbase_debug_ktrace_defs_csf.h        |   85 +
 .../backend/mali_kbase_debug_ktrace_defs_jm.h |   39 +-
 .../backend/mali_kbase_debug_ktrace_jm.c      |    2 +
 .../backend/mali_kbase_debug_ktrace_jm.h      |  111 +-
 .../mali_kbase_debug_linux_ktrace_csf.h       |  147 +
 .../mali_kbase_debug_linux_ktrace_jm.h        |   45 +-
 .../bifrost/debug/mali_kbase_debug_ktrace.c   |   36 +-
 .../bifrost/debug/mali_kbase_debug_ktrace.h   |   11 +-
 .../debug/mali_kbase_debug_ktrace_codes.h     |    7 +
 .../debug/mali_kbase_debug_ktrace_defs.h      |   51 +-
 .../debug/mali_kbase_debug_linux_ktrace.h     |   26 +-
 .../device/backend/mali_kbase_device_csf.c    |  274 ++
 .../device/backend/mali_kbase_device_hw_csf.c |  161 +
 .../device/backend/mali_kbase_device_hw_jm.c  |  100 +
 .../device/backend/mali_kbase_device_jm.c     |    8 +-
 .../arm/bifrost/device/mali_kbase_device.c    |   60 +-
 .../arm/bifrost/device/mali_kbase_device.h    |  106 +
 .../arm/bifrost/device/mali_kbase_device_hw.c |  184 +
 .../device/mali_kbase_device_internal.h       |    6 +-
 .../gpu/backend/mali_kbase_gpu_fault_csf.c    |  105 +
 .../gpu/backend/mali_kbase_gpu_fault_jm.c     |    6 +-
 .../gpu/backend/mali_kbase_gpu_regmap_csf.h   |  297 ++
 .../gpu/backend/mali_kbase_gpu_regmap_jm.h    |   28 +-
 .../arm/bifrost/gpu/mali_kbase_gpu_fault.h    |   11 -
 .../gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h   |    1 +
 .../arm/bifrost/gpu/mali_kbase_gpu_regmap.h   |   21 +-
 drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c  |    7 +-
 drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.h  |    3 +-
 .../ipa/mali_kbase_ipa_vinstr_common.c        |    5 +-
 .../bifrost/ipa/mali_kbase_ipa_vinstr_g7x.c   |   36 +-
 .../gpu/arm/bifrost/jm/mali_base_jm_kernel.h  |   86 +-
 .../gpu/arm/bifrost/jm/mali_kbase_jm_defs.h   |   30 +-
 .../gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h  |   86 +-
 .../arm/bifrost/mali_base_hwconfig_features.h |   31 +-
 .../arm/bifrost/mali_base_hwconfig_issues.h   |   49 +
 drivers/gpu/arm/bifrost/mali_base_kernel.h    |   32 +-
 drivers/gpu/arm/bifrost/mali_kbase.h          |   96 +-
 drivers/gpu/arm/bifrost/mali_kbase_caps.h     |   65 +
 drivers/gpu/arm/bifrost/mali_kbase_ccswe.c    |  105 +
 drivers/gpu/arm/bifrost/mali_kbase_ccswe.h    |   97 +
 drivers/gpu/arm/bifrost/mali_kbase_config.h   |   84 +-
 .../gpu/arm/bifrost/mali_kbase_core_linux.c   |  665 ++-
 .../arm/bifrost/mali_kbase_cs_experimental.h  |    3 -
 .../gpu/arm/bifrost/mali_kbase_ctx_sched.c    |   19 +-
 drivers/gpu/arm/bifrost/mali_kbase_defs.h     |  189 +-
 .../gpu/arm/bifrost/mali_kbase_dma_fence.c    |   27 +-
 .../gpu/arm/bifrost/mali_kbase_dma_fence.h    |   14 +-
 .../gpu/arm/bifrost/mali_kbase_dummy_job_wa.c |    4 +-
 .../gpu/arm/bifrost/mali_kbase_dummy_job_wa.h |   29 +
 drivers/gpu/arm/bifrost/mali_kbase_event.c    |   10 +
 drivers/gpu/arm/bifrost/mali_kbase_fence.c    |   60 -
 drivers/gpu/arm/bifrost/mali_kbase_fence.h    |    2 +
 .../gpu/arm/bifrost/mali_kbase_fence_defs.h   |    6 +-
 .../gpu/arm/bifrost/mali_kbase_fence_ops.c    |   84 +
 .../bifrost/mali_kbase_gpu_memory_debugfs.h   |   16 +-
 drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c |   13 +
 drivers/gpu/arm/bifrost/mali_kbase_gwt.c      |    4 +
 drivers/gpu/arm/bifrost/mali_kbase_hw.c       |   14 +-
 .../arm/bifrost/mali_kbase_hwaccess_instr.h   |    4 +-
 .../gpu/arm/bifrost/mali_kbase_hwaccess_jm.h  |    2 +
 .../arm/bifrost/mali_kbase_hwaccess_time.h    |   14 +
 drivers/gpu/arm/bifrost/mali_kbase_hwcnt.c    |   24 +-
 .../arm/bifrost/mali_kbase_hwcnt_backend.h    |    7 +-
 .../bifrost/mali_kbase_hwcnt_backend_gpu.c    |  510 --
 .../arm/bifrost/mali_kbase_hwcnt_backend_jm.c |  736 +++
 ...nd_gpu.h => mali_kbase_hwcnt_backend_jm.h} |   18 +-
 .../gpu/arm/bifrost/mali_kbase_hwcnt_gpu.c    |  276 +-
 .../gpu/arm/bifrost/mali_kbase_hwcnt_gpu.h    |   48 +-
 .../gpu/arm/bifrost/mali_kbase_hwcnt_legacy.c |    4 +-
 .../gpu/arm/bifrost/mali_kbase_hwcnt_reader.h |   41 +-
 .../gpu/arm/bifrost/mali_kbase_hwcnt_types.c  |   94 +-
 .../gpu/arm/bifrost/mali_kbase_hwcnt_types.h  |   81 +-
 drivers/gpu/arm/bifrost/mali_kbase_ioctl.h    |   26 +-
 drivers/gpu/arm/bifrost/mali_kbase_jd.c       |  168 +-
 .../gpu/arm/bifrost/mali_kbase_jd_debugfs.c   |   10 +-
 drivers/gpu/arm/bifrost/mali_kbase_jm.c       |    4 +
 drivers/gpu/arm/bifrost/mali_kbase_jm.h       |    4 +
 .../gpu/arm/bifrost/mali_kbase_kinstr_jm.c    |  911 ++++
 .../gpu/arm/bifrost/mali_kbase_kinstr_jm.h    |  283 ++
 .../arm/bifrost/mali_kbase_kinstr_jm_reader.h |   70 +
 drivers/gpu/arm/bifrost/mali_kbase_mem.c      |  631 ++-
 drivers/gpu/arm/bifrost/mali_kbase_mem.h      |  309 +-
 .../gpu/arm/bifrost/mali_kbase_mem_linux.c    |  432 +-
 .../gpu/arm/bifrost/mali_kbase_mem_linux.h    |   18 +-
 .../bifrost/mali_kbase_mem_profile_debugfs.c  |    9 +-
 .../arm/bifrost/mali_kbase_mipe_gen_header.h  |    2 +
 drivers/gpu/arm/bifrost/mali_kbase_pm.c       |   32 +-
 drivers/gpu/arm/bifrost/mali_kbase_pm.h       |    7 +
 .../bifrost/mali_kbase_regs_history_debugfs.c |  125 +-
 .../bifrost/mali_kbase_regs_history_debugfs.h |   36 +-
 .../gpu/arm/bifrost/mali_kbase_reset_gpu.h    |   12 +-
 drivers/gpu/arm/bifrost/mali_kbase_softjobs.c |   32 +-
 drivers/gpu/arm/bifrost/mali_kbase_sync.h     |    8 +
 .../gpu/arm/bifrost/mali_kbase_sync_common.c  |    2 +
 .../gpu/arm/bifrost/mali_kbase_sync_file.c    |   10 +-
 .../arm/bifrost/mali_kbase_trace_gpu_mem.c    |  227 +
 .../arm/bifrost/mali_kbase_trace_gpu_mem.h    |  103 +
 drivers/gpu/arm/bifrost/mali_kbase_vinstr.c   |  185 +-
 drivers/gpu/arm/bifrost/mali_linux_trace.h    |   28 +-
 .../bifrost/mali_power_gpu_frequency_trace.c  |   27 +
 .../bifrost/mali_power_gpu_frequency_trace.h  |   69 +
 .../bifrost/mmu/backend/mali_kbase_mmu_csf.c  |  532 +++
 .../bifrost/mmu/backend/mali_kbase_mmu_jm.c   |   40 +-
 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c  |   96 +-
 drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h  |   40 +-
 .../bifrost/mmu/mali_kbase_mmu_hw_direct.c    |   10 +-
 .../arm/bifrost/mmu/mali_kbase_mmu_internal.h |   42 +-
 .../arm/bifrost/platform/devicetree/Kbuild    |    5 +-
 .../devicetree/mali_kbase_clk_rate_trace.c    |   68 +
 .../devicetree/mali_kbase_config_platform.h   |    5 +-
 drivers/gpu/arm/bifrost/tests/Kbuild          |    3 +-
 drivers/gpu/arm/bifrost/tests/Kconfig         |    3 +-
 .../bifrost/tests/include/kutf/kutf_helpers.h |   10 +-
 .../gpu/arm/bifrost/tests/kutf/kutf_helpers.c |    8 +-
 .../gpu/arm/bifrost/tests/kutf/kutf_suite.c   |    5 +
 .../mali_kutf_clk_rate_trace/kernel/Kbuild    |   26 +
 .../mali_kutf_clk_rate_trace/kernel/Kconfig   |   30 +
 .../mali_kutf_clk_rate_trace/kernel/Makefile  |   57 +
 .../mali_kutf_clk_rate_trace/kernel/build.bp  |   34 +
 .../kernel/mali_kutf_clk_rate_trace_test.c    |  890 ++++
 .../mali_kutf_clk_rate_trace_test.h           |  148 +
 .../mali_kutf_irq_test_main.c                 |    4 +-
 .../arm/bifrost/thirdparty/mali_kbase_mmap.c  |    2 +
 .../tl/backend/mali_kbase_timeline_csf.c      |  172 +
 .../gpu/arm/bifrost/tl/mali_kbase_timeline.c  |   34 +
 .../arm/bifrost/tl/mali_kbase_timeline_io.c   |   33 +
 .../arm/bifrost/tl/mali_kbase_timeline_priv.h |    7 +
 .../gpu/arm/bifrost/tl/mali_kbase_tlstream.c  |    8 +
 .../gpu/arm/bifrost/tl/mali_kbase_tlstream.h  |    3 +
 .../arm/bifrost/tl/mali_kbase_tracepoints.c   |  326 +-
 .../arm/bifrost/tl/mali_kbase_tracepoints.h   |  865 +++-
 209 files changed, 34033 insertions(+), 2984 deletions(-)
 create mode 100644 drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c
 create mode 100644 drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.h
 delete mode 100644 drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_hw.c
 delete mode 100644 drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_internal.h
 create mode 100644 drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_mcu_states.h
 create mode 100644 drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/Kbuild
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_base_csf_kernel.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_gpu_csf_control_registers.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_gpu_csf_registers.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_def.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.h
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c
 create mode 100644 drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h
 create mode 100644 drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h
 create mode 100644 drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c
 create mode 100644 drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.h
 create mode 100644 drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_csf.h
 create mode 100644 drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h
 create mode 100644 drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c
 create mode 100644 drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c
 create mode 100644 drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c
 create mode 100644 drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c
 create mode 100644 drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c
 create mode 100644 drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_csf.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_caps.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_ccswe.c
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_ccswe.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c
 delete mode 100644 drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_gpu.c
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_jm.c
 rename drivers/gpu/arm/bifrost/{mali_kbase_hwcnt_backend_gpu.h => mali_kbase_hwcnt_backend_jm.h} (79%)
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm_reader.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.c
 create mode 100644 drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.h
 create mode 100644 drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.c
 create mode 100644 drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.h
 create mode 100644 drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c
 create mode 100644 drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_clk_rate_trace.c
 create mode 100644 drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kbuild
 create mode 100644 drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kconfig
 create mode 100644 drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Makefile
 create mode 100644 drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/build.bp
 create mode 100644 drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c
 create mode 100644 drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/mali_kutf_clk_rate_trace_test.h
 create mode 100644 drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c

diff --git a/Documentation/devicetree/bindings/arm/mali-bifrost.txt b/Documentation/devicetree/bindings/arm/mali-bifrost.txt
index dd8f733ce71b..8aaca67d3c9a 100644
--- a/Documentation/devicetree/bindings/arm/mali-bifrost.txt
+++ b/Documentation/devicetree/bindings/arm/mali-bifrost.txt
@@ -64,8 +64,8 @@ for details.
 - power_model : Sets the power model parameters. Defined power models include:
 	  "mali-simple-power-model", "mali-g51-power-model", "mali-g52-power-model",
 	  "mali-g52_r1-power-model", "mali-g71-power-model", "mali-g72-power-model",
-	  "mali-g76-power-model", "mali-g77-power-model", "mali-tnax-power-model"
-	  and "mali-tbex-power-model".
+	  "mali-g76-power-model", "mali-g77-power-model", "mali-tnax-power-model",
+	  "mali-tbex-power-model" and "mali-tbax-power-model".
 	- mali-simple-power-model: this model derives the GPU power usage based
 	  on the GPU voltage scaled by the system temperature. Note: it was
 	  designed for the Juno platform, and may not be suitable for others.
diff --git a/drivers/gpu/arm/bifrost/Kbuild b/drivers/gpu/arm/bifrost/Kbuild
index 443a9b85fdee..c05dc8399027 100644
--- a/drivers/gpu/arm/bifrost/Kbuild
+++ b/drivers/gpu/arm/bifrost/Kbuild
@@ -20,7 +20,7 @@
 #
 
 # Driver version string which is returned to userspace via an ioctl
-MALI_RELEASE_NAME ?= "r25p1-01bet0"
+MALI_RELEASE_NAME ?= "g2p0-01eac0"
 
 # Paths required for build
 
@@ -36,6 +36,7 @@ MALI_USE_CSF ?= 0
 MALI_UNIT_TEST ?= 0
 MALI_KERNEL_TEST_API ?= 0
 MALI_COVERAGE ?= 0
+MALI_JIT_PRESSURE_LIMIT_BASE ?= 1
 CONFIG_MALI_PLATFORM_NAME ?= "devicetree"
 # Experimental features (corresponding -D definition should be appended to
 # DEFINES below, e.g. for MALI_EXPERIMENTAL_FEATURE,
@@ -43,7 +44,6 @@ CONFIG_MALI_PLATFORM_NAME ?= "devicetree"
 #
 # Experimental features must default to disabled, e.g.:
 # MALI_EXPERIMENTAL_FEATURE ?= 0
-MALI_JIT_PRESSURE_LIMIT ?= 0
 MALI_INCREMENTAL_RENDERING ?= 0
 
 # Set up our defines, which will be passed to gcc
@@ -54,7 +54,7 @@ DEFINES = \
 	-DMALI_UNIT_TEST=$(MALI_UNIT_TEST) \
 	-DMALI_COVERAGE=$(MALI_COVERAGE) \
 	-DMALI_RELEASE_NAME=\"$(MALI_RELEASE_NAME)\" \
-	-DMALI_JIT_PRESSURE_LIMIT=$(MALI_JIT_PRESSURE_LIMIT) \
+	-DMALI_JIT_PRESSURE_LIMIT_BASE=$(MALI_JIT_PRESSURE_LIMIT_BASE) \
 	-DMALI_INCREMENTAL_RENDERING=$(MALI_INCREMENTAL_RENDERING)
 
 ifeq ($(KBUILD_EXTMOD),)
@@ -77,7 +77,9 @@ SRC := \
 	context/mali_kbase_context.c \
 	debug/mali_kbase_debug_ktrace.c \
 	device/mali_kbase_device.c \
+	device/mali_kbase_device_hw.c \
 	mali_kbase_cache_policy.c \
+	mali_kbase_ccswe.c \
 	mali_kbase_mem.c \
 	mali_kbase_mem_pool_group.c \
 	mali_kbase_native_mgm.c \
@@ -88,7 +90,7 @@ SRC := \
 	mali_kbase_config.c \
 	mali_kbase_vinstr.c \
 	mali_kbase_hwcnt.c \
-	mali_kbase_hwcnt_backend_gpu.c \
+	mali_kbase_hwcnt_backend_jm.c \
 	mali_kbase_hwcnt_gpu.c \
 	mali_kbase_hwcnt_legacy.c \
 	mali_kbase_hwcnt_types.c \
@@ -113,6 +115,8 @@ SRC := \
 	mali_kbase_strings.c \
 	mali_kbase_as_fault_debugfs.c \
 	mali_kbase_regs_history_debugfs.c \
+	mali_power_gpu_frequency_trace.c \
+	mali_kbase_trace_gpu_mem.c \
 	thirdparty/mali_kbase_mmap.c \
 	tl/mali_kbase_timeline.c \
 	tl/mali_kbase_timeline_io.c \
@@ -124,6 +128,7 @@ ifeq ($(MALI_USE_CSF),1)
 	SRC += \
 		debug/backend/mali_kbase_debug_ktrace_csf.c \
 		device/backend/mali_kbase_device_csf.c \
+		device/backend/mali_kbase_device_hw_csf.c \
 		gpu/backend/mali_kbase_gpu_fault_csf.c \
 		tl/backend/mali_kbase_timeline_csf.c \
 		mmu/backend/mali_kbase_mmu_csf.c \
@@ -137,8 +142,10 @@ else
 		mali_kbase_jd_debugfs.c \
 		mali_kbase_js.c \
 		mali_kbase_js_ctx_attr.c \
+		mali_kbase_kinstr_jm.c \
 		debug/backend/mali_kbase_debug_ktrace_jm.c \
 		device/backend/mali_kbase_device_jm.c \
+		device/backend/mali_kbase_device_hw_jm.c \
 		gpu/backend/mali_kbase_gpu_fault_jm.c \
 		tl/backend/mali_kbase_timeline_jm.c \
 		mmu/backend/mali_kbase_mmu_jm.c \
@@ -191,16 +198,25 @@ else
 # empty
 endif
 
-bifrost_kbase-$(CONFIG_MALI_BIFROST_DMA_FENCE) += \
-	mali_kbase_dma_fence.o \
-	mali_kbase_fence.o
+ifeq ($(MALI_USE_CSF),0)
+	bifrost_kbase-$(CONFIG_MALI_BIFROST_DMA_FENCE) += \
+		mali_kbase_fence_ops.o \
+		mali_kbase_dma_fence.o \
+		mali_kbase_fence.o
+
+	bifrost_kbase-$(CONFIG_SYNC_FILE) += \
+		mali_kbase_fence_ops.o \
+		mali_kbase_fence.o
+endif
+
 bifrost_kbase-$(CONFIG_SYNC) += \
 	mali_kbase_sync_android.o \
 	mali_kbase_sync_common.o
+
 bifrost_kbase-$(CONFIG_SYNC_FILE) += \
+	mali_kbase_fence_ops.o \
 	mali_kbase_sync_file.o \
-	mali_kbase_sync_common.o \
-	mali_kbase_fence.o
+	mali_kbase_sync_common.o
 
 include  $(src)/backend/gpu/Kbuild
 bifrost_kbase-y += $(BACKEND:.c=.o)
diff --git a/drivers/gpu/arm/bifrost/Mconfig b/drivers/gpu/arm/bifrost/Mconfig
index 25d384325a77..99ababfc2d16 100644
--- a/drivers/gpu/arm/bifrost/Mconfig
+++ b/drivers/gpu/arm/bifrost/Mconfig
@@ -83,7 +83,7 @@ config MALI_PLATFORM_NAME
 
 config MALI_ARBITER_SUPPORT
 	bool "Enable arbiter support for Mali"
-	depends on MALI_BIFROST
+	depends on MALI_BIFROST && !GPU_HAS_CSF
 	default n
 	help
 	  Enable support for the arbiter interface in the driver.
diff --git a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c
index d193cb99d881..ddf1a0ce0b05 100644
--- a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c
+++ b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbif.c
@@ -38,7 +38,7 @@ static void on_gpu_stop(struct device *dev)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
 
-	KBASE_TLSTREAM_TL_EVENT_ARB_STOP_REQUESTED(kbdev, kbdev);
+	KBASE_TLSTREAM_TL_ARBITER_STOP_REQUESTED(kbdev, kbdev);
 	kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_STOP_EVT);
 }
 
@@ -46,7 +46,7 @@ static void on_gpu_granted(struct device *dev)
 {
 	struct kbase_device *kbdev = dev_get_drvdata(dev);
 
-	KBASE_TLSTREAM_TL_EVENT_ARB_GRANTED(kbdev, kbdev);
+	KBASE_TLSTREAM_TL_ARBITER_GRANTED(kbdev, kbdev);
 	kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_GRANTED_EVT);
 }
 
@@ -106,7 +106,7 @@ int kbase_arbif_init(struct kbase_device *kbdev)
 		err = arb_if->vm_ops.vm_arb_register_dev(arb_if,
 			kbdev->dev, &ops);
 		if (err) {
-			dev_err(kbdev->dev, "Arbiter registration failed.\n");
+			dev_err(&pdev->dev, "Failed to register with arbiter\n");
 			module_put(pdev->dev.driver->owner);
 			return err;
 		}
@@ -149,7 +149,7 @@ void kbase_arbif_gpu_stopped(struct kbase_device *kbdev, u8 gpu_required)
 
 	if (arb_if && arb_if->vm_ops.vm_arb_gpu_stopped) {
 		dev_dbg(kbdev->dev, "%s\n", __func__);
-		KBASE_TLSTREAM_TL_EVENT_ARB_STOPPED(kbdev, kbdev);
+		KBASE_TLSTREAM_TL_ARBITER_STOPPED(kbdev, kbdev);
 		arb_if->vm_ops.vm_arb_gpu_stopped(arb_if, gpu_required);
 	}
 }
diff --git a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c
index 6c35e165009b..02b5de2436ea 100644
--- a/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c
+++ b/drivers/gpu/arm/bifrost/arbiter/mali_kbase_arbiter_pm.c
@@ -189,6 +189,7 @@ int kbase_arbiter_pm_early_init(struct kbase_device *kbdev)
 
 	err = kbase_arbif_init(kbdev);
 	if (err) {
+		dev_err(kbdev->dev, "Failed to initialise arbif module\n");
 		goto arbif_init_fail;
 	}
 	if (kbdev->arb.arb_if) {
@@ -214,9 +215,10 @@ void kbase_arbiter_pm_early_term(struct kbase_device *kbdev)
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 
 	mutex_lock(&arb_vm_state->vm_state_lock);
-	if (arb_vm_state->vm_state > KBASE_VM_STATE_STOPPED_GPU_REQUESTED)
+	if (arb_vm_state->vm_state > KBASE_VM_STATE_STOPPED_GPU_REQUESTED) {
+		kbase_pm_set_gpu_lost(kbdev, false);
 		kbase_arbif_gpu_stopped(kbdev, false);
-
+	}
 	mutex_unlock(&arb_vm_state->vm_state_lock);
 	kbase_arbif_destroy(kbdev);
 	destroy_workqueue(arb_vm_state->vm_arb_wq);
@@ -271,6 +273,7 @@ void kbase_arbiter_pm_vm_stopped(struct kbase_device *kbdev)
 		break;
 	}
 
+	kbase_pm_set_gpu_lost(kbdev, false);
 	kbase_arbif_gpu_stopped(kbdev, request_gpu);
 }
 
@@ -291,6 +294,7 @@ static void kbase_arbiter_pm_vm_gpu_start(struct kbase_device *kbdev)
 			&arb_vm_state->vm_resume_work);
 		break;
 	case KBASE_VM_STATE_SUSPEND_WAIT_FOR_GRANT:
+		kbase_pm_set_gpu_lost(kbdev, false);
 		kbase_arbif_gpu_stopped(kbdev, false);
 		kbase_arbiter_pm_vm_set_state(kbdev, KBASE_VM_STATE_SUSPENDED);
 		break;
@@ -347,6 +351,7 @@ static void kbase_arbiter_pm_vm_gpu_stop(struct kbase_device *kbdev)
 static void kbase_gpu_lost(struct kbase_device *kbdev)
 {
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
+	bool handle_gpu_lost = false;
 
 	lockdep_assert_held(&arb_vm_state->vm_state_lock);
 
@@ -357,31 +362,38 @@ static void kbase_gpu_lost(struct kbase_device *kbdev)
 		dev_warn(kbdev->dev, "GPU lost in state %s",
 		kbase_arbiter_pm_vm_state_str(arb_vm_state->vm_state));
 		kbase_arbiter_pm_vm_gpu_stop(kbdev);
-		mutex_unlock(&arb_vm_state->vm_state_lock);
-		kbase_pm_handle_gpu_lost(kbdev);
-		mutex_lock(&arb_vm_state->vm_state_lock);
+		handle_gpu_lost = true;
 		break;
 	case KBASE_VM_STATE_STOPPING_IDLE:
 	case KBASE_VM_STATE_STOPPING_ACTIVE:
 	case KBASE_VM_STATE_SUSPEND_PENDING:
-		dev_info(kbdev->dev, "GPU lost while stopping");
-		mutex_unlock(&arb_vm_state->vm_state_lock);
-		kbase_pm_handle_gpu_lost(kbdev);
-		mutex_lock(&arb_vm_state->vm_state_lock);
+		dev_dbg(kbdev->dev, "GPU lost while stopping");
+		handle_gpu_lost = true;
 		break;
 	case KBASE_VM_STATE_SUSPENDED:
 	case KBASE_VM_STATE_STOPPED:
 	case KBASE_VM_STATE_STOPPED_GPU_REQUESTED:
-		dev_info(kbdev->dev, "GPU lost while already stopped");
+		dev_dbg(kbdev->dev, "GPU lost while already stopped");
 		break;
 	case KBASE_VM_STATE_SUSPEND_WAIT_FOR_GRANT:
-		dev_info(kbdev->dev, "GPU lost while waiting to suspend");
+		dev_dbg(kbdev->dev, "GPU lost while waiting to suspend");
 		kbase_arbiter_pm_vm_set_state(kbdev, KBASE_VM_STATE_SUSPENDED);
 		break;
 	default:
 		break;
 	}
-
+	if (handle_gpu_lost) {
+		/* Releasing the VM state lock here is safe because
+		 * we are guaranteed to be in either STOPPING_IDLE,
+		 * STOPPING_ACTIVE or SUSPEND_PENDING at this point.
+		 * The only transitions that are valid from here are to
+		 * STOPPED, STOPPED_GPU_REQUESTED or SUSPENDED which can
+		 * only happen at the completion of the GPU lost handling.
+		 */
+		mutex_unlock(&arb_vm_state->vm_state_lock);
+		kbase_pm_handle_gpu_lost(kbdev);
+		mutex_lock(&arb_vm_state->vm_state_lock);
+	}
 }
 
 static inline bool kbase_arbiter_pm_vm_os_suspend_ready_state(
@@ -506,7 +518,7 @@ void kbase_arbiter_pm_vm_event(struct kbase_device *kbdev,
 		kbase_arbiter_pm_vm_gpu_stop(kbdev);
 		break;
 	case KBASE_VM_GPU_LOST_EVT:
-		dev_info(kbdev->dev, "KBASE_ARBIF_GPU_LOST_EVT!");
+		dev_dbg(kbdev->dev, "KBASE_ARBIF_GPU_LOST_EVT!");
 		kbase_gpu_lost(kbdev);
 		break;
 	case KBASE_VM_OS_SUSPEND_EVENT:
@@ -530,7 +542,7 @@ void kbase_arbiter_pm_vm_event(struct kbase_device *kbdev,
 	case KBASE_VM_REF_EVENT:
 		switch (arb_vm_state->vm_state) {
 		case KBASE_VM_STATE_STARTING:
-			KBASE_TLSTREAM_TL_EVENT_ARB_STARTED(kbdev, kbdev);
+			KBASE_TLSTREAM_TL_ARBITER_STARTED(kbdev, kbdev);
 			/* FALL THROUGH */
 		case KBASE_VM_STATE_IDLE:
 			kbase_arbiter_pm_vm_set_state(kbdev,
@@ -547,15 +559,21 @@ void kbase_arbiter_pm_vm_event(struct kbase_device *kbdev,
 		break;
 
 	case KBASE_VM_GPU_INITIALIZED_EVT:
-		lockdep_assert_held(&kbdev->pm.lock);
-		if (kbdev->pm.active_count > 0) {
-			kbase_arbiter_pm_vm_set_state(kbdev,
-				KBASE_VM_STATE_ACTIVE);
-			kbase_arbif_gpu_active(kbdev);
-		} else {
-			kbase_arbiter_pm_vm_set_state(kbdev,
-				KBASE_VM_STATE_IDLE);
-			kbase_arbif_gpu_idle(kbdev);
+		switch (arb_vm_state->vm_state) {
+		case KBASE_VM_STATE_INITIALIZING_WITH_GPU:
+			lockdep_assert_held(&kbdev->pm.lock);
+			if (kbdev->pm.active_count > 0) {
+				kbase_arbiter_pm_vm_set_state(kbdev,
+					KBASE_VM_STATE_ACTIVE);
+				kbase_arbif_gpu_active(kbdev);
+			} else {
+				kbase_arbiter_pm_vm_set_state(kbdev,
+					KBASE_VM_STATE_IDLE);
+				kbase_arbif_gpu_idle(kbdev);
+			}
+			break;
+		default:
+			break;
 		}
 		break;
 
@@ -566,6 +584,8 @@ void kbase_arbiter_pm_vm_event(struct kbase_device *kbdev,
 	mutex_unlock(&arb_vm_state->vm_state_lock);
 }
 
+KBASE_EXPORT_TEST_API(kbase_arbiter_pm_vm_event);
+
 static void kbase_arbiter_pm_vm_wait_gpu_assignment(struct kbase_device *kbdev)
 {
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
@@ -592,6 +612,7 @@ int kbase_arbiter_pm_ctx_active_handle_suspend(struct kbase_device *kbdev,
 {
 	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
 	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
+	int res = 0;
 
 	if (kbdev->arb.arb_if) {
 		mutex_lock(&arb_vm_state->vm_state_lock);
@@ -612,21 +633,31 @@ int kbase_arbiter_pm_ctx_active_handle_suspend(struct kbase_device *kbdev,
 
 			if (suspend_handler !=
 				KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE) {
-				if (suspend_handler ==
-					KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED
-						||
-						kbdev->pm.active_count > 0)
-					break;
 
-				mutex_unlock(&arb_vm_state->vm_state_lock);
-				mutex_unlock(&kbdev->pm.lock);
-				mutex_unlock(&js_devdata->runpool_mutex);
-				return 1;
-			}
+				/* In case of GPU lost, even if
+				 * active_count > 0, we no longer have GPU
+				 * access
+				 */
+				if (kbase_pm_is_gpu_lost(kbdev))
+					res = 1;
 
-			if (arb_vm_state->vm_state ==
-					KBASE_VM_STATE_INITIALIZING_WITH_GPU)
+				switch (suspend_handler) {
+				case KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE:
+					res = 1;
+					break;
+				case KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE:
+					if (kbdev->pm.active_count == 0)
+						res = 1;
+					break;
+				case KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED:
+					break;
+				default:
+					WARN(1, "Unknown suspend_handler\n");
+					res = 1;
+					break;
+				}
 				break;
+			}
 
 			/* Need to synchronously wait for GPU assignment */
 			arb_vm_state->vm_arb_users_waiting++;
@@ -641,5 +672,5 @@ int kbase_arbiter_pm_ctx_active_handle_suspend(struct kbase_device *kbdev,
 		}
 		mutex_unlock(&arb_vm_state->vm_state_lock);
 	}
-	return 0;
+	return res;
 }
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/Kbuild b/drivers/gpu/arm/bifrost/backend/gpu/Kbuild
index f0090e97dd48..b48ab4c51875 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/Kbuild
+++ b/drivers/gpu/arm/bifrost/backend/gpu/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2014-2019 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2014-2020 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -21,7 +21,6 @@
 
 BACKEND += \
 	backend/gpu/mali_kbase_cache_policy_backend.c \
-	backend/gpu/mali_kbase_device_hw.c \
 	backend/gpu/mali_kbase_gpuprops_backend.c \
 	backend/gpu/mali_kbase_irq_linux.c \
 	backend/gpu/mali_kbase_instr_backend.c \
@@ -34,7 +33,8 @@ BACKEND += \
 	backend/gpu/mali_kbase_pm_coarse_demand.c \
 	backend/gpu/mali_kbase_pm_policy.c \
 	backend/gpu/mali_kbase_time.c \
-	backend/gpu/mali_kbase_l2_mmu_config.c
+	backend/gpu/mali_kbase_l2_mmu_config.c \
+	backend/gpu/mali_kbase_clk_rate_trace_mgr.c
 
 ifeq ($(MALI_USE_CSF),1)
 # empty
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.c
index 7378bfd7b397..4e07a3f9d83f 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_cache_policy_backend.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015-2016,2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2016, 2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -21,7 +21,7 @@
  */
 
 #include "backend/gpu/mali_kbase_cache_policy_backend.h"
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 
 void kbase_cache_set_coherency_mode(struct kbase_device *kbdev,
 		u32 mode)
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c
new file mode 100644
index 000000000000..187d7d6f6926
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.c
@@ -0,0 +1,287 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * Implementation of the GPU clock rate trace manager.
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_config_defaults.h>
+#include <linux/clk.h>
+#include <asm/div64.h>
+#include "mali_kbase_clk_rate_trace_mgr.h"
+
+#ifdef CONFIG_TRACE_POWER_GPU_FREQUENCY
+#include <trace/events/power_gpu_frequency.h>
+#else
+#include "mali_power_gpu_frequency_trace.h"
+#endif
+
+#ifndef CLK_RATE_TRACE_OPS
+#define CLK_RATE_TRACE_OPS (NULL)
+#endif
+
+static int gpu_clk_rate_change_notifier(struct notifier_block *nb,
+			unsigned long event, void *data)
+{
+	struct kbase_gpu_clk_notifier_data *ndata = data;
+	struct kbase_clk_data *clk_data =
+		container_of(nb, struct kbase_clk_data, clk_rate_change_nb);
+	struct kbase_clk_rate_trace_manager *clk_rtm = clk_data->clk_rtm;
+	unsigned long flags;
+
+	if (WARN_ON_ONCE(clk_data->gpu_clk_handle != ndata->gpu_clk_handle))
+		return NOTIFY_BAD;
+
+	spin_lock_irqsave(&clk_rtm->lock, flags);
+	if (event == POST_RATE_CHANGE) {
+		if (!clk_rtm->gpu_idle &&
+		    (clk_data->clock_val != ndata->new_rate)) {
+			kbase_clk_rate_trace_manager_notify_all(
+				clk_rtm, clk_data->index, ndata->new_rate);
+		}
+
+		clk_data->clock_val = ndata->new_rate;
+	}
+	spin_unlock_irqrestore(&clk_rtm->lock, flags);
+
+	return NOTIFY_DONE;
+}
+
+static int gpu_clk_data_init(struct kbase_device *kbdev,
+		void *gpu_clk_handle, unsigned int index)
+{
+	struct kbase_clk_rate_trace_op_conf *callbacks =
+		(struct kbase_clk_rate_trace_op_conf *)CLK_RATE_TRACE_OPS;
+	struct kbase_clk_data *clk_data;
+	struct kbase_clk_rate_trace_manager *clk_rtm = &kbdev->pm.clk_rtm;
+	int ret = 0;
+
+	if (WARN_ON(!callbacks) ||
+	    WARN_ON(!gpu_clk_handle) ||
+	    WARN_ON(index >= BASE_MAX_NR_CLOCKS_REGULATORS))
+		return -EINVAL;
+
+	clk_data = kzalloc(sizeof(*clk_data), GFP_KERNEL);
+	if (!clk_data) {
+		dev_err(kbdev->dev, "Failed to allocate data for clock enumerated at index %u", index);
+		return -ENOMEM;
+	}
+
+	clk_data->index = (u8)index;
+	clk_data->gpu_clk_handle = gpu_clk_handle;
+	/* Store the initial value of clock */
+	clk_data->clock_val =
+		callbacks->get_gpu_clk_rate(kbdev, gpu_clk_handle);
+
+	{
+		/* At the initialization time, GPU is powered off. */
+		unsigned long flags;
+
+		spin_lock_irqsave(&clk_rtm->lock, flags);
+		kbase_clk_rate_trace_manager_notify_all(
+			clk_rtm, clk_data->index, 0);
+		spin_unlock_irqrestore(&clk_rtm->lock, flags);
+	}
+
+	clk_data->clk_rtm = clk_rtm;
+	clk_rtm->clks[index] = clk_data;
+
+	clk_data->clk_rate_change_nb.notifier_call =
+			gpu_clk_rate_change_notifier;
+
+	ret = callbacks->gpu_clk_notifier_register(kbdev, gpu_clk_handle,
+			&clk_data->clk_rate_change_nb);
+	if (ret) {
+		dev_err(kbdev->dev, "Failed to register notifier for clock enumerated at index %u", index);
+		kfree(clk_data);
+	}
+
+	return ret;
+}
+
+int kbase_clk_rate_trace_manager_init(struct kbase_device *kbdev)
+{
+	struct kbase_clk_rate_trace_op_conf *callbacks =
+		(struct kbase_clk_rate_trace_op_conf *)CLK_RATE_TRACE_OPS;
+	struct kbase_clk_rate_trace_manager *clk_rtm = &kbdev->pm.clk_rtm;
+	unsigned int i;
+	int ret = 0;
+
+	/* Return early if no callbacks provided for clock rate tracing */
+	if (!callbacks)
+		return 0;
+
+	spin_lock_init(&clk_rtm->lock);
+	INIT_LIST_HEAD(&clk_rtm->listeners);
+
+	clk_rtm->gpu_idle = true;
+
+	for (i = 0; i < BASE_MAX_NR_CLOCKS_REGULATORS; i++) {
+		void *gpu_clk_handle =
+			callbacks->enumerate_gpu_clk(kbdev, i);
+
+		if (!gpu_clk_handle)
+			break;
+
+		ret = gpu_clk_data_init(kbdev, gpu_clk_handle, i);
+		if (ret)
+			goto error;
+	}
+
+	/* Activate clock rate trace manager if at least one GPU clock was
+	 * enumerated.
+	 */
+	if (i)
+		WRITE_ONCE(clk_rtm->clk_rate_trace_ops, callbacks);
+	else
+		dev_info(kbdev->dev, "No clock(s) available for rate tracing");
+
+	return 0;
+
+error:
+	while (i--) {
+		clk_rtm->clk_rate_trace_ops->gpu_clk_notifier_unregister(
+				kbdev, clk_rtm->clks[i]->gpu_clk_handle,
+				&clk_rtm->clks[i]->clk_rate_change_nb);
+		kfree(clk_rtm->clks[i]);
+	}
+
+	return ret;
+}
+
+void kbase_clk_rate_trace_manager_term(struct kbase_device *kbdev)
+{
+	struct kbase_clk_rate_trace_manager *clk_rtm = &kbdev->pm.clk_rtm;
+	unsigned int i;
+
+	WARN_ON(!list_empty(&clk_rtm->listeners));
+
+	if (!clk_rtm->clk_rate_trace_ops)
+		return;
+
+	for (i = 0; i < BASE_MAX_NR_CLOCKS_REGULATORS; i++) {
+		if (!clk_rtm->clks[i])
+			break;
+
+		clk_rtm->clk_rate_trace_ops->gpu_clk_notifier_unregister(
+				kbdev, clk_rtm->clks[i]->gpu_clk_handle,
+				&clk_rtm->clks[i]->clk_rate_change_nb);
+		kfree(clk_rtm->clks[i]);
+	}
+
+	WRITE_ONCE(clk_rtm->clk_rate_trace_ops, NULL);
+}
+
+void kbase_clk_rate_trace_manager_gpu_active(struct kbase_device *kbdev)
+{
+	struct kbase_clk_rate_trace_manager *clk_rtm = &kbdev->pm.clk_rtm;
+	unsigned int i;
+	unsigned long flags;
+
+	if (!clk_rtm->clk_rate_trace_ops)
+		return;
+
+	spin_lock_irqsave(&clk_rtm->lock, flags);
+
+	for (i = 0; i < BASE_MAX_NR_CLOCKS_REGULATORS; i++) {
+		struct kbase_clk_data *clk_data = clk_rtm->clks[i];
+
+		if (!clk_data)
+			break;
+
+		if (unlikely(!clk_data->clock_val))
+			continue;
+
+		kbase_clk_rate_trace_manager_notify_all(
+			clk_rtm, clk_data->index, clk_data->clock_val);
+	}
+
+	clk_rtm->gpu_idle = false;
+	spin_unlock_irqrestore(&clk_rtm->lock, flags);
+}
+
+void kbase_clk_rate_trace_manager_gpu_idle(struct kbase_device *kbdev)
+{
+	struct kbase_clk_rate_trace_manager *clk_rtm = &kbdev->pm.clk_rtm;
+	unsigned int i;
+	unsigned long flags;
+
+	if (!clk_rtm->clk_rate_trace_ops)
+		return;
+
+	spin_lock_irqsave(&clk_rtm->lock, flags);
+
+	for (i = 0; i < BASE_MAX_NR_CLOCKS_REGULATORS; i++) {
+		struct kbase_clk_data *clk_data = clk_rtm->clks[i];
+
+		if (!clk_data)
+			break;
+
+		if (unlikely(!clk_data->clock_val))
+			continue;
+
+		kbase_clk_rate_trace_manager_notify_all(
+			clk_rtm, clk_data->index, 0);
+	}
+
+	clk_rtm->gpu_idle = true;
+	spin_unlock_irqrestore(&clk_rtm->lock, flags);
+}
+
+void kbase_clk_rate_trace_manager_notify_all(
+	struct kbase_clk_rate_trace_manager *clk_rtm,
+	u32 clk_index,
+	unsigned long new_rate)
+{
+	struct kbase_clk_rate_listener *pos;
+	struct kbase_device *kbdev;
+
+	lockdep_assert_held(&clk_rtm->lock);
+
+	kbdev = container_of(clk_rtm, struct kbase_device, pm.clk_rtm);
+
+	dev_dbg(kbdev->dev, "GPU clock %u rate changed to %lu",
+		clk_index, new_rate);
+
+	/* Raise standard `power/gpu_frequency` ftrace event */
+	{
+		unsigned long new_rate_khz = new_rate;
+
+#if BITS_PER_LONG == 64
+		do_div(new_rate_khz, 1000);
+#elif BITS_PER_LONG == 32
+		new_rate_khz /= 1000;
+#else
+#error "unsigned long division is not supported for this architecture"
+#endif
+
+		trace_gpu_frequency(new_rate_khz, clk_index);
+	}
+
+	/* Notify the listeners. */
+	list_for_each_entry(pos, &clk_rtm->listeners, node) {
+		pos->notify(pos, clk_index, new_rate);
+	}
+}
+KBASE_EXPORT_TEST_API(kbase_clk_rate_trace_manager_notify_all);
+
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.h
new file mode 100644
index 000000000000..dcafb26ea4c0
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_clk_rate_trace_mgr.h
@@ -0,0 +1,155 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CLK_RATE_TRACE_MGR_
+#define _KBASE_CLK_RATE_TRACE_MGR_
+
+/** The index of top clock domain in kbase_clk_rate_trace_manager:clks. */
+#define KBASE_CLOCK_DOMAIN_TOP (0)
+
+/** The index of shader-cores clock domain in
+ * kbase_clk_rate_trace_manager:clks.
+ */
+#define KBASE_CLOCK_DOMAIN_SHADER_CORES (1)
+
+/**
+ * struct kbase_clk_data - Data stored per enumerated GPU clock.
+ *
+ * @clk_rtm:            Pointer to clock rate trace manager object.
+ * @gpu_clk_handle:     Handle unique to the enumerated GPU clock.
+ * @plat_private:       Private data for the platform to store into
+ * @clk_rate_change_nb: notifier block containing the pointer to callback
+ *                      function that is invoked whenever the rate of
+ *                      enumerated GPU clock changes.
+ * @clock_val:          Current rate of the enumerated GPU clock.
+ * @index:              Index at which the GPU clock was enumerated.
+ */
+struct kbase_clk_data {
+	struct kbase_clk_rate_trace_manager *clk_rtm;
+	void *gpu_clk_handle;
+	void *plat_private;
+	struct notifier_block clk_rate_change_nb;
+	unsigned long clock_val;
+	u8 index;
+};
+
+/**
+ * kbase_clk_rate_trace_manager_init - Initialize GPU clock rate trace manager.
+ *
+ * @kbdev:      Device pointer
+ *
+ * Return: 0 if success, or an error code on failure.
+ */
+int kbase_clk_rate_trace_manager_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_clk_rate_trace_manager_term - Terminate GPU clock rate trace manager.
+ *
+ *  @kbdev:      Device pointer
+ */
+void kbase_clk_rate_trace_manager_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_clk_rate_trace_manager_gpu_active - Inform GPU clock rate trace
+ *                                           manager of GPU becoming active.
+ *
+ * @kbdev:      Device pointer
+ */
+void kbase_clk_rate_trace_manager_gpu_active(struct kbase_device *kbdev);
+
+/**
+ * kbase_clk_rate_trace_manager_gpu_idle - Inform GPU clock rate trace
+ *                                         manager of GPU becoming idle.
+ * @kbdev:      Device pointer
+ */
+void kbase_clk_rate_trace_manager_gpu_idle(struct kbase_device *kbdev);
+
+/**
+ * kbase_clk_rate_trace_manager_subscribe_no_lock() - Add freq change listener.
+ *
+ * @clk_rtm:    Clock rate manager instance.
+ * @listener:   Listener handle
+ *
+ * kbase_clk_rate_trace_manager:lock must be held by the caller.
+ */
+static inline void kbase_clk_rate_trace_manager_subscribe_no_lock(
+	struct kbase_clk_rate_trace_manager *clk_rtm,
+	struct kbase_clk_rate_listener *listener)
+{
+	lockdep_assert_held(&clk_rtm->lock);
+	list_add(&listener->node, &clk_rtm->listeners);
+}
+
+/**
+ * kbase_clk_rate_trace_manager_subscribe() - Add freq change listener.
+ *
+ * @clk_rtm:    Clock rate manager instance.
+ * @listener:   Listener handle
+ */
+static inline void kbase_clk_rate_trace_manager_subscribe(
+	struct kbase_clk_rate_trace_manager *clk_rtm,
+	struct kbase_clk_rate_listener *listener)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&clk_rtm->lock, flags);
+	kbase_clk_rate_trace_manager_subscribe_no_lock(
+		clk_rtm, listener);
+	spin_unlock_irqrestore(&clk_rtm->lock, flags);
+}
+
+/**
+ * kbase_clk_rate_trace_manager_unsubscribe() - Remove freq change listener.
+ *
+ * @clk_rtm:    Clock rate manager instance.
+ * @listener:   Listener handle
+ */
+static inline void kbase_clk_rate_trace_manager_unsubscribe(
+	struct kbase_clk_rate_trace_manager *clk_rtm,
+	struct kbase_clk_rate_listener *listener)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&clk_rtm->lock, flags);
+	list_del(&listener->node);
+	spin_unlock_irqrestore(&clk_rtm->lock, flags);
+}
+
+/**
+ * kbase_clk_rate_trace_manager_notify_all() - Notify all clock \
+ *                                             rate listeners.
+ *
+ * @clk_rtm:     Clock rate manager instance.
+ * @clk_index:   Clock index.
+ * @new_rate:    New clock frequency(Hz)
+ *
+ * kbase_clk_rate_trace_manager:lock must be locked.
+ * This function is exported to be used by clock rate trace test
+ * portal.
+ */
+void kbase_clk_rate_trace_manager_notify_all(
+	struct kbase_clk_rate_trace_manager *clk_rtm,
+	u32 clock_index,
+	unsigned long new_rate);
+
+#endif /* _KBASE_CLK_RATE_TRACE_MGR_ */
+
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c
index b05844ef4f50..3aadcb04160c 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_debug_job_fault_backend.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2015,2018-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2015, 2018-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -21,7 +21,7 @@
  */
 
 #include <mali_kbase.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include "mali_kbase_debug_job_fault.h"
 
 #ifdef CONFIG_DEBUG_FS
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_hw.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_hw.c
deleted file mode 100644
index 55cff3e2f5b6..000000000000
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_hw.c
+++ /dev/null
@@ -1,387 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014-2016, 2018-2020 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- * SPDX-License-Identifier: GPL-2.0
- *
- */
-
-
-/*
- *
- */
-#include <mali_kbase.h>
-#include <gpu/mali_kbase_gpu_fault.h>
-#include <backend/gpu/mali_kbase_instr_internal.h>
-#include <backend/gpu/mali_kbase_pm_internal.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
-#include <mali_kbase_reset_gpu.h>
-#include <mmu/mali_kbase_mmu.h>
-
-#if !defined(CONFIG_MALI_BIFROST_NO_MALI)
-
-#ifdef CONFIG_DEBUG_FS
-
-int kbase_io_history_resize(struct kbase_io_history *h, u16 new_size)
-{
-	struct kbase_io_access *old_buf;
-	struct kbase_io_access *new_buf;
-	unsigned long flags;
-
-	if (!new_size)
-		goto out_err; /* The new size must not be 0 */
-
-	new_buf = vmalloc(new_size * sizeof(*h->buf));
-	if (!new_buf)
-		goto out_err;
-
-	spin_lock_irqsave(&h->lock, flags);
-
-	old_buf = h->buf;
-
-	/* Note: we won't bother with copying the old data over. The dumping
-	 * logic wouldn't work properly as it relies on 'count' both as a
-	 * counter and as an index to the buffer which would have changed with
-	 * the new array. This is a corner case that we don't need to support.
-	 */
-	h->count = 0;
-	h->size = new_size;
-	h->buf = new_buf;
-
-	spin_unlock_irqrestore(&h->lock, flags);
-
-	vfree(old_buf);
-
-	return 0;
-
-out_err:
-	return -1;
-}
-
-
-int kbase_io_history_init(struct kbase_io_history *h, u16 n)
-{
-	h->enabled = false;
-	spin_lock_init(&h->lock);
-	h->count = 0;
-	h->size = 0;
-	h->buf = NULL;
-	if (kbase_io_history_resize(h, n))
-		return -1;
-
-	return 0;
-}
-
-
-void kbase_io_history_term(struct kbase_io_history *h)
-{
-	vfree(h->buf);
-	h->buf = NULL;
-}
-
-
-/* kbase_io_history_add - add new entry to the register access history
- *
- * @h: Pointer to the history data structure
- * @addr: Register address
- * @value: The value that is either read from or written to the register
- * @write: 1 if it's a register write, 0 if it's a read
- */
-static void kbase_io_history_add(struct kbase_io_history *h,
-		void __iomem const *addr, u32 value, u8 write)
-{
-	struct kbase_io_access *io;
-	unsigned long flags;
-
-	spin_lock_irqsave(&h->lock, flags);
-
-	io = &h->buf[h->count % h->size];
-	io->addr = (uintptr_t)addr | write;
-	io->value = value;
-	++h->count;
-	/* If count overflows, move the index by the buffer size so the entire
-	 * buffer will still be dumped later */
-	if (unlikely(!h->count))
-		h->count = h->size;
-
-	spin_unlock_irqrestore(&h->lock, flags);
-}
-
-
-void kbase_io_history_dump(struct kbase_device *kbdev)
-{
-	struct kbase_io_history *const h = &kbdev->io_history;
-	u16 i;
-	size_t iters;
-	unsigned long flags;
-
-	if (!unlikely(h->enabled))
-		return;
-
-	spin_lock_irqsave(&h->lock, flags);
-
-	dev_err(kbdev->dev, "Register IO History:");
-	iters = (h->size > h->count) ? h->count : h->size;
-	dev_err(kbdev->dev, "Last %zu register accesses of %zu total:\n", iters,
-			h->count);
-	for (i = 0; i < iters; ++i) {
-		struct kbase_io_access *io =
-			&h->buf[(h->count - iters + i) % h->size];
-		char const access = (io->addr & 1) ? 'w' : 'r';
-
-		dev_err(kbdev->dev, "%6i: %c: reg 0x%016lx val %08x\n", i,
-			access, (unsigned long)(io->addr & ~0x1), io->value);
-	}
-
-	spin_unlock_irqrestore(&h->lock, flags);
-}
-
-
-#endif /* CONFIG_DEBUG_FS */
-
-
-void kbase_reg_write(struct kbase_device *kbdev, u32 offset, u32 value)
-{
-	KBASE_DEBUG_ASSERT(kbdev->pm.backend.gpu_powered);
-	KBASE_DEBUG_ASSERT(kbdev->dev != NULL);
-
-	writel(value, kbdev->reg + offset);
-
-#ifdef CONFIG_DEBUG_FS
-	if (unlikely(kbdev->io_history.enabled))
-		kbase_io_history_add(&kbdev->io_history, kbdev->reg + offset,
-				value, 1);
-#endif /* CONFIG_DEBUG_FS */
-	dev_dbg(kbdev->dev, "w: reg %08x val %08x", offset, value);
-}
-
-KBASE_EXPORT_TEST_API(kbase_reg_write);
-
-u32 kbase_reg_read(struct kbase_device *kbdev, u32 offset)
-{
-	u32 val;
-	KBASE_DEBUG_ASSERT(kbdev->pm.backend.gpu_powered);
-	KBASE_DEBUG_ASSERT(kbdev->dev != NULL);
-
-	val = readl(kbdev->reg + offset);
-
-#ifdef CONFIG_DEBUG_FS
-	if (unlikely(kbdev->io_history.enabled))
-		kbase_io_history_add(&kbdev->io_history, kbdev->reg + offset,
-				val, 0);
-#endif /* CONFIG_DEBUG_FS */
-	dev_dbg(kbdev->dev, "r: reg %08x val %08x", offset, val);
-
-	return val;
-}
-
-KBASE_EXPORT_TEST_API(kbase_reg_read);
-
-bool kbase_is_gpu_lost(struct kbase_device *kbdev)
-{
-	u32 val;
-
-	val = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_ID));
-
-	return val == 0;
-}
-#endif /* !defined(CONFIG_MALI_BIFROST_NO_MALI) */
-
-/**
- * kbase_report_gpu_fault - Report a GPU fault.
- * @kbdev:    Kbase device pointer
- * @multiple: Zero if only GPU_FAULT was raised, non-zero if MULTIPLE_GPU_FAULTS
- *            was also set
- *
- * This function is called from the interrupt handler when a GPU fault occurs.
- * It reports the details of the fault using dev_warn().
- */
-static void kbase_report_gpu_fault(struct kbase_device *kbdev, int multiple)
-{
-	u32 status = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_FAULTSTATUS));
-	u64 address = (u64) kbase_reg_read(kbdev,
-			GPU_CONTROL_REG(GPU_FAULTADDRESS_HI)) << 32;
-
-	address |= kbase_reg_read(kbdev,
-			GPU_CONTROL_REG(GPU_FAULTADDRESS_LO));
-
-	dev_warn(kbdev->dev, "GPU Fault 0x%08x (%s) at 0x%016llx",
-		status,
-		kbase_gpu_exception_name(status & 0xFF),
-		address);
-	if (multiple)
-		dev_warn(kbdev->dev, "There were multiple GPU faults - some have not been reported\n");
-}
-
-static bool kbase_gpu_fault_interrupt(struct kbase_device *kbdev, int multiple)
-{
-	kbase_report_gpu_fault(kbdev, multiple);
-	return false;
-}
-
-void kbase_gpu_start_cache_clean_nolock(struct kbase_device *kbdev)
-{
-	u32 irq_mask;
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	if (kbdev->cache_clean_in_progress) {
-		/* If this is called while another clean is in progress, we
-		 * can't rely on the current one to flush any new changes in
-		 * the cache. Instead, trigger another cache clean immediately
-		 * after this one finishes.
-		 */
-		kbdev->cache_clean_queued = true;
-		return;
-	}
-
-	/* Enable interrupt */
-	irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK));
-	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK),
-				irq_mask | CLEAN_CACHES_COMPLETED);
-
-	KBASE_KTRACE_ADD(kbdev, CORE_GPU_CLEAN_INV_CACHES, NULL, 0);
-	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
-					GPU_COMMAND_CLEAN_INV_CACHES);
-
-	kbdev->cache_clean_in_progress = true;
-}
-
-void kbase_gpu_start_cache_clean(struct kbase_device *kbdev)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbase_gpu_start_cache_clean_nolock(kbdev);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-}
-
-void kbase_gpu_cache_clean_wait_complete(struct kbase_device *kbdev)
-{
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	kbdev->cache_clean_queued = false;
-	kbdev->cache_clean_in_progress = false;
-	wake_up(&kbdev->cache_clean_wait);
-}
-
-static void kbase_clean_caches_done(struct kbase_device *kbdev)
-{
-	u32 irq_mask;
-	unsigned long flags;
-
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-
-	if (kbdev->cache_clean_queued) {
-		kbdev->cache_clean_queued = false;
-
-		KBASE_KTRACE_ADD(kbdev, CORE_GPU_CLEAN_INV_CACHES, NULL, 0);
-		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
-				GPU_COMMAND_CLEAN_INV_CACHES);
-	} else {
-		/* Disable interrupt */
-		irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK));
-		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK),
-				irq_mask & ~CLEAN_CACHES_COMPLETED);
-
-		kbase_gpu_cache_clean_wait_complete(kbdev);
-	}
-
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-}
-
-static inline bool get_cache_clean_flag(struct kbase_device *kbdev)
-{
-	bool cache_clean_in_progress;
-	unsigned long flags;
-
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	cache_clean_in_progress = kbdev->cache_clean_in_progress;
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-
-	return cache_clean_in_progress;
-}
-
-void kbase_gpu_wait_cache_clean(struct kbase_device *kbdev)
-{
-	while (get_cache_clean_flag(kbdev)) {
-		wait_event_interruptible(kbdev->cache_clean_wait,
-				!kbdev->cache_clean_in_progress);
-	}
-}
-
-int kbase_gpu_wait_cache_clean_timeout(struct kbase_device *kbdev,
-				unsigned int wait_timeout_ms)
-{
-	long remaining = msecs_to_jiffies(wait_timeout_ms);
-
-	while (remaining && get_cache_clean_flag(kbdev)) {
-		remaining = wait_event_timeout(kbdev->cache_clean_wait,
-					!kbdev->cache_clean_in_progress,
-					remaining);
-	}
-
-	return (remaining ? 0 : -ETIMEDOUT);
-}
-
-void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
-{
-	bool clear_gpu_fault = false;
-
-	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ, NULL, val);
-	if (val & GPU_FAULT)
-		clear_gpu_fault = kbase_gpu_fault_interrupt(kbdev,
-					val & MULTIPLE_GPU_FAULTS);
-
-	if (val & RESET_COMPLETED)
-		kbase_pm_reset_done(kbdev);
-
-	if (val & PRFCNT_SAMPLE_COMPLETED)
-		kbase_instr_hwcnt_sample_done(kbdev);
-
-	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_CLEAR, NULL, val);
-	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), val);
-
-	/* kbase_pm_check_transitions (called by kbase_pm_power_changed) must
-	 * be called after the IRQ has been cleared. This is because it might
-	 * trigger further power transitions and we don't want to miss the
-	 * interrupt raised to notify us that these further transitions have
-	 * finished. The same applies to kbase_clean_caches_done() - if another
-	 * clean was queued, it might trigger another clean, which might
-	 * generate another interrupt which shouldn't be missed.
-	 */
-
-	if (val & CLEAN_CACHES_COMPLETED)
-		kbase_clean_caches_done(kbdev);
-
-	if (val & POWER_CHANGED_ALL) {
-		kbase_pm_power_changed(kbdev);
-	} else if (val & CLEAN_CACHES_COMPLETED) {
-		/* If cache line evict messages can be lost when shader cores
-		 * power down then we need to flush the L2 cache before powering
-		 * down cores. When the flush completes, the shaders' state
-		 * machine needs to be re-invoked to proceed with powering down
-		 * cores.
-		 */
-		if (kbdev->pm.backend.l2_always_on ||
-				kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_921))
-			kbase_pm_power_changed(kbdev);
-	}
-
-
-	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_DONE, NULL, val);
-}
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_internal.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_internal.h
deleted file mode 100644
index 2e1d0112172e..000000000000
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_device_internal.h
+++ /dev/null
@@ -1,127 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2014,2019-2020 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- * SPDX-License-Identifier: GPL-2.0
- *
- */
-
-
-
-/*
- * Backend-specific HW access device APIs
- */
-
-#ifndef _KBASE_DEVICE_INTERNAL_H_
-#define _KBASE_DEVICE_INTERNAL_H_
-
-/**
- * kbase_reg_write - write to GPU register
- * @kbdev:  Kbase device pointer
- * @offset: Offset of register
- * @value:  Value to write
- *
- * Caller must ensure the GPU is powered (@kbdev->pm.gpu_powered != false).
- */
-void kbase_reg_write(struct kbase_device *kbdev, u32 offset, u32 value);
-
-/**
- * kbase_reg_read - read from GPU register
- * @kbdev:  Kbase device pointer
- * @offset: Offset of register
- *
- * Caller must ensure the GPU is powered (@kbdev->pm.gpu_powered != false).
- *
- * Return: Value in desired register
- */
-u32 kbase_reg_read(struct kbase_device *kbdev, u32 offset);
-
-/**
- * kbase_is_gpu_lost() - Has the GPU been lost.
- * @kbdev:    Kbase device pointer
- *
- * This function will return true if the GPU has been lost.
- * When this happens register reads will be zero. A zero GPU_ID is
- * invalid so this is used to detect GPU_LOST
- *
- * Return: True if GPU LOST
- */
-bool kbase_is_gpu_lost(struct kbase_device *kbdev);
-
-/**
- * kbase_gpu_start_cache_clean - Start a cache clean
- * @kbdev: Kbase device
- *
- * Issue a cache clean and invalidate command to hardware. This function will
- * take hwaccess_lock.
- */
-void kbase_gpu_start_cache_clean(struct kbase_device *kbdev);
-
-/**
- * kbase_gpu_start_cache_clean_nolock - Start a cache clean
- * @kbdev: Kbase device
- *
- * Issue a cache clean and invalidate command to hardware. hwaccess_lock
- * must be held by the caller.
- */
-void kbase_gpu_start_cache_clean_nolock(struct kbase_device *kbdev);
-
-/**
- * kbase_gpu_wait_cache_clean - Wait for cache cleaning to finish
- * @kbdev: Kbase device
- *
- * This function will take hwaccess_lock, and may sleep.
- */
-void kbase_gpu_wait_cache_clean(struct kbase_device *kbdev);
-
-/**
- * kbase_gpu_wait_cache_clean_timeout - Wait for certain time for cache
- *                                      cleaning to finish
- * @kbdev: Kbase device
- * @wait_timeout_ms: Time, in milli seconds, to wait for cache clean to complete.
- *
- * This function will take hwaccess_lock, and may sleep. This is supposed to be
- * called from paths (like GPU reset) where an indefinite wait for the completion
- * of cache clean operation can cause deadlock, as the operation may never
- * complete.
- *
- * Return: 0 if successful or a negative error code on failure.
- */
-int kbase_gpu_wait_cache_clean_timeout(struct kbase_device *kbdev,
-		unsigned int wait_timeout_ms);
-
-/**
- * kbase_gpu_cache_clean_wait_complete - Called after the cache cleaning is
- *                                       finished. Would also be called after
- *                                       the GPU reset.
- * @kbdev: Kbase device
- *
- * Caller must hold the hwaccess_lock.
- */
-void kbase_gpu_cache_clean_wait_complete(struct kbase_device *kbdev);
-
-/**
- * kbase_gpu_interrupt - GPU interrupt handler
- * @kbdev: Kbase device pointer
- * @val:   The value of the GPU IRQ status register which triggered the call
- *
- * This function is called from the interrupt handler when a GPU irq is to be
- * handled.
- */
-void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val);
-
-#endif /* _KBASE_DEVICE_INTERNAL_H_ */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c
index 352afa11907a..60ae0206d6a8 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_gpuprops_backend.c
@@ -26,7 +26,7 @@
  */
 
 #include <mali_kbase.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
 #include <mali_kbase_hwaccess_gpuprops.h>
 
@@ -41,8 +41,12 @@ int kbase_backend_gpuprops_get(struct kbase_device *kbdev,
 
 	registers.l2_features = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(L2_FEATURES));
+#if !MALI_USE_CSF
 	registers.core_features = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(CORE_FEATURES));
+#else /* !MALI_USE_CSF */
+	registers.core_features = 0;
+#endif /* !MALI_USE_CSF */
 	registers.tiler_features = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(TILER_FEATURES));
 	registers.mem_features = kbase_reg_read(kbdev,
@@ -51,12 +55,20 @@ int kbase_backend_gpuprops_get(struct kbase_device *kbdev,
 				GPU_CONTROL_REG(MMU_FEATURES));
 	registers.as_present = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(AS_PRESENT));
+#if !MALI_USE_CSF
 	registers.js_present = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(JS_PRESENT));
+#else /* !MALI_USE_CSF */
+	registers.js_present = 0;
+#endif /* !MALI_USE_CSF */
 
 	for (i = 0; i < GPU_MAX_JOB_SLOTS; i++)
+#if !MALI_USE_CSF
 		registers.js_features[i] = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(JS_FEATURES_REG(i)));
+#else /* !MALI_USE_CSF */
+		registers.js_features[i] = 0;
+#endif /* !MALI_USE_CSF */
 
 	for (i = 0; i < BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS; i++)
 		registers.texture_features[i] = kbase_reg_read(kbdev,
@@ -93,7 +105,7 @@ int kbase_backend_gpuprops_get(struct kbase_device *kbdev,
 	registers.stack_present_hi = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(STACK_PRESENT_HI));
 
-	if (!kbase_is_gpu_lost(kbdev)) {
+	if (!kbase_is_gpu_removed(kbdev)) {
 		*regdump = registers;
 		return 0;
 	} else
@@ -112,7 +124,7 @@ int kbase_backend_gpuprops_get_features(struct kbase_device *kbdev,
 		coherency_features = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(COHERENCY_FEATURES));
 
-		if (kbase_is_gpu_lost(kbdev))
+		if (kbase_is_gpu_removed(kbdev))
 			return -EIO;
 
 		regdump->coherency_features = coherency_features;
@@ -136,7 +148,7 @@ int kbase_backend_gpuprops_get_l2_features(struct kbase_device *kbdev,
 		u32 l2_features = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(L2_FEATURES));
 
-		if (kbase_is_gpu_lost(kbdev))
+		if (kbase_is_gpu_removed(kbdev))
 			return -EIO;
 
 		regdump->l2_features = l2_features;
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c
index 8b320c7ba950..54b07483dee6 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_backend.c
@@ -29,7 +29,7 @@
 #include <mali_kbase.h>
 #include <gpu/mali_kbase_gpu_regmap.h>
 #include <mali_kbase_hwaccess_instr.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_instr_internal.h>
 
 
@@ -39,7 +39,9 @@ int kbase_instr_hwcnt_enable_internal(struct kbase_device *kbdev,
 {
 	unsigned long flags;
 	int err = -EINVAL;
+#if !MALI_USE_CSF
 	u32 irq_mask;
+#endif
 	u32 prfcnt_config;
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
@@ -56,10 +58,12 @@ int kbase_instr_hwcnt_enable_internal(struct kbase_device *kbdev,
 		goto out_err;
 	}
 
+#if !MALI_USE_CSF
 	/* Enable interrupt */
 	irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK));
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), irq_mask |
 						PRFCNT_SAMPLE_COMPLETED);
+#endif
 
 	/* In use, this context is the owner */
 	kbdev->hwcnt.kctx = kctx;
@@ -78,6 +82,29 @@ int kbase_instr_hwcnt_enable_internal(struct kbase_device *kbdev,
 #endif
 		prfcnt_config |= 1 << PRFCNT_CONFIG_SETSELECT_SHIFT;
 
+#if MALI_USE_CSF
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_CONFIG),
+			prfcnt_config | PRFCNT_CONFIG_MODE_OFF);
+
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_BASE_LO),
+					enable->dump_buffer & 0xFFFFFFFF);
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_BASE_HI),
+					enable->dump_buffer >> 32);
+
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_CSHW_EN),
+					enable->fe_bm);
+
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_SHADER_EN),
+					enable->shader_bm);
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_MMU_L2_EN),
+					enable->mmu_l2_bm);
+
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_TILER_EN),
+					enable->tiler_bm);
+
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_CONFIG),
+			prfcnt_config | PRFCNT_CONFIG_MODE_MANUAL);
+#else
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG),
 			prfcnt_config | PRFCNT_CONFIG_MODE_OFF);
 
@@ -87,7 +114,7 @@ int kbase_instr_hwcnt_enable_internal(struct kbase_device *kbdev,
 					enable->dump_buffer >> 32);
 
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_JM_EN),
-					enable->jm_bm);
+					enable->fe_bm);
 
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_SHADER_EN),
 					enable->shader_bm);
@@ -99,6 +126,7 @@ int kbase_instr_hwcnt_enable_internal(struct kbase_device *kbdev,
 
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG),
 			prfcnt_config | PRFCNT_CONFIG_MODE_MANUAL);
+#endif
 
 	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
 
@@ -120,7 +148,9 @@ int kbase_instr_hwcnt_disable_internal(struct kbase_context *kctx)
 {
 	unsigned long flags, pm_flags;
 	int err = -EINVAL;
+#if !MALI_USE_CSF
 	u32 irq_mask;
+#endif
 	struct kbase_device *kbdev = kctx->kbdev;
 
 	while (1) {
@@ -155,6 +185,10 @@ int kbase_instr_hwcnt_disable_internal(struct kbase_context *kctx)
 	kbdev->hwcnt.backend.state = KBASE_INSTR_STATE_DISABLED;
 	kbdev->hwcnt.backend.triggered = 0;
 
+#if MALI_USE_CSF
+	/* Disable the counters */
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_CONFIG), 0);
+#else
 	/* Disable interrupt */
 	irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK));
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK),
@@ -162,6 +196,7 @@ int kbase_instr_hwcnt_disable_internal(struct kbase_context *kctx)
 
 	/* Disable the counters */
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG), 0);
+#endif
 
 	kbdev->hwcnt.kctx = NULL;
 	kbdev->hwcnt.addr = 0ULL;
@@ -205,18 +240,31 @@ int kbase_instr_hwcnt_request_dump(struct kbase_context *kctx)
 	kbdev->hwcnt.backend.state = KBASE_INSTR_STATE_DUMPING;
 
 
+#if MALI_USE_CSF
+	/* Reconfigure the dump address */
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_BASE_LO),
+					kbdev->hwcnt.addr & 0xFFFFFFFF);
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(PRFCNT_BASE_HI),
+					kbdev->hwcnt.addr >> 32);
+#else
 	/* Reconfigure the dump address */
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_LO),
 					kbdev->hwcnt.addr & 0xFFFFFFFF);
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_HI),
 					kbdev->hwcnt.addr >> 32);
+#endif
 
 	/* Start dumping */
 	KBASE_KTRACE_ADD(kbdev, CORE_GPU_PRFCNT_SAMPLE, NULL,
 			kbdev->hwcnt.addr);
 
+#if MALI_USE_CSF
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(GPU_COMMAND),
+					GPU_COMMAND_PRFCNT_SAMPLE);
+#else
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
 					GPU_COMMAND_PRFCNT_SAMPLE);
+#endif
 
 	dev_dbg(kbdev->dev, "HW counters dumping done for context %p", kctx);
 
@@ -225,6 +273,9 @@ int kbase_instr_hwcnt_request_dump(struct kbase_context *kctx)
  unlock:
 	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
 
+#if MALI_USE_CSF
+	tasklet_schedule(&kbdev->hwcnt.backend.csf_hwc_irq_poll_tasklet);
+#endif
 
 	return err;
 }
@@ -287,6 +338,52 @@ void kbasep_cache_clean_worker(struct work_struct *data)
 	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
 }
 
+#if MALI_USE_CSF
+/**
+ * kbasep_hwcnt_irq_poll_tasklet - tasklet to poll MCU IRQ status register
+ *
+ * @data: tasklet parameter which pointer to kbdev
+ *
+ * This tasklet poll GPU_IRQ_STATUS register in GPU_CONTROL_MCU page to check
+ * PRFCNT_SAMPLE_COMPLETED bit.
+ *
+ * Tasklet is needed here since work_queue is too slow and cuased some test
+ * cases timeout, the poll_count variable is introduced to avoid infinite
+ * loop in unexpected cases, the poll_count is 1 or 2 in normal case, 128
+ * should be big enough to exit the tasklet in abnormal cases.
+ *
+ * Return: void
+ */
+static void kbasep_hwcnt_irq_poll_tasklet(unsigned long int data)
+{
+	struct kbase_device *kbdev = (struct kbase_device *)data;
+	unsigned long flags, pm_flags;
+	u32 mcu_gpu_irq_raw_status = 0;
+	u32 poll_count = 0;
+
+	while (1) {
+		spin_lock_irqsave(&kbdev->hwaccess_lock, pm_flags);
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+		mcu_gpu_irq_raw_status = kbase_reg_read(kbdev,
+			GPU_CONTROL_MCU_REG(GPU_IRQ_RAWSTAT));
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, pm_flags);
+		if (mcu_gpu_irq_raw_status & PRFCNT_SAMPLE_COMPLETED) {
+			kbase_reg_write(kbdev,
+				GPU_CONTROL_MCU_REG(GPU_IRQ_CLEAR),
+				PRFCNT_SAMPLE_COMPLETED);
+			kbase_instr_hwcnt_sample_done(kbdev);
+			break;
+		} else if (poll_count++ > 128) {
+			dev_err(kbdev->dev,
+				"Err: HWC dump timeout, count: %u", poll_count);
+			/* Still call sample_done to unblock waiting thread */
+			kbase_instr_hwcnt_sample_done(kbdev);
+			break;
+		}
+	}
+}
+#endif
 
 void kbase_instr_hwcnt_sample_done(struct kbase_device *kbdev)
 {
@@ -360,8 +457,13 @@ int kbase_instr_hwcnt_clear(struct kbase_context *kctx)
 
 	/* Clear the counters */
 	KBASE_KTRACE_ADD(kbdev, CORE_GPU_PRFCNT_CLEAR, NULL, 0);
+#if MALI_USE_CSF
+	kbase_reg_write(kbdev, GPU_CONTROL_MCU_REG(GPU_COMMAND),
+					GPU_COMMAND_PRFCNT_CLEAR);
+#else
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
 						GPU_COMMAND_PRFCNT_CLEAR);
+#endif
 
 	err = 0;
 
@@ -381,6 +483,10 @@ int kbase_instr_backend_init(struct kbase_device *kbdev)
 	INIT_WORK(&kbdev->hwcnt.backend.cache_clean_work,
 						kbasep_cache_clean_worker);
 
+#if MALI_USE_CSF
+	tasklet_init(&kbdev->hwcnt.backend.csf_hwc_irq_poll_tasklet,
+		     kbasep_hwcnt_irq_poll_tasklet, (unsigned long int)kbdev);
+#endif
 
 	kbdev->hwcnt.backend.triggered = 0;
 
@@ -398,6 +504,9 @@ int kbase_instr_backend_init(struct kbase_device *kbdev)
 
 void kbase_instr_backend_term(struct kbase_device *kbdev)
 {
+#if MALI_USE_CSF
+	tasklet_kill(&kbdev->hwcnt.backend.csf_hwc_irq_poll_tasklet);
+#endif
 	destroy_workqueue(kbdev->hwcnt.backend.cache_clean_wq);
 }
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_defs.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_defs.h
index 99309685c4ff..9f785ce16e17 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_defs.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_instr_defs.h
@@ -54,6 +54,9 @@ struct kbase_instr_backend {
 	enum kbase_instr_state state;
 	struct workqueue_struct *cache_clean_wq;
 	struct work_struct  cache_clean_work;
+#if MALI_USE_CSF
+	struct tasklet_struct csf_hwc_irq_poll_tasklet;
+#endif
 };
 
 #endif /* _KBASE_INSTR_DEFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c
index 8cf3c1e6790e..b09db552e639 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_irq_linux.c
@@ -21,7 +21,7 @@
  */
 
 #include <mali_kbase.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
 
 #include <linux/interrupt.h>
@@ -72,7 +72,12 @@ static irqreturn_t kbase_job_irq_handler(int irq, void *data)
 
 	dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
 
+#if MALI_USE_CSF
+	/* call the csf interrupt handler */
+	kbase_csf_interrupt(kbdev, val);
+#else
 	kbase_job_done(kbdev, val);
+#endif
 
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_as.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_as.c
index bb4f548e9a4d..9b775898dac2 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_as.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_as.c
@@ -58,8 +58,10 @@ static void assign_and_activate_kctx_addr_space(struct kbase_device *kbdev,
 	lockdep_assert_held(&js_devdata->runpool_mutex);
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
+#if !MALI_USE_CSF
 	/* Attribute handling */
 	kbasep_js_ctx_attr_runpool_retain_ctx(kbdev, kctx);
+#endif
 
 	/* Allow it to run jobs */
 	kbasep_js_set_submit_allowed(js_devdata, kctx);
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_defs.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_defs.h
index 7cda61ac6cdb..9cccf224999e 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_defs.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_defs.h
@@ -80,9 +80,11 @@ struct slot_rb {
 struct kbase_backend_data {
 	struct slot_rb slot_rb[BASE_JM_MAX_NR_SLOTS];
 
+#if !MALI_USE_CSF
 	struct hrtimer scheduling_timer;
 
 	bool timer_running;
+#endif
 	bool suspend_timer;
 
 	atomic_t reset_gpu;
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c
index 4255e0b373bb..19661c9766c6 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_hw.c
@@ -33,10 +33,12 @@
 #include <mali_kbase_hwaccess_jm.h>
 #include <mali_kbase_reset_gpu.h>
 #include <mali_kbase_ctx_sched.h>
+#include <mali_kbase_kinstr_jm.h>
 #include <mali_kbase_hwcnt_context.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
 #include <backend/gpu/mali_kbase_jm_internal.h>
+#include <mali_kbase_regs_history_debugfs.h>
 
 static void kbasep_try_reset_gpu_early_locked(struct kbase_device *kbdev);
 
@@ -277,6 +279,7 @@ void kbase_job_hw_submit(struct kbase_device *kbdev,
 			katom,
 			&kbdev->gpu_props.props.raw_props.js_features[js],
 			"ctx_nr,atom_nr");
+	kbase_kinstr_jm_atom_hw_submit(katom);
 #ifdef CONFIG_GPU_TRACEPOINTS
 	if (!kbase_backend_nr_atoms_submitted(kbdev, js)) {
 		/* If this is the only job on the slot, trace it as starting */
@@ -692,12 +695,40 @@ void kbase_backend_jm_kill_running_jobs_from_kctx(struct kbase_context *kctx)
 		kbase_job_slot_hardstop(kctx, i, NULL);
 }
 
+/**
+ * kbase_is_existing_atom_submitted_later_than_ready
+ * @ready: sequence number of the ready atom
+ * @existing: sequence number of the existing atom
+ *
+ * Returns true if the existing atom has been submitted later than the
+ * ready atom. It is used to understand if an atom that is ready has been
+ * submitted earlier than the currently running atom, so that the currently
+ * running atom should be preempted to allow the ready atom to run.
+ */
+static inline bool kbase_is_existing_atom_submitted_later_than_ready(u64 ready, u64 existing)
+{
+	/* No seq_nr set? */
+	if (!ready || !existing)
+		return false;
+
+	/* Efficiently handle the unlikely case of wrapping.
+	 * The following code assumes that the delta between the sequence number
+	 * of the two atoms is less than INT64_MAX.
+	 * In the extremely unlikely case where the delta is higher, the comparison
+	 * defaults for no preemption.
+	 * The code also assumes that the conversion from unsigned to signed types
+	 * works because the signed integers are 2's complement.
+	 */
+	return (s64)(ready - existing) < 0;
+}
+
 void kbase_job_slot_ctx_priority_check_locked(struct kbase_context *kctx,
 				struct kbase_jd_atom *target_katom)
 {
 	struct kbase_device *kbdev;
 	int js = target_katom->slot_nr;
 	int priority = target_katom->sched_priority;
+	int seq_nr = target_katom->seq_nr;
 	int i;
 	bool stop_sent = false;
 
@@ -719,7 +750,8 @@ void kbase_job_slot_ctx_priority_check_locked(struct kbase_context *kctx,
 				(katom->kctx != kctx))
 			continue;
 
-		if (katom->sched_priority > priority) {
+		if ((katom->sched_priority > priority) ||
+		    (katom->kctx == kctx && kbase_is_existing_atom_submitted_later_than_ready(seq_nr, katom->seq_nr))) {
 			if (!stop_sent)
 				KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITIZED(
 						kbdev,
@@ -1277,6 +1309,15 @@ bool kbase_prepare_to_reset_gpu_locked(struct kbase_device *kbdev)
 
 	KBASE_DEBUG_ASSERT(kbdev);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbase_pm_is_gpu_lost(kbdev)) {
+		/* GPU access has been removed, reset will be done by
+		 * Arbiter instead
+		 */
+		return false;
+	}
+#endif
+
 	if (atomic_cmpxchg(&kbdev->hwaccess.backend.reset_gpu,
 						KBASE_RESET_GPU_NOT_PENDING,
 						KBASE_RESET_GPU_PREPARED) !=
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_internal.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_internal.h
index 1419b5987eff..cd1f9794fdc4 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_internal.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_internal.h
@@ -34,7 +34,7 @@
 #include <linux/atomic.h>
 
 #include <backend/gpu/mali_kbase_jm_rb.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 
 /**
  * kbase_job_submit_nolock() - Submit a job to a certain job-slot
@@ -71,11 +71,13 @@ static inline char *kbasep_make_job_slot_string(int js, char *js_string,
 }
 #endif
 
+#if !MALI_USE_CSF
 static inline int kbasep_jm_is_js_free(struct kbase_device *kbdev, int js,
 						struct kbase_context *kctx)
 {
 	return !kbase_reg_read(kbdev, JOB_SLOT_REG(js, JS_COMMAND_NEXT));
 }
+#endif
 
 
 /**
@@ -94,6 +96,7 @@ void kbase_job_hw_submit(struct kbase_device *kbdev,
 				struct kbase_jd_atom *katom,
 				int js);
 
+#if !MALI_USE_CSF
 /**
  * kbasep_job_slot_soft_or_hard_stop_do_action() - Perform a soft or hard stop
  *						   on the specified atom
@@ -112,6 +115,7 @@ void kbasep_job_slot_soft_or_hard_stop_do_action(struct kbase_device *kbdev,
 					u32 action,
 					base_jd_core_req core_reqs,
 					struct kbase_jd_atom *target_katom);
+#endif /* !MALI_USE_CSF */
 
 /**
  * kbase_backend_soft_hard_stop_slot() - Soft or hard stop jobs on a given job
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c
index ec7bcb19f8d1..afaaef27883d 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_jm_rb.c
@@ -33,8 +33,9 @@
 #include <tl/mali_kbase_tracepoints.h>
 #include <mali_kbase_hwcnt_context.h>
 #include <mali_kbase_reset_gpu.h>
+#include <mali_kbase_kinstr_jm.h>
 #include <backend/gpu/mali_kbase_cache_policy_backend.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_jm_internal.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
 
@@ -253,6 +254,8 @@ static bool kbase_gpu_check_secure_atoms(struct kbase_device *kbdev,
 
 int kbase_backend_slot_free(struct kbase_device *kbdev, int js)
 {
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
 	if (atomic_read(&kbdev->hwaccess.backend.reset_gpu) !=
 						KBASE_RESET_GPU_NOT_PENDING) {
 		/* The GPU is being reset - so prevent submission */
@@ -278,6 +281,7 @@ static void kbase_gpu_release_atom(struct kbase_device *kbdev,
 		break;
 
 	case KBASE_ATOM_GPU_RB_SUBMITTED:
+		kbase_kinstr_jm_atom_hw_release(katom);
 		/* Inform power management at start/finish of atom so it can
 		 * update its GPU utilisation metrics. Mark atom as not
 		 * submitted beforehand. */
@@ -618,7 +622,7 @@ static int kbase_jm_enter_protected_mode(struct kbase_device *kbdev,
 						KBASE_PM_CORE_L2) ||
 				kbase_pm_get_trans_cores(kbdev,
 						KBASE_PM_CORE_L2) ||
-				kbase_is_gpu_lost(kbdev)) {
+				kbase_is_gpu_removed(kbdev)) {
 				/*
 				 * The L2 is still powered, wait for all
 				 * the users to finish with it before doing
@@ -811,7 +815,8 @@ void kbase_backend_slot_update(struct kbase_device *kbdev)
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
-	if (kbase_reset_gpu_is_active(kbdev) || kbase_is_gpu_lost(kbdev))
+	if (kbase_reset_gpu_is_active(kbdev) ||
+			kbase_is_gpu_removed(kbdev))
 #else
 	if (kbase_reset_gpu_is_active(kbdev))
 #endif
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c
index 4ffc1232d306..8187e73767be 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_js_backend.c
@@ -31,13 +31,14 @@
 #include <backend/gpu/mali_kbase_jm_internal.h>
 #include <backend/gpu/mali_kbase_js_internal.h>
 
+#if !MALI_USE_CSF
 /*
  * Hold the runpool_mutex for this
  */
 static inline bool timer_callback_should_run(struct kbase_device *kbdev)
 {
 	struct kbase_backend_data *backend = &kbdev->hwaccess.backend;
-	s8 nr_running_ctxs;
+	int nr_running_ctxs;
 
 	lockdep_assert_held(&kbdev->js_data.runpool_mutex);
 
@@ -69,10 +70,10 @@ static inline bool timer_callback_should_run(struct kbase_device *kbdev)
 		 * don't check KBASEP_JS_CTX_ATTR_NON_COMPUTE).
 		 */
 		{
-			s8 nr_compute_ctxs =
+			int nr_compute_ctxs =
 				kbasep_js_ctx_attr_count_on_runpool(kbdev,
 						KBASEP_JS_CTX_ATTR_COMPUTE);
-			s8 nr_noncompute_ctxs = nr_running_ctxs -
+			int nr_noncompute_ctxs = nr_running_ctxs -
 							nr_compute_ctxs;
 
 			return (bool) (nr_compute_ctxs >= 2 ||
@@ -270,9 +271,11 @@ static enum hrtimer_restart timer_callback(struct hrtimer *timer)
 
 	return HRTIMER_NORESTART;
 }
+#endif /* !MALI_USE_CSF */
 
 void kbase_backend_ctx_count_changed(struct kbase_device *kbdev)
 {
+#if !MALI_USE_CSF
 	struct kbasep_js_device_data *js_devdata = &kbdev->js_data;
 	struct kbase_backend_data *backend = &kbdev->hwaccess.backend;
 	unsigned long flags;
@@ -303,25 +306,36 @@ void kbase_backend_ctx_count_changed(struct kbase_device *kbdev)
 
 		KBASE_KTRACE_ADD_JM(kbdev, JS_POLICY_TIMER_START, NULL, NULL, 0u, 0u);
 	}
+#else /* !MALI_USE_CSF */
+	CSTD_UNUSED(kbdev);
+#endif /* !MALI_USE_CSF */
 }
 
 int kbase_backend_timer_init(struct kbase_device *kbdev)
 {
+#if !MALI_USE_CSF
 	struct kbase_backend_data *backend = &kbdev->hwaccess.backend;
 
 	hrtimer_init(&backend->scheduling_timer, CLOCK_MONOTONIC,
 							HRTIMER_MODE_REL);
 	backend->scheduling_timer.function = timer_callback;
 	backend->timer_running = false;
+#else /* !MALI_USE_CSF */
+	CSTD_UNUSED(kbdev);
+#endif /* !MALI_USE_CSF */
 
 	return 0;
 }
 
 void kbase_backend_timer_term(struct kbase_device *kbdev)
 {
+#if !MALI_USE_CSF
 	struct kbase_backend_data *backend = &kbdev->hwaccess.backend;
 
 	hrtimer_cancel(&backend->scheduling_timer);
+#else /* !MALI_USE_CSF */
+	CSTD_UNUSED(kbdev);
+#endif /* !MALI_USE_CSF */
 }
 
 void kbase_backend_timer_suspend(struct kbase_device *kbdev)
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_l2_mmu_config.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_l2_mmu_config.c
index e67d12b1ba3d..d5526caa5899 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_l2_mmu_config.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_l2_mmu_config.c
@@ -24,7 +24,7 @@
 #include <mali_kbase.h>
 #include <mali_kbase_bits.h>
 #include <mali_kbase_config_defaults.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include "mali_kbase_l2_mmu_config.h"
 
 /**
@@ -56,23 +56,34 @@ struct l2_mmu_config_limit {
 /*
  * Zero represents no limit
  *
- * For LBEX TBEX TTRX and TNAX:
+ * For LBEX TBEX TBAX TTRX and TNAX:
  *   The value represents the number of outstanding reads (6 bits) or writes (5 bits)
  *
  * For all other GPUS it is a fraction see: mali_kbase_config_defaults.h
  */
 static const struct l2_mmu_config_limit limits[] = {
-	 /* GPU                       read                  write            */
-	 {GPU_ID2_PRODUCT_LBEX, {0, GENMASK(10, 5), 5}, {0, GENMASK(16, 12), 12} },
-	 {GPU_ID2_PRODUCT_TBEX, {0, GENMASK(10, 5), 5}, {0, GENMASK(16, 12), 12} },
-	 {GPU_ID2_PRODUCT_TTRX, {0, GENMASK(12, 7), 7}, {0, GENMASK(17, 13), 13} },
-	 {GPU_ID2_PRODUCT_TNAX, {0, GENMASK(12, 7), 7}, {0, GENMASK(17, 13), 13} },
-	 {GPU_ID2_PRODUCT_TGOX,
-	   {KBASE_3BIT_AID_32, GENMASK(14, 12), 12},
-	   {KBASE_3BIT_AID_32, GENMASK(17, 15), 15} },
-	 {GPU_ID2_PRODUCT_TNOX,
-	   {KBASE_3BIT_AID_32, GENMASK(14, 12), 12},
-	   {KBASE_3BIT_AID_32, GENMASK(17, 15), 15} },
+	/* GPU, read, write */
+	{GPU_ID2_PRODUCT_LBEX,
+		{0, GENMASK(10, 5), 5},
+		{0, GENMASK(16, 12), 12} },
+	{GPU_ID2_PRODUCT_TBEX,
+		{0, GENMASK(10, 5), 5},
+		{0, GENMASK(16, 12), 12} },
+	{GPU_ID2_PRODUCT_TBAX,
+		{0, GENMASK(10, 5), 5},
+		{0, GENMASK(16, 12), 12} },
+	{GPU_ID2_PRODUCT_TTRX,
+		{0, GENMASK(12, 7), 7},
+		{0, GENMASK(17, 13), 13} },
+	{GPU_ID2_PRODUCT_TNAX,
+		{0, GENMASK(12, 7), 7},
+		{0, GENMASK(17, 13), 13} },
+	{GPU_ID2_PRODUCT_TGOX,
+		{KBASE_3BIT_AID_32, GENMASK(14, 12), 12},
+		{KBASE_3BIT_AID_32, GENMASK(17, 15), 15} },
+	{GPU_ID2_PRODUCT_TNOX,
+		{KBASE_3BIT_AID_32, GENMASK(14, 12), 12},
+		{KBASE_3BIT_AID_32, GENMASK(17, 15), 15} },
 };
 
 int kbase_set_mmu_quirks(struct kbase_device *kbdev)
@@ -100,7 +111,7 @@ int kbase_set_mmu_quirks(struct kbase_device *kbdev)
 
 	mmu_config = kbase_reg_read(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG));
 
-	if (kbase_is_gpu_lost(kbdev))
+	if (kbase_is_gpu_removed(kbdev))
 		return -EIO;
 
 	mmu_config &= ~(limit.read.mask | limit.write.mask);
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c
index 948080b991e8..7b10d06c5fdb 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_backend.c
@@ -30,9 +30,11 @@
 #include <mali_kbase_config_defaults.h>
 
 #include <mali_kbase_pm.h>
+#if !MALI_USE_CSF
 #include <mali_kbase_hwaccess_jm.h>
 #include <backend/gpu/mali_kbase_js_internal.h>
 #include <backend/gpu/mali_kbase_jm_internal.h>
+#endif /* !MALI_USE_CSF */
 #include <mali_kbase_hwcnt_context.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
 #include <backend/gpu/mali_kbase_devfreq.h>
@@ -106,6 +108,11 @@ void kbase_pm_register_access_enable(struct kbase_device *kbdev)
 	if (callbacks)
 		callbacks->power_on_callback(kbdev);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (WARN_ON(kbase_pm_is_gpu_lost(kbdev)))
+		dev_err(kbdev->dev, "Attempting to power on while GPU lost\n");
+#endif
+
 	kbdev->pm.backend.gpu_powered = true;
 }
 
@@ -139,9 +146,10 @@ int kbase_hwaccess_pm_init(struct kbase_device *kbdev)
 
 	kbdev->pm.backend.ca_cores_enabled = ~0ull;
 	kbdev->pm.backend.gpu_powered = false;
+	kbdev->pm.backend.gpu_ready = false;
 	kbdev->pm.suspending = false;
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
-	kbdev->pm.gpu_lost = false;
+	kbase_pm_set_gpu_lost(kbdev, false);
 #endif
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 	kbdev->pm.backend.driver_ready_for_irqs = false;
@@ -251,13 +259,20 @@ static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data)
 	struct kbase_pm_backend_data *backend = &pm->backend;
 	unsigned long flags;
 
+#if !MALI_USE_CSF
 	/* Wait for power transitions to complete. We do this with no locks held
 	 * so that we don't deadlock with any pending workqueues.
 	 */
 	kbase_pm_wait_for_desired_state(kbdev);
+#endif
 
 	kbase_pm_lock(kbdev);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbase_pm_is_gpu_lost(kbdev))
+		backend->poweron_required = false;
+#endif
+
 	if (!backend->poweron_required) {
 		unsigned long flags;
 
@@ -278,6 +293,14 @@ static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data)
 			kbase_flush_mmu_wqs(kbdev);
 			kbase_pm_lock(kbdev);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+			/* poweron_required may have changed while pm lock
+			 * was released.
+			 */
+			if (kbase_pm_is_gpu_lost(kbdev))
+				backend->poweron_required = false;
+#endif
+
 			/* Turn off clock now that fault have been handled. We
 			 * dropped locks so poweron_required may have changed -
 			 * power back on if this is the case (effectively only
@@ -296,9 +319,14 @@ static void kbase_pm_gpu_poweroff_wait_wq(struct work_struct *data)
 	if (backend->poweron_required) {
 		backend->poweron_required = false;
 		kbdev->pm.backend.l2_desired = true;
+#if MALI_USE_CSF
+		kbdev->pm.backend.mcu_desired = true;
+#endif
 		kbase_pm_update_state(kbdev);
 		kbase_pm_update_cores_state_nolock(kbdev);
+#if !MALI_USE_CSF
 		kbase_backend_slot_update(kbdev);
+#endif /* !MALI_USE_CSF */
 	}
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
@@ -451,7 +479,9 @@ static void kbase_pm_hwcnt_disable_worker(struct work_struct *data)
 		 */
 		backend->hwcnt_disabled = true;
 		kbase_pm_update_state(kbdev);
+#if !MALI_USE_CSF
 		kbase_backend_slot_update(kbdev);
+#endif /* !MALI_USE_CSF */
 	} else {
 		/* PM state was updated while we were doing the disable,
 		 * so we need to undo the disable we just performed.
@@ -476,8 +506,12 @@ void kbase_pm_do_poweroff(struct kbase_device *kbdev)
 	if (kbdev->pm.backend.poweroff_wait_in_progress)
 		goto unlock_hwaccess;
 
+#if MALI_USE_CSF
+	kbdev->pm.backend.mcu_desired = false;
+#else
 	/* Force all cores off */
 	kbdev->pm.backend.shaders_desired = false;
+#endif
 	kbdev->pm.backend.l2_desired = false;
 
 	kbdev->pm.backend.poweroff_wait_in_progress = true;
@@ -559,7 +593,20 @@ int kbase_hwaccess_pm_powerup(struct kbase_device *kbdev,
 #endif
 	kbase_pm_enable_interrupts(kbdev);
 
+	WARN_ON(!kbdev->pm.backend.gpu_powered);
+	/* GPU has been powered up (by kbase_pm_init_hw) and interrupts have
+	 * been enabled, so GPU is ready for use and PM state machine can be
+	 * exercised from this point onwards.
+	 */
+	kbdev->pm.backend.gpu_ready = true;
+
 	/* Turn on the GPU and any cores needed by the policy */
+#if MALI_USE_CSF
+	/* Turn on the L2 caches, needed for firmware boot */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
+	kbdev->pm.backend.l2_desired = true;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
+#endif
 	kbase_pm_do_poweron(kbdev, false);
 	kbase_pm_unlock(kbdev);
 
@@ -611,7 +658,9 @@ void kbase_pm_power_changed(struct kbase_device *kbdev)
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	kbase_pm_update_state(kbdev);
 
+#if !MALI_USE_CSF
 		kbase_backend_slot_update(kbdev);
+#endif /* !MALI_USE_CSF */
 
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 }
@@ -656,7 +705,9 @@ void kbase_hwaccess_pm_suspend(struct kbase_device *kbdev)
 
 	kbase_pm_do_poweroff(kbdev);
 
+#if !MALI_USE_CSF
 	kbase_backend_timer_suspend(kbdev);
+#endif /* !MALI_USE_CSF */
 
 	kbase_pm_unlock(kbdev);
 
@@ -672,11 +723,17 @@ void kbase_hwaccess_pm_resume(struct kbase_device *kbdev)
 
 	kbdev->pm.suspending = false;
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
-	kbdev->pm.gpu_lost = false;
+	if (kbase_pm_is_gpu_lost(kbdev)) {
+		dev_dbg(kbdev->dev, "%s: GPU lost in progress\n", __func__);
+		kbase_pm_unlock(kbdev);
+		return;
+	}
 #endif
 	kbase_pm_do_poweron(kbdev, true);
 
+#if !MALI_USE_CSF
 	kbase_backend_timer_resume(kbdev);
+#endif /* !MALI_USE_CSF */
 
 	kbase_pm_unlock(kbdev);
 }
@@ -685,63 +742,47 @@ void kbase_hwaccess_pm_resume(struct kbase_device *kbdev)
 void kbase_pm_handle_gpu_lost(struct kbase_device *kbdev)
 {
 	unsigned long flags;
-	struct kbase_pm_backend_data *backend = &kbdev->pm.backend;
 	ktime_t end_timestamp = ktime_get();
+	struct kbase_arbiter_vm_state *arb_vm_state = kbdev->pm.arb_vm_state;
 
-	/* Full GPU reset will have been done by hypervisor, so cancel */
-	atomic_set(&kbdev->hwaccess.backend.reset_gpu,
-			KBASE_RESET_GPU_NOT_PENDING);
-	hrtimer_cancel(&kbdev->hwaccess.backend.reset_timer);
-
-	/* GPU is no longer mapped to VM.  So no interrupts will be received
-	 * and Mali registers have been replaced by dummy RAM
-	 */
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	spin_lock(&kbdev->mmu_mask_change);
-	kbdev->irq_reset_flush = true;
-	spin_unlock(&kbdev->mmu_mask_change);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-	kbase_synchronize_irqs(kbdev);
-	kbase_flush_mmu_wqs(kbdev);
-	kbdev->irq_reset_flush = false;
-
-	/* Clear all jobs running on the GPU */
 	mutex_lock(&kbdev->pm.lock);
-	kbdev->pm.gpu_lost = true;
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbdev->protected_mode = false;
-	if (!kbdev->pm.backend.protected_entry_transition_override)
-		kbase_backend_reset(kbdev, &end_timestamp);
-	kbase_pm_metrics_update(kbdev, NULL);
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	mutex_lock(&arb_vm_state->vm_state_lock);
+	if (kbdev->pm.backend.gpu_powered &&
+			!kbase_pm_is_gpu_lost(kbdev)) {
+		kbase_pm_set_gpu_lost(kbdev, true);
+
+		/* GPU is no longer mapped to VM.  So no interrupts will
+		 * be received and Mali registers have been replaced by
+		 * dummy RAM
+		 */
+		WARN(!kbase_is_gpu_removed(kbdev),
+			"GPU is still available after GPU lost event\n");
 
-	/* Cancel any pending HWC dumps */
-	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
-	kbdev->hwcnt.backend.state = KBASE_INSTR_STATE_IDLE;
-	kbdev->hwcnt.backend.triggered = 1;
-	wake_up(&kbdev->hwcnt.backend.wait);
-	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		/* Full GPU reset will have been done by hypervisor, so
+		 * cancel
+		 */
+		atomic_set(&kbdev->hwaccess.backend.reset_gpu,
+				KBASE_RESET_GPU_NOT_PENDING);
+		hrtimer_cancel(&kbdev->hwaccess.backend.reset_timer);
+		kbase_synchronize_irqs(kbdev);
 
-	/* Wait for all threads keeping GPU active to complete */
-	mutex_unlock(&kbdev->pm.lock);
-	wait_event(kbdev->pm.zero_active_count_wait,
-			kbdev->pm.active_count == 0);
-	mutex_lock(&kbdev->pm.lock);
+		/* Clear all jobs running on the GPU */
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbdev->protected_mode = false;
+		kbase_backend_reset(kbdev, &end_timestamp);
+		kbase_pm_metrics_update(kbdev, NULL);
+		kbase_pm_update_state(kbdev);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-	/* Update state to GPU off */
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-	kbdev->pm.backend.shaders_desired = false;
-	kbdev->pm.backend.l2_desired = false;
-	backend->l2_state = KBASE_L2_OFF;
-	backend->shaders_state = KBASE_SHADERS_OFF_CORESTACK_OFF;
-	kbdev->pm.backend.gpu_powered = false;
-	backend->poweroff_wait_in_progress = false;
-	KBASE_KTRACE_ADD(kbdev, PM_WAKE_WAITERS, NULL, 0);
-	wake_up(&kbdev->pm.backend.gpu_in_desired_state_wait);
-	kbase_gpu_cache_clean_wait_complete(kbdev);
-	backend->poweroff_wait_in_progress = false;
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-	wake_up(&kbdev->pm.backend.poweroff_wait);
+		/* Cancel any pending HWC dumps */
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+		kbdev->hwcnt.backend.state = KBASE_INSTR_STATE_IDLE;
+		kbdev->hwcnt.backend.triggered = 1;
+		wake_up(&kbdev->hwcnt.backend.wait);
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+	}
+	mutex_unlock(&arb_vm_state->vm_state_lock);
 	mutex_unlock(&kbdev->pm.lock);
 }
+
 #endif /* CONFIG_MALI_ARBITER_SUPPORT */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c
index dc22b6e25bb8..984e12503009 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_ca.c
@@ -106,6 +106,8 @@ u64 kbase_pm_ca_get_instr_core_mask(struct kbase_device *kbdev)
 
 #ifdef CONFIG_MALI_BIFROST_NO_MALI
 	return (((1ull) << KBASE_DUMMY_MODEL_MAX_SHADER_CORES) - 1);
+#elif MALI_USE_CSF
+	return kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_SHADER);
 #else
 	return kbdev->pm.backend.pm_shaders_core_mask;
 #endif
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h
index abe9713fd9af..7322c093c7b6 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_defs.h
@@ -87,6 +87,33 @@ enum kbase_l2_core_state {
 #undef KBASEP_L2_STATE
 };
 
+#if MALI_USE_CSF
+/**
+ * enum kbase_mcu_state - The states used for the MCU state machine.
+ *
+ * @KBASE_MCU_OFF:            The MCU is powered off.
+ * @KBASE_MCU_PEND_ON_RELOAD: The warm boot of MCU or cold boot of MCU (with
+ *                            firmware reloading) is in progress.
+ * @KBASE_MCU_ON_GLB_REINIT_PEND: The MCU is enabled and Global configuration
+ *                                requests have been sent to the firmware.
+ * @KBASE_MCU_ON_HWCNT_ENABLE: The Global requests have completed and MCU is
+ *                             now ready for use and hwcnt is being enabled.
+ * @KBASE_MCU_ON:             The MCU is active and hwcnt has been enabled.
+ * @KBASE_MCU_ON_HWCNT_DISABLE: The MCU is on and hwcnt is being disabled.
+ * @KBASE_MCU_ON_HALT:        The MCU is on and hwcnt has been disabled,
+ *                            MCU halt would be triggered.
+ * @KBASE_MCU_ON_PEND_HALT:   MCU halt in progress, confirmation pending.
+ * @KBASE_MCU_POWER_DOWN:     MCU halted operations, pending being disabled.
+ * @KBASE_MCU_PEND_OFF:       MCU is being disabled, pending on powering off.
+ * @KBASE_MCU_RESET_WAIT:     The GPU is resetting, MCU state is unknown.
+ */
+enum kbase_mcu_state {
+#define KBASEP_MCU_STATE(n) KBASE_MCU_ ## n,
+#include "mali_kbase_pm_mcu_states.h"
+#undef KBASEP_MCU_STATE
+};
+#endif
+
 /**
  * enum kbase_shader_core_state - The states used for the shaders' state machine.
  *
@@ -254,6 +281,11 @@ union kbase_pm_policy_data {
  *                     variable should be protected by: both the hwaccess_lock
  *                     spinlock and the pm.lock mutex for writes; or at least
  *                     one of either lock for reads.
+ * @gpu_ready:         Indicates whether the GPU is in a state in which it is
+ *                     safe to perform PM changes. When false, the PM state
+ *                     machine needs to wait before making changes to the GPU
+ *                     power policy, DevFreq or core_mask, so as to avoid these
+ *                     changing while implicit GPU resets are ongoing.
  * @pm_shaders_core_mask: Shader PM state synchronised shaders core mask. It
  *                     holds the cores enabled in a hardware counters dump,
  *                     and may differ from @shaders_avail when under different
@@ -373,6 +405,7 @@ struct kbase_pm_backend_data {
 	wait_queue_head_t gpu_in_desired_state_wait;
 
 	bool gpu_powered;
+	bool gpu_ready;
 
 	u64 pm_shaders_core_mask;
 
@@ -406,10 +439,20 @@ struct kbase_pm_backend_data {
 
 	u64 ca_cores_enabled;
 
+#if MALI_USE_CSF
+	/* The current state of the micro-control unit, only applicable
+	 * to GPUs that has such a component
+	 */
+	enum kbase_mcu_state mcu_state;
+#endif
 	enum kbase_l2_core_state l2_state;
 	enum kbase_shader_core_state shaders_state;
 	u64 shaders_avail;
 	u64 shaders_desired_mask;
+#if MALI_USE_CSF
+	/* True if the micro-control unit should be powered on */
+	bool mcu_desired;
+#endif
 	bool l2_desired;
 	bool l2_always_on;
 	bool shaders_desired;
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c
index 6b821f7d7134..e9e30ebadc2d 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_driver.c
@@ -32,12 +32,14 @@
 #include <mali_kbase_pm.h>
 #include <mali_kbase_config_defaults.h>
 #include <mali_kbase_smc.h>
+#if !MALI_USE_CSF
 #include <mali_kbase_hwaccess_jm.h>
+#endif /* !MALI_USE_CSF */
 #include <mali_kbase_reset_gpu.h>
 #include <mali_kbase_ctx_sched.h>
 #include <mali_kbase_hwcnt_context.h>
 #include <backend/gpu/mali_kbase_cache_policy_backend.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_irq_internal.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
 #include <backend/gpu/mali_kbase_l2_mmu_config.h>
@@ -89,6 +91,28 @@ static u64 kbase_pm_get_state(
 		enum kbase_pm_core_type core_type,
 		enum kbasep_pm_action action);
 
+#if MALI_USE_CSF
+bool kbase_pm_is_mcu_desired(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (unlikely(!kbdev->csf.firmware_inited))
+		return false;
+
+	if (kbdev->csf.scheduler.pm_active_count)
+		return true;
+
+	/* MCU is supposed to be ON, only when scheduler.pm_active_count is
+	 * non zero. But for always_on policy also MCU needs to be ON.
+	 * GPUCORE-24926 will add the proper handling for always_on
+	 * power policy.
+	 */
+	return (kbdev->pm.backend.mcu_desired &&
+		(kbdev->pm.backend.pm_current_policy ==
+		 &kbase_pm_always_on_policy_ops));
+}
+#endif
+
 bool kbase_pm_is_l2_desired(struct kbase_device *kbdev)
 {
 	if (kbdev->pm.backend.protected_entry_transition_override)
@@ -523,6 +547,141 @@ static void kbase_pm_control_gpu_clock(struct kbase_device *kbdev)
 	queue_work(system_wq, &backend->gpu_clock_control_work);
 }
 
+#if MALI_USE_CSF
+static const char *kbase_mcu_state_to_string(enum kbase_mcu_state state)
+{
+	const char *const strings[] = {
+#define KBASEP_MCU_STATE(n) #n,
+#include "mali_kbase_pm_mcu_states.h"
+#undef KBASEP_MCU_STATE
+	};
+	if (WARN_ON((size_t)state >= ARRAY_SIZE(strings)))
+		return "Bad MCU state";
+	else
+		return strings[state];
+}
+
+static int kbase_pm_mcu_update_state(struct kbase_device *kbdev)
+{
+	struct kbase_pm_backend_data *backend = &kbdev->pm.backend;
+	enum kbase_mcu_state prev_state;
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	/*
+	 * Initial load of firmare should have been done to
+	 * exercise the MCU state machine.
+	 */
+	if (unlikely(!kbdev->csf.firmware_inited)) {
+		WARN_ON(backend->mcu_state != KBASE_MCU_OFF);
+		return -EIO;
+	}
+
+	do {
+		prev_state = backend->mcu_state;
+
+		switch (backend->mcu_state) {
+		case KBASE_MCU_OFF:
+			if (kbase_pm_is_mcu_desired(kbdev) &&
+			    backend->l2_state == KBASE_L2_ON) {
+				kbase_csf_firmware_trigger_reload(kbdev);
+				backend->mcu_state = KBASE_MCU_PEND_ON_RELOAD;
+			}
+			break;
+
+		case KBASE_MCU_PEND_ON_RELOAD:
+			if (kbdev->csf.firmware_reloaded) {
+				kbase_csf_firmware_global_reinit(kbdev);
+				backend->mcu_state =
+					KBASE_MCU_ON_GLB_REINIT_PEND;
+			}
+			break;
+
+		case KBASE_MCU_ON_GLB_REINIT_PEND:
+			if (kbase_csf_firmware_global_reinit_complete(kbdev))
+				backend->mcu_state = KBASE_MCU_ON_HWCNT_ENABLE;
+			break;
+
+		case KBASE_MCU_ON_HWCNT_ENABLE:
+			backend->hwcnt_desired = true;
+			if (backend->hwcnt_disabled) {
+				kbase_hwcnt_context_enable(
+					kbdev->hwcnt_gpu_ctx);
+				backend->hwcnt_disabled = false;
+			}
+			backend->mcu_state = KBASE_MCU_ON;
+			break;
+
+		case KBASE_MCU_ON:
+			if (!kbase_pm_is_mcu_desired(kbdev))
+				backend->mcu_state = KBASE_MCU_ON_HWCNT_DISABLE;
+			break;
+
+		/* ToDo. Add new state(s) if shader cores mask change for DVFS
+		 * has to be accommodated in the MCU state machine.
+		 */
+
+		case KBASE_MCU_ON_HWCNT_DISABLE:
+			if (kbase_pm_is_mcu_desired(kbdev)) {
+				backend->mcu_state = KBASE_MCU_ON_HWCNT_ENABLE;
+				break;
+			}
+
+			backend->hwcnt_desired = false;
+			if (!backend->hwcnt_disabled)
+				kbase_pm_trigger_hwcnt_disable(kbdev);
+
+			if (backend->hwcnt_disabled)
+				backend->mcu_state = KBASE_MCU_ON_HALT;
+			break;
+
+		case KBASE_MCU_ON_HALT:
+			if (!kbase_pm_is_mcu_desired(kbdev)) {
+				kbase_csf_firmware_trigger_mcu_halt(kbdev);
+				backend->mcu_state = KBASE_MCU_ON_PEND_HALT;
+			} else if (kbase_pm_is_mcu_desired(kbdev)) {
+				backend->mcu_state = KBASE_MCU_ON_HWCNT_ENABLE;
+			}
+			break;
+
+		case KBASE_MCU_ON_PEND_HALT:
+			if (kbase_csf_firmware_mcu_halted(kbdev))
+				backend->mcu_state = KBASE_MCU_POWER_DOWN;
+			break;
+
+		case KBASE_MCU_POWER_DOWN:
+			kbase_csf_firmware_disable_mcu(kbdev);
+			backend->mcu_state = KBASE_MCU_PEND_OFF;
+			break;
+
+		case KBASE_MCU_PEND_OFF:
+			/* wait synchronously for the MCU to get disabled */
+			kbase_csf_firmware_disable_mcu_wait(kbdev);
+			backend->mcu_state = KBASE_MCU_OFF;
+			break;
+
+		case KBASE_MCU_RESET_WAIT:
+			/* Reset complete  */
+			if (!backend->in_reset)
+				backend->mcu_state = KBASE_MCU_OFF;
+			break;
+
+		default:
+			WARN(1, "Invalid state in mcu_state: %d",
+			     backend->mcu_state);
+		}
+
+		if (backend->mcu_state != prev_state)
+			dev_dbg(kbdev->dev, "MCU state transition: %s to %s\n",
+				kbase_mcu_state_to_string(prev_state),
+				kbase_mcu_state_to_string(backend->mcu_state));
+
+	} while (backend->mcu_state != prev_state);
+
+	return 0;
+}
+#endif
+
 static const char *kbase_l2_core_state_to_string(enum kbase_l2_core_state state)
 {
 	const char *const strings[] = {
@@ -560,8 +719,18 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 		 * kbase_pm_get_ready_cores and kbase_pm_get_trans_cores
 		 * are vulnerable to corruption if gpu is lost
 		 */
-		if (kbase_is_gpu_lost(kbdev))
-			return -EIO;
+		if (kbase_is_gpu_removed(kbdev)
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+				|| kbase_pm_is_gpu_lost(kbdev)) {
+#else
+				) {
+#endif
+			backend->shaders_state =
+				KBASE_SHADERS_OFF_CORESTACK_OFF;
+			backend->l2_state = KBASE_L2_OFF;
+			dev_dbg(kbdev->dev, "GPU lost has occurred - L2 off\n");
+			break;
+		}
 
 		/* mask off ready from trans in case transitions finished
 		 * between the register reads
@@ -575,8 +744,8 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 		case KBASE_L2_OFF:
 			if (kbase_pm_is_l2_desired(kbdev)) {
 				/*
-				 * Set the desired config for L2 before powering
-				 * it on
+				 * Set the desired config for L2 before
+				 * powering it on
 				 */
 				kbase_pm_l2_config_override(kbdev);
 
@@ -658,22 +827,30 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 			break;
 
 		case KBASE_L2_ON_HWCNT_ENABLE:
+#if !MALI_USE_CSF
 			backend->hwcnt_desired = true;
 			if (backend->hwcnt_disabled) {
 				kbase_hwcnt_context_enable(
 					kbdev->hwcnt_gpu_ctx);
 				backend->hwcnt_disabled = false;
 			}
+#endif
 			backend->l2_state = KBASE_L2_ON;
 			break;
 
 		case KBASE_L2_ON:
 			if (!kbase_pm_is_l2_desired(kbdev)) {
+#if !MALI_USE_CSF
 				/* Do not power off L2 until the shaders and
 				 * core stacks are off.
 				 */
 				if (backend->shaders_state != KBASE_SHADERS_OFF_CORESTACK_OFF)
 					break;
+#else
+				/* Do not power off L2 until the MCU has been stopped */
+				if (backend->mcu_state != KBASE_MCU_OFF)
+					break;
+#endif
 
 				/* We need to make sure hardware counters are
 				 * disabled before powering down the L2, to
@@ -690,6 +867,7 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 			break;
 
 		case KBASE_L2_ON_HWCNT_DISABLE:
+#if !MALI_USE_CSF
 			/* If the L2 became desired while we were waiting on the
 			 * worker to do the actual hwcnt disable (which might
 			 * happen if some work was submitted immediately after
@@ -719,6 +897,7 @@ static int kbase_pm_l2_update_state(struct kbase_device *kbdev)
 			if (!backend->hwcnt_disabled) {
 				kbase_pm_trigger_hwcnt_disable(kbdev);
 			}
+#endif
 
 			if (backend->hwcnt_disabled) {
 				if (kbdev->pm.backend.gpu_clock_slow_down_wa)
@@ -877,6 +1056,7 @@ static void shader_poweroff_timer_queue_cancel(struct kbase_device *kbdev)
 	}
 }
 
+#if !MALI_USE_CSF
 static const char *kbase_shader_core_state_to_string(
 	enum kbase_shader_core_state state)
 {
@@ -924,8 +1104,15 @@ static int kbase_pm_shaders_update_state(struct kbase_device *kbdev)
 		 * kbase_pm_get_ready_cores and kbase_pm_get_trans_cores
 		 * are vulnerable to corruption if gpu is lost
 		 */
-		if (kbase_is_gpu_lost(kbdev)) {
-			err = -EIO;
+		if (kbase_is_gpu_removed(kbdev)
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+				|| kbase_pm_is_gpu_lost(kbdev)) {
+#else
+				) {
+#endif
+			backend->shaders_state =
+				KBASE_SHADERS_OFF_CORESTACK_OFF;
+			dev_dbg(kbdev->dev, "GPU lost has occurred - shaders off\n");
 			break;
 		}
 
@@ -1197,6 +1384,7 @@ static int kbase_pm_shaders_update_state(struct kbase_device *kbdev)
 
 	return err;
 }
+#endif
 
 static bool kbase_pm_is_in_desired_state_nolock(struct kbase_device *kbdev)
 {
@@ -1211,12 +1399,21 @@ static bool kbase_pm_is_in_desired_state_nolock(struct kbase_device *kbdev)
 			kbdev->pm.backend.l2_state != KBASE_L2_OFF)
 		in_desired_state = false;
 
+#if !MALI_USE_CSF
 	if (kbdev->pm.backend.shaders_desired &&
 			kbdev->pm.backend.shaders_state != KBASE_SHADERS_ON_CORESTACK_ON)
 		in_desired_state = false;
 	else if (!kbdev->pm.backend.shaders_desired &&
 			kbdev->pm.backend.shaders_state != KBASE_SHADERS_OFF_CORESTACK_OFF)
 		in_desired_state = false;
+#else
+	if (kbase_pm_is_mcu_desired(kbdev) &&
+	    kbdev->pm.backend.mcu_state != KBASE_MCU_ON)
+		in_desired_state = false;
+	else if (!kbase_pm_is_mcu_desired(kbdev) &&
+		 kbdev->pm.backend.mcu_state != KBASE_MCU_OFF)
+		in_desired_state = false;
+#endif
 
 	return in_desired_state;
 }
@@ -1280,17 +1477,22 @@ static void kbase_pm_trace_power_state(struct kbase_device *kbdev)
 
 void kbase_pm_update_state(struct kbase_device *kbdev)
 {
+#if !MALI_USE_CSF
 	enum kbase_shader_core_state prev_shaders_state =
 			kbdev->pm.backend.shaders_state;
+#else
+	enum kbase_mcu_state prev_mcu_state = kbdev->pm.backend.mcu_state;
+#endif
 
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 
-	if (!kbdev->pm.backend.gpu_powered)
-		return; /* Do nothing if the GPU is off */
+	if (!kbdev->pm.backend.gpu_ready)
+		return; /* Do nothing if the GPU is not ready */
 
 	if (kbase_pm_l2_update_state(kbdev))
 		return;
 
+#if !MALI_USE_CSF
 	if (kbase_pm_shaders_update_state(kbdev))
 		return;
 
@@ -1304,9 +1506,20 @@ void kbase_pm_update_state(struct kbase_device *kbdev)
 		if (kbase_pm_l2_update_state(kbdev))
 			return;
 		}
+#else
+	if (kbase_pm_mcu_update_state(kbdev))
+		return;
+
+	if (prev_mcu_state != KBASE_MCU_OFF &&
+	    kbdev->pm.backend.mcu_state == KBASE_MCU_OFF) {
+		if (kbase_pm_l2_update_state(kbdev))
+			return;
+	}
+#endif
 
 	if (kbase_pm_is_in_desired_state_nolock(kbdev)) {
-		KBASE_KTRACE_ADD(kbdev, PM_DESIRED_REACHED, NULL, kbdev->pm.backend.shaders_avail);
+		KBASE_KTRACE_ADD(kbdev, PM_DESIRED_REACHED, NULL,
+				 kbdev->pm.backend.shaders_avail);
 
 		kbase_pm_trace_power_state(kbdev);
 
@@ -1382,7 +1595,19 @@ void kbase_pm_reset_start_locked(struct kbase_device *kbdev)
 
 	backend->in_reset = true;
 	backend->l2_state = KBASE_L2_RESET_WAIT;
+#if !MALI_USE_CSF
 	backend->shaders_state = KBASE_SHADERS_RESET_WAIT;
+#else
+	/* MCU state machine is exercised only after the initial load/boot
+	 * of the firmware.
+	 */
+	if (likely(kbdev->csf.firmware_inited)) {
+		backend->mcu_state = KBASE_MCU_RESET_WAIT;
+		kbdev->csf.firmware_reload_needed = true;
+	} else {
+		WARN_ON(backend->mcu_state != KBASE_MCU_OFF);
+	}
+#endif
 
 	/* We're in a reset, so hwcnt will have been synchronously disabled by
 	 * this function's caller as part of the reset process. We therefore
@@ -1423,14 +1648,26 @@ void kbase_pm_reset_complete(struct kbase_device *kbdev)
 /* Timeout for kbase_pm_wait_for_desired_state when wait_event_killable has
  * aborted due to a fatal signal. If the time spent waiting has exceeded this
  * threshold then there is most likely a hardware issue. */
-#define PM_TIMEOUT (5*HZ) /* 5s */
+#define PM_TIMEOUT_MS (5000) /* 5s */
 
 static void kbase_pm_timed_out(struct kbase_device *kbdev)
 {
+	unsigned long flags;
+
 	dev_err(kbdev->dev, "Power transition timed out unexpectedly\n");
+#if !MALI_USE_CSF
+	CSTD_UNUSED(flags);
 	dev_err(kbdev->dev, "Desired state :\n");
 	dev_err(kbdev->dev, "\tShader=%016llx\n",
 			kbdev->pm.backend.shaders_desired ? kbdev->pm.backend.shaders_avail : 0);
+#else
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	dev_err(kbdev->dev, "\tMCU desired = %d\n",
+		kbase_pm_is_mcu_desired(kbdev));
+	dev_err(kbdev->dev, "\tMCU sw state = %d\n",
+		kbdev->pm.backend.mcu_state);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+#endif
 	dev_err(kbdev->dev, "Current state :\n");
 	dev_err(kbdev->dev, "\tShader=%08x%08x\n",
 			kbase_reg_read(kbdev,
@@ -1447,6 +1684,10 @@ static void kbase_pm_timed_out(struct kbase_device *kbdev)
 				GPU_CONTROL_REG(L2_READY_HI)),
 			kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(L2_READY_LO)));
+#if MALI_USE_CSF
+	dev_err(kbdev->dev, "\tMCU status = %d\n",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(MCU_STATUS)));
+#endif
 	dev_err(kbdev->dev, "Cores transitioning :\n");
 	dev_err(kbdev->dev, "\tShader=%08x%08x\n",
 			kbase_reg_read(kbdev, GPU_CONTROL_REG(
@@ -1479,7 +1720,7 @@ void kbase_pm_wait_for_l2_powered(struct kbase_device *kbdev)
 	kbase_pm_update_state(kbdev);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-	timeout = jiffies + PM_TIMEOUT;
+	timeout = jiffies + msecs_to_jiffies(PM_TIMEOUT_MS);
 
 	/* Wait for cores */
 	err = wait_event_killable(kbdev->pm.backend.gpu_in_desired_state_wait,
@@ -1489,25 +1730,43 @@ void kbase_pm_wait_for_l2_powered(struct kbase_device *kbdev)
 		kbase_pm_timed_out(kbdev);
 }
 
-void kbase_pm_wait_for_desired_state(struct kbase_device *kbdev)
+int kbase_pm_wait_for_desired_state(struct kbase_device *kbdev)
 {
 	unsigned long flags;
-	unsigned long timeout;
-	int err;
+	long remaining;
+#if MALI_USE_CSF
+	long timeout = kbase_csf_timeout_in_jiffies(PM_TIMEOUT_MS);
+#else
+	long timeout = msecs_to_jiffies(PM_TIMEOUT_MS);
+#endif
+	int err = 0;
 
 	/* Let the state machine latch the most recent desired state. */
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
 	kbase_pm_update_state(kbdev);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
-	timeout = jiffies + PM_TIMEOUT;
-
 	/* Wait for cores */
-	err = wait_event_killable(kbdev->pm.backend.gpu_in_desired_state_wait,
-			kbase_pm_is_in_desired_state(kbdev));
+#if KERNEL_VERSION(4, 13, 1) <= LINUX_VERSION_CODE
+	remaining = wait_event_killable_timeout(
+		kbdev->pm.backend.gpu_in_desired_state_wait,
+		kbase_pm_is_in_desired_state(kbdev), timeout);
+#else
+	remaining = wait_event_timeout(
+		kbdev->pm.backend.gpu_in_desired_state_wait,
+		kbase_pm_is_in_desired_state(kbdev), timeout);
+#endif
 
-	if (err < 0 && time_after(jiffies, timeout))
+	if (!remaining) {
 		kbase_pm_timed_out(kbdev);
+		err = -ETIMEDOUT;
+	} else if (remaining < 0) {
+		dev_info(kbdev->dev,
+			 "Wait for desired PM state got interrupted");
+		err = (int)remaining;
+	}
+
+	return err;
 }
 KBASE_EXPORT_TEST_API(kbase_pm_wait_for_desired_state);
 
@@ -1529,7 +1788,12 @@ void kbase_pm_enable_interrupts(struct kbase_device *kbdev)
 	kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_MASK), 0xFFFFFFFF);
 
 	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_CLEAR), 0xFFFFFFFF);
+#if MALI_USE_CSF
+	/* Enable only the Page fault bits part */
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), 0xFFFF);
+#else
 	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), 0xFFFFFFFF);
+#endif
 }
 
 KBASE_EXPORT_TEST_API(kbase_pm_enable_interrupts);
@@ -1575,9 +1839,19 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 	unsigned long flags;
 
 	KBASE_DEBUG_ASSERT(NULL != kbdev);
+#if !MALI_USE_CSF
 	lockdep_assert_held(&kbdev->js_data.runpool_mutex);
+#endif /* !MALI_USE_CSF */
 	lockdep_assert_held(&kbdev->pm.lock);
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (WARN_ON(kbase_pm_is_gpu_lost(kbdev))) {
+		dev_err(kbdev->dev,
+			"%s: Cannot power up while GPU lost", __func__);
+		return;
+	}
+#endif
+
 	if (kbdev->pm.backend.gpu_powered) {
 		/* Already turned on */
 		if (kbdev->poweroff_pending)
@@ -1607,6 +1881,20 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 		 * consistent state */
 		kbase_pm_init_hw(kbdev, PM_ENABLE_IRQS);
 	}
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	else {
+		struct kbase_arbiter_vm_state *arb_vm_state =
+				kbdev->pm.arb_vm_state;
+
+		/* In the case that the GPU has just been granted by
+		 * the Arbiter, a reset will have already been done.
+		 * However, it is still necessary to initialize the GPU.
+		 */
+		if (arb_vm_state->vm_arb_starting)
+			kbase_pm_init_hw(kbdev, PM_ENABLE_IRQS |
+					PM_NO_RESET);
+	}
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
 	mutex_lock(&kbdev->mmu_hw_mutex);
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
@@ -1628,6 +1916,7 @@ void kbase_pm_clock_on(struct kbase_device *kbdev, bool is_resume)
 
 	/* Turn on the L2 caches */
 	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbdev->pm.backend.gpu_ready = true;
 	kbdev->pm.backend.l2_desired = true;
 	kbase_pm_update_state(kbdev);
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
@@ -1670,8 +1959,20 @@ bool kbase_pm_clock_off(struct kbase_device *kbdev)
 
 	kbase_pm_cache_snoop_disable(kbdev);
 
+	kbdev->pm.backend.gpu_ready = false;
+
 	/* The GPU power may be turned off from this point */
 	kbdev->pm.backend.gpu_powered = false;
+
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (kbase_pm_is_gpu_lost(kbdev)) {
+		/* Ensure we unblock any threads that are stuck waiting
+		 * for the GPU
+		 */
+		kbase_gpu_cache_clean_wait_complete(kbdev);
+	}
+#endif
+
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
 	kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_GPU_IDLE_EVENT);
@@ -1731,6 +2032,10 @@ static enum hrtimer_restart kbasep_reset_timeout(struct hrtimer *timer)
 
 static int kbase_set_jm_quirks(struct kbase_device *kbdev, const u32 prod_id)
 {
+#if MALI_USE_CSF
+	kbdev->hw_quirks_jm = kbase_reg_read(kbdev,
+				GPU_CONTROL_REG(CSF_CONFIG));
+#else
 	u32 hw_quirks_jm = kbase_reg_read(kbdev,
 				GPU_CONTROL_REG(JM_CONFIG));
 
@@ -1752,11 +2057,12 @@ static int kbase_set_jm_quirks(struct kbase_device *kbdev, const u32 prod_id)
 		}
 	}
 
-	if (kbase_is_gpu_lost(kbdev))
+	if (kbase_is_gpu_removed(kbdev))
 		return -EIO;
 
 	kbdev->hw_quirks_jm = hw_quirks_jm;
 
+#endif /* !MALI_USE_CSF */
 	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_IDVS_GROUP_SIZE)) {
 		int default_idvs_group_size = 0xF;
 		u32 tmp;
@@ -1787,7 +2093,7 @@ static int kbase_set_sc_quirks(struct kbase_device *kbdev, const u32 prod_id)
 	u32 hw_quirks_sc = kbase_reg_read(kbdev,
 					GPU_CONTROL_REG(SHADER_CONFIG));
 
-	if (kbase_is_gpu_lost(kbdev))
+	if (kbase_is_gpu_removed(kbdev))
 		return -EIO;
 
 	if (prod_id < 0x750 || prod_id == 0x6956) /* T60x, T62x, T72x */
@@ -1811,7 +2117,7 @@ static int kbase_set_tiler_quirks(struct kbase_device *kbdev)
 	u32 hw_quirks_tiler = kbase_reg_read(kbdev,
 					GPU_CONTROL_REG(TILER_CONFIG));
 
-	if (kbase_is_gpu_lost(kbdev))
+	if (kbase_is_gpu_removed(kbdev))
 		return -EIO;
 
 	/* Set tiler clock gate override if required */
@@ -1891,8 +2197,13 @@ static void kbase_pm_hw_issues_apply(struct kbase_device *kbdev)
 
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG),
 			kbdev->hw_quirks_mmu);
+#if MALI_USE_CSF
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(CSF_CONFIG),
+			kbdev->hw_quirks_jm);
+#else
 	kbase_reg_write(kbdev, GPU_CONTROL_REG(JM_CONFIG),
 			kbdev->hw_quirks_jm);
+#endif
 }
 
 void kbase_pm_cache_snoop_enable(struct kbase_device *kbdev)
@@ -1982,8 +2293,7 @@ static int kbase_pm_do_reset(struct kbase_device *kbdev)
 	/* No interrupt has been received - check if the RAWSTAT register says
 	 * the reset has completed */
 	if ((kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT)) &
-							RESET_COMPLETED)
-		|| kbase_is_gpu_lost(kbdev)) {
+							RESET_COMPLETED)) {
 		/* The interrupt is set in the RAWSTAT; this suggests that the
 		 * interrupts are not getting to the CPU */
 		dev_err(kbdev->dev, "Reset interrupt didn't reach CPU. Check interrupt assignments.\n");
@@ -1992,6 +2302,12 @@ static int kbase_pm_do_reset(struct kbase_device *kbdev)
 		return -EINVAL;
 	}
 
+	if (kbase_is_gpu_removed(kbdev)) {
+		dev_dbg(kbdev->dev, "GPU has been removed, reset no longer needed.\n");
+		destroy_hrtimer_on_stack(&rtdata.timer);
+		return -EINVAL;
+	}
+
 	/* The GPU doesn't seem to be responding to the reset so try a hard
 	 * reset */
 	dev_err(kbdev->dev, "Failed to soft-reset GPU (timed out after %d ms), now attempting a hard reset\n",
@@ -2041,7 +2357,7 @@ int kbase_pm_protected_mode_disable(struct kbase_device *const kbdev)
 int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags)
 {
 	unsigned long irq_flags;
-	int err;
+	int err = 0;
 
 	KBASE_DEBUG_ASSERT(NULL != kbdev);
 	lockdep_assert_held(&kbdev->pm.lock);
@@ -2069,8 +2385,11 @@ int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags)
 	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
 
 	/* Soft reset the GPU */
-	err = kbdev->protected_ops->protected_mode_disable(
-			kbdev->protected_dev);
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	if (!(flags & PM_NO_RESET))
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
+		err = kbdev->protected_ops->protected_mode_disable(
+				kbdev->protected_dev);
 
 	spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
 	kbdev->protected_mode = false;
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h
index f8da114003f1..50ca016bbd6d 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_internal.h
@@ -205,6 +205,29 @@ int kbase_pm_init_hw(struct kbase_device *kbdev, unsigned int flags);
  */
 void kbase_pm_reset_done(struct kbase_device *kbdev);
 
+#if MALI_USE_CSF
+/**
+ * kbase_pm_wait_for_desired_state - Wait for the desired power state to be
+ *                                   reached
+ *
+ * Wait for the L2 and MCU state machines to reach the states corresponding
+ * to the values of 'kbase_pm_is_l2_desired' and 'kbase_pm_is_mcu_desired'.
+ *
+ * The usual use-case for this is to ensure that all parts of GPU have been
+ * powered up after performing a GPU Reset.
+ *
+ * Unlike kbase_pm_update_state(), the caller must not hold hwaccess_lock,
+ * because this function will take that lock itself.
+ *
+ * NOTE: This may not wait until the correct state is reached if there is a
+ * power off in progress and kbase_pm_context_active() was called instead of
+ * kbase_csf_scheduler_pm_active().
+ *
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
+ *
+ * Return: 0 on success, error code on error
+ */
+#else
 /**
  * kbase_pm_wait_for_desired_state - Wait for the desired power state to be
  *                                   reached
@@ -224,15 +247,17 @@ void kbase_pm_reset_done(struct kbase_device *kbdev);
  * kbase_pm_wait_for_poweroff_complete()
  *
  * @kbdev: The kbase device structure for the device (must be a valid pointer)
+ *
+ * Return: 0 on success, error code on error
  */
-void kbase_pm_wait_for_desired_state(struct kbase_device *kbdev);
+#endif
+int kbase_pm_wait_for_desired_state(struct kbase_device *kbdev);
 
 /**
  * kbase_pm_wait_for_l2_powered - Wait for the L2 cache to be powered on
  *
- * Wait for the L2 to be powered on, and for the L2 and shader state machines to
- * stabilise by reaching the states corresponding to the values of 'l2_desired'
- * and 'shaders_desired'.
+ * Wait for the L2 to be powered on, and for the L2 and the state machines of
+ * its dependent stack components to stabilise.
  *
  * kbdev->pm.active_count must be non-zero when calling this function.
  *
@@ -692,7 +717,9 @@ bool kbase_pm_is_l2_desired(struct kbase_device *kbdev);
  */
 static inline void kbase_pm_lock(struct kbase_device *kbdev)
 {
+#if !MALI_USE_CSF
 	mutex_lock(&kbdev->js_data.runpool_mutex);
+#endif /* !MALI_USE_CSF */
 	mutex_lock(&kbdev->pm.lock);
 }
 
@@ -704,7 +731,9 @@ static inline void kbase_pm_lock(struct kbase_device *kbdev)
 static inline void kbase_pm_unlock(struct kbase_device *kbdev)
 {
 	mutex_unlock(&kbdev->pm.lock);
+#if !MALI_USE_CSF
 	mutex_unlock(&kbdev->js_data.runpool_mutex);
+#endif /* !MALI_USE_CSF */
 }
 
 #endif /* _KBASE_BACKEND_PM_INTERNAL_H_ */
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_mcu_states.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_mcu_states.h
new file mode 100644
index 000000000000..e163bd4f4094
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_mcu_states.h
@@ -0,0 +1,39 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * Backend-specific Power Manager MCU state definitions.
+ * The function-like macro KBASEP_MCU_STATE() must be defined before including
+ * this header file. This header file can be included multiple times in the
+ * same compilation unit with different definitions of KBASEP_MCU_STATE().
+ */
+KBASEP_MCU_STATE(OFF)
+KBASEP_MCU_STATE(PEND_ON_RELOAD)
+KBASEP_MCU_STATE(ON_GLB_REINIT_PEND)
+KBASEP_MCU_STATE(ON_HWCNT_ENABLE)
+KBASEP_MCU_STATE(ON)
+KBASEP_MCU_STATE(ON_HWCNT_DISABLE)
+KBASEP_MCU_STATE(ON_HALT)
+KBASEP_MCU_STATE(ON_PEND_HALT)
+KBASEP_MCU_STATE(POWER_DOWN)
+KBASEP_MCU_STATE(PEND_OFF)
+KBASEP_MCU_STATE(RESET_WAIT)
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c
index 519fc41a272b..b714971ba17c 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_metrics.c
@@ -29,7 +29,9 @@
 #include <mali_kbase.h>
 #include <mali_kbase_pm.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
+#if !MALI_USE_CSF
 #include <backend/gpu/mali_kbase_jm_rb.h>
+#endif /* !MALI_USE_CSF */
 #include <backend/gpu/mali_kbase_pm_defs.h>
 #include <mali_linux_trace.h>
 
@@ -246,6 +248,7 @@ void kbase_pm_metrics_stop(struct kbase_device *kbdev)
 
 #endif /* CONFIG_MALI_BIFROST_DVFS */
 
+#if !MALI_USE_CSF
 /**
  * kbase_pm_metrics_active_calc - Update PM active counts based on currently
  *                                running atoms
@@ -293,6 +296,7 @@ static void kbase_pm_metrics_active_calc(struct kbase_device *kbdev)
 		}
 	}
 }
+#endif /* !MALI_USE_CSF */
 
 /* called when job is submitted to or removed from a GPU slot */
 void kbase_pm_metrics_update(struct kbase_device *kbdev, ktime_t *timestamp)
@@ -312,7 +316,9 @@ void kbase_pm_metrics_update(struct kbase_device *kbdev, ktime_t *timestamp)
 	/* Track how long CL and/or GL jobs have been busy for */
 	kbase_pm_get_dvfs_utilisation_calc(kbdev, *timestamp);
 
+#if !MALI_USE_CSF
 	kbase_pm_metrics_active_calc(kbdev);
+#endif /* !MALI_USE_CSF */
 
 	spin_unlock_irqrestore(&kbdev->pm.backend.metrics.lock, flags);
 }
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c
index 8e7b3de9a945..48b24b1c866e 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_policy.c
@@ -93,6 +93,9 @@ void kbase_pm_update_active(struct kbase_device *kbdev)
 			pm->backend.invoke_poweroff_wait_wq_when_l2_off = false;
 			pm->backend.poweroff_wait_in_progress = false;
 			pm->backend.l2_desired = true;
+#if MALI_USE_CSF
+			pm->backend.mcu_desired = true;
+#endif
 
 			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 			kbase_pm_do_poweron(kbdev, false);
@@ -123,6 +126,14 @@ void kbase_pm_update_dynamic_cores_onoff(struct kbase_device *kbdev)
 	lockdep_assert_held(&kbdev->hwaccess_lock);
 	lockdep_assert_held(&kbdev->pm.lock);
 
+#if MALI_USE_CSF
+	/* On CSF GPUs, Host driver isn't supposed to do the power management
+	 * for shader cores. CSF firmware will power up the cores appropriately
+	 * and so from Driver's standpoint 'shaders_desired' flag shall always
+	 * remain 0.
+	 */
+	return;
+#endif
 	if (kbdev->pm.backend.pm_current_policy == NULL)
 		return;
 	if (kbdev->pm.backend.poweroff_wait_in_progress)
@@ -158,6 +169,14 @@ void kbase_pm_update_cores_state_nolock(struct kbase_device *kbdev)
 	else
 		shaders_desired = kbdev->pm.backend.pm_current_policy->shaders_needed(kbdev);
 
+#if MALI_USE_CSF
+	/* On CSF GPUs, Host driver isn't supposed to do the power management
+	 * for shader cores. CSF firmware will power up the cores appropriately
+	 * and so from Driver's standpoint 'shaders_desired' flag shall always
+	 * remain 0.
+	 */
+	shaders_desired = false;
+#endif
 	if (kbdev->pm.backend.shaders_desired != shaders_desired) {
 		KBASE_KTRACE_ADD(kbdev, PM_CORES_CHANGE_DESIRED, NULL, kbdev->pm.backend.shaders_desired);
 
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_shader_states.h b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_shader_states.h
index 2bd9e4798e93..6cafaa171962 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_shader_states.h
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_pm_shader_states.h
@@ -33,7 +33,9 @@ KBASEP_SHADER_STATE(PEND_ON_CORESTACK_ON)
 KBASEP_SHADER_STATE(ON_CORESTACK_ON)
 KBASEP_SHADER_STATE(ON_CORESTACK_ON_RECHECK)
 KBASEP_SHADER_STATE(WAIT_OFF_CORESTACK_ON)
+#if !MALI_USE_CSF
 KBASEP_SHADER_STATE(WAIT_GPU_IDLE)
+#endif /* !MALI_USE_CSF */
 KBASEP_SHADER_STATE(WAIT_FINISHED_CORESTACK_ON)
 KBASEP_SHADER_STATE(L2_FLUSHING_CORESTACK_ON)
 KBASEP_SHADER_STATE(READY_OFF_CORESTACK_ON)
diff --git a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c
index cb105186d798..e19f53b2cbe8 100644
--- a/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c
+++ b/drivers/gpu/arm/bifrost/backend/gpu/mali_kbase_time.c
@@ -22,16 +22,16 @@
 
 #include <mali_kbase.h>
 #include <mali_kbase_hwaccess_time.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
 
-void kbase_backend_get_gpu_time(struct kbase_device *kbdev, u64 *cycle_counter,
-				u64 *system_time, struct timespec64 *ts)
+void kbase_backend_get_gpu_time_norequest(struct kbase_device *kbdev,
+					  u64 *cycle_counter,
+					  u64 *system_time,
+					  struct timespec64 *ts)
 {
 	u32 hi1, hi2;
 
-	kbase_pm_request_gpu_cycle_counter(kbdev);
-
 	if (cycle_counter) {
 		/* Read hi, lo, hi to ensure a coherent u64 */
 		do {
@@ -65,6 +65,17 @@ void kbase_backend_get_gpu_time(struct kbase_device *kbdev, u64 *cycle_counter,
 #else
 		ktime_get_raw_ts64(ts);
 #endif
+}
 
+void kbase_backend_get_gpu_time(struct kbase_device *kbdev, u64 *cycle_counter,
+				u64 *system_time, struct timespec64 *ts)
+{
+#if !MALI_USE_CSF
+	kbase_pm_request_gpu_cycle_counter(kbdev);
+#endif
+	kbase_backend_get_gpu_time_norequest(
+		kbdev, cycle_counter, system_time, ts);
+#if !MALI_USE_CSF
 	kbase_pm_release_gpu_cycle_counter(kbdev);
+#endif
 }
diff --git a/drivers/gpu/arm/bifrost/build.bp b/drivers/gpu/arm/bifrost/build.bp
index a74677522f8e..b9b86184f3be 100644
--- a/drivers/gpu/arm/bifrost/build.bp
+++ b/drivers/gpu/arm/bifrost/build.bp
@@ -25,6 +25,9 @@ bob_defaults {
     mali_real_hw: {
         kbuild_options: ["CONFIG_MALI_REAL_HW=y"],
     },
+    mali_dma_fence: {
+        kbuild_options: ["CONFIG_MALI_BIFROST_DMA_FENCE=y"],
+    },
     mali_devfreq: {
         kbuild_options: ["CONFIG_MALI_BIFROST_DEVFREQ=y"],
     },
diff --git a/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c
new file mode 100644
index 000000000000..7c68eb2f860a
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_csf.c
@@ -0,0 +1,177 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * Base kernel context APIs for CSF GPUs
+ */
+
+#include <context/mali_kbase_context_internal.h>
+#include <gpu/mali_kbase_gpu_regmap.h>
+#include <mali_kbase.h>
+#include <mali_kbase_ctx_sched.h>
+#include <mali_kbase_dma_fence.h>
+#include <mali_kbase_mem_linux.h>
+#include <mali_kbase_mem_pool_group.h>
+#include <mmu/mali_kbase_mmu.h>
+#include <tl/mali_kbase_timeline.h>
+#include <tl/mali_kbase_tracepoints.h>
+
+#ifdef CONFIG_DEBUG_FS
+#include <csf/mali_kbase_csf_csg_debugfs.h>
+#include <csf/mali_kbase_csf_kcpu_debugfs.h>
+#include <csf/mali_kbase_csf_tiler_heap_debugfs.h>
+#include <mali_kbase_debug_mem_view.h>
+#include <mali_kbase_mem_pool_debugfs.h>
+
+void kbase_context_debugfs_init(struct kbase_context *const kctx)
+{
+	kbase_debug_mem_view_init(kctx);
+	kbase_mem_pool_debugfs_init(kctx->kctx_dentry, kctx);
+	kbase_jit_debugfs_init(kctx);
+	kbase_csf_queue_group_debugfs_init(kctx);
+	kbase_csf_kcpu_debugfs_init(kctx);
+	kbase_csf_tiler_heap_debugfs_init(kctx);
+}
+KBASE_EXPORT_SYMBOL(kbase_context_debugfs_init);
+
+void kbase_context_debugfs_term(struct kbase_context *const kctx)
+{
+	debugfs_remove_recursive(kctx->kctx_dentry);
+}
+KBASE_EXPORT_SYMBOL(kbase_context_debugfs_term);
+#else
+void kbase_context_debugfs_init(struct kbase_context *const kctx)
+{
+	CSTD_UNUSED(kctx);
+}
+KBASE_EXPORT_SYMBOL(kbase_context_debugfs_init);
+
+void kbase_context_debugfs_term(struct kbase_context *const kctx)
+{
+	CSTD_UNUSED(kctx);
+}
+KBASE_EXPORT_SYMBOL(kbase_context_debugfs_term);
+#endif /* CONFIG_DEBUG_FS */
+
+static const struct kbase_context_init context_init[] = {
+	{kbase_context_common_init, kbase_context_common_term, NULL},
+	{kbase_context_mem_pool_group_init, kbase_context_mem_pool_group_term,
+			"Memory pool goup initialization failed"},
+	{kbase_mem_evictable_init, kbase_mem_evictable_deinit,
+			"Memory evictable initialization failed"},
+	{kbase_context_mmu_init, kbase_context_mmu_term,
+			"MMU initialization failed"},
+	{kbase_context_mem_alloc_page, kbase_context_mem_pool_free,
+			"Memory alloc page failed"},
+	{kbase_region_tracker_init, kbase_region_tracker_term,
+			"Region tracker initialization failed"},
+	{kbase_sticky_resource_init, kbase_context_sticky_resource_term,
+			"Sticky resource initialization failed"},
+	{kbase_jit_init, kbase_jit_term,
+			"JIT initialization failed"},
+	{kbase_csf_ctx_init, kbase_csf_ctx_term,
+			"CSF context initialization failed"},
+};
+
+static void kbase_context_term_partial(
+	struct kbase_context *kctx,
+	unsigned int i)
+{
+	while (i-- > 0) {
+		if (context_init[i].term)
+			context_init[i].term(kctx);
+	}
+}
+
+struct kbase_context *kbase_create_context(struct kbase_device *kbdev,
+	bool is_compat,
+	base_context_create_flags const flags,
+	unsigned long const api_version,
+	struct file *const filp)
+{
+	struct kbase_context *kctx;
+	unsigned int i = 0;
+
+	if (WARN_ON(!kbdev))
+		return NULL;
+
+	/* Validate flags */
+	if (WARN_ON(flags != (flags & BASEP_CONTEXT_CREATE_KERNEL_FLAGS)))
+		return NULL;
+
+	/* zero-inited as lot of code assume it's zero'ed out on create */
+	kctx = vzalloc(sizeof(*kctx));
+	if (WARN_ON(!kctx))
+		return NULL;
+
+	kctx->kbdev = kbdev;
+	kctx->api_version = api_version;
+	kctx->filp = filp;
+	kctx->create_flags = flags;
+
+	if (is_compat)
+		kbase_ctx_flag_set(kctx, KCTX_COMPAT);
+#if defined(CONFIG_64BIT)
+	else
+		kbase_ctx_flag_set(kctx, KCTX_FORCE_SAME_VA);
+#endif /* !defined(CONFIG_64BIT) */
+
+	for (i = 0; i < ARRAY_SIZE(context_init); i++) {
+		int err = context_init[i].init(kctx);
+
+		if (err) {
+			dev_err(kbdev->dev, "%s error = %d\n",
+						context_init[i].err_mes, err);
+			kbase_context_term_partial(kctx, i);
+			return NULL;
+		}
+	}
+
+	return kctx;
+}
+KBASE_EXPORT_SYMBOL(kbase_create_context);
+
+void kbase_destroy_context(struct kbase_context *kctx)
+{
+	struct kbase_device *kbdev;
+
+	if (WARN_ON(!kctx))
+		return;
+
+	kbdev = kctx->kbdev;
+	if (WARN_ON(!kbdev))
+		return;
+
+	/* Ensure the core is powered up for the destroy process
+	 * A suspend won't happen here, because we're in a syscall
+	 * from a userspace thread.
+	 */
+	kbase_pm_context_active(kbdev);
+
+	kbase_mem_pool_group_mark_dying(&kctx->mem_pools);
+
+	kbase_context_term_partial(kctx, ARRAY_SIZE(context_init));
+
+	kbase_pm_context_idle(kbdev);
+}
+KBASE_EXPORT_SYMBOL(kbase_destroy_context);
diff --git a/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c
index 2cd2551b433e..0eb42589fe46 100644
--- a/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c
+++ b/drivers/gpu/arm/bifrost/context/backend/mali_kbase_context_jm.c
@@ -30,6 +30,7 @@
 #include <mali_kbase.h>
 #include <mali_kbase_ctx_sched.h>
 #include <mali_kbase_dma_fence.h>
+#include <mali_kbase_kinstr_jm.h>
 #include <mali_kbase_mem_linux.h>
 #include <mali_kbase_mem_pool_group.h>
 #include <mmu/mali_kbase_mmu.h>
@@ -70,6 +71,21 @@ void kbase_context_debugfs_term(struct kbase_context *const kctx)
 KBASE_EXPORT_SYMBOL(kbase_context_debugfs_term);
 #endif /* CONFIG_DEBUG_FS */
 
+static int kbase_context_kbase_kinstr_jm_init(struct kbase_context *kctx)
+{
+	int ret = kbase_kinstr_jm_init(&kctx->kinstr_jm);
+
+	if (!ret)
+		return ret;
+
+	return 0;
+}
+
+static void kbase_context_kbase_kinstr_jm_term(struct kbase_context *kctx)
+{
+	kbase_kinstr_jm_term(kctx->kinstr_jm);
+}
+
 static int kbase_context_kbase_timer_setup(struct kbase_context *kctx)
 {
 	kbase_timer_setup(&kctx->soft_job_timeout,
@@ -99,31 +115,32 @@ static int kbase_context_submit_check(struct kbase_context *kctx)
 }
 
 static const struct kbase_context_init context_init[] = {
-	{kbase_context_common_init, kbase_context_common_term, NULL},
-	{kbase_context_mem_pool_group_init, kbase_context_mem_pool_group_term,
-			"Memory pool goup initialization failed"},
-	{kbase_mem_evictable_init, kbase_mem_evictable_deinit,
-			"Memory evictable initialization failed"},
-	{kbasep_js_kctx_init, kbasep_js_kctx_term,
-			"JS kctx initialization failed"},
-	{kbase_jd_init, kbase_jd_exit,
-			"JD initialization failed"},
-	{kbase_event_init, kbase_event_cleanup,
-			"Event initialization failed"},
-	{kbase_dma_fence_init, kbase_dma_fence_term,
-			"DMA fence initialization failed"},
-	{kbase_context_mmu_init, kbase_context_mmu_term,
-			"MMU initialization failed"},
-	{kbase_context_mem_alloc_page, kbase_context_mem_pool_free,
-			"Memory alloc page failed"},
-	{kbase_region_tracker_init, kbase_region_tracker_term,
-			"Region tracker initialization failed"},
-	{kbase_sticky_resource_init, kbase_context_sticky_resource_term,
-			"Sticky resource initialization failed"},
-	{kbase_jit_init, kbase_jit_term,
-			"JIT initialization failed"},
-	{kbase_context_kbase_timer_setup, NULL, NULL},
-	{kbase_context_submit_check, NULL, NULL},
+	{ kbase_context_common_init, kbase_context_common_term, NULL },
+	{ kbase_dma_fence_init, kbase_dma_fence_term,
+	  "DMA fence initialization failed" },
+	{ kbase_context_mem_pool_group_init, kbase_context_mem_pool_group_term,
+	  "Memory pool goup initialization failed" },
+	{ kbase_mem_evictable_init, kbase_mem_evictable_deinit,
+	  "Memory evictable initialization failed" },
+	{ kbase_context_mmu_init, kbase_context_mmu_term,
+	  "MMU initialization failed" },
+	{ kbase_context_mem_alloc_page, kbase_context_mem_pool_free,
+	  "Memory alloc page failed" },
+	{ kbase_region_tracker_init, kbase_region_tracker_term,
+	  "Region tracker initialization failed" },
+	{ kbase_sticky_resource_init, kbase_context_sticky_resource_term,
+	  "Sticky resource initialization failed" },
+	{ kbase_jit_init, kbase_jit_term, "JIT initialization failed" },
+	{ kbase_context_kbase_kinstr_jm_init,
+	  kbase_context_kbase_kinstr_jm_term,
+	  "JM instrumentation initialization failed" },
+	{ kbase_context_kbase_timer_setup, NULL, NULL },
+	{ kbase_event_init, kbase_event_cleanup,
+	  "Event initialization failed" },
+	{ kbasep_js_kctx_init, kbasep_js_kctx_term,
+	  "JS kctx initialization failed" },
+	{ kbase_jd_init, kbase_jd_exit, "JD initialization failed" },
+	{ kbase_context_submit_check, NULL, NULL },
 };
 
 static void kbase_context_term_partial(
diff --git a/drivers/gpu/arm/bifrost/context/mali_kbase_context.c b/drivers/gpu/arm/bifrost/context/mali_kbase_context.c
index 93fe43147536..83182f983467 100644
--- a/drivers/gpu/arm/bifrost/context/mali_kbase_context.c
+++ b/drivers/gpu/arm/bifrost/context/mali_kbase_context.c
@@ -36,9 +36,99 @@
 #include <mmu/mali_kbase_mmu.h>
 #include <context/mali_kbase_context_internal.h>
 
+/**
+ * find_process_node - Used to traverse the process rb_tree to find if
+ *                     process exists already in process rb_tree.
+ *
+ * @node: Pointer to root node to start search.
+ * @tgid: Thread group PID to search for.
+ *
+ * Return: Pointer to kbase_process if exists otherwise NULL.
+ */
+static struct kbase_process *find_process_node(struct rb_node *node, pid_t tgid)
+{
+	struct kbase_process *kprcs = NULL;
+
+	/* Check if the kctx creation request is from a existing process.*/
+	while (node) {
+		struct kbase_process *prcs_node =
+			rb_entry(node, struct kbase_process, kprcs_node);
+		if (prcs_node->tgid == tgid) {
+			kprcs = prcs_node;
+			break;
+		}
+
+		if (tgid < prcs_node->tgid)
+			node = node->rb_left;
+		else
+			node = node->rb_right;
+	}
+
+	return kprcs;
+}
+
+/**
+ * kbase_insert_kctx_to_process - Initialise kbase process context.
+ *
+ * @kctx: Pointer to kbase context.
+ *
+ * Here we initialise per process rb_tree managed by kbase_device.
+ * We maintain a rb_tree of each unique process that gets created.
+ * and Each process maintains a list of kbase context.
+ * This setup is currently used by kernel trace functionality
+ * to trace and visualise gpu memory consumption.
+ *
+ * Return: 0 on success and error number on failure.
+ */
+static int kbase_insert_kctx_to_process(struct kbase_context *kctx)
+{
+	struct rb_root *const prcs_root = &kctx->kbdev->process_root;
+	const pid_t tgid = kctx->tgid;
+	struct kbase_process *kprcs = NULL;
+
+	lockdep_assert_held(&kctx->kbdev->kctx_list_lock);
+
+	kprcs = find_process_node(prcs_root->rb_node, tgid);
+
+	/* if the kctx is from new process then create a new kbase_process
+	 * and add it to the &kbase_device->rb_tree
+	 */
+	if (!kprcs) {
+		struct rb_node **new = &prcs_root->rb_node, *parent = NULL;
+
+		kprcs = kzalloc(sizeof(*kprcs), GFP_KERNEL);
+		if (kprcs == NULL)
+			return -ENOMEM;
+		kprcs->tgid = tgid;
+		INIT_LIST_HEAD(&kprcs->kctx_list);
+		kprcs->dma_buf_root = RB_ROOT;
+		kprcs->total_gpu_pages = 0;
+
+		while (*new) {
+			struct kbase_process *prcs_node;
+
+			parent = *new;
+			prcs_node = rb_entry(parent, struct kbase_process,
+					     kprcs_node);
+			if (tgid < prcs_node->tgid)
+				new = &(*new)->rb_left;
+			else
+				new = &(*new)->rb_right;
+		}
+		rb_link_node(&kprcs->kprcs_node, parent, new);
+		rb_insert_color(&kprcs->kprcs_node, prcs_root);
+	}
+
+	kctx->kprcs = kprcs;
+	list_add(&kctx->kprcs_link, &kprcs->kctx_list);
+
+	return 0;
+}
+
 int kbase_context_common_init(struct kbase_context *kctx)
 {
 	const unsigned long cookies_mask = KBASE_COOKIE_MASK;
+	int err = 0;
 
 	/* creating a context is considered a disjoint event */
 	kbase_disjoint_event(kctx->kbdev);
@@ -66,13 +156,14 @@ int kbase_context_common_init(struct kbase_context *kctx)
 
 	init_waitqueue_head(&kctx->event_queue);
 	atomic_set(&kctx->event_count, 0);
+#if !MALI_USE_CSF
 	atomic_set(&kctx->event_closed, false);
-
-	bitmap_copy(kctx->cookies, &cookies_mask, BITS_PER_LONG);
-
 #ifdef CONFIG_GPU_TRACEPOINTS
 	atomic_set(&kctx->jctx.work_id, 0);
 #endif
+#endif
+
+	bitmap_copy(kctx->cookies, &cookies_mask, BITS_PER_LONG);
 
 	kctx->id = atomic_add_return(1, &(kctx->kbdev->ctx_num)) - 1;
 
@@ -81,13 +172,50 @@ int kbase_context_common_init(struct kbase_context *kctx)
 	mutex_lock(&kctx->kbdev->kctx_list_lock);
 	list_add(&kctx->kctx_list_link, &kctx->kbdev->kctx_list);
 
+	err = kbase_insert_kctx_to_process(kctx);
+	if (err)
+		dev_err(kctx->kbdev->dev,
+		"(err:%d) failed to insert kctx to kbase_process\n", err);
+
 	KBASE_TLSTREAM_TL_KBASE_NEW_CTX(kctx->kbdev, kctx->id,
 		kctx->kbdev->gpu_props.props.raw_props.gpu_id);
 	KBASE_TLSTREAM_TL_NEW_CTX(kctx->kbdev, kctx, kctx->id,
 			(u32)(kctx->tgid));
 	mutex_unlock(&kctx->kbdev->kctx_list_lock);
 
-	return 0;
+	return err;
+}
+
+/**
+ * kbase_remove_kctx_from_process - remove a terminating context from
+ *                                    the process list.
+ *
+ * @kctx: Pointer to kbase context.
+ *
+ * Remove the tracking of context from the list of contexts maintained under
+ * kbase process and if the list if empty then there no outstanding contexts
+ * we can remove the process node as well.
+ */
+
+static void kbase_remove_kctx_from_process(struct kbase_context *kctx)
+{
+	struct kbase_process *kprcs = kctx->kprcs;
+
+	lockdep_assert_held(&kctx->kbdev->kctx_list_lock);
+	list_del(&kctx->kprcs_link);
+
+	/* if there are no outstanding contexts in current process node,
+	 * we can remove it from the process rb_tree.
+	 */
+	if (list_empty(&kprcs->kctx_list)) {
+		rb_erase(&kprcs->kprcs_node, &kctx->kbdev->process_root);
+		/* Add checks, so that the terminating process Should not
+		 * hold any gpu_memory.
+		 */
+		WARN_ON(kprcs->total_gpu_pages);
+		WARN_ON(!RB_EMPTY_ROOT(&kprcs->dma_buf_root));
+		kfree(kprcs);
+	}
 }
 
 void kbase_context_common_term(struct kbase_context *kctx)
@@ -109,6 +237,7 @@ void kbase_context_common_term(struct kbase_context *kctx)
 	WARN_ON(atomic_read(&kctx->nonmapped_pages) != 0);
 
 	mutex_lock(&kctx->kbdev->kctx_list_lock);
+	kbase_remove_kctx_from_process(kctx);
 
 	KBASE_TLSTREAM_TL_KBASE_DEL_CTX(kctx->kbdev, kctx->id);
 
diff --git a/drivers/gpu/arm/bifrost/csf/Kbuild b/drivers/gpu/arm/bifrost/csf/Kbuild
new file mode 100644
index 000000000000..bb61811e6c85
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/Kbuild
@@ -0,0 +1,40 @@
+#
+# (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, you can access it online at
+# http://www.gnu.org/licenses/gpl-2.0.html.
+#
+# SPDX-License-Identifier: GPL-2.0
+#
+#
+
+mali_kbase-y += \
+	csf/mali_kbase_csf_firmware_cfg.o \
+	csf/mali_kbase_csf_trace_buffer.o \
+	csf/mali_kbase_csf.o \
+	csf/mali_kbase_csf_scheduler.o \
+	csf/mali_kbase_csf_kcpu.o \
+	csf/mali_kbase_csf_tiler_heap.o \
+	csf/mali_kbase_csf_timeout.o \
+	csf/mali_kbase_csf_tl_reader.o \
+	csf/mali_kbase_csf_heap_context_alloc.o \
+	csf/mali_kbase_csf_reset_gpu.o \
+	csf/mali_kbase_csf_csg_debugfs.o \
+	csf/mali_kbase_csf_kcpu_debugfs.o \
+	csf/mali_kbase_csf_protected_memory.o \
+	csf/mali_kbase_csf_tiler_heap_debugfs.o
+
+mali_kbase-$(CONFIG_MALI_REAL_HW) += csf/mali_kbase_csf_firmware.o
+
+mali_kbase-$(CONFIG_MALI_BIFROST_NO_MALI) += csf/mali_kbase_csf_firmware_no_mali.o
diff --git a/drivers/gpu/arm/bifrost/csf/mali_base_csf_kernel.h b/drivers/gpu/arm/bifrost/csf/mali_base_csf_kernel.h
new file mode 100644
index 000000000000..301146cbedd3
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_base_csf_kernel.h
@@ -0,0 +1,598 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _BASE_CSF_KERNEL_H_
+#define _BASE_CSF_KERNEL_H_
+
+/* Memory allocation, access/hint flags.
+ *
+ * See base_mem_alloc_flags.
+ */
+
+/* IN */
+/* Read access CPU side
+ */
+#define BASE_MEM_PROT_CPU_RD ((base_mem_alloc_flags)1 << 0)
+
+/* Write access CPU side
+ */
+#define BASE_MEM_PROT_CPU_WR ((base_mem_alloc_flags)1 << 1)
+
+/* Read access GPU side
+ */
+#define BASE_MEM_PROT_GPU_RD ((base_mem_alloc_flags)1 << 2)
+
+/* Write access GPU side
+ */
+#define BASE_MEM_PROT_GPU_WR ((base_mem_alloc_flags)1 << 3)
+
+/* Execute allowed on the GPU side
+ */
+#define BASE_MEM_PROT_GPU_EX ((base_mem_alloc_flags)1 << 4)
+
+/* Will be permanently mapped in kernel space.
+ * Flag is only allowed on allocations originating from kbase.
+ */
+#define BASEP_MEM_PERMANENT_KERNEL_MAPPING ((base_mem_alloc_flags)1 << 5)
+
+/* The allocation will completely reside within the same 4GB chunk in the GPU
+ * virtual space.
+ * Since this flag is primarily required only for the TLS memory which will
+ * not be used to contain executable code and also not used for Tiler heap,
+ * it can't be used along with BASE_MEM_PROT_GPU_EX and TILER_ALIGN_TOP flags.
+ */
+#define BASE_MEM_GPU_VA_SAME_4GB_PAGE ((base_mem_alloc_flags)1 << 6)
+
+/* Userspace is not allowed to free this memory.
+ * Flag is only allowed on allocations originating from kbase.
+ */
+#define BASEP_MEM_NO_USER_FREE ((base_mem_alloc_flags)1 << 7)
+
+#define BASE_MEM_RESERVED_BIT_8 ((base_mem_alloc_flags)1 << 8)
+
+/* Grow backing store on GPU Page Fault
+ */
+#define BASE_MEM_GROW_ON_GPF ((base_mem_alloc_flags)1 << 9)
+
+/* Page coherence Outer shareable, if available
+ */
+#define BASE_MEM_COHERENT_SYSTEM ((base_mem_alloc_flags)1 << 10)
+
+/* Page coherence Inner shareable
+ */
+#define BASE_MEM_COHERENT_LOCAL ((base_mem_alloc_flags)1 << 11)
+
+/* IN/OUT */
+/* Should be cached on the CPU, returned if actually cached
+ */
+#define BASE_MEM_CACHED_CPU ((base_mem_alloc_flags)1 << 12)
+
+/* IN/OUT */
+/* Must have same VA on both the GPU and the CPU
+ */
+#define BASE_MEM_SAME_VA ((base_mem_alloc_flags)1 << 13)
+
+/* OUT */
+/* Must call mmap to acquire a GPU address for the alloc
+ */
+#define BASE_MEM_NEED_MMAP ((base_mem_alloc_flags)1 << 14)
+
+/* IN */
+/* Page coherence Outer shareable, required.
+ */
+#define BASE_MEM_COHERENT_SYSTEM_REQUIRED ((base_mem_alloc_flags)1 << 15)
+
+/* Protected memory
+ */
+#define BASE_MEM_PROTECTED ((base_mem_alloc_flags)1 << 16)
+
+/* Not needed physical memory
+ */
+#define BASE_MEM_DONT_NEED ((base_mem_alloc_flags)1 << 17)
+
+/* Must use shared CPU/GPU zone (SAME_VA zone) but doesn't require the
+ * addresses to be the same
+ */
+#define BASE_MEM_IMPORT_SHARED ((base_mem_alloc_flags)1 << 18)
+
+/* CSF event memory
+ *
+ * If Outer shareable coherence is not specified or not available, then on
+ * allocation kbase will automatically use the uncached GPU mapping.
+ * There is no need for the client to specify BASE_MEM_UNCACHED_GPU
+ * themselves when allocating memory with the BASE_MEM_CSF_EVENT flag.
+ *
+ * This memory requires a permanent mapping
+ *
+ * See also kbase_reg_needs_kernel_mapping()
+ */
+#define BASE_MEM_CSF_EVENT ((base_mem_alloc_flags)1 << 19)
+
+#define BASE_MEM_RESERVED_BIT_20 ((base_mem_alloc_flags)1 << 20)
+
+/* Should be uncached on the GPU, will work only for GPUs using AARCH64 mmu
+ * mode. Some components within the GPU might only be able to access memory
+ * that is GPU cacheable. Refer to the specific GPU implementation for more
+ * details. The 3 shareability flags will be ignored for GPU uncached memory.
+ * If used while importing USER_BUFFER type memory, then the import will fail
+ * if the memory is not aligned to GPU and CPU cache line width.
+ */
+#define BASE_MEM_UNCACHED_GPU ((base_mem_alloc_flags)1 << 21)
+
+/*
+ * Bits [22:25] for group_id (0~15).
+ *
+ * base_mem_group_id_set() should be used to pack a memory group ID into a
+ * base_mem_alloc_flags value instead of accessing the bits directly.
+ * base_mem_group_id_get() should be used to extract the memory group ID from
+ * a base_mem_alloc_flags value.
+ */
+#define BASEP_MEM_GROUP_ID_SHIFT 22
+#define BASE_MEM_GROUP_ID_MASK \
+	((base_mem_alloc_flags)0xF << BASEP_MEM_GROUP_ID_SHIFT)
+
+/* Must do CPU cache maintenance when imported memory is mapped/unmapped
+ * on GPU. Currently applicable to dma-buf type only.
+ */
+#define BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP ((base_mem_alloc_flags)1 << 26)
+
+/* OUT */
+/* Kernel side cache sync ops required */
+#define BASE_MEM_KERNEL_SYNC ((base_mem_alloc_flags)1 << 28)
+
+/* Number of bits used as flags for base memory management
+ *
+ * Must be kept in sync with the base_mem_alloc_flags flags
+ */
+#define BASE_MEM_FLAGS_NR_BITS 29
+
+/* A mask of all the flags which are only valid for allocations within kbase,
+ * and may not be passed from user space.
+ */
+#define BASEP_MEM_FLAGS_KERNEL_ONLY \
+	(BASEP_MEM_PERMANENT_KERNEL_MAPPING | BASEP_MEM_NO_USER_FREE)
+
+/* A mask for all output bits, excluding IN/OUT bits.
+ */
+#define BASE_MEM_FLAGS_OUTPUT_MASK BASE_MEM_NEED_MMAP
+
+/* A mask for all input bits, including IN/OUT bits.
+ */
+#define BASE_MEM_FLAGS_INPUT_MASK \
+	(((1 << BASE_MEM_FLAGS_NR_BITS) - 1) & ~BASE_MEM_FLAGS_OUTPUT_MASK)
+
+/* A mask of all currently reserved flags
+ */
+#define BASE_MEM_FLAGS_RESERVED \
+	BASE_MEM_RESERVED_BIT_8 | BASE_MEM_RESERVED_BIT_20
+
+#define BASEP_MEM_INVALID_HANDLE               (0ull  << 12)
+#define BASE_MEM_MMU_DUMP_HANDLE               (1ull  << 12)
+#define BASE_MEM_TRACE_BUFFER_HANDLE           (2ull  << 12)
+#define BASE_MEM_MAP_TRACKING_HANDLE           (3ull  << 12)
+#define BASEP_MEM_WRITE_ALLOC_PAGES_HANDLE     (4ull  << 12)
+/* reserved handles ..-47<<PAGE_SHIFT> for future special handles */
+#define BASEP_MEM_CSF_USER_REG_PAGE_HANDLE     (47ul  << 12)
+#define BASEP_MEM_CSF_USER_IO_PAGES_HANDLE     (48ul  << 12)
+#define BASE_MEM_COOKIE_BASE                   (64ul  << 12)
+#define BASE_MEM_FIRST_FREE_ADDRESS            ((BITS_PER_LONG << 12) + \
+						BASE_MEM_COOKIE_BASE)
+
+#define KBASE_CSF_NUM_USER_IO_PAGES_HANDLE \
+	((BASE_MEM_COOKIE_BASE - BASEP_MEM_CSF_USER_IO_PAGES_HANDLE) >> \
+	 LOCAL_PAGE_SHIFT)
+
+/**
+ * Valid set of just-in-time memory allocation flags
+ */
+#define BASE_JIT_ALLOC_VALID_FLAGS ((u8)0)
+
+/* Flags to pass to ::base_context_init.
+ * Flags can be ORed together to enable multiple things.
+ *
+ * These share the same space as BASEP_CONTEXT_FLAG_*, and so must
+ * not collide with them.
+ */
+typedef u32 base_context_create_flags;
+
+/* No flags set */
+#define BASE_CONTEXT_CREATE_FLAG_NONE ((base_context_create_flags)0)
+
+/* Base context is embedded in a cctx object (flag used for CINSTR
+ * software counter macros)
+ */
+#define BASE_CONTEXT_CCTX_EMBEDDED ((base_context_create_flags)1 << 0)
+
+/* Base context is a 'System Monitor' context for Hardware counters.
+ *
+ * One important side effect of this is that job submission is disabled.
+ */
+#define BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED \
+	((base_context_create_flags)1 << 1)
+
+/* Create CSF event thread.
+ *
+ * The creation of a CSF event thread is conditional and only allowed in
+ * unit tests for the moment, in order to avoid clashes with the existing
+ * Base unit tests.
+ */
+#define BASE_CONTEXT_CSF_EVENT_THREAD ((base_context_create_flags)1 << 2)
+
+/* Bit-shift used to encode a memory group ID in base_context_create_flags
+ */
+#define BASEP_CONTEXT_MMU_GROUP_ID_SHIFT (3)
+
+/* Bitmask used to encode a memory group ID in base_context_create_flags
+ */
+#define BASEP_CONTEXT_MMU_GROUP_ID_MASK \
+	((base_context_create_flags)0xF << BASEP_CONTEXT_MMU_GROUP_ID_SHIFT)
+
+/* Bitpattern describing the base_context_create_flags that can be
+ * passed to the kernel
+ */
+#define BASEP_CONTEXT_CREATE_KERNEL_FLAGS \
+	(BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED | \
+	 BASEP_CONTEXT_MMU_GROUP_ID_MASK)
+
+/* Bitpattern describing the ::base_context_create_flags that can be
+ * passed to base_context_init()
+ */
+#define BASEP_CONTEXT_CREATE_ALLOWED_FLAGS \
+	(BASE_CONTEXT_CCTX_EMBEDDED | \
+	 BASE_CONTEXT_CSF_EVENT_THREAD | \
+	 BASEP_CONTEXT_CREATE_KERNEL_FLAGS)
+
+/* Enable additional tracepoints for latency measurements (TL_ATOM_READY,
+ * TL_ATOM_DONE, TL_ATOM_PRIO_CHANGE, TL_ATOM_EVENT_POST)
+ */
+#define BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS (1 << 0)
+
+/* Indicate that job dumping is enabled. This could affect certain timers
+ * to account for the performance impact.
+ */
+#define BASE_TLSTREAM_JOB_DUMPING_ENABLED (1 << 1)
+
+/* Enable KBase tracepoints for CSF builds */
+#define BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS (1 << 2)
+
+/* Enable additional CSF Firmware side tracepoints */
+#define BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS (1 << 3)
+
+#define BASE_TLSTREAM_FLAGS_MASK (BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS | \
+		BASE_TLSTREAM_JOB_DUMPING_ENABLED | \
+		BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS | \
+		BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)
+
+/* Number of pages mapped into the process address space for a bound GPU
+ * command queue. A pair of input/output pages and a Hw doorbell page
+ * are mapped to enable direct submission of commands to Hw.
+ */
+#define BASEP_QUEUE_NR_MMAP_USER_PAGES ((size_t)3)
+
+#define BASE_QUEUE_MAX_PRIORITY (15U)
+
+/* CQS Sync object is an array of u32 event_mem[2], error field index is 1 */
+#define BASEP_EVENT_VAL_INDEX (0U)
+#define BASEP_EVENT_ERR_INDEX (1U)
+
+/* The upper limit for number of objects that could be waited/set per command.
+ * This limit is now enforced as internally the error inherit inputs are
+ * converted to 32-bit flags in a u32 variable occupying a previously padding
+ * field.
+ */
+#define BASEP_KCPU_CQS_MAX_NUM_OBJS ((size_t)32)
+
+/**
+ * enum base_kcpu_command_type - Kernel CPU queue command type.
+ */
+enum base_kcpu_command_type {
+	BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL,
+	BASE_KCPU_COMMAND_TYPE_FENCE_WAIT,
+	BASE_KCPU_COMMAND_TYPE_CQS_WAIT,
+	BASE_KCPU_COMMAND_TYPE_CQS_SET,
+	BASE_KCPU_COMMAND_TYPE_MAP_IMPORT,
+	BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT,
+	BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE,
+	BASE_KCPU_COMMAND_TYPE_JIT_ALLOC,
+	BASE_KCPU_COMMAND_TYPE_JIT_FREE,
+	BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND,
+	BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER,
+};
+
+/**
+ * enum base_queue_group_priority - Priority of a GPU Command Queue Group.
+ * @BASE_QUEUE_GROUP_PRIORITY_HIGH:   GPU Command Queue Group is of high
+ *                                    priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_MEDIUM: GPU Command Queue Group is of medium
+ *                                    priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_LOW:    GPU Command Queue Group is of low
+ *                                    priority.
+ * @BASE_QUEUE_GROUP_PRIORITY_COUNT:  Number of GPU Command Queue Group
+ *                                    priority levels.
+ *
+ * Currently this is in order of highest to lowest, but if new levels are added
+ * then those new levels may be out of order to preserve the ABI compatibility
+ * with previous releases. At that point, ensure assignment to
+ * the 'priority' member in &kbase_queue_group is updated to ensure it remains
+ * a linear ordering.
+ *
+ * There should be no gaps in the enum, otherwise use of
+ * BASE_QUEUE_GROUP_PRIORITY_COUNT in kbase must be updated.
+ */
+enum base_queue_group_priority {
+	BASE_QUEUE_GROUP_PRIORITY_HIGH = 0,
+	BASE_QUEUE_GROUP_PRIORITY_MEDIUM,
+	BASE_QUEUE_GROUP_PRIORITY_LOW,
+	BASE_QUEUE_GROUP_PRIORITY_COUNT
+};
+
+struct base_kcpu_command_fence_info {
+	u64 fence;
+};
+
+struct base_cqs_wait {
+	u64 addr;
+	u32 val;
+	u32 padding;
+};
+
+struct base_kcpu_command_cqs_wait_info {
+	u64 objs;
+	u32 nr_objs;
+	u32 inherit_err_flags;
+};
+
+struct base_cqs_set {
+	u64 addr;
+};
+
+struct base_kcpu_command_cqs_set_info {
+	u64 objs;
+	u32 nr_objs;
+	u32 propagate_flags;
+};
+
+/**
+ * struct base_kcpu_command_import_info - structure which contains information
+ *		about the imported buffer.
+ *
+ * @handle:	Address of imported user buffer.
+ */
+struct base_kcpu_command_import_info {
+	u64 handle;
+};
+
+/**
+ * struct base_kcpu_command_jit_alloc_info - structure which contains
+ *		information about jit memory allocation.
+ *
+ * @info:	An array of elements of the
+ *		struct base_jit_alloc_info type.
+ * @count:	The number of elements in the info array.
+ * @padding:	Padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_jit_alloc_info {
+	u64 info;
+	u8 count;
+	u8 padding[7];
+};
+
+/**
+ * struct base_kcpu_command_jit_free_info - structure which contains
+ *		information about jit memory which is to be freed.
+ *
+ * @ids:	An array containing the JIT IDs to free.
+ * @count:	The number of elements in the ids array.
+ * @padding:	Padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_jit_free_info {
+	u64 ids;
+	u8 count;
+	u8 padding[7];
+};
+
+/**
+ * struct base_kcpu_command_group_suspend_info - structure which contains
+ *		suspend buffer data captured for a suspended queue group.
+ *
+ * @buffer:		Pointer to an array of elements of the type char.
+ * @size:		Number of elements in the @buffer array.
+ * @group_handle:	Handle to the mapping of command stream group.
+ * @padding:		padding to a multiple of 64 bits.
+ */
+struct base_kcpu_command_group_suspend_info {
+	u64 buffer;
+	u32 size;
+	u8 group_handle;
+	u8 padding[3];
+};
+
+/**
+ * struct base_kcpu_command - kcpu command.
+ *
+ * @type:	type of the kcpu command, one enum base_kcpu_command_type
+ * @info:	structure which contains information about the kcpu command;
+ *		actual type is determined by @p type
+ * @padding:	padding to a multiple of 64 bits
+ */
+struct base_kcpu_command {
+	u8 type;
+	u8 padding[sizeof(u64) - sizeof(u8)];
+	union {
+		struct base_kcpu_command_fence_info fence;
+		struct base_kcpu_command_cqs_wait_info cqs_wait;
+		struct base_kcpu_command_cqs_set_info cqs_set;
+		struct base_kcpu_command_import_info import;
+		struct base_kcpu_command_jit_alloc_info jit_alloc;
+		struct base_kcpu_command_jit_free_info jit_free;
+		struct base_kcpu_command_group_suspend_info suspend_buf_copy;
+		u64 padding[2]; /* No sub-struct should be larger */
+	} info;
+};
+
+/**
+ * struct basep_cs_stream_control - Command Stream interface capabilities.
+ *
+ * @features: Features of this stream
+ * @padding:  Padding to a multiple of 64 bits.
+ */
+struct basep_cs_stream_control {
+	u32 features;
+	u32 padding;
+};
+
+/**
+ * struct basep_cs_group_control - Command Stream Group interface capabilities.
+ *
+ * @features:     Features of this group
+ * @stream_num:   Number of streams in this group
+ * @suspend_size: Size in bytes of the suspend buffer for this group
+ * @padding:      Padding to a multiple of 64 bits.
+ */
+struct basep_cs_group_control {
+	u32 features;
+	u32 stream_num;
+	u32 suspend_size;
+	u32 padding;
+};
+
+/**
+ * struct base_gpu_queue_group_error_fatal_payload - Unrecoverable fault
+ *        error information associated with GPU command queue group.
+ *
+ * @sideband:     Additional information of the unrecoverable fault.
+ * @status:       Unrecoverable fault information.
+ *                This consists of exception type (least significant byte) and
+ *                data (remaining bytes). One example of exception type is
+ *                CS_INVALID_INSTRUCTION (0x49).
+ * @padding:      Padding to make multiple of 64bits
+ */
+struct base_gpu_queue_group_error_fatal_payload {
+	u64 sideband;
+	u32 status;
+	u32 padding;
+};
+
+/**
+ * struct base_gpu_queue_error_fatal_payload - Unrecoverable fault
+ *        error information related to GPU command queue.
+ *
+ * @sideband:     Additional information about this unrecoverable fault.
+ * @status:       Unrecoverable fault information.
+ *                This consists of exception type (least significant byte) and
+ *                data (remaining bytes). One example of exception type is
+ *                CS_INVALID_INSTRUCTION (0x49).
+ * @csi_index:    Index of the CSF interface the queue is bound to.
+ * @padding:      Padding to make multiple of 64bits
+ */
+struct base_gpu_queue_error_fatal_payload {
+	u64 sideband;
+	u32 status;
+	u8 csi_index;
+	u8 padding[3];
+};
+
+/**
+ * enum base_gpu_queue_group_error_type - GPU Fatal error type.
+ *
+ * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL:       Fatal error associated with GPU
+ *                                          command queue group.
+ * @BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL: Fatal error associated with GPU
+ *                                          command queue.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT:     Fatal error associated with
+ *                                          progress timeout.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM: Fatal error due to running out
+ *                                             of tiler heap memory.
+ * @BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT: The number of fatal error types
+ *
+ * This type is used for &struct_base_gpu_queue_group_error.error_type.
+ */
+enum base_gpu_queue_group_error_type {
+	BASE_GPU_QUEUE_GROUP_ERROR_FATAL = 0,
+	BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL,
+	BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT,
+	BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM,
+	BASE_GPU_QUEUE_GROUP_ERROR_FATAL_COUNT
+};
+
+/**
+ * struct base_gpu_queue_group_error - Unrecoverable fault information
+ *
+ * @error_type:   Error type of @base_gpu_queue_group_error_type
+ *                indicating which field in union payload is filled
+ * @padding:      Unused bytes for 64bit boundary
+ * @fatal_group:  Unrecoverable fault error associated with
+ *                GPU command queue group
+ * @fatal_queue:  Unrecoverable fault error associated with command queue
+ *
+ * @payload:      Input Payload
+ */
+struct base_gpu_queue_group_error {
+	u8 error_type;
+	u8 padding[7];
+	union {
+		struct base_gpu_queue_group_error_fatal_payload fatal_group;
+		struct base_gpu_queue_error_fatal_payload fatal_queue;
+	} payload;
+};
+
+/**
+ * enum base_csf_notification_type - Notification type
+ *
+ * @BASE_CSF_NOTIFICATION_EVENT:                 Notification with kernel event
+ * @BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR: Notification with GPU fatal
+ *                                               error
+ * @BASE_CSF_NOTIFICATION_COUNT:                 The number of notification type
+ *
+ * This type is used for &struct_base_csf_notification.type.
+ */
+enum base_csf_notification_type {
+	BASE_CSF_NOTIFICATION_EVENT = 0,
+	BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+	BASE_CSF_NOTIFICATION_COUNT
+};
+
+/**
+ * struct base_csf_notification - Event or error notification
+ *
+ * @type:         Notification type of @base_csf_notification_type
+ * @padding:      Padding for 64bit boundary
+ * @handle:       Handle of GPU command queue group associated with fatal error
+ * @error:        Unrecoverable fault error
+ * @align:        To fit the struct into a 64-byte cache line
+ *
+ * @payload:      Input Payload
+ */
+struct base_csf_notification {
+	u8 type;
+	u8 padding[7];
+	union {
+		struct {
+			u8 handle;
+			u8 padding[7];
+			struct base_gpu_queue_group_error error;
+		} csg_error;
+		u8 align[56];
+	} payload;
+};
+
+#endif /* _BASE_CSF_KERNEL_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_gpu_csf_control_registers.h b/drivers/gpu/arm/bifrost/csf/mali_gpu_csf_control_registers.h
new file mode 100644
index 000000000000..4fff80ca4023
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_gpu_csf_control_registers.h
@@ -0,0 +1,33 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * This header was autogenerated, it should not be edited.
+ */
+
+#ifndef _GPU_CSF_CONTROL_REGISTERS_H_
+#define _GPU_CSF_CONTROL_REGISTERS_H_
+
+/* GPU_REGISTERS register offsets */
+#define GPU_CONTROL_MCU 0x3000 /* () MCU control registers */
+
+#endif /* _GPU_CSF_CONTROL_REGISTERS_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_gpu_csf_registers.h b/drivers/gpu/arm/bifrost/csf/mali_gpu_csf_registers.h
new file mode 100644
index 000000000000..5c03445f3c79
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_gpu_csf_registers.h
@@ -0,0 +1,1252 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * This header was autogenerated, it should not be edited.
+ */
+
+#ifndef _GPU_CSF_REGISTERS_H_
+#define _GPU_CSF_REGISTERS_H_
+
+/*
+ * Begin register sets
+ */
+
+/* DOORBELLS base address */
+#define DOORBELLS_BASE 0x0080000
+#define DOORBELLS_REG(r) (DOORBELLS_BASE + (r))
+
+/* CS_KERNEL_INPUT_BLOCK base address */
+#define CS_KERNEL_INPUT_BLOCK_BASE 0x0000
+#define CS_KERNEL_INPUT_BLOCK_REG(r) (CS_KERNEL_INPUT_BLOCK_BASE + (r))
+
+/* CS_KERNEL_OUTPUT_BLOCK base address */
+#define CS_KERNEL_OUTPUT_BLOCK_BASE 0x0000
+#define CS_KERNEL_OUTPUT_BLOCK_REG(r) (CS_KERNEL_OUTPUT_BLOCK_BASE + (r))
+
+/* CS_USER_INPUT_BLOCK base address */
+#define CS_USER_INPUT_BLOCK_BASE 0x0000
+#define CS_USER_INPUT_BLOCK_REG(r) (CS_USER_INPUT_BLOCK_BASE + (r))
+
+/* CS_USER_OUTPUT_BLOCK base address */
+#define CS_USER_OUTPUT_BLOCK_BASE 0x0000
+#define CS_USER_OUTPUT_BLOCK_REG(r) (CS_USER_OUTPUT_BLOCK_BASE + (r))
+
+/* CSG_INPUT_BLOCK base address */
+#define CSG_INPUT_BLOCK_BASE 0x0000
+#define CSG_INPUT_BLOCK_REG(r) (CSG_INPUT_BLOCK_BASE + (r))
+
+/* CSG_OUTPUT_BLOCK base address */
+#define CSG_OUTPUT_BLOCK_BASE 0x0000
+#define CSG_OUTPUT_BLOCK_REG(r) (CSG_OUTPUT_BLOCK_BASE + (r))
+
+/* GLB_CONTROL_BLOCK base address */
+#define GLB_CONTROL_BLOCK_BASE 0x04000000
+#define GLB_CONTROL_BLOCK_REG(r) (GLB_CONTROL_BLOCK_BASE + (r))
+
+/* GLB_INPUT_BLOCK base address */
+#define GLB_INPUT_BLOCK_BASE 0x0000
+#define GLB_INPUT_BLOCK_REG(r) (GLB_INPUT_BLOCK_BASE + (r))
+
+/* GLB_OUTPUT_BLOCK base address */
+#define GLB_OUTPUT_BLOCK_BASE 0x0000
+#define GLB_OUTPUT_BLOCK_REG(r) (GLB_OUTPUT_BLOCK_BASE + (r))
+
+/* USER base address */
+#define USER_BASE 0x0010000
+#define USER_REG(r) (USER_BASE + (r))
+
+/* End register sets */
+
+/*
+ * Begin register offsets
+ */
+
+/* DOORBELLS register offsets */
+#define DOORBELL_0 0x0000 /* () Doorbell 0 register */
+#define DOORBELL(n) (DOORBELL_0 + (n)*65536)
+#define DOORBELL_REG(n, r) (DOORBELL(n) + DOORBELL_BLOCK_REG(r))
+#define DOORBELL_COUNT 1024
+
+/* DOORBELL_BLOCK register offsets */
+#define DB_BLK_DOORBELL 0x0000 /* (WO) Doorbell request */
+
+/* CS_KERNEL_INPUT_BLOCK register offsets */
+#define CS_REQ 0x0000 /* () Command stream request flags */
+#define CS_CONFIG 0x0004 /* () Command stream configuration */
+#define CS_ACK_IRQ_MASK 0x000C /* () Command steam interrupt mask */
+#define CS_BASE_LO 0x0010 /* () Base pointer for the ring buffer, low word */
+#define CS_BASE_HI 0x0014 /* () Base pointer for the ring buffer, high word */
+#define CS_SIZE 0x0018 /* () Size of the ring buffer */
+#define CS_TILER_HEAP_START_LO 0x0020 /* () Pointer to heap start, low word */
+#define CS_TILER_HEAP_START_HI 0x0024 /* () Pointer to heap start, high word */
+#define CS_TILER_HEAP_END_LO 0x0028 /* () Tiler heap descriptor address, low word */
+#define CS_TILER_HEAP_END_HI 0x002C /* () Tiler heap descriptor address, high word */
+#define CS_USER_INPUT_LO 0x0030 /* () CS user mode input page address, low word */
+#define CS_USER_INPUT_HI 0x0034 /* () CS user mode input page address, high word */
+#define CS_USER_OUTPUT_LO 0x0038 /* () CS user mode input page address, low word */
+#define CS_USER_OUTPUT_HI 0x003C /* () CS user mode input page address, high word */
+
+/* CS_KERNEL_OUTPUT_BLOCK register offsets */
+#define CS_ACK 0x0000 /* () Command stream acknowledge flags */
+#define CS_STATUS_CMD_PTR_LO 0x0040 /* () Program pointer current value, low word */
+#define CS_STATUS_CMD_PTR_HI 0x0044 /* () Program pointer current value, high word */
+#define CS_STATUS_WAIT 0x0048 /* () Wait condition status register */
+#define CS_STATUS_REQ_RESOURCE 0x004C /* () Indicates the resources requested by the command stream */
+#define CS_STATUS_WAIT_SYNC_POINTER_LO 0x0050 /* () Sync object pointer, low word */
+#define CS_STATUS_WAIT_SYNC_POINTER_HI 0x0054 /* () Sync object pointer, high word */
+#define CS_STATUS_WAIT_SYNC_VALUE 0x0058 /* () Sync object test value */
+#define CS_FAULT 0x0080 /* () Recoverable fault information */
+#define CS_FATAL 0x0084 /* () Unrecoverable fault information */
+#define CS_FAULT_INFO_LO 0x0088 /* () Additional information about a recoverable fault, low word */
+#define CS_FAULT_INFO_HI 0x008C /* () Additional information about a recoverable fault, high word */
+#define CS_FATAL_INFO_LO 0x0090 /* () Additional information about a non-recoverable fault, low word */
+#define CS_FATAL_INFO_HI 0x0094 /* () Additional information about a non-recoverable fault, high word */
+#define CS_HEAP_VT_START 0x00C0 /* () Number of vertex/tiling operations started */
+#define CS_HEAP_VT_END 0x00C4 /* () Number of vertex/tiling operations completed */
+#define CS_HEAP_FRAG_END 0x00CC /* () Number of fragment completed */
+#define CS_HEAP_ADDRESS_LO 0x00D0 /* () Heap address, low word */
+#define CS_HEAP_ADDRESS_HI 0x00D4 /* () Heap address, high word */
+
+/* CS_USER_INPUT_BLOCK register offsets */
+#define CS_INSERT_LO 0x0000 /* () Current insert offset for ring buffer, low word */
+#define CS_INSERT_HI 0x0004 /* () Current insert offset for ring buffer, high word */
+#define CS_EXTRACT_INIT_LO 0x0008 /* () Initial extract offset for ring buffer, low word */
+#define CS_EXTRACT_INIT_HI 0x000C /* () Initial extract offset for ring buffer, high word */
+
+/* CS_USER_OUTPUT_BLOCK register offsets */
+#define CS_EXTRACT_LO 0x0000 /* () Current extract offset for ring buffer, low word */
+#define CS_EXTRACT_HI 0x0004 /* () Current extract offset for ring buffer, high word */
+#define CS_ACTIVE 0x0008 /* () Initial extract offset when the command stream is started */
+
+/* CSG_INPUT_BLOCK register offsets */
+#define CSG_REQ 0x0000 /* () CSG request */
+#define CSG_ACK_IRQ_MASK 0x0004 /* () Global acknowledge interrupt mask */
+#define CSG_DB_REQ 0x0008 /* () Global doorbell request */
+#define CSG_IRQ_ACK 0x000C /* () Command stream IRQ acknowledge */
+#define CSG_ALLOW_COMPUTE_LO 0x0020 /* () Allowed compute endpoints, low word */
+#define CSG_ALLOW_COMPUTE_HI 0x0024 /* () Allowed compute endpoints, high word */
+#define CSG_ALLOW_FRAGMENT_LO 0x0028 /* () Allowed fragment endpoints, low word */
+#define CSG_ALLOW_FRAGMENT_HI 0x002C /* () Allowed fragment endpoints, high word */
+#define CSG_ALLOW_OTHER 0x0030 /* () Allowed other endpoints */
+#define CSG_EP_REQ 0x0034 /* () Maximum number of endpoints allowed */
+#define CSG_SUSPEND_BUF_LO 0x0040 /* () Normal mode suspend buffer, low word */
+#define CSG_SUSPEND_BUF_HI 0x0044 /* () Normal mode suspend buffer, high word */
+#define CSG_PROTM_SUSPEND_BUF_LO 0x0048 /* () Protected mode suspend buffer, low word */
+#define CSG_PROTM_SUSPEND_BUF_HI 0x004C /* () Protected mode suspend buffer, high word */
+#define CSG_CONFIG 0x0050 /* () CSG configuration options */
+
+/* CSG_OUTPUT_BLOCK register offsets */
+#define CSG_ACK 0x0000 /* () Command stream group acknowledge flags */
+#define CSG_DB_ACK 0x0008 /* () Command stream kernel doorbell acknowledge flags */
+#define CSG_IRQ_REQ 0x000C /* () Command stream interrupt request flags */
+#define CSG_STATUS_EP_CURRENT 0x0010 /* () Endpoint allocation status register */
+#define CSG_STATUS_EP_REQ 0x0014 /* () Endpoint request status register */
+#define CSG_RESOURCE_DEP 0x001C /* () Current resource dependencies */
+
+/* GLB_CONTROL_BLOCK register offsets */
+#define GLB_VERSION 0x0000 /* () Global interface version */
+#define GLB_FEATURES 0x0004 /* () Global interface features */
+#define GLB_INPUT_VA 0x0008 /* () Address of GLB_INPUT_BLOCK */
+#define GLB_OUTPUT_VA 0x000C /* () Address of GLB_OUTPUT_BLOCK */
+#define GLB_GROUP_NUM 0x0010 /* () Number of CSG interfaces */
+#define GLB_GROUP_STRIDE 0x0014 /* () Stride between CSG interfaces */
+#define GLB_PRFCNT_SIZE 0x0018 /* () Size of CSF performance counters */
+#define GROUP_CONTROL_0 0x1000 /* () CSG control and capabilities */
+#define GROUP_CONTROL(n) (GROUP_CONTROL_0 + (n)*256)
+#define GROUP_CONTROL_REG(n, r) (GROUP_CONTROL(n) + GROUP_CONTROL_BLOCK_REG(r))
+#define GROUP_CONTROL_COUNT 16
+
+/* STREAM_CONTROL_BLOCK register offsets */
+#define STREAM_FEATURES 0x0000 /* () Command Stream interface features */
+#define STREAM_INPUT_VA 0x0004 /* () Address of CS_KERNEL_INPUT_BLOCK */
+#define STREAM_OUTPUT_VA 0x0008 /* () Address of CS_KERNEL_OUTPUT_BLOCK */
+
+/* GROUP_CONTROL_BLOCK register offsets */
+#define GROUP_FEATURES 0x0000 /* () Command Stream Group interface features */
+#define GROUP_INPUT_VA 0x0004 /* () Address of CSG_INPUT_BLOCK */
+#define GROUP_OUTPUT_VA 0x0008 /* () Address of CSG_OUTPUT_BLOCK */
+#define GROUP_SUSPEND_SIZE 0x000C /* () Size of CSG suspend buffer */
+#define GROUP_PROTM_SUSPEND_SIZE 0x0010 /* () Size of CSG protected-mode suspend buffer */
+#define GROUP_STREAM_NUM 0x0014 /* () Number of CS interfaces */
+#define GROUP_STREAM_STRIDE 0x0018 /* () Stride between CS interfaces  */
+#define STREAM_CONTROL_0 0x0040 /* () CS control and capabilities */
+#define STREAM_CONTROL(n) (STREAM_CONTROL_0 + (n)*12)
+#define STREAM_CONTROL_REG(n, r) (STREAM_CONTROL(n) + STREAM_CONTROL_BLOCK_REG(r))
+#define STREAM_CONTROL_COUNT 16
+
+/* GLB_INPUT_BLOCK register offsets */
+#define GLB_REQ 0x0000 /* () Global request */
+#define GLB_ACK_IRQ_MASK 0x0004 /* () Global acknowledge interrupt mask */
+#define GLB_DB_REQ 0x0008 /* () Global doorbell request */
+#define GLB_PROGRESS_TIMER 0x0010 /* () Global progress timeout */
+#define GLB_PWROFF_TIMER 0x0014 /* () Global shader core power off timer */
+#define GLB_ALLOC_EN_LO 0x0018 /* () Global shader core allocation enable mask, low word */
+#define GLB_ALLOC_EN_HI 0x001C /* () Global shader core allocation enable mask, high word */
+#define GLB_PROTM_COHERENCY 0x0020 /* () Configure COHERENCY_ENABLE register value to use in protected mode execution */
+
+#define GLB_PRFCNT_JASID 0x0024 /* () Performance counter address space */
+#define GLB_PRFCNT_BASE_LO 0x0028 /* () Performance counter buffer address, low word */
+#define GLB_PRFCNT_BASE_HI 0x002C /* () Performance counter buffer address, high word */
+#define GLB_PRFCNT_CONFIG 0x0040 /* () Performance counter configuration */
+#define GLB_PRFCNT_CSG_SELECT 0x0044 /* () CSG performance counting enable */
+#define GLB_PRFCNT_FW_EN 0x0048 /* () Performance counter enable for firmware */
+#define GLB_PRFCNT_CSG_EN 0x004C /* () Performance counter enable for CSG */
+#define GLB_PRFCNT_CSF_EN 0x0050 /* () Performance counter enable for CSF */
+#define GLB_PRFCNT_SHADER_EN 0x0054 /* () Performance counter enable for shader cores */
+#define GLB_PRFCNT_TILER_EN 0x0058 /* () Performance counter enable for tiler */
+#define GLB_PRFCNT_MMU_L2_EN 0x005C /* () Performance counter enable for MMU/L2 cache */
+
+#define GLB_DEBUG_FWUTF_DESTROY 0x0FE0 /* () Test fixture destroy function address */
+#define GLB_DEBUG_FWUTF_TEST 0x0FE4 /* () Test index */
+#define GLB_DEBUG_FWUTF_FIXTURE 0x0FE8 /* () Test fixture index */
+#define GLB_DEBUG_FWUTF_CREATE 0x0FEC /* () Test fixture create function address */
+#define GLB_DEBUG_ACK_IRQ_MASK 0x0FF8 /* () Global debug acknowledge interrupt mask */
+#define GLB_DEBUG_REQ 0x0FFC /* () Global debug request */
+
+/* GLB_OUTPUT_BLOCK register offsets */
+#define GLB_ACK 0x0000 /* () Global acknowledge */
+#define GLB_DB_ACK 0x0008 /* () Global doorbell acknowledge */
+#define GLB_HALT_STATUS 0x0010 /* () Global halt status */
+#define GLB_PRFCNT_STATUS 0x0014 /* () Performance counter status */
+#define GLB_DEBUG_FWUTF_RESULT 0x0FE0 /* () Firmware debug test result */
+#define GLB_DEBUG_ACK 0x0FFC /* () Global debug acknowledge */
+
+/* End register offsets */
+
+/* CS_KERNEL_INPUT_BLOCK register set definitions */
+
+/* CS_REQ register */
+#define CS_REQ_STATE_SHIFT 0
+#define CS_REQ_STATE_MASK (0x7 << CS_REQ_STATE_SHIFT)
+#define CS_REQ_STATE_GET(reg_val) (((reg_val)&CS_REQ_STATE_MASK) >> CS_REQ_STATE_SHIFT)
+#define CS_REQ_STATE_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_STATE_MASK) | (((value) << CS_REQ_STATE_SHIFT) & CS_REQ_STATE_MASK))
+/* CS_REQ_STATE values */
+#define CS_REQ_STATE_STOP 0x0
+#define CS_REQ_STATE_START 0x1
+/* End of CS_REQ_STATE values */
+#define CS_REQ_EXTRACT_EVENT_SHIFT 4
+#define CS_REQ_EXTRACT_EVENT_MASK (0x1 << CS_REQ_EXTRACT_EVENT_SHIFT)
+#define CS_REQ_EXTRACT_EVENT_GET(reg_val) (((reg_val)&CS_REQ_EXTRACT_EVENT_MASK) >> CS_REQ_EXTRACT_EVENT_SHIFT)
+#define CS_REQ_EXTRACT_EVENT_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_EXTRACT_EVENT_MASK) | (((value) << CS_REQ_EXTRACT_EVENT_SHIFT) & CS_REQ_EXTRACT_EVENT_MASK))
+
+/* From 10.x.5, CS_REQ_ERROR_MODE is removed but TI2 bitfile upload not finished.
+ * Need to remove on GPUCORE-23972
+ */
+#define CS_REQ_ERROR_MODE_SHIFT 5
+#define CS_REQ_ERROR_MODE_MASK (0x1 << CS_REQ_ERROR_MODE_SHIFT)
+#define CS_REQ_ERROR_MODE_GET(reg_val) ((reg_val & CS_REQ_ERROR_MODE_MASK) >> CS_REQ_ERROR_MODE_SHIFT)
+#define CS_REQ_ERROR_MODE_SET(reg_val, value) \
+         ((reg_val & ~CS_REQ_ERROR_MODE_MASK) | ((value << CS_REQ_ERROR_MODE_SHIFT) & CS_REQ_ERROR_MODE_MASK))
+
+#define CS_REQ_IDLE_SYNC_WAIT_SHIFT 8
+#define CS_REQ_IDLE_SYNC_WAIT_MASK (0x1 << CS_REQ_IDLE_SYNC_WAIT_SHIFT)
+#define CS_REQ_IDLE_SYNC_WAIT_GET(reg_val) (((reg_val)&CS_REQ_IDLE_SYNC_WAIT_MASK) >> CS_REQ_IDLE_SYNC_WAIT_SHIFT)
+#define CS_REQ_IDLE_SYNC_WAIT_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_IDLE_SYNC_WAIT_MASK) |  \
+	 (((value) << CS_REQ_IDLE_SYNC_WAIT_SHIFT) & CS_REQ_IDLE_SYNC_WAIT_MASK))
+#define CS_REQ_IDLE_PROTM_PEND_SHIFT 9
+#define CS_REQ_IDLE_PROTM_PEND_MASK (0x1 << CS_REQ_IDLE_PROTM_PEND_SHIFT)
+#define CS_REQ_IDLE_PROTM_PEND_GET(reg_val) (((reg_val)&CS_REQ_IDLE_PROTM_PEND_MASK) >> CS_REQ_IDLE_PROTM_PEND_SHIFT)
+#define CS_REQ_IDLE_PROTM_PEND_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_IDLE_PROTM_PEND_MASK) |  \
+	 (((value) << CS_REQ_IDLE_PROTM_PEND_SHIFT) & CS_REQ_IDLE_PROTM_PEND_MASK))
+#define CS_REQ_IDLE_EMPTY_SHIFT 10
+#define CS_REQ_IDLE_EMPTY_MASK (0x1 << CS_REQ_IDLE_EMPTY_SHIFT)
+#define CS_REQ_IDLE_EMPTY_GET(reg_val) (((reg_val)&CS_REQ_IDLE_EMPTY_MASK) >> CS_REQ_IDLE_EMPTY_SHIFT)
+#define CS_REQ_IDLE_EMPTY_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_IDLE_EMPTY_MASK) | (((value) << CS_REQ_IDLE_EMPTY_SHIFT) & CS_REQ_IDLE_EMPTY_MASK))
+#define CS_REQ_IDLE_RESOURCE_REQ_SHIFT 11
+#define CS_REQ_IDLE_RESOURCE_REQ_MASK (0x1 << CS_REQ_IDLE_RESOURCE_REQ_SHIFT)
+#define CS_REQ_IDLE_RESOURCE_REQ_GET(reg_val) \
+	(((reg_val)&CS_REQ_IDLE_RESOURCE_REQ_MASK) >> CS_REQ_IDLE_RESOURCE_REQ_SHIFT)
+#define CS_REQ_IDLE_RESOURCE_REQ_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_IDLE_RESOURCE_REQ_MASK) |  \
+	 (((value) << CS_REQ_IDLE_RESOURCE_REQ_SHIFT) & CS_REQ_IDLE_RESOURCE_REQ_MASK))
+#define CS_REQ_TILER_OOM_SHIFT 26
+#define CS_REQ_TILER_OOM_MASK (0x1 << CS_REQ_TILER_OOM_SHIFT)
+#define CS_REQ_TILER_OOM_GET(reg_val) (((reg_val)&CS_REQ_TILER_OOM_MASK) >> CS_REQ_TILER_OOM_SHIFT)
+#define CS_REQ_TILER_OOM_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_TILER_OOM_MASK) | (((value) << CS_REQ_TILER_OOM_SHIFT) & CS_REQ_TILER_OOM_MASK))
+#define CS_REQ_PROTM_PEND_SHIFT 27
+#define CS_REQ_PROTM_PEND_MASK (0x1 << CS_REQ_PROTM_PEND_SHIFT)
+#define CS_REQ_PROTM_PEND_GET(reg_val) (((reg_val)&CS_REQ_PROTM_PEND_MASK) >> CS_REQ_PROTM_PEND_SHIFT)
+#define CS_REQ_PROTM_PEND_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_PROTM_PEND_MASK) | (((value) << CS_REQ_PROTM_PEND_SHIFT) & CS_REQ_PROTM_PEND_MASK))
+#define CS_REQ_FATAL_SHIFT 30
+#define CS_REQ_FATAL_MASK (0x1 << CS_REQ_FATAL_SHIFT)
+#define CS_REQ_FATAL_GET(reg_val) (((reg_val)&CS_REQ_FATAL_MASK) >> CS_REQ_FATAL_SHIFT)
+#define CS_REQ_FATAL_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_FATAL_MASK) | (((value) << CS_REQ_FATAL_SHIFT) & CS_REQ_FATAL_MASK))
+#define CS_REQ_FAULT_SHIFT 31
+#define CS_REQ_FAULT_MASK (0x1 << CS_REQ_FAULT_SHIFT)
+#define CS_REQ_FAULT_GET(reg_val) (((reg_val)&CS_REQ_FAULT_MASK) >> CS_REQ_FAULT_SHIFT)
+#define CS_REQ_FAULT_SET(reg_val, value) \
+	(((reg_val) & ~CS_REQ_FAULT_MASK) | (((value) << CS_REQ_FAULT_SHIFT) & CS_REQ_FAULT_MASK))
+
+/* CS_CONFIG register */
+#define CS_CONFIG_PRIORITY_SHIFT 0
+#define CS_CONFIG_PRIORITY_MASK (0xF << CS_CONFIG_PRIORITY_SHIFT)
+#define CS_CONFIG_PRIORITY_GET(reg_val) (((reg_val)&CS_CONFIG_PRIORITY_MASK) >> CS_CONFIG_PRIORITY_SHIFT)
+#define CS_CONFIG_PRIORITY_SET(reg_val, value) \
+	(((reg_val) & ~CS_CONFIG_PRIORITY_MASK) | (((value) << CS_CONFIG_PRIORITY_SHIFT) & CS_CONFIG_PRIORITY_MASK))
+#define CS_CONFIG_USER_DOORBELL_SHIFT 8
+#define CS_CONFIG_USER_DOORBELL_MASK (0xFF << CS_CONFIG_USER_DOORBELL_SHIFT)
+#define CS_CONFIG_USER_DOORBELL_GET(reg_val) (((reg_val)&CS_CONFIG_USER_DOORBELL_MASK) >> CS_CONFIG_USER_DOORBELL_SHIFT)
+#define CS_CONFIG_USER_DOORBELL_SET(reg_val, value) \
+	(((reg_val) & ~CS_CONFIG_USER_DOORBELL_MASK) |  \
+	 (((value) << CS_CONFIG_USER_DOORBELL_SHIFT) & CS_CONFIG_USER_DOORBELL_MASK))
+
+/* CS_ACK_IRQ_MASK register */
+#define CS_ACK_IRQ_MASK_STATE_SHIFT 0
+#define CS_ACK_IRQ_MASK_STATE_MASK (0x7 << CS_ACK_IRQ_MASK_STATE_SHIFT)
+#define CS_ACK_IRQ_MASK_STATE_GET(reg_val) (((reg_val)&CS_ACK_IRQ_MASK_STATE_MASK) >> CS_ACK_IRQ_MASK_STATE_SHIFT)
+#define CS_ACK_IRQ_MASK_STATE_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_IRQ_MASK_STATE_MASK) |  \
+	 (((value) << CS_ACK_IRQ_MASK_STATE_SHIFT) & CS_ACK_IRQ_MASK_STATE_MASK))
+/* CS_ACK_IRQ_MASK_STATE values */
+#define CS_ACK_IRQ_MASK_STATE_DISABLED 0x0
+#define CS_ACK_IRQ_MASK_STATE_ENABLED 0x7
+/* End of CS_ACK_IRQ_MASK_STATE values */
+#define CS_ACK_IRQ_MASK_EXTRACT_EVENT_SHIFT 4
+#define CS_ACK_IRQ_MASK_EXTRACT_EVENT_MASK (0x1 << CS_ACK_IRQ_MASK_EXTRACT_EVENT_SHIFT)
+#define CS_ACK_IRQ_MASK_EXTRACT_EVENT_GET(reg_val) \
+	(((reg_val)&CS_ACK_IRQ_MASK_EXTRACT_EVENT_MASK) >> CS_ACK_IRQ_MASK_EXTRACT_EVENT_SHIFT)
+#define CS_ACK_IRQ_MASK_EXTRACT_EVENT_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_IRQ_MASK_EXTRACT_EVENT_MASK) |  \
+	 (((value) << CS_ACK_IRQ_MASK_EXTRACT_EVENT_SHIFT) & CS_ACK_IRQ_MASK_EXTRACT_EVENT_MASK))
+#define CS_ACK_IRQ_MASK_TILER_OOM_SHIFT 26
+#define CS_ACK_IRQ_MASK_TILER_OOM_MASK (0x1 << CS_ACK_IRQ_MASK_TILER_OOM_SHIFT)
+#define CS_ACK_IRQ_MASK_TILER_OOM_GET(reg_val) \
+	(((reg_val)&CS_ACK_IRQ_MASK_TILER_OOM_MASK) >> CS_ACK_IRQ_MASK_TILER_OOM_SHIFT)
+#define CS_ACK_IRQ_MASK_TILER_OOM_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_IRQ_MASK_TILER_OOM_MASK) |  \
+	 (((value) << CS_ACK_IRQ_MASK_TILER_OOM_SHIFT) & CS_ACK_IRQ_MASK_TILER_OOM_MASK))
+#define CS_ACK_IRQ_MASK_PROTM_PEND_SHIFT 27
+#define CS_ACK_IRQ_MASK_PROTM_PEND_MASK (0x1 << CS_ACK_IRQ_MASK_PROTM_PEND_SHIFT)
+#define CS_ACK_IRQ_MASK_PROTM_PEND_GET(reg_val) \
+	(((reg_val)&CS_ACK_IRQ_MASK_PROTM_PEND_MASK) >> CS_ACK_IRQ_MASK_PROTM_PEND_SHIFT)
+#define CS_ACK_IRQ_MASK_PROTM_PEND_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_IRQ_MASK_PROTM_PEND_MASK) |  \
+	 (((value) << CS_ACK_IRQ_MASK_PROTM_PEND_SHIFT) & CS_ACK_IRQ_MASK_PROTM_PEND_MASK))
+#define CS_ACK_IRQ_MASK_FATAL_SHIFT 30
+#define CS_ACK_IRQ_MASK_FATAL_MASK (0x1 << CS_ACK_IRQ_MASK_FATAL_SHIFT)
+#define CS_ACK_IRQ_MASK_FATAL_GET(reg_val) (((reg_val)&CS_ACK_IRQ_MASK_FATAL_MASK) >> CS_ACK_IRQ_MASK_FATAL_SHIFT)
+#define CS_ACK_IRQ_MASK_FATAL_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_IRQ_MASK_FATAL_MASK) |  \
+	 (((value) << CS_ACK_IRQ_MASK_FATAL_SHIFT) & CS_ACK_IRQ_MASK_FATAL_MASK))
+#define CS_ACK_IRQ_MASK_FAULT_SHIFT 31
+#define CS_ACK_IRQ_MASK_FAULT_MASK (0x1 << CS_ACK_IRQ_MASK_FAULT_SHIFT)
+#define CS_ACK_IRQ_MASK_FAULT_GET(reg_val) (((reg_val)&CS_ACK_IRQ_MASK_FAULT_MASK) >> CS_ACK_IRQ_MASK_FAULT_SHIFT)
+#define CS_ACK_IRQ_MASK_FAULT_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_IRQ_MASK_FAULT_MASK) |  \
+	 (((value) << CS_ACK_IRQ_MASK_FAULT_SHIFT) & CS_ACK_IRQ_MASK_FAULT_MASK))
+
+/* CS_BASE register */
+#define CS_BASE_POINTER_SHIFT 0
+#define CS_BASE_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_BASE_POINTER_SHIFT)
+#define CS_BASE_POINTER_GET(reg_val) (((reg_val)&CS_BASE_POINTER_MASK) >> CS_BASE_POINTER_SHIFT)
+#define CS_BASE_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_BASE_POINTER_MASK) | (((value) << CS_BASE_POINTER_SHIFT) & CS_BASE_POINTER_MASK))
+
+/* CS_SIZE register */
+#define CS_SIZE_SIZE_SHIFT 0
+#define CS_SIZE_SIZE_MASK (0xFFFFFFFF << CS_SIZE_SIZE_SHIFT)
+#define CS_SIZE_SIZE_GET(reg_val) (((reg_val)&CS_SIZE_SIZE_MASK) >> CS_SIZE_SIZE_SHIFT)
+#define CS_SIZE_SIZE_SET(reg_val, value) \
+	(((reg_val) & ~CS_SIZE_SIZE_MASK) | (((value) << CS_SIZE_SIZE_SHIFT) & CS_SIZE_SIZE_MASK))
+
+/* CS_TILER_HEAP_START register */
+#define CS_TILER_HEAP_START_POINTER_SHIFT 0
+#define CS_TILER_HEAP_START_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_TILER_HEAP_START_POINTER_SHIFT)
+#define CS_TILER_HEAP_START_POINTER_GET(reg_val) \
+	(((reg_val)&CS_TILER_HEAP_START_POINTER_MASK) >> CS_TILER_HEAP_START_POINTER_SHIFT)
+#define CS_TILER_HEAP_START_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_TILER_HEAP_START_POINTER_MASK) |  \
+	 (((value) << CS_TILER_HEAP_START_POINTER_SHIFT) & CS_TILER_HEAP_START_POINTER_MASK))
+/* HeapChunkPointer nested in CS_TILER_HEAP_START_POINTER */
+/* End of HeapChunkPointer nested in CS_TILER_HEAP_START_POINTER */
+
+/* CS_TILER_HEAP_END register */
+#define CS_TILER_HEAP_END_POINTER_SHIFT 0
+#define CS_TILER_HEAP_END_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_TILER_HEAP_END_POINTER_SHIFT)
+#define CS_TILER_HEAP_END_POINTER_GET(reg_val) \
+	(((reg_val)&CS_TILER_HEAP_END_POINTER_MASK) >> CS_TILER_HEAP_END_POINTER_SHIFT)
+#define CS_TILER_HEAP_END_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_TILER_HEAP_END_POINTER_MASK) |  \
+	 (((value) << CS_TILER_HEAP_END_POINTER_SHIFT) & CS_TILER_HEAP_END_POINTER_MASK))
+/* HeapChunkPointer nested in CS_TILER_HEAP_END_POINTER */
+/* End of HeapChunkPointer nested in CS_TILER_HEAP_END_POINTER */
+
+/* CS_USER_INPUT register */
+#define CS_USER_INPUT_POINTER_SHIFT 0
+#define CS_USER_INPUT_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_USER_INPUT_POINTER_SHIFT)
+#define CS_USER_INPUT_POINTER_GET(reg_val) (((reg_val)&CS_USER_INPUT_POINTER_MASK) >> CS_USER_INPUT_POINTER_SHIFT)
+#define CS_USER_INPUT_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_USER_INPUT_POINTER_MASK) |  \
+	 (((value) << CS_USER_INPUT_POINTER_SHIFT) & CS_USER_INPUT_POINTER_MASK))
+
+/* CS_USER_OUTPUT register */
+#define CS_USER_OUTPUT_POINTER_SHIFT 0
+#define CS_USER_OUTPUT_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_USER_OUTPUT_POINTER_SHIFT)
+#define CS_USER_OUTPUT_POINTER_GET(reg_val) (((reg_val)&CS_USER_OUTPUT_POINTER_MASK) >> CS_USER_OUTPUT_POINTER_SHIFT)
+#define CS_USER_OUTPUT_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_USER_OUTPUT_POINTER_MASK) |  \
+	 (((value) << CS_USER_OUTPUT_POINTER_SHIFT) & CS_USER_OUTPUT_POINTER_MASK))
+/* End of CS_KERNEL_INPUT_BLOCK register set definitions */
+
+/* CS_KERNEL_OUTPUT_BLOCK register set definitions */
+
+/* CS_ACK register */
+#define CS_ACK_STATE_SHIFT 0
+#define CS_ACK_STATE_MASK (0x7 << CS_ACK_STATE_SHIFT)
+#define CS_ACK_STATE_GET(reg_val) (((reg_val)&CS_ACK_STATE_MASK) >> CS_ACK_STATE_SHIFT)
+#define CS_ACK_STATE_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_STATE_MASK) | (((value) << CS_ACK_STATE_SHIFT) & CS_ACK_STATE_MASK))
+/* CS_ACK_STATE values */
+#define CS_ACK_STATE_STOP 0x0
+#define CS_ACK_STATE_START 0x1
+/* End of CS_ACK_STATE values */
+#define CS_ACK_EXTRACT_EVENT_SHIFT 4
+#define CS_ACK_EXTRACT_EVENT_MASK (0x1 << CS_ACK_EXTRACT_EVENT_SHIFT)
+#define CS_ACK_EXTRACT_EVENT_GET(reg_val) (((reg_val)&CS_ACK_EXTRACT_EVENT_MASK) >> CS_ACK_EXTRACT_EVENT_SHIFT)
+#define CS_ACK_EXTRACT_EVENT_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_EXTRACT_EVENT_MASK) | (((value) << CS_ACK_EXTRACT_EVENT_SHIFT) & CS_ACK_EXTRACT_EVENT_MASK))
+#define CS_ACK_TILER_OOM_SHIFT 26
+#define CS_ACK_TILER_OOM_MASK (0x1 << CS_ACK_TILER_OOM_SHIFT)
+#define CS_ACK_TILER_OOM_GET(reg_val) (((reg_val)&CS_ACK_TILER_OOM_MASK) >> CS_ACK_TILER_OOM_SHIFT)
+#define CS_ACK_TILER_OOM_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_TILER_OOM_MASK) | (((value) << CS_ACK_TILER_OOM_SHIFT) & CS_ACK_TILER_OOM_MASK))
+#define CS_ACK_PROTM_PEND_SHIFT 27
+#define CS_ACK_PROTM_PEND_MASK (0x1 << CS_ACK_PROTM_PEND_SHIFT)
+#define CS_ACK_PROTM_PEND_GET(reg_val) (((reg_val)&CS_ACK_PROTM_PEND_MASK) >> CS_ACK_PROTM_PEND_SHIFT)
+#define CS_ACK_PROTM_PEND_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_PROTM_PEND_MASK) | (((value) << CS_ACK_PROTM_PEND_SHIFT) & CS_ACK_PROTM_PEND_MASK))
+#define CS_ACK_FATAL_SHIFT 30
+#define CS_ACK_FATAL_MASK (0x1 << CS_ACK_FATAL_SHIFT)
+#define CS_ACK_FATAL_GET(reg_val) (((reg_val)&CS_ACK_FATAL_MASK) >> CS_ACK_FATAL_SHIFT)
+#define CS_ACK_FATAL_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_FATAL_MASK) | (((value) << CS_ACK_FATAL_SHIFT) & CS_ACK_FATAL_MASK))
+#define CS_ACK_FAULT_SHIFT 31
+#define CS_ACK_FAULT_MASK (0x1 << CS_ACK_FAULT_SHIFT)
+#define CS_ACK_FAULT_GET(reg_val) (((reg_val)&CS_ACK_FAULT_MASK) >> CS_ACK_FAULT_SHIFT)
+#define CS_ACK_FAULT_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACK_FAULT_MASK) | (((value) << CS_ACK_FAULT_SHIFT) & CS_ACK_FAULT_MASK))
+
+/* CS_STATUS_CMD_PTR register */
+#define CS_STATUS_CMD_PTR_POINTER_SHIFT 0
+#define CS_STATUS_CMD_PTR_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_STATUS_CMD_PTR_POINTER_SHIFT)
+#define CS_STATUS_CMD_PTR_POINTER_GET(reg_val) \
+	(((reg_val)&CS_STATUS_CMD_PTR_POINTER_MASK) >> CS_STATUS_CMD_PTR_POINTER_SHIFT)
+#define CS_STATUS_CMD_PTR_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_CMD_PTR_POINTER_MASK) |  \
+	 (((value) << CS_STATUS_CMD_PTR_POINTER_SHIFT) & CS_STATUS_CMD_PTR_POINTER_MASK))
+
+/* CS_STATUS_WAIT register */
+#define CS_STATUS_WAIT_SB_MASK_SHIFT 0
+#define CS_STATUS_WAIT_SB_MASK_MASK (0xFFFF << CS_STATUS_WAIT_SB_MASK_SHIFT)
+#define CS_STATUS_WAIT_SB_MASK_GET(reg_val) (((reg_val)&CS_STATUS_WAIT_SB_MASK_MASK) >> CS_STATUS_WAIT_SB_MASK_SHIFT)
+#define CS_STATUS_WAIT_SB_MASK_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_WAIT_SB_MASK_MASK) |  \
+	 (((value) << CS_STATUS_WAIT_SB_MASK_SHIFT) & CS_STATUS_WAIT_SB_MASK_MASK))
+#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT 24
+#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_MASK (0xF << CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT)
+#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_GET(reg_val) \
+	(((reg_val)&CS_STATUS_WAIT_SYNC_WAIT_CONDITION_MASK) >> CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT)
+#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_WAIT_SYNC_WAIT_CONDITION_MASK) |  \
+	 (((value) << CS_STATUS_WAIT_SYNC_WAIT_CONDITION_SHIFT) & CS_STATUS_WAIT_SYNC_WAIT_CONDITION_MASK))
+/* CS_STATUS_WAIT_SYNC_WAIT_CONDITION values */
+#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_LE 0x0
+#define CS_STATUS_WAIT_SYNC_WAIT_CONDITION_GT 0x1
+/* End of CS_STATUS_WAIT_SYNC_WAIT_CONDITION values */
+#define CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT 28
+#define CS_STATUS_WAIT_PROGRESS_WAIT_MASK (0x1 << CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT)
+#define CS_STATUS_WAIT_PROGRESS_WAIT_GET(reg_val) \
+	(((reg_val)&CS_STATUS_WAIT_PROGRESS_WAIT_MASK) >> CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT)
+#define CS_STATUS_WAIT_PROGRESS_WAIT_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_WAIT_PROGRESS_WAIT_MASK) |  \
+	 (((value) << CS_STATUS_WAIT_PROGRESS_WAIT_SHIFT) & CS_STATUS_WAIT_PROGRESS_WAIT_MASK))
+#define CS_STATUS_WAIT_PROTM_PEND_SHIFT 29
+#define CS_STATUS_WAIT_PROTM_PEND_MASK (0x1 << CS_STATUS_WAIT_PROTM_PEND_SHIFT)
+#define CS_STATUS_WAIT_PROTM_PEND_GET(reg_val) \
+	(((reg_val)&CS_STATUS_WAIT_PROTM_PEND_MASK) >> CS_STATUS_WAIT_PROTM_PEND_SHIFT)
+#define CS_STATUS_WAIT_PROTM_PEND_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_WAIT_PROTM_PEND_MASK) |  \
+	 (((value) << CS_STATUS_WAIT_PROTM_PEND_SHIFT) & CS_STATUS_WAIT_PROTM_PEND_MASK))
+#define CS_STATUS_WAIT_SYNC_WAIT_SHIFT 31
+#define CS_STATUS_WAIT_SYNC_WAIT_MASK (0x1 << CS_STATUS_WAIT_SYNC_WAIT_SHIFT)
+#define CS_STATUS_WAIT_SYNC_WAIT_GET(reg_val) \
+	(((reg_val)&CS_STATUS_WAIT_SYNC_WAIT_MASK) >> CS_STATUS_WAIT_SYNC_WAIT_SHIFT)
+#define CS_STATUS_WAIT_SYNC_WAIT_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_WAIT_SYNC_WAIT_MASK) |  \
+	 (((value) << CS_STATUS_WAIT_SYNC_WAIT_SHIFT) & CS_STATUS_WAIT_SYNC_WAIT_MASK))
+
+/* CS_STATUS_REQ_RESOURCE register */
+#define CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_SHIFT 0
+#define CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_MASK (0x1 << CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_GET(reg_val) \
+	(((reg_val)&CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_MASK) >> CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_MASK) |  \
+	 (((value) << CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_SHIFT) & CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_MASK))
+#define CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_SHIFT 1
+#define CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_MASK (0x1 << CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_GET(reg_val) \
+	(((reg_val)&CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_MASK) >> CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_MASK) |  \
+	 (((value) << CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_SHIFT) & CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_MASK))
+#define CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_SHIFT 2
+#define CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_MASK (0x1 << CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_GET(reg_val) \
+	(((reg_val)&CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_MASK) >> CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_MASK) |  \
+	 (((value) << CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_SHIFT) & CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_MASK))
+#define CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_SHIFT 3
+#define CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_MASK (0x1 << CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_GET(reg_val) \
+	(((reg_val)&CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_MASK) >> CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_SHIFT)
+#define CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_MASK) |  \
+	 (((value) << CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_SHIFT) & CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_MASK))
+
+/* CS_STATUS_WAIT_SYNC_POINTER register */
+#define CS_STATUS_WAIT_SYNC_POINTER_POINTER_SHIFT 0
+#define CS_STATUS_WAIT_SYNC_POINTER_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_STATUS_WAIT_SYNC_POINTER_POINTER_SHIFT)
+#define CS_STATUS_WAIT_SYNC_POINTER_POINTER_GET(reg_val) \
+	(((reg_val)&CS_STATUS_WAIT_SYNC_POINTER_POINTER_MASK) >> CS_STATUS_WAIT_SYNC_POINTER_POINTER_SHIFT)
+#define CS_STATUS_WAIT_SYNC_POINTER_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_WAIT_SYNC_POINTER_POINTER_MASK) |  \
+	 (((value) << CS_STATUS_WAIT_SYNC_POINTER_POINTER_SHIFT) & CS_STATUS_WAIT_SYNC_POINTER_POINTER_MASK))
+
+/* CS_STATUS_WAIT_SYNC_VALUE register */
+#define CS_STATUS_WAIT_SYNC_VALUE_VALUE_SHIFT 0
+#define CS_STATUS_WAIT_SYNC_VALUE_VALUE_MASK (0xFFFFFFFF << CS_STATUS_WAIT_SYNC_VALUE_VALUE_SHIFT)
+#define CS_STATUS_WAIT_SYNC_VALUE_VALUE_GET(reg_val) \
+	(((reg_val)&CS_STATUS_WAIT_SYNC_VALUE_VALUE_MASK) >> CS_STATUS_WAIT_SYNC_VALUE_VALUE_SHIFT)
+#define CS_STATUS_WAIT_SYNC_VALUE_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~CS_STATUS_WAIT_SYNC_VALUE_VALUE_MASK) |  \
+	 (((value) << CS_STATUS_WAIT_SYNC_VALUE_VALUE_SHIFT) & CS_STATUS_WAIT_SYNC_VALUE_VALUE_MASK))
+
+/* CS_FAULT register */
+#define CS_FAULT_EXCEPTION_TYPE_SHIFT 0
+#define CS_FAULT_EXCEPTION_TYPE_MASK (0xFF << CS_FAULT_EXCEPTION_TYPE_SHIFT)
+#define CS_FAULT_EXCEPTION_TYPE_GET(reg_val) (((reg_val)&CS_FAULT_EXCEPTION_TYPE_MASK) >> CS_FAULT_EXCEPTION_TYPE_SHIFT)
+#define CS_FAULT_EXCEPTION_TYPE_SET(reg_val, value) \
+	(((reg_val) & ~CS_FAULT_EXCEPTION_TYPE_MASK) |  \
+	 (((value) << CS_FAULT_EXCEPTION_TYPE_SHIFT) & CS_FAULT_EXCEPTION_TYPE_MASK))
+/* CS_FAULT_EXCEPTION_TYPE values */
+#define CS_FAULT_EXCEPTION_TYPE_CS_RESOURCE_TERMINATED 0x0F
+#define CS_FAULT_EXCEPTION_TYPE_CS_INHERIT_FAULT 0x4B
+#define CS_FAULT_EXCEPTION_TYPE_INSTR_INVALID_PC 0x50
+#define CS_FAULT_EXCEPTION_TYPE_INSTR_INVALID_ENC 0x51
+#define CS_FAULT_EXCEPTION_TYPE_INSTR_BARRIER_FAULT 0x55
+#define CS_FAULT_EXCEPTION_TYPE_DATA_INVALID_FAULT 0x58
+#define CS_FAULT_EXCEPTION_TYPE_TILE_RANGE_FAULT 0x59
+#define CS_FAULT_EXCEPTION_TYPE_ADDR_RANGE_FAULT 0x5A
+#define CS_FAULT_EXCEPTION_TYPE_IMPRECISE_FAULT 0x5B
+#define CS_FAULT_EXCEPTION_TYPE_RESOURCE_EVICTION_TIMEOUT 0x69
+/* End of CS_FAULT_EXCEPTION_TYPE values */
+#define CS_FAULT_EXCEPTION_DATA_SHIFT 8
+#define CS_FAULT_EXCEPTION_DATA_MASK (0xFFFFFF << CS_FAULT_EXCEPTION_DATA_SHIFT)
+#define CS_FAULT_EXCEPTION_DATA_GET(reg_val) (((reg_val)&CS_FAULT_EXCEPTION_DATA_MASK) >> CS_FAULT_EXCEPTION_DATA_SHIFT)
+#define CS_FAULT_EXCEPTION_DATA_SET(reg_val, value) \
+	(((reg_val) & ~CS_FAULT_EXCEPTION_DATA_MASK) |  \
+	 (((value) << CS_FAULT_EXCEPTION_DATA_SHIFT) & CS_FAULT_EXCEPTION_DATA_MASK))
+
+/* CS_FATAL register */
+#define CS_FATAL_EXCEPTION_TYPE_SHIFT 0
+#define CS_FATAL_EXCEPTION_TYPE_MASK (0xFF << CS_FATAL_EXCEPTION_TYPE_SHIFT)
+#define CS_FATAL_EXCEPTION_TYPE_GET(reg_val) (((reg_val)&CS_FATAL_EXCEPTION_TYPE_MASK) >> CS_FATAL_EXCEPTION_TYPE_SHIFT)
+#define CS_FATAL_EXCEPTION_TYPE_SET(reg_val, value) \
+	(((reg_val) & ~CS_FATAL_EXCEPTION_TYPE_MASK) |  \
+	 (((value) << CS_FATAL_EXCEPTION_TYPE_SHIFT) & CS_FATAL_EXCEPTION_TYPE_MASK))
+/* CS_FATAL_EXCEPTION_TYPE values */
+#define CS_FATAL_EXCEPTION_TYPE_CS_CONFIG_FAULT 0x40
+#define CS_FATAL_EXCEPTION_TYPE_CS_ENDPOINT_FAULT 0x44
+#define CS_FATAL_EXCEPTION_TYPE_CS_BUS_FAULT 0x48
+#define CS_FATAL_EXCEPTION_TYPE_CS_INVALID_INSTRUCTION 0x49
+#define CS_FATAL_EXCEPTION_TYPE_CS_CALL_STACK_OVERFLOW 0x4A
+#define CS_FATAL_EXCEPTION_TYPE_FIRMWARE_INTERNAL_ERROR 0x68
+/* End of CS_FATAL_EXCEPTION_TYPE values */
+#define CS_FATAL_EXCEPTION_DATA_SHIFT 8
+#define CS_FATAL_EXCEPTION_DATA_MASK (0xFFFFFF << CS_FATAL_EXCEPTION_DATA_SHIFT)
+#define CS_FATAL_EXCEPTION_DATA_GET(reg_val) (((reg_val)&CS_FATAL_EXCEPTION_DATA_MASK) >> CS_FATAL_EXCEPTION_DATA_SHIFT)
+#define CS_FATAL_EXCEPTION_DATA_SET(reg_val, value) \
+	(((reg_val) & ~CS_FATAL_EXCEPTION_DATA_MASK) |  \
+	 (((value) << CS_FATAL_EXCEPTION_DATA_SHIFT) & CS_FATAL_EXCEPTION_DATA_MASK))
+
+/* CS_FAULT_INFO register */
+#define CS_FAULT_INFO_EXCEPTION_DATA_SHIFT 0
+#define CS_FAULT_INFO_EXCEPTION_DATA_MASK (0xFFFFFFFFFFFFFFFF << CS_FAULT_INFO_EXCEPTION_DATA_SHIFT)
+#define CS_FAULT_INFO_EXCEPTION_DATA_GET(reg_val) \
+	(((reg_val)&CS_FAULT_INFO_EXCEPTION_DATA_MASK) >> CS_FAULT_INFO_EXCEPTION_DATA_SHIFT)
+#define CS_FAULT_INFO_EXCEPTION_DATA_SET(reg_val, value) \
+	(((reg_val) & ~CS_FAULT_INFO_EXCEPTION_DATA_MASK) |  \
+	 (((value) << CS_FAULT_INFO_EXCEPTION_DATA_SHIFT) & CS_FAULT_INFO_EXCEPTION_DATA_MASK))
+
+/* CS_FATAL_INFO register */
+#define CS_FATAL_INFO_EXCEPTION_DATA_SHIFT 0
+#define CS_FATAL_INFO_EXCEPTION_DATA_MASK (0xFFFFFFFFFFFFFFFF << CS_FATAL_INFO_EXCEPTION_DATA_SHIFT)
+#define CS_FATAL_INFO_EXCEPTION_DATA_GET(reg_val) \
+	(((reg_val)&CS_FATAL_INFO_EXCEPTION_DATA_MASK) >> CS_FATAL_INFO_EXCEPTION_DATA_SHIFT)
+#define CS_FATAL_INFO_EXCEPTION_DATA_SET(reg_val, value) \
+	(((reg_val) & ~CS_FATAL_INFO_EXCEPTION_DATA_MASK) |  \
+	 (((value) << CS_FATAL_INFO_EXCEPTION_DATA_SHIFT) & CS_FATAL_INFO_EXCEPTION_DATA_MASK))
+
+/* CS_HEAP_VT_START register */
+#define CS_HEAP_VT_START_VALUE_SHIFT 0
+#define CS_HEAP_VT_START_VALUE_MASK (0xFFFFFFFF << CS_HEAP_VT_START_VALUE_SHIFT)
+#define CS_HEAP_VT_START_VALUE_GET(reg_val) (((reg_val)&CS_HEAP_VT_START_VALUE_MASK) >> CS_HEAP_VT_START_VALUE_SHIFT)
+#define CS_HEAP_VT_START_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~CS_HEAP_VT_START_VALUE_MASK) |  \
+	 (((value) << CS_HEAP_VT_START_VALUE_SHIFT) & CS_HEAP_VT_START_VALUE_MASK))
+
+/* CS_HEAP_VT_END register */
+#define CS_HEAP_VT_END_VALUE_SHIFT 0
+#define CS_HEAP_VT_END_VALUE_MASK (0xFFFFFFFF << CS_HEAP_VT_END_VALUE_SHIFT)
+#define CS_HEAP_VT_END_VALUE_GET(reg_val) (((reg_val)&CS_HEAP_VT_END_VALUE_MASK) >> CS_HEAP_VT_END_VALUE_SHIFT)
+#define CS_HEAP_VT_END_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~CS_HEAP_VT_END_VALUE_MASK) | (((value) << CS_HEAP_VT_END_VALUE_SHIFT) & CS_HEAP_VT_END_VALUE_MASK))
+
+/* CS_HEAP_FRAG_END register */
+#define CS_HEAP_FRAG_END_VALUE_SHIFT 0
+#define CS_HEAP_FRAG_END_VALUE_MASK (0xFFFFFFFF << CS_HEAP_FRAG_END_VALUE_SHIFT)
+#define CS_HEAP_FRAG_END_VALUE_GET(reg_val) (((reg_val)&CS_HEAP_FRAG_END_VALUE_MASK) >> CS_HEAP_FRAG_END_VALUE_SHIFT)
+#define CS_HEAP_FRAG_END_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~CS_HEAP_FRAG_END_VALUE_MASK) |  \
+	 (((value) << CS_HEAP_FRAG_END_VALUE_SHIFT) & CS_HEAP_FRAG_END_VALUE_MASK))
+
+/* CS_HEAP_ADDRESS register */
+#define CS_HEAP_ADDRESS_POINTER_SHIFT 0
+#define CS_HEAP_ADDRESS_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CS_HEAP_ADDRESS_POINTER_SHIFT)
+#define CS_HEAP_ADDRESS_POINTER_GET(reg_val) (((reg_val)&CS_HEAP_ADDRESS_POINTER_MASK) >> CS_HEAP_ADDRESS_POINTER_SHIFT)
+#define CS_HEAP_ADDRESS_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CS_HEAP_ADDRESS_POINTER_MASK) |  \
+	 (((value) << CS_HEAP_ADDRESS_POINTER_SHIFT) & CS_HEAP_ADDRESS_POINTER_MASK))
+/* End of CS_KERNEL_OUTPUT_BLOCK register set definitions */
+
+/* CS_USER_INPUT_BLOCK register set definitions */
+
+/* CS_INSERT register */
+#define CS_INSERT_VALUE_SHIFT 0
+#define CS_INSERT_VALUE_MASK (0xFFFFFFFFFFFFFFFF << CS_INSERT_VALUE_SHIFT)
+#define CS_INSERT_VALUE_GET(reg_val) (((reg_val)&CS_INSERT_VALUE_MASK) >> CS_INSERT_VALUE_SHIFT)
+#define CS_INSERT_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~CS_INSERT_VALUE_MASK) | (((value) << CS_INSERT_VALUE_SHIFT) & CS_INSERT_VALUE_MASK))
+
+/* CS_EXTRACT_INIT register */
+#define CS_EXTRACT_INIT_VALUE_SHIFT 0
+#define CS_EXTRACT_INIT_VALUE_MASK (0xFFFFFFFFFFFFFFFF << CS_EXTRACT_INIT_VALUE_SHIFT)
+#define CS_EXTRACT_INIT_VALUE_GET(reg_val) (((reg_val)&CS_EXTRACT_INIT_VALUE_MASK) >> CS_EXTRACT_INIT_VALUE_SHIFT)
+#define CS_EXTRACT_INIT_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~CS_EXTRACT_INIT_VALUE_MASK) |  \
+	 (((value) << CS_EXTRACT_INIT_VALUE_SHIFT) & CS_EXTRACT_INIT_VALUE_MASK))
+/* End of CS_USER_INPUT_BLOCK register set definitions */
+
+/* CS_USER_OUTPUT_BLOCK register set definitions */
+
+/* CS_EXTRACT register */
+#define CS_EXTRACT_VALUE_SHIFT 0
+#define CS_EXTRACT_VALUE_MASK (0xFFFFFFFFFFFFFFFF << CS_EXTRACT_VALUE_SHIFT)
+#define CS_EXTRACT_VALUE_GET(reg_val) (((reg_val)&CS_EXTRACT_VALUE_MASK) >> CS_EXTRACT_VALUE_SHIFT)
+#define CS_EXTRACT_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~CS_EXTRACT_VALUE_MASK) | (((value) << CS_EXTRACT_VALUE_SHIFT) & CS_EXTRACT_VALUE_MASK))
+
+/* CS_ACTIVE register */
+#define CS_ACTIVE_HW_ACTIVE_SHIFT 0
+#define CS_ACTIVE_HW_ACTIVE_MASK (0x1 << CS_ACTIVE_HW_ACTIVE_SHIFT)
+#define CS_ACTIVE_HW_ACTIVE_GET(reg_val) (((reg_val)&CS_ACTIVE_HW_ACTIVE_MASK) >> CS_ACTIVE_HW_ACTIVE_SHIFT)
+#define CS_ACTIVE_HW_ACTIVE_SET(reg_val, value) \
+	(((reg_val) & ~CS_ACTIVE_HW_ACTIVE_MASK) | (((value) << CS_ACTIVE_HW_ACTIVE_SHIFT) & CS_ACTIVE_HW_ACTIVE_MASK))
+/* End of CS_USER_OUTPUT_BLOCK register set definitions */
+
+/* CSG_INPUT_BLOCK register set definitions */
+
+/* CSG_REQ register */
+#define CSG_REQ_STATE_SHIFT 0
+#define CSG_REQ_STATE_MASK (0x7 << CSG_REQ_STATE_SHIFT)
+#define CSG_REQ_STATE_GET(reg_val) (((reg_val)&CSG_REQ_STATE_MASK) >> CSG_REQ_STATE_SHIFT)
+#define CSG_REQ_STATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_REQ_STATE_MASK) | (((value) << CSG_REQ_STATE_SHIFT) & CSG_REQ_STATE_MASK))
+/* CSG_REQ_STATE values */
+#define CSG_REQ_STATE_TERMINATE 0x0
+#define CSG_REQ_STATE_START 0x1
+#define CSG_REQ_STATE_SUSPEND 0x2
+#define CSG_REQ_STATE_RESUME 0x3
+/* End of CSG_REQ_STATE values */
+#define CSG_REQ_EP_CFG_SHIFT 4
+#define CSG_REQ_EP_CFG_MASK (0x1 << CSG_REQ_EP_CFG_SHIFT)
+#define CSG_REQ_EP_CFG_GET(reg_val) (((reg_val)&CSG_REQ_EP_CFG_MASK) >> CSG_REQ_EP_CFG_SHIFT)
+#define CSG_REQ_EP_CFG_SET(reg_val, value) \
+	(((reg_val) & ~CSG_REQ_EP_CFG_MASK) | (((value) << CSG_REQ_EP_CFG_SHIFT) & CSG_REQ_EP_CFG_MASK))
+#define CSG_REQ_STATUS_UPDATE_SHIFT 5
+#define CSG_REQ_STATUS_UPDATE_MASK (0x1 << CSG_REQ_STATUS_UPDATE_SHIFT)
+#define CSG_REQ_STATUS_UPDATE_GET(reg_val) (((reg_val)&CSG_REQ_STATUS_UPDATE_MASK) >> CSG_REQ_STATUS_UPDATE_SHIFT)
+#define CSG_REQ_STATUS_UPDATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_REQ_STATUS_UPDATE_MASK) |  \
+	 (((value) << CSG_REQ_STATUS_UPDATE_SHIFT) & CSG_REQ_STATUS_UPDATE_MASK))
+#define CSG_REQ_SYNC_UPDATE_SHIFT 28
+#define CSG_REQ_SYNC_UPDATE_MASK (0x1 << CSG_REQ_SYNC_UPDATE_SHIFT)
+#define CSG_REQ_SYNC_UPDATE_GET(reg_val) (((reg_val)&CSG_REQ_SYNC_UPDATE_MASK) >> CSG_REQ_SYNC_UPDATE_SHIFT)
+#define CSG_REQ_SYNC_UPDATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_REQ_SYNC_UPDATE_MASK) | (((value) << CSG_REQ_SYNC_UPDATE_SHIFT) & CSG_REQ_SYNC_UPDATE_MASK))
+#define CSG_REQ_IDLE_SHIFT 29
+#define CSG_REQ_IDLE_MASK (0x1 << CSG_REQ_IDLE_SHIFT)
+#define CSG_REQ_IDLE_GET(reg_val) (((reg_val)&CSG_REQ_IDLE_MASK) >> CSG_REQ_IDLE_SHIFT)
+#define CSG_REQ_IDLE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_REQ_IDLE_MASK) | (((value) << CSG_REQ_IDLE_SHIFT) & CSG_REQ_IDLE_MASK))
+#define CSG_REQ_DOORBELL_SHIFT 30
+#define CSG_REQ_DOORBELL_MASK (0x1 << CSG_REQ_DOORBELL_SHIFT)
+#define CSG_REQ_DOORBELL_GET(reg_val) (((reg_val)&CSG_REQ_DOORBELL_MASK) >> CSG_REQ_DOORBELL_SHIFT)
+#define CSG_REQ_DOORBELL_SET(reg_val, value) \
+	(((reg_val) & ~CSG_REQ_DOORBELL_MASK) | (((value) << CSG_REQ_DOORBELL_SHIFT) & CSG_REQ_DOORBELL_MASK))
+#define CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT 31
+#define CSG_REQ_PROGRESS_TIMER_EVENT_MASK (0x1 << CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_REQ_PROGRESS_TIMER_EVENT_GET(reg_val) \
+	(((reg_val)&CSG_REQ_PROGRESS_TIMER_EVENT_MASK) >> CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_REQ_PROGRESS_TIMER_EVENT_SET(reg_val, value) \
+	(((reg_val) & ~CSG_REQ_PROGRESS_TIMER_EVENT_MASK) |  \
+	 (((value) << CSG_REQ_PROGRESS_TIMER_EVENT_SHIFT) & CSG_REQ_PROGRESS_TIMER_EVENT_MASK))
+
+/* CSG_ACK_IRQ_MASK register */
+#define CSG_ACK_IRQ_MASK_STATE_SHIFT 0
+#define CSG_ACK_IRQ_MASK_STATE_MASK (0x7 << CSG_ACK_IRQ_MASK_STATE_SHIFT)
+#define CSG_ACK_IRQ_MASK_STATE_GET(reg_val) (((reg_val)&CSG_ACK_IRQ_MASK_STATE_MASK) >> CSG_ACK_IRQ_MASK_STATE_SHIFT)
+#define CSG_ACK_IRQ_MASK_STATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IRQ_MASK_STATE_MASK) |  \
+	 (((value) << CSG_ACK_IRQ_MASK_STATE_SHIFT) & CSG_ACK_IRQ_MASK_STATE_MASK))
+/* CSG_ACK_IRQ_MASK_STATE values */
+#define CSG_ACK_IRQ_MASK_STATE_DISABLED 0x0
+#define CSG_ACK_IRQ_MASK_STATE_ENABLED 0x7
+/* End of CSG_ACK_IRQ_MASK_STATE values */
+#define CSG_ACK_IRQ_MASK_EP_CFG_SHIFT 4
+#define CSG_ACK_IRQ_MASK_EP_CFG_MASK (0x1 << CSG_ACK_IRQ_MASK_EP_CFG_SHIFT)
+#define CSG_ACK_IRQ_MASK_EP_CFG_GET(reg_val) (((reg_val)&CSG_ACK_IRQ_MASK_EP_CFG_MASK) >> CSG_ACK_IRQ_MASK_EP_CFG_SHIFT)
+#define CSG_ACK_IRQ_MASK_EP_CFG_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IRQ_MASK_EP_CFG_MASK) |  \
+	 (((value) << CSG_ACK_IRQ_MASK_EP_CFG_SHIFT) & CSG_ACK_IRQ_MASK_EP_CFG_MASK))
+#define CSG_ACK_IRQ_MASK_STATUS_UPDATE_SHIFT 5
+#define CSG_ACK_IRQ_MASK_STATUS_UPDATE_MASK (0x1 << CSG_ACK_IRQ_MASK_STATUS_UPDATE_SHIFT)
+#define CSG_ACK_IRQ_MASK_STATUS_UPDATE_GET(reg_val) \
+	(((reg_val)&CSG_ACK_IRQ_MASK_STATUS_UPDATE_MASK) >> CSG_ACK_IRQ_MASK_STATUS_UPDATE_SHIFT)
+#define CSG_ACK_IRQ_MASK_STATUS_UPDATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IRQ_MASK_STATUS_UPDATE_MASK) |  \
+	 (((value) << CSG_ACK_IRQ_MASK_STATUS_UPDATE_SHIFT) & CSG_ACK_IRQ_MASK_STATUS_UPDATE_MASK))
+#define CSG_ACK_IRQ_MASK_SYNC_UPDATE_SHIFT 28
+#define CSG_ACK_IRQ_MASK_SYNC_UPDATE_MASK (0x1 << CSG_ACK_IRQ_MASK_SYNC_UPDATE_SHIFT)
+#define CSG_ACK_IRQ_MASK_SYNC_UPDATE_GET(reg_val) \
+	(((reg_val)&CSG_ACK_IRQ_MASK_SYNC_UPDATE_MASK) >> CSG_ACK_IRQ_MASK_SYNC_UPDATE_SHIFT)
+#define CSG_ACK_IRQ_MASK_SYNC_UPDATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IRQ_MASK_SYNC_UPDATE_MASK) |  \
+	 (((value) << CSG_ACK_IRQ_MASK_SYNC_UPDATE_SHIFT) & CSG_ACK_IRQ_MASK_SYNC_UPDATE_MASK))
+#define CSG_ACK_IRQ_MASK_IDLE_SHIFT 29
+#define CSG_ACK_IRQ_MASK_IDLE_MASK (0x1 << CSG_ACK_IRQ_MASK_IDLE_SHIFT)
+#define CSG_ACK_IRQ_MASK_IDLE_GET(reg_val) (((reg_val)&CSG_ACK_IRQ_MASK_IDLE_MASK) >> CSG_ACK_IRQ_MASK_IDLE_SHIFT)
+#define CSG_ACK_IRQ_MASK_IDLE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IRQ_MASK_IDLE_MASK) |  \
+	 (((value) << CSG_ACK_IRQ_MASK_IDLE_SHIFT) & CSG_ACK_IRQ_MASK_IDLE_MASK))
+#define CSG_ACK_IRQ_MASK_DOORBELL_SHIFT 30
+#define CSG_ACK_IRQ_MASK_DOORBELL_MASK (0x1 << CSG_ACK_IRQ_MASK_DOORBELL_SHIFT)
+#define CSG_ACK_IRQ_MASK_DOORBELL_GET(reg_val) \
+	(((reg_val)&CSG_ACK_IRQ_MASK_DOORBELL_MASK) >> CSG_ACK_IRQ_MASK_DOORBELL_SHIFT)
+#define CSG_ACK_IRQ_MASK_DOORBELL_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IRQ_MASK_DOORBELL_MASK) |  \
+	 (((value) << CSG_ACK_IRQ_MASK_DOORBELL_SHIFT) & CSG_ACK_IRQ_MASK_DOORBELL_MASK))
+#define CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_SHIFT 31
+#define CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_MASK (0x1 << CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_GET(reg_val) \
+	(((reg_val)&CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_MASK) >> CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_MASK) |  \
+	 (((value) << CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_SHIFT) & CSG_ACK_IRQ_MASK_PROGRESS_TIMER_EVENT_MASK))
+
+/* CSG_EP_REQ register */
+#define CSG_EP_REQ_COMPUTE_EP_SHIFT 0
+#define CSG_EP_REQ_COMPUTE_EP_MASK (0xFF << CSG_EP_REQ_COMPUTE_EP_SHIFT)
+#define CSG_EP_REQ_COMPUTE_EP_GET(reg_val) (((reg_val)&CSG_EP_REQ_COMPUTE_EP_MASK) >> CSG_EP_REQ_COMPUTE_EP_SHIFT)
+#define CSG_EP_REQ_COMPUTE_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_EP_REQ_COMPUTE_EP_MASK) |  \
+	 (((value) << CSG_EP_REQ_COMPUTE_EP_SHIFT) & CSG_EP_REQ_COMPUTE_EP_MASK))
+#define CSG_EP_REQ_FRAGMENT_EP_SHIFT 8
+#define CSG_EP_REQ_FRAGMENT_EP_MASK (0xFF << CSG_EP_REQ_FRAGMENT_EP_SHIFT)
+#define CSG_EP_REQ_FRAGMENT_EP_GET(reg_val) (((reg_val)&CSG_EP_REQ_FRAGMENT_EP_MASK) >> CSG_EP_REQ_FRAGMENT_EP_SHIFT)
+#define CSG_EP_REQ_FRAGMENT_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_EP_REQ_FRAGMENT_EP_MASK) |  \
+	 (((value) << CSG_EP_REQ_FRAGMENT_EP_SHIFT) & CSG_EP_REQ_FRAGMENT_EP_MASK))
+#define CSG_EP_REQ_TILER_EP_SHIFT 16
+#define CSG_EP_REQ_TILER_EP_MASK (0xF << CSG_EP_REQ_TILER_EP_SHIFT)
+#define CSG_EP_REQ_TILER_EP_GET(reg_val) (((reg_val)&CSG_EP_REQ_TILER_EP_MASK) >> CSG_EP_REQ_TILER_EP_SHIFT)
+#define CSG_EP_REQ_TILER_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_EP_REQ_TILER_EP_MASK) | (((value) << CSG_EP_REQ_TILER_EP_SHIFT) & CSG_EP_REQ_TILER_EP_MASK))
+#define CSG_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT 20
+#define CSG_EP_REQ_EXCLUSIVE_COMPUTE_MASK (0x1 << CSG_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT)
+#define CSG_EP_REQ_EXCLUSIVE_COMPUTE_GET(reg_val) \
+	(((reg_val)&CSG_EP_REQ_EXCLUSIVE_COMPUTE_MASK) >> CSG_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT)
+#define CSG_EP_REQ_EXCLUSIVE_COMPUTE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_EP_REQ_EXCLUSIVE_COMPUTE_MASK) |  \
+	 (((value) << CSG_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT) & CSG_EP_REQ_EXCLUSIVE_COMPUTE_MASK))
+#define CSG_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT 21
+#define CSG_EP_REQ_EXCLUSIVE_FRAGMENT_MASK (0x1 << CSG_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT)
+#define CSG_EP_REQ_EXCLUSIVE_FRAGMENT_GET(reg_val) \
+	(((reg_val)&CSG_EP_REQ_EXCLUSIVE_FRAGMENT_MASK) >> CSG_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT)
+#define CSG_EP_REQ_EXCLUSIVE_FRAGMENT_SET(reg_val, value) \
+	(((reg_val) & ~CSG_EP_REQ_EXCLUSIVE_FRAGMENT_MASK) |  \
+	 (((value) << CSG_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT) & CSG_EP_REQ_EXCLUSIVE_FRAGMENT_MASK))
+#define CSG_EP_REQ_PRIORITY_SHIFT 28
+#define CSG_EP_REQ_PRIORITY_MASK (0xF << CSG_EP_REQ_PRIORITY_SHIFT)
+#define CSG_EP_REQ_PRIORITY_GET(reg_val) (((reg_val)&CSG_EP_REQ_PRIORITY_MASK) >> CSG_EP_REQ_PRIORITY_SHIFT)
+#define CSG_EP_REQ_PRIORITY_SET(reg_val, value) \
+	(((reg_val) & ~CSG_EP_REQ_PRIORITY_MASK) | (((value) << CSG_EP_REQ_PRIORITY_SHIFT) & CSG_EP_REQ_PRIORITY_MASK))
+
+/* CSG_SUSPEND_BUF register */
+#define CSG_SUSPEND_BUF_POINTER_SHIFT 0
+#define CSG_SUSPEND_BUF_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CSG_SUSPEND_BUF_POINTER_SHIFT)
+#define CSG_SUSPEND_BUF_POINTER_GET(reg_val) (((reg_val)&CSG_SUSPEND_BUF_POINTER_MASK) >> CSG_SUSPEND_BUF_POINTER_SHIFT)
+#define CSG_SUSPEND_BUF_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CSG_SUSPEND_BUF_POINTER_MASK) |  \
+	 (((value) << CSG_SUSPEND_BUF_POINTER_SHIFT) & CSG_SUSPEND_BUF_POINTER_MASK))
+
+/* CSG_PROTM_SUSPEND_BUF register */
+#define CSG_PROTM_SUSPEND_BUF_POINTER_SHIFT 0
+#define CSG_PROTM_SUSPEND_BUF_POINTER_MASK (0xFFFFFFFFFFFFFFFF << CSG_PROTM_SUSPEND_BUF_POINTER_SHIFT)
+#define CSG_PROTM_SUSPEND_BUF_POINTER_GET(reg_val) \
+	(((reg_val)&CSG_PROTM_SUSPEND_BUF_POINTER_MASK) >> CSG_PROTM_SUSPEND_BUF_POINTER_SHIFT)
+#define CSG_PROTM_SUSPEND_BUF_POINTER_SET(reg_val, value) \
+	(((reg_val) & ~CSG_PROTM_SUSPEND_BUF_POINTER_MASK) |  \
+	 (((value) << CSG_PROTM_SUSPEND_BUF_POINTER_SHIFT) & CSG_PROTM_SUSPEND_BUF_POINTER_MASK))
+
+/* End of CSG_INPUT_BLOCK register set definitions */
+
+/* CSG_OUTPUT_BLOCK register set definitions */
+
+/* CSG_ACK register */
+#define CSG_ACK_STATE_SHIFT 0
+#define CSG_ACK_STATE_MASK (0x7 << CSG_ACK_STATE_SHIFT)
+#define CSG_ACK_STATE_GET(reg_val) (((reg_val)&CSG_ACK_STATE_MASK) >> CSG_ACK_STATE_SHIFT)
+#define CSG_ACK_STATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_STATE_MASK) | (((value) << CSG_ACK_STATE_SHIFT) & CSG_ACK_STATE_MASK))
+/* CSG_ACK_STATE values */
+#define CSG_ACK_STATE_TERMINATE 0x0
+#define CSG_ACK_STATE_START 0x1
+#define CSG_ACK_STATE_SUSPEND 0x2
+#define CSG_ACK_STATE_RESUME 0x3
+/* End of CSG_ACK_STATE values */
+#define CSG_ACK_EP_CFG_SHIFT 4
+#define CSG_ACK_EP_CFG_MASK (0x1 << CSG_ACK_EP_CFG_SHIFT)
+#define CSG_ACK_EP_CFG_GET(reg_val) (((reg_val)&CSG_ACK_EP_CFG_MASK) >> CSG_ACK_EP_CFG_SHIFT)
+#define CSG_ACK_EP_CFG_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_EP_CFG_MASK) | (((value) << CSG_ACK_EP_CFG_SHIFT) & CSG_ACK_EP_CFG_MASK))
+#define CSG_ACK_STATUS_UPDATE_SHIFT 5
+#define CSG_ACK_STATUS_UPDATE_MASK (0x1 << CSG_ACK_STATUS_UPDATE_SHIFT)
+#define CSG_ACK_STATUS_UPDATE_GET(reg_val) (((reg_val)&CSG_ACK_STATUS_UPDATE_MASK) >> CSG_ACK_STATUS_UPDATE_SHIFT)
+#define CSG_ACK_STATUS_UPDATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_STATUS_UPDATE_MASK) |  \
+	 (((value) << CSG_ACK_STATUS_UPDATE_SHIFT) & CSG_ACK_STATUS_UPDATE_MASK))
+#define CSG_ACK_SYNC_UPDATE_SHIFT 28
+#define CSG_ACK_SYNC_UPDATE_MASK (0x1 << CSG_ACK_SYNC_UPDATE_SHIFT)
+#define CSG_ACK_SYNC_UPDATE_GET(reg_val) (((reg_val)&CSG_ACK_SYNC_UPDATE_MASK) >> CSG_ACK_SYNC_UPDATE_SHIFT)
+#define CSG_ACK_SYNC_UPDATE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_SYNC_UPDATE_MASK) | (((value) << CSG_ACK_SYNC_UPDATE_SHIFT) & CSG_ACK_SYNC_UPDATE_MASK))
+#define CSG_ACK_IDLE_SHIFT 29
+#define CSG_ACK_IDLE_MASK (0x1 << CSG_ACK_IDLE_SHIFT)
+#define CSG_ACK_IDLE_GET(reg_val) (((reg_val)&CSG_ACK_IDLE_MASK) >> CSG_ACK_IDLE_SHIFT)
+#define CSG_ACK_IDLE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_IDLE_MASK) | (((value) << CSG_ACK_IDLE_SHIFT) & CSG_ACK_IDLE_MASK))
+#define CSG_ACK_DOORBELL_SHIFT 30
+#define CSG_ACK_DOORBELL_MASK (0x1 << CSG_ACK_DOORBELL_SHIFT)
+#define CSG_ACK_DOORBELL_GET(reg_val) (((reg_val)&CSG_ACK_DOORBELL_MASK) >> CSG_ACK_DOORBELL_SHIFT)
+#define CSG_ACK_DOORBELL_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_DOORBELL_MASK) | (((value) << CSG_ACK_DOORBELL_SHIFT) & CSG_ACK_DOORBELL_MASK))
+#define CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT 31
+#define CSG_ACK_PROGRESS_TIMER_EVENT_MASK (0x1 << CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_ACK_PROGRESS_TIMER_EVENT_GET(reg_val) \
+	(((reg_val)&CSG_ACK_PROGRESS_TIMER_EVENT_MASK) >> CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT)
+#define CSG_ACK_PROGRESS_TIMER_EVENT_SET(reg_val, value) \
+	(((reg_val) & ~CSG_ACK_PROGRESS_TIMER_EVENT_MASK) |  \
+	 (((value) << CSG_ACK_PROGRESS_TIMER_EVENT_SHIFT) & CSG_ACK_PROGRESS_TIMER_EVENT_MASK))
+
+/* CSG_STATUS_EP_CURRENT register */
+#define CSG_STATUS_EP_CURRENT_COMPUTE_EP_SHIFT 0
+#define CSG_STATUS_EP_CURRENT_COMPUTE_EP_MASK (0xFF << CSG_STATUS_EP_CURRENT_COMPUTE_EP_SHIFT)
+#define CSG_STATUS_EP_CURRENT_COMPUTE_EP_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_CURRENT_COMPUTE_EP_MASK) >> CSG_STATUS_EP_CURRENT_COMPUTE_EP_SHIFT)
+#define CSG_STATUS_EP_CURRENT_COMPUTE_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_CURRENT_COMPUTE_EP_MASK) |  \
+	 (((value) << CSG_STATUS_EP_CURRENT_COMPUTE_EP_SHIFT) & CSG_STATUS_EP_CURRENT_COMPUTE_EP_MASK))
+#define CSG_STATUS_EP_CURRENT_FRAGMENT_EP_SHIFT 8
+#define CSG_STATUS_EP_CURRENT_FRAGMENT_EP_MASK (0xFF << CSG_STATUS_EP_CURRENT_FRAGMENT_EP_SHIFT)
+#define CSG_STATUS_EP_CURRENT_FRAGMENT_EP_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_CURRENT_FRAGMENT_EP_MASK) >> CSG_STATUS_EP_CURRENT_FRAGMENT_EP_SHIFT)
+#define CSG_STATUS_EP_CURRENT_FRAGMENT_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_CURRENT_FRAGMENT_EP_MASK) |  \
+	 (((value) << CSG_STATUS_EP_CURRENT_FRAGMENT_EP_SHIFT) & CSG_STATUS_EP_CURRENT_FRAGMENT_EP_MASK))
+#define CSG_STATUS_EP_CURRENT_TILER_EP_SHIFT 16
+#define CSG_STATUS_EP_CURRENT_TILER_EP_MASK (0xF << CSG_STATUS_EP_CURRENT_TILER_EP_SHIFT)
+#define CSG_STATUS_EP_CURRENT_TILER_EP_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_CURRENT_TILER_EP_MASK) >> CSG_STATUS_EP_CURRENT_TILER_EP_SHIFT)
+#define CSG_STATUS_EP_CURRENT_TILER_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_CURRENT_TILER_EP_MASK) |  \
+	 (((value) << CSG_STATUS_EP_CURRENT_TILER_EP_SHIFT) & CSG_STATUS_EP_CURRENT_TILER_EP_MASK))
+
+/* CSG_STATUS_EP_REQ register */
+#define CSG_STATUS_EP_REQ_COMPUTE_EP_SHIFT 0
+#define CSG_STATUS_EP_REQ_COMPUTE_EP_MASK (0xFF << CSG_STATUS_EP_REQ_COMPUTE_EP_SHIFT)
+#define CSG_STATUS_EP_REQ_COMPUTE_EP_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_REQ_COMPUTE_EP_MASK) >> CSG_STATUS_EP_REQ_COMPUTE_EP_SHIFT)
+#define CSG_STATUS_EP_REQ_COMPUTE_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_REQ_COMPUTE_EP_MASK) |  \
+	 (((value) << CSG_STATUS_EP_REQ_COMPUTE_EP_SHIFT) & CSG_STATUS_EP_REQ_COMPUTE_EP_MASK))
+#define CSG_STATUS_EP_REQ_FRAGMENT_EP_SHIFT 8
+#define CSG_STATUS_EP_REQ_FRAGMENT_EP_MASK (0xFF << CSG_STATUS_EP_REQ_FRAGMENT_EP_SHIFT)
+#define CSG_STATUS_EP_REQ_FRAGMENT_EP_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_REQ_FRAGMENT_EP_MASK) >> CSG_STATUS_EP_REQ_FRAGMENT_EP_SHIFT)
+#define CSG_STATUS_EP_REQ_FRAGMENT_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_REQ_FRAGMENT_EP_MASK) |  \
+	 (((value) << CSG_STATUS_EP_REQ_FRAGMENT_EP_SHIFT) & CSG_STATUS_EP_REQ_FRAGMENT_EP_MASK))
+#define CSG_STATUS_EP_REQ_TILER_EP_SHIFT 16
+#define CSG_STATUS_EP_REQ_TILER_EP_MASK (0xF << CSG_STATUS_EP_REQ_TILER_EP_SHIFT)
+#define CSG_STATUS_EP_REQ_TILER_EP_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_REQ_TILER_EP_MASK) >> CSG_STATUS_EP_REQ_TILER_EP_SHIFT)
+#define CSG_STATUS_EP_REQ_TILER_EP_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_REQ_TILER_EP_MASK) |  \
+	 (((value) << CSG_STATUS_EP_REQ_TILER_EP_SHIFT) & CSG_STATUS_EP_REQ_TILER_EP_MASK))
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT 20
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_MASK (0x1 << CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT)
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_MASK) >> CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT)
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_MASK) |  \
+	 (((value) << CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_SHIFT) & CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_MASK))
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT 21
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_MASK (0x1 << CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT)
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_GET(reg_val) \
+	(((reg_val)&CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_MASK) >> CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT)
+#define CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_SET(reg_val, value) \
+	(((reg_val) & ~CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_MASK) |  \
+	 (((value) << CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_SHIFT) & CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_MASK))
+
+/* End of CSG_OUTPUT_BLOCK register set definitions */
+
+/* STREAM_CONTROL_BLOCK register set definitions */
+
+/* STREAM_FEATURES register */
+#define STREAM_FEATURES_WORK_REGISTERS_SHIFT 0
+#define STREAM_FEATURES_WORK_REGISTERS_MASK (0xFF << STREAM_FEATURES_WORK_REGISTERS_SHIFT)
+#define STREAM_FEATURES_WORK_REGISTERS_GET(reg_val) \
+	(((reg_val)&STREAM_FEATURES_WORK_REGISTERS_MASK) >> STREAM_FEATURES_WORK_REGISTERS_SHIFT)
+#define STREAM_FEATURES_WORK_REGISTERS_SET(reg_val, value) \
+	(((reg_val) & ~STREAM_FEATURES_WORK_REGISTERS_MASK) |  \
+	 (((value) << STREAM_FEATURES_WORK_REGISTERS_SHIFT) & STREAM_FEATURES_WORK_REGISTERS_MASK))
+#define STREAM_FEATURES_SCOREBOARDS_SHIFT 8
+#define STREAM_FEATURES_SCOREBOARDS_MASK (0xFF << STREAM_FEATURES_SCOREBOARDS_SHIFT)
+#define STREAM_FEATURES_SCOREBOARDS_GET(reg_val) \
+	(((reg_val)&STREAM_FEATURES_SCOREBOARDS_MASK) >> STREAM_FEATURES_SCOREBOARDS_SHIFT)
+#define STREAM_FEATURES_SCOREBOARDS_SET(reg_val, value) \
+	(((reg_val) & ~STREAM_FEATURES_SCOREBOARDS_MASK) |  \
+	 (((value) << STREAM_FEATURES_SCOREBOARDS_SHIFT) & STREAM_FEATURES_SCOREBOARDS_MASK))
+#define STREAM_FEATURES_COMPUTE_SHIFT 16
+#define STREAM_FEATURES_COMPUTE_MASK (0x1 << STREAM_FEATURES_COMPUTE_SHIFT)
+#define STREAM_FEATURES_COMPUTE_GET(reg_val) (((reg_val)&STREAM_FEATURES_COMPUTE_MASK) >> STREAM_FEATURES_COMPUTE_SHIFT)
+#define STREAM_FEATURES_COMPUTE_SET(reg_val, value) \
+	(((reg_val) & ~STREAM_FEATURES_COMPUTE_MASK) |  \
+	 (((value) << STREAM_FEATURES_COMPUTE_SHIFT) & STREAM_FEATURES_COMPUTE_MASK))
+#define STREAM_FEATURES_FRAGMENT_SHIFT 17
+#define STREAM_FEATURES_FRAGMENT_MASK (0x1 << STREAM_FEATURES_FRAGMENT_SHIFT)
+#define STREAM_FEATURES_FRAGMENT_GET(reg_val) \
+	(((reg_val)&STREAM_FEATURES_FRAGMENT_MASK) >> STREAM_FEATURES_FRAGMENT_SHIFT)
+#define STREAM_FEATURES_FRAGMENT_SET(reg_val, value) \
+	(((reg_val) & ~STREAM_FEATURES_FRAGMENT_MASK) |  \
+	 (((value) << STREAM_FEATURES_FRAGMENT_SHIFT) & STREAM_FEATURES_FRAGMENT_MASK))
+#define STREAM_FEATURES_TILER_SHIFT 18
+#define STREAM_FEATURES_TILER_MASK (0x1 << STREAM_FEATURES_TILER_SHIFT)
+#define STREAM_FEATURES_TILER_GET(reg_val) (((reg_val)&STREAM_FEATURES_TILER_MASK) >> STREAM_FEATURES_TILER_SHIFT)
+#define STREAM_FEATURES_TILER_SET(reg_val, value) \
+	(((reg_val) & ~STREAM_FEATURES_TILER_MASK) |  \
+	 (((value) << STREAM_FEATURES_TILER_SHIFT) & STREAM_FEATURES_TILER_MASK))
+
+/* STREAM_INPUT_VA register */
+#define STREAM_INPUT_VA_VALUE_SHIFT 0
+#define STREAM_INPUT_VA_VALUE_MASK (0xFFFFFFFF << STREAM_INPUT_VA_VALUE_SHIFT)
+#define STREAM_INPUT_VA_VALUE_GET(reg_val) (((reg_val)&STREAM_INPUT_VA_VALUE_MASK) >> STREAM_INPUT_VA_VALUE_SHIFT)
+#define STREAM_INPUT_VA_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~STREAM_INPUT_VA_VALUE_MASK) |  \
+	 (((value) << STREAM_INPUT_VA_VALUE_SHIFT) & STREAM_INPUT_VA_VALUE_MASK))
+
+/* STREAM_OUTPUT_VA register */
+#define STREAM_OUTPUT_VA_VALUE_SHIFT 0
+#define STREAM_OUTPUT_VA_VALUE_MASK (0xFFFFFFFF << STREAM_OUTPUT_VA_VALUE_SHIFT)
+#define STREAM_OUTPUT_VA_VALUE_GET(reg_val) (((reg_val)&STREAM_OUTPUT_VA_VALUE_MASK) >> STREAM_OUTPUT_VA_VALUE_SHIFT)
+#define STREAM_OUTPUT_VA_VALUE_SET(reg_val, value) \
+	(((reg_val) & ~STREAM_OUTPUT_VA_VALUE_MASK) |  \
+	 (((value) << STREAM_OUTPUT_VA_VALUE_SHIFT) & STREAM_OUTPUT_VA_VALUE_MASK))
+/* End of STREAM_CONTROL_BLOCK register set definitions */
+
+/* GLB_INPUT_BLOCK register set definitions */
+
+/* GLB_REQ register */
+#define GLB_REQ_HALT_SHIFT 0
+#define GLB_REQ_HALT_MASK (0x1 << GLB_REQ_HALT_SHIFT)
+#define GLB_REQ_HALT_GET(reg_val) (((reg_val)&GLB_REQ_HALT_MASK) >> GLB_REQ_HALT_SHIFT)
+#define GLB_REQ_HALT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_HALT_MASK) | (((value) << GLB_REQ_HALT_SHIFT) & GLB_REQ_HALT_MASK))
+#define GLB_REQ_CFG_PROGRESS_TIMER_SHIFT 1
+#define GLB_REQ_CFG_PROGRESS_TIMER_MASK (0x1 << GLB_REQ_CFG_PROGRESS_TIMER_SHIFT)
+#define GLB_REQ_CFG_PROGRESS_TIMER_GET(reg_val) \
+	(((reg_val)&GLB_REQ_CFG_PROGRESS_TIMER_MASK) >> GLB_REQ_CFG_PROGRESS_TIMER_SHIFT)
+#define GLB_REQ_CFG_PROGRESS_TIMER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_CFG_PROGRESS_TIMER_MASK) |  \
+	 (((value) << GLB_REQ_CFG_PROGRESS_TIMER_SHIFT) & GLB_REQ_CFG_PROGRESS_TIMER_MASK))
+#define GLB_REQ_CFG_ALLOC_EN_SHIFT 2
+#define GLB_REQ_CFG_ALLOC_EN_MASK (0x1 << GLB_REQ_CFG_ALLOC_EN_SHIFT)
+#define GLB_REQ_CFG_ALLOC_EN_GET(reg_val) (((reg_val)&GLB_REQ_CFG_ALLOC_EN_MASK) >> GLB_REQ_CFG_ALLOC_EN_SHIFT)
+#define GLB_REQ_CFG_ALLOC_EN_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_CFG_ALLOC_EN_MASK) | (((value) << GLB_REQ_CFG_ALLOC_EN_SHIFT) & GLB_REQ_CFG_ALLOC_EN_MASK))
+#define GLB_REQ_CFG_PWROFF_TIMER_SHIFT 3
+#define GLB_REQ_CFG_PWROFF_TIMER_MASK (0x1 << GLB_REQ_CFG_PWROFF_TIMER_SHIFT)
+#define GLB_REQ_CFG_PWROFF_TIMER_GET(reg_val) \
+	(((reg_val)&GLB_REQ_CFG_PWROFF_TIMER_MASK) >> GLB_REQ_CFG_PWROFF_TIMER_SHIFT)
+#define GLB_REQ_CFG_PWROFF_TIMER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_CFG_PWROFF_TIMER_MASK) |  \
+	 (((value) << GLB_REQ_CFG_PWROFF_TIMER_SHIFT) & GLB_REQ_CFG_PWROFF_TIMER_MASK))
+#define GLB_REQ_PROTM_ENTER_SHIFT 4
+#define GLB_REQ_PROTM_ENTER_MASK (0x1 << GLB_REQ_PROTM_ENTER_SHIFT)
+#define GLB_REQ_PROTM_ENTER_GET(reg_val) (((reg_val)&GLB_REQ_PROTM_ENTER_MASK) >> GLB_REQ_PROTM_ENTER_SHIFT)
+#define GLB_REQ_PROTM_ENTER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_PROTM_ENTER_MASK) | (((value) << GLB_REQ_PROTM_ENTER_SHIFT) & GLB_REQ_PROTM_ENTER_MASK))
+#define GLB_REQ_PRFCNT_ENABLE_SHIFT 5
+#define GLB_REQ_PRFCNT_ENABLE_MASK (0x1 << GLB_REQ_PRFCNT_ENABLE_SHIFT)
+#define GLB_REQ_PRFCNT_ENABLE_GET(reg_val) (((reg_val)&GLB_REQ_PRFCNT_ENABLE_MASK) >> GLB_REQ_PRFCNT_ENABLE_SHIFT)
+#define GLB_REQ_PRFCNT_ENABLE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_PRFCNT_ENABLE_MASK) |  \
+	 (((value) << GLB_REQ_PRFCNT_ENABLE_SHIFT) & GLB_REQ_PRFCNT_ENABLE_MASK))
+#define GLB_REQ_PRFCNT_SAMPLE_SHIFT 6
+#define GLB_REQ_PRFCNT_SAMPLE_MASK (0x1 << GLB_REQ_PRFCNT_SAMPLE_SHIFT)
+#define GLB_REQ_PRFCNT_SAMPLE_GET(reg_val) (((reg_val)&GLB_REQ_PRFCNT_SAMPLE_MASK) >> GLB_REQ_PRFCNT_SAMPLE_SHIFT)
+#define GLB_REQ_PRFCNT_SAMPLE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_PRFCNT_SAMPLE_MASK) |  \
+	 (((value) << GLB_REQ_PRFCNT_SAMPLE_SHIFT) & GLB_REQ_PRFCNT_SAMPLE_MASK))
+#define GLB_REQ_COUNTER_ENABLE_SHIFT 7
+#define GLB_REQ_COUNTER_ENABLE_MASK (0x1 << GLB_REQ_COUNTER_ENABLE_SHIFT)
+#define GLB_REQ_COUNTER_ENABLE_GET(reg_val) (((reg_val)&GLB_REQ_COUNTER_ENABLE_MASK) >> GLB_REQ_COUNTER_ENABLE_SHIFT)
+#define GLB_REQ_COUNTER_ENABLE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_COUNTER_ENABLE_MASK) |  \
+	 (((value) << GLB_REQ_COUNTER_ENABLE_SHIFT) & GLB_REQ_COUNTER_ENABLE_MASK))
+#define GLB_REQ_PING_SHIFT 8
+#define GLB_REQ_PING_MASK (0x1 << GLB_REQ_PING_SHIFT)
+#define GLB_REQ_PING_GET(reg_val) (((reg_val)&GLB_REQ_PING_MASK) >> GLB_REQ_PING_SHIFT)
+#define GLB_REQ_PING_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_PING_MASK) | (((value) << GLB_REQ_PING_SHIFT) & GLB_REQ_PING_MASK))
+#define GLB_REQ_INACTIVE_COMPUTE_SHIFT 20
+#define GLB_REQ_INACTIVE_COMPUTE_MASK (0x1 << GLB_REQ_INACTIVE_COMPUTE_SHIFT)
+#define GLB_REQ_INACTIVE_COMPUTE_GET(reg_val) \
+	(((reg_val)&GLB_REQ_INACTIVE_COMPUTE_MASK) >> GLB_REQ_INACTIVE_COMPUTE_SHIFT)
+#define GLB_REQ_INACTIVE_COMPUTE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_INACTIVE_COMPUTE_MASK) |  \
+	 (((value) << GLB_REQ_INACTIVE_COMPUTE_SHIFT) & GLB_REQ_INACTIVE_COMPUTE_MASK))
+#define GLB_REQ_INACTIVE_FRAGMENT_SHIFT 21
+#define GLB_REQ_INACTIVE_FRAGMENT_MASK (0x1 << GLB_REQ_INACTIVE_FRAGMENT_SHIFT)
+#define GLB_REQ_INACTIVE_FRAGMENT_GET(reg_val) \
+	(((reg_val)&GLB_REQ_INACTIVE_FRAGMENT_MASK) >> GLB_REQ_INACTIVE_FRAGMENT_SHIFT)
+#define GLB_REQ_INACTIVE_FRAGMENT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_INACTIVE_FRAGMENT_MASK) |  \
+	 (((value) << GLB_REQ_INACTIVE_FRAGMENT_SHIFT) & GLB_REQ_INACTIVE_FRAGMENT_MASK))
+#define GLB_REQ_INACTIVE_TILER_SHIFT 22
+#define GLB_REQ_INACTIVE_TILER_MASK (0x1 << GLB_REQ_INACTIVE_TILER_SHIFT)
+#define GLB_REQ_INACTIVE_TILER_GET(reg_val) (((reg_val)&GLB_REQ_INACTIVE_TILER_MASK) >> GLB_REQ_INACTIVE_TILER_SHIFT)
+#define GLB_REQ_INACTIVE_TILER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_INACTIVE_TILER_MASK) |  \
+	 (((value) << GLB_REQ_INACTIVE_TILER_SHIFT) & GLB_REQ_INACTIVE_TILER_MASK))
+#define GLB_REQ_PROTM_EXIT_SHIFT 23
+#define GLB_REQ_PROTM_EXIT_MASK (0x1 << GLB_REQ_PROTM_EXIT_SHIFT)
+#define GLB_REQ_PROTM_EXIT_GET(reg_val) (((reg_val)&GLB_REQ_PROTM_EXIT_MASK) >> GLB_REQ_PROTM_EXIT_SHIFT)
+#define GLB_REQ_PROTM_EXIT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_PROTM_EXIT_MASK) | (((value) << GLB_REQ_PROTM_EXIT_SHIFT) & GLB_REQ_PROTM_EXIT_MASK))
+#define GLB_REQ_DEBUG_CSF_REQ_SHIFT 30
+#define GLB_REQ_DEBUG_CSF_REQ_MASK (0x1 << GLB_REQ_DEBUG_CSF_REQ_SHIFT)
+#define GLB_REQ_DEBUG_CSF_REQ_GET(reg_val) (((reg_val)&GLB_REQ_DEBUG_CSF_REQ_MASK) >> GLB_REQ_DEBUG_CSF_REQ_SHIFT)
+#define GLB_REQ_DEBUG_CSF_REQ_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_DEBUG_CSF_REQ_MASK) |  \
+	 (((value) << GLB_REQ_DEBUG_CSF_REQ_SHIFT) & GLB_REQ_DEBUG_CSF_REQ_MASK))
+#define GLB_REQ_DEBUG_HOST_REQ_SHIFT 31
+#define GLB_REQ_DEBUG_HOST_REQ_MASK (0x1 << GLB_REQ_DEBUG_HOST_REQ_SHIFT)
+#define GLB_REQ_DEBUG_HOST_REQ_GET(reg_val) (((reg_val)&GLB_REQ_DEBUG_HOST_REQ_MASK) >> GLB_REQ_DEBUG_HOST_REQ_SHIFT)
+#define GLB_REQ_DEBUG_HOST_REQ_SET(reg_val, value) \
+	(((reg_val) & ~GLB_REQ_DEBUG_HOST_REQ_MASK) |  \
+	 (((value) << GLB_REQ_DEBUG_HOST_REQ_SHIFT) & GLB_REQ_DEBUG_HOST_REQ_MASK))
+
+/* GLB_ACK_IRQ_MASK register */
+#define GLB_ACK_IRQ_MASK_HALT_SHIFT 0
+#define GLB_ACK_IRQ_MASK_HALT_MASK (0x1 << GLB_ACK_IRQ_MASK_HALT_SHIFT)
+#define GLB_ACK_IRQ_MASK_HALT_GET(reg_val) (((reg_val)&GLB_ACK_IRQ_MASK_HALT_MASK) >> GLB_ACK_IRQ_MASK_HALT_SHIFT)
+#define GLB_ACK_IRQ_MASK_HALT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_HALT_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_HALT_SHIFT) & GLB_ACK_IRQ_MASK_HALT_MASK))
+#define GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_SHIFT 1
+#define GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_MASK (0x1 << GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_SHIFT)
+#define GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_MASK) >> GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_SHIFT)
+#define GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_SHIFT) & GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_MASK))
+#define GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_SHIFT 2
+#define GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_MASK (0x1 << GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_SHIFT)
+#define GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_MASK) >> GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_SHIFT)
+#define GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_SHIFT) & GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_MASK))
+#define GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_SHIFT 3
+#define GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_MASK (0x1 << GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_SHIFT)
+#define GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_MASK) >> GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_SHIFT)
+#define GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_SHIFT) & GLB_ACK_IRQ_MASK_CFG_PWROFF_TIMER_MASK))
+#define GLB_ACK_IRQ_MASK_PROTM_ENTER_SHIFT 4
+#define GLB_ACK_IRQ_MASK_PROTM_ENTER_MASK (0x1 << GLB_ACK_IRQ_MASK_PROTM_ENTER_SHIFT)
+#define GLB_ACK_IRQ_MASK_PROTM_ENTER_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_PROTM_ENTER_MASK) >> GLB_ACK_IRQ_MASK_PROTM_ENTER_SHIFT)
+#define GLB_ACK_IRQ_MASK_PROTM_ENTER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_PROTM_ENTER_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_PROTM_ENTER_SHIFT) & GLB_ACK_IRQ_MASK_PROTM_ENTER_MASK))
+#define GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_SHIFT 5
+#define GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_MASK (0x1 << GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_SHIFT)
+#define GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_MASK) >> GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_SHIFT)
+#define GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_SHIFT) & GLB_ACK_IRQ_MASK_PRFCNT_ENABLE_MASK))
+#define GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_SHIFT 6
+#define GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_MASK (0x1 << GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_SHIFT)
+#define GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_MASK) >> GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_SHIFT)
+#define GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_SHIFT) & GLB_ACK_IRQ_MASK_PRFCNT_SAMPLE_MASK))
+#define GLB_ACK_IRQ_MASK_COUNTER_ENABLE_SHIFT 7
+#define GLB_ACK_IRQ_MASK_COUNTER_ENABLE_MASK (0x1 << GLB_ACK_IRQ_MASK_COUNTER_ENABLE_SHIFT)
+#define GLB_ACK_IRQ_MASK_COUNTER_ENABLE_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_COUNTER_ENABLE_MASK) >> GLB_ACK_IRQ_MASK_COUNTER_ENABLE_SHIFT)
+#define GLB_ACK_IRQ_MASK_COUNTER_ENABLE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_COUNTER_ENABLE_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_COUNTER_ENABLE_SHIFT) & GLB_ACK_IRQ_MASK_COUNTER_ENABLE_MASK))
+#define GLB_ACK_IRQ_MASK_PING_SHIFT 8
+#define GLB_ACK_IRQ_MASK_PING_MASK (0x1 << GLB_ACK_IRQ_MASK_PING_SHIFT)
+#define GLB_ACK_IRQ_MASK_PING_GET(reg_val) (((reg_val)&GLB_ACK_IRQ_MASK_PING_MASK) >> GLB_ACK_IRQ_MASK_PING_SHIFT)
+#define GLB_ACK_IRQ_MASK_PING_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_PING_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_PING_SHIFT) & GLB_ACK_IRQ_MASK_PING_MASK))
+#define GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_SHIFT 20
+#define GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_MASK (0x1 << GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_SHIFT)
+#define GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_MASK) >> GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_SHIFT)
+#define GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_SHIFT) & GLB_ACK_IRQ_MASK_INACTIVE_COMPUTE_MASK))
+#define GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_SHIFT 21
+#define GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_MASK (0x1 << GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_SHIFT)
+#define GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_MASK) >> GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_SHIFT)
+#define GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_SHIFT) & GLB_ACK_IRQ_MASK_INACTIVE_FRAGMENT_MASK))
+#define GLB_ACK_IRQ_MASK_INACTIVE_TILER_SHIFT 22
+#define GLB_ACK_IRQ_MASK_INACTIVE_TILER_MASK (0x1 << GLB_ACK_IRQ_MASK_INACTIVE_TILER_SHIFT)
+#define GLB_ACK_IRQ_MASK_INACTIVE_TILER_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_INACTIVE_TILER_MASK) >> GLB_ACK_IRQ_MASK_INACTIVE_TILER_SHIFT)
+#define GLB_ACK_IRQ_MASK_INACTIVE_TILER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_INACTIVE_TILER_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_INACTIVE_TILER_SHIFT) & GLB_ACK_IRQ_MASK_INACTIVE_TILER_MASK))
+#define GLB_ACK_IRQ_MASK_PROTM_EXIT_SHIFT 23
+#define GLB_ACK_IRQ_MASK_PROTM_EXIT_MASK (0x1 << GLB_ACK_IRQ_MASK_PROTM_EXIT_SHIFT)
+#define GLB_ACK_IRQ_MASK_PROTM_EXIT_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_PROTM_EXIT_MASK) >> GLB_ACK_IRQ_MASK_PROTM_EXIT_SHIFT)
+#define GLB_ACK_IRQ_MASK_PROTM_EXIT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_PROTM_EXIT_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_PROTM_EXIT_SHIFT) & GLB_ACK_IRQ_MASK_PROTM_EXIT_MASK))
+#define GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_SHIFT 30
+#define GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_MASK (0x1 << GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_SHIFT)
+#define GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_MASK) >> GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_SHIFT)
+#define GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_SHIFT) & GLB_ACK_IRQ_MASK_DEBUG_CSF_REQ_MASK))
+#define GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_SHIFT 31
+#define GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_MASK (0x1 << GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_SHIFT)
+#define GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_GET(reg_val) \
+	(((reg_val)&GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_MASK) >> GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_SHIFT)
+#define GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_MASK) |  \
+	 (((value) << GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_SHIFT) & GLB_ACK_IRQ_MASK_DEBUG_HOST_REQ_MASK))
+
+/* GLB_PROGRESS_TIMER register */
+#define GLB_PROGRESS_TIMER_TIMEOUT_SHIFT 0
+#define GLB_PROGRESS_TIMER_TIMEOUT_MASK (0xFFFFFFFF << GLB_PROGRESS_TIMER_TIMEOUT_SHIFT)
+#define GLB_PROGRESS_TIMER_TIMEOUT_GET(reg_val) \
+	(((reg_val)&GLB_PROGRESS_TIMER_TIMEOUT_MASK) >> GLB_PROGRESS_TIMER_TIMEOUT_SHIFT)
+#define GLB_PROGRESS_TIMER_TIMEOUT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_PROGRESS_TIMER_TIMEOUT_MASK) |  \
+	 (((value) << GLB_PROGRESS_TIMER_TIMEOUT_SHIFT) & GLB_PROGRESS_TIMER_TIMEOUT_MASK))
+
+/* GLB_ALLOC_EN register */
+#define GLB_ALLOC_EN_MASK_SHIFT 0
+#define GLB_ALLOC_EN_MASK_MASK (0xFFFFFFFFFFFFFFFF << GLB_ALLOC_EN_MASK_SHIFT)
+#define GLB_ALLOC_EN_MASK_GET(reg_val) (((reg_val)&GLB_ALLOC_EN_MASK_MASK) >> GLB_ALLOC_EN_MASK_SHIFT)
+#define GLB_ALLOC_EN_MASK_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ALLOC_EN_MASK_MASK) | (((value) << GLB_ALLOC_EN_MASK_SHIFT) & GLB_ALLOC_EN_MASK_MASK))
+
+/* GLB_PROTM_COHERENCY register */
+#define GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_SHIFT 0
+#define GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_MASK \
+	(0xFFFFFFFF << GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_SHIFT)
+#define GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_GET(reg_val)     \
+	(((reg_val)&GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_MASK) >> \
+	 GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_SHIFT)
+#define GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_SET(reg_val, value) \
+	(((reg_val) & ~GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_MASK) |  \
+	 (((value) << GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_SHIFT) &  \
+	  GLB_PROTM_COHERENCY_L2_CACHE_PROTOCOL_SELECT_MASK))
+/* End of GLB_INPUT_BLOCK register set definitions */
+
+/* GLB_OUTPUT_BLOCK register set definitions */
+
+/* GLB_ACK register */
+#define GLB_ACK_CFG_PROGRESS_TIMER_SHIFT 1
+#define GLB_ACK_CFG_PROGRESS_TIMER_MASK (0x1 << GLB_ACK_CFG_PROGRESS_TIMER_SHIFT)
+#define GLB_ACK_CFG_PROGRESS_TIMER_GET(reg_val) \
+	(((reg_val)&GLB_ACK_CFG_PROGRESS_TIMER_MASK) >> GLB_ACK_CFG_PROGRESS_TIMER_SHIFT)
+#define GLB_ACK_CFG_PROGRESS_TIMER_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_CFG_PROGRESS_TIMER_MASK) |  \
+	 (((value) << GLB_ACK_CFG_PROGRESS_TIMER_SHIFT) & GLB_ACK_CFG_PROGRESS_TIMER_MASK))
+#define GLB_ACK_CFG_ALLOC_EN_SHIFT 2
+#define GLB_ACK_CFG_ALLOC_EN_MASK (0x1 << GLB_ACK_CFG_ALLOC_EN_SHIFT)
+#define GLB_ACK_CFG_ALLOC_EN_GET(reg_val) (((reg_val)&GLB_ACK_CFG_ALLOC_EN_MASK) >> GLB_ACK_CFG_ALLOC_EN_SHIFT)
+#define GLB_ACK_CFG_ALLOC_EN_SET(reg_val, value) \
+	(((reg_val) & ~GLB_ACK_CFG_ALLOC_EN_MASK) | (((value) << GLB_ACK_CFG_ALLOC_EN_SHIFT) & GLB_ACK_CFG_ALLOC_EN_MASK))
+/* End of GLB_OUTPUT_BLOCK register set definitions */
+
+#endif /* _GPU_CSF_REGISTERS_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c
new file mode 100644
index 000000000000..83d7513e78d9
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.c
@@ -0,0 +1,2547 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <gpu/mali_kbase_gpu_fault.h>
+#include <mali_kbase_ctx_sched.h>
+#include <mali_kbase_reset_gpu.h>
+#include "mali_kbase_csf.h"
+#include "backend/gpu/mali_kbase_pm_internal.h"
+#include <linux/export.h>
+#include <linux/shmem_fs.h>
+#include "mali_gpu_csf_registers.h"
+#include "mali_kbase_csf_tiler_heap.h"
+#include <mmu/mali_kbase_mmu.h>
+#include <mali_kbase_ctx_sched.h>
+
+#define CS_REQ_EXCEPTION_MASK (CS_REQ_FAULT_MASK | CS_REQ_FATAL_MASK)
+#define CS_ACK_EXCEPTION_MASK (CS_ACK_FAULT_MASK | CS_ACK_FATAL_MASK)
+
+/**
+ * struct kbase_csf_event - CSF event callback.
+ *
+ * This structure belongs to the list of events which is part of a Kbase
+ * context, and describes a callback function with a custom parameter to pass
+ * to it when a CSF event is signalled.
+ *
+ * @link:      Link to the rest of the list.
+ * @kctx:      Pointer to the Kbase context this event belongs to.
+ * @callback:  Callback function to call when a CSF event is signalled.
+ * @param:     Parameter to pass to the callback function.
+ */
+struct kbase_csf_event {
+	struct list_head link;
+	struct kbase_context *kctx;
+	kbase_csf_event_callback *callback;
+	void *param;
+};
+
+static void put_user_pages_mmap_handle(struct kbase_context *kctx,
+			struct kbase_queue *queue)
+{
+	unsigned long cookie_nr;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	if (queue->handle == BASEP_MEM_INVALID_HANDLE)
+		return;
+
+	cookie_nr =
+		PFN_DOWN(queue->handle - BASEP_MEM_CSF_USER_IO_PAGES_HANDLE);
+
+	if (!WARN_ON(kctx->csf.user_pages_info[cookie_nr] != queue)) {
+		/* free up cookie */
+		kctx->csf.user_pages_info[cookie_nr] = NULL;
+		bitmap_set(kctx->csf.cookies, cookie_nr, 1);
+	}
+
+	queue->handle = BASEP_MEM_INVALID_HANDLE;
+}
+
+/* Reserve a cookie, to be returned as a handle to userspace for creating
+ * the CPU mapping of the pair of input/output pages and Hw doorbell page.
+ * Will return 0 in case of success otherwise negative on failure.
+ */
+static int get_user_pages_mmap_handle(struct kbase_context *kctx,
+			struct kbase_queue *queue)
+{
+	unsigned long cookie, cookie_nr;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	if (bitmap_empty(kctx->csf.cookies,
+				KBASE_CSF_NUM_USER_IO_PAGES_HANDLE)) {
+		dev_err(kctx->kbdev->dev,
+			"No csf cookies available for allocation!");
+		return -ENOMEM;
+	}
+
+	/* allocate a cookie */
+	cookie_nr = find_first_bit(kctx->csf.cookies,
+				KBASE_CSF_NUM_USER_IO_PAGES_HANDLE);
+	if (kctx->csf.user_pages_info[cookie_nr]) {
+		dev_err(kctx->kbdev->dev,
+			"Inconsistent state of csf cookies!");
+		return -EINVAL;
+	}
+	kctx->csf.user_pages_info[cookie_nr] = queue;
+	bitmap_clear(kctx->csf.cookies, cookie_nr, 1);
+
+	/* relocate to correct base */
+	cookie = cookie_nr + PFN_DOWN(BASEP_MEM_CSF_USER_IO_PAGES_HANDLE);
+	cookie <<= PAGE_SHIFT;
+
+	queue->handle = (u64)cookie;
+
+	return 0;
+}
+
+static void gpu_munmap_user_io_pages(struct kbase_context *kctx,
+			struct kbase_va_region *reg)
+{
+	size_t num_pages = 2;
+
+	kbase_mmu_teardown_pages(kctx->kbdev, &kctx->kbdev->csf.mcu_mmu,
+				 reg->start_pfn, num_pages, MCU_AS_NR);
+
+	WARN_ON(reg->flags & KBASE_REG_FREE);
+
+	mutex_lock(&kctx->kbdev->csf.reg_lock);
+	kbase_remove_va_region(reg);
+	mutex_unlock(&kctx->kbdev->csf.reg_lock);
+}
+
+static void init_user_output_page(struct kbase_queue *queue)
+{
+	u32 *addr = (u32 *)(queue->user_io_addr + PAGE_SIZE);
+
+	addr[CS_EXTRACT_LO/4] = 0;
+	addr[CS_EXTRACT_HI/4] = 0;
+
+	addr[CS_ACTIVE/4] = 0;
+}
+
+/* Map the input/output pages in the shared interface segment of MCU firmware
+ * address space.
+ */
+static int gpu_mmap_user_io_pages(struct kbase_device *kbdev,
+		struct tagged_addr *phys, struct kbase_va_region *reg)
+{
+	unsigned long mem_flags = KBASE_REG_GPU_RD;
+	const size_t num_pages = 2;
+	int ret;
+
+#if ((KERNEL_VERSION(4, 4, 147) >= LINUX_VERSION_CODE) || \
+		((KERNEL_VERSION(4, 6, 0) > LINUX_VERSION_CODE) && \
+		 (KERNEL_VERSION(4, 5, 0) <= LINUX_VERSION_CODE)))
+	mem_flags |=
+		KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_NON_CACHEABLE);
+#else
+	if (kbdev->system_coherency == COHERENCY_NONE) {
+		mem_flags |=
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_NON_CACHEABLE);
+	} else {
+		mem_flags |= KBASE_REG_SHARE_BOTH |
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_SHARED);
+	}
+#endif
+
+	mutex_lock(&kbdev->csf.reg_lock);
+	ret = kbase_add_va_region_rbtree(kbdev, reg, 0, num_pages, 1);
+	reg->flags &= ~KBASE_REG_FREE;
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+	if (ret)
+		return ret;
+
+	/* Map input page */
+	ret = kbase_mmu_insert_pages(kbdev, &kbdev->csf.mcu_mmu,
+				     reg->start_pfn, &phys[0],
+				     1, mem_flags, MCU_AS_NR,
+				     KBASE_MEM_GROUP_CSF_IO);
+	if (ret)
+		goto bad_insert;
+
+	/* Map output page, it needs rw access */
+	mem_flags |= KBASE_REG_GPU_WR;
+	ret = kbase_mmu_insert_pages(kbdev, &kbdev->csf.mcu_mmu,
+				     reg->start_pfn + 1, &phys[1],
+				     1, mem_flags, MCU_AS_NR,
+				     KBASE_MEM_GROUP_CSF_IO);
+	if (ret)
+		goto bad_insert_output_page;
+
+	return 0;
+
+bad_insert_output_page:
+	kbase_mmu_teardown_pages(kbdev, &kbdev->csf.mcu_mmu,
+				 reg->start_pfn, 1, MCU_AS_NR);
+bad_insert:
+	mutex_lock(&kbdev->csf.reg_lock);
+	kbase_remove_va_region(reg);
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+	return ret;
+}
+
+static void kernel_unmap_user_io_pages(struct kbase_context *kctx,
+			struct kbase_queue *queue)
+{
+	const size_t num_pages = 2;
+
+	kbase_gpu_vm_lock(kctx);
+
+	vunmap(queue->user_io_addr);
+
+	WARN_ON(num_pages > atomic_read(&kctx->permanent_mapped_pages));
+	atomic_sub(num_pages, &kctx->permanent_mapped_pages);
+
+	kbase_gpu_vm_unlock(kctx);
+}
+
+static int kernel_map_user_io_pages(struct kbase_context *kctx,
+			struct kbase_queue *queue)
+{
+	struct page *page_list[2];
+	pgprot_t cpu_map_prot;
+	int ret = 0;
+	size_t i;
+
+	kbase_gpu_vm_lock(kctx);
+
+	if (ARRAY_SIZE(page_list) > (KBASE_PERMANENTLY_MAPPED_MEM_LIMIT_PAGES -
+			 atomic_read(&kctx->permanent_mapped_pages))) {
+		ret = -ENOMEM;
+		goto unlock;
+	}
+
+	/* The pages are mapped to Userspace also, so use the same mapping
+	 * attributes as used inside the CPU page fault handler.
+	 */
+#if ((KERNEL_VERSION(4, 4, 147) >= LINUX_VERSION_CODE) || \
+		((KERNEL_VERSION(4, 6, 0) > LINUX_VERSION_CODE) && \
+		 (KERNEL_VERSION(4, 5, 0) <= LINUX_VERSION_CODE)))
+	cpu_map_prot = pgprot_device(PAGE_KERNEL);
+#else
+	if (kctx->kbdev->system_coherency == COHERENCY_NONE)
+		cpu_map_prot = pgprot_writecombine(PAGE_KERNEL);
+	else
+		cpu_map_prot = PAGE_KERNEL;
+#endif
+
+	for (i = 0; i < ARRAY_SIZE(page_list); i++)
+		page_list[i] = as_page(queue->phys[i]);
+
+	queue->user_io_addr = vmap(page_list, ARRAY_SIZE(page_list), VM_MAP, cpu_map_prot);
+
+	if (!queue->user_io_addr)
+		ret = -ENOMEM;
+	else
+		atomic_add(ARRAY_SIZE(page_list), &kctx->permanent_mapped_pages);
+
+unlock:
+	kbase_gpu_vm_unlock(kctx);
+	return ret;
+}
+
+static void get_queue(struct kbase_queue *queue);
+static void release_queue(struct kbase_queue *queue);
+
+/**
+ * kbase_csf_free_command_stream_user_pages() - Free the resources allocated
+ *				    for a queue at the time of bind.
+ *
+ * @kctx:	Address of the kbase context within which the queue was created.
+ * @queue:	Pointer to the queue to be unlinked.
+ *
+ * This function will free the pair of physical pages allocated for a GPU
+ * command queue, and also release the hardware doorbell page, that were mapped
+ * into the process address space to enable direct submission of commands to
+ * the hardware. Also releases the reference taken on the queue when the mapping
+ * was created.
+ *
+ * This function will be called only when the mapping is being removed and
+ * so the resources for queue will not get freed up until the mapping is
+ * removed even though userspace could have terminated the queue.
+ * Kernel will ensure that the termination of Kbase context would only be
+ * triggered after the mapping is removed.
+ *
+ * If an explicit or implicit unbind was missed by the userspace then the
+ * mapping will persist. On process exit kernel itself will remove the mapping.
+ */
+static void kbase_csf_free_command_stream_user_pages(struct kbase_context *kctx,
+		struct kbase_queue *queue)
+{
+	const size_t num_pages = 2;
+
+	gpu_munmap_user_io_pages(kctx, queue->reg);
+	kernel_unmap_user_io_pages(kctx, queue);
+
+	kbase_mem_pool_free_pages(
+		&kctx->mem_pools.small[KBASE_MEM_GROUP_CSF_IO],
+		num_pages, queue->phys, true, false);
+
+	kfree(queue->reg);
+	queue->reg = NULL;
+
+	/* If the queue has already been terminated by userspace
+	 * then the ref count for queue object will drop to 0 here.
+	 */
+	release_queue(queue);
+}
+
+int kbase_csf_alloc_command_stream_user_pages(struct kbase_context *kctx,
+			struct kbase_queue *queue)
+{
+	struct kbase_device *kbdev = kctx->kbdev;
+	struct kbase_va_region *reg;
+	const size_t num_pages = 2;
+	int ret;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	reg = kbase_alloc_free_region(&kctx->kbdev->csf.shared_reg_rbtree, 0,
+				      num_pages, KBASE_REG_ZONE_MCU_SHARED);
+	if (!reg)
+		return -ENOMEM;
+
+	ret = kbase_mem_pool_alloc_pages(
+				&kctx->mem_pools.small[KBASE_MEM_GROUP_CSF_IO],
+				num_pages, queue->phys, false);
+
+	if (ret != num_pages)
+		goto phys_alloc_failed;
+
+	ret = kernel_map_user_io_pages(kctx, queue);
+	if (ret)
+		goto kernel_map_failed;
+
+	init_user_output_page(queue);
+
+	ret = gpu_mmap_user_io_pages(kctx->kbdev, queue->phys, reg);
+	if (ret)
+		goto gpu_mmap_failed;
+
+	queue->reg = reg;
+
+	mutex_lock(&kbdev->csf.reg_lock);
+	if (kbdev->csf.db_file_offsets >
+			(U32_MAX - BASEP_QUEUE_NR_MMAP_USER_PAGES + 1))
+		kbdev->csf.db_file_offsets = 0;
+
+	queue->db_file_offset = kbdev->csf.db_file_offsets;
+	kbdev->csf.db_file_offsets += BASEP_QUEUE_NR_MMAP_USER_PAGES;
+
+	WARN(atomic_read(&queue->refcount) != 1, "Incorrect refcounting for queue object\n");
+	/* This is the second reference taken on the queue object and
+	 * would be dropped only when the IO mapping is removed either
+	 * explicitly by userspace or implicitly by kernel on process exit.
+	 */
+	get_queue(queue);
+	queue->bind_state = KBASE_CSF_QUEUE_BOUND;
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+	return 0;
+
+gpu_mmap_failed:
+	kernel_unmap_user_io_pages(kctx, queue);
+
+kernel_map_failed:
+	kbase_mem_pool_free_pages(
+		&kctx->mem_pools.small[KBASE_MEM_GROUP_CSF_IO],
+		num_pages, queue->phys, false, false);
+
+phys_alloc_failed:
+	kfree(reg);
+
+	return -ENOMEM;
+}
+
+static struct kbase_queue_group *find_queue_group(struct kbase_context *kctx,
+	u8 group_handle)
+{
+	uint index = group_handle;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	if (index < MAX_QUEUE_GROUP_NUM && kctx->csf.queue_groups[index]) {
+		if (WARN_ON(kctx->csf.queue_groups[index]->handle != index))
+			return NULL;
+		return kctx->csf.queue_groups[index];
+	}
+
+	return NULL;
+}
+
+int kbase_csf_queue_group_handle_is_valid(struct kbase_context *kctx,
+	u8 group_handle)
+{
+	struct kbase_queue_group *group;
+
+	mutex_lock(&kctx->csf.lock);
+	group = find_queue_group(kctx, group_handle);
+	mutex_unlock(&kctx->csf.lock);
+
+	return group ? 0 : -EINVAL;
+}
+
+static struct kbase_queue *find_queue(struct kbase_context *kctx, u64 base_addr)
+{
+	struct kbase_queue *queue;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	list_for_each_entry(queue, &kctx->csf.queue_list, link) {
+		if (base_addr == queue->base_addr)
+			return queue;
+	}
+
+	return NULL;
+}
+
+static void get_queue(struct kbase_queue *queue)
+{
+	WARN_ON(!atomic_inc_not_zero(&queue->refcount));
+}
+
+static void release_queue(struct kbase_queue *queue)
+{
+	lockdep_assert_held(&queue->kctx->csf.lock);
+
+	WARN_ON(atomic_read(&queue->refcount) <= 0);
+
+	if (atomic_dec_and_test(&queue->refcount)) {
+		/* The queue can't still be on the per context list. */
+		WARN_ON(!list_empty(&queue->link));
+		WARN_ON(queue->group);
+		kfree(queue);
+	}
+}
+
+static void oom_event_worker(struct work_struct *data);
+static void fault_event_worker(struct work_struct *data);
+
+int kbase_csf_queue_register(struct kbase_context *kctx,
+			     struct kbase_ioctl_cs_queue_register *reg)
+{
+	struct kbase_queue *queue;
+	int ret = 0;
+	struct kbase_va_region *region;
+	u64 queue_addr = reg->buffer_gpu_addr;
+	size_t queue_size = reg->buffer_size >> PAGE_SHIFT;
+
+	/* Validate the queue priority */
+	if (reg->priority > BASE_QUEUE_MAX_PRIORITY)
+		return -EINVAL;
+
+	mutex_lock(&kctx->csf.lock);
+
+	/* Check if queue is already registered */
+	if (find_queue(kctx, queue_addr) != NULL) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* Check if the queue address is valid */
+	kbase_gpu_vm_lock(kctx);
+	region = kbase_region_tracker_find_region_enclosing_address(kctx,
+								    queue_addr);
+
+	if (kbase_is_region_invalid_or_free(region)) {
+		ret = -ENOENT;
+		goto out_unlock_vm;
+	}
+
+	if (queue_size > (region->nr_pages -
+			  ((queue_addr >> PAGE_SHIFT) - region->start_pfn))) {
+		ret = -EINVAL;
+		goto out_unlock_vm;
+	}
+
+	queue = kzalloc(sizeof(struct kbase_queue), GFP_KERNEL);
+
+	if (!queue) {
+		ret = -ENOMEM;
+		goto out_unlock_vm;
+	}
+
+	queue->kctx = kctx;
+	queue->base_addr = queue_addr;
+	queue->queue_reg = region;
+	queue->size = (queue_size << PAGE_SHIFT);
+	queue->csi_index = KBASEP_IF_NR_INVALID;
+	queue->enabled = false;
+
+	queue->priority = reg->priority;
+	atomic_set(&queue->refcount, 1);
+
+	queue->group = NULL;
+	queue->bind_state = KBASE_CSF_QUEUE_UNBOUND;
+	queue->handle = BASEP_MEM_INVALID_HANDLE;
+	queue->doorbell_nr = KBASEP_USER_DB_NR_INVALID;
+
+	queue->status_wait = 0;
+	queue->sync_ptr = 0;
+	queue->sync_value = 0;
+
+	INIT_LIST_HEAD(&queue->link);
+	INIT_LIST_HEAD(&queue->error.link);
+	INIT_WORK(&queue->oom_event_work, oom_event_worker);
+	INIT_WORK(&queue->fault_event_work, fault_event_worker);
+	list_add(&queue->link, &kctx->csf.queue_list);
+
+	region->flags |= KBASE_REG_NO_USER_FREE;
+
+out_unlock_vm:
+	kbase_gpu_vm_unlock(kctx);
+out:
+	mutex_unlock(&kctx->csf.lock);
+
+	return ret;
+}
+
+static void unbind_queue(struct kbase_context *kctx,
+		struct kbase_queue *queue);
+
+void kbase_csf_queue_terminate(struct kbase_context *kctx,
+			      struct kbase_ioctl_cs_queue_terminate *term)
+{
+	struct kbase_queue *queue;
+
+	mutex_lock(&kctx->csf.lock);
+
+	queue = find_queue(kctx, term->buffer_gpu_addr);
+
+	if (queue) {
+		/* As the GPU queue has been terminated by the
+		 * user space, undo the actions that were performed when the
+		 * queue was registered i.e. remove the queue from the per
+		 * context list & release the initial reference. The subsequent
+		 * lookups for the queue in find_queue() would fail.
+		 */
+		list_del_init(&queue->link);
+
+		/* Stop the CSI to which queue was bound */
+		unbind_queue(kctx, queue);
+
+		kbase_gpu_vm_lock(kctx);
+		if (!WARN_ON(!queue->queue_reg)) {
+			/* After this the Userspace would be able to free the
+			 * memory for GPU queue. In case the Userspace missed
+			 * terminating the queue, the cleanup will happen on
+			 * context termination where teardown of region tracker
+			 * would free up the GPU queue memory.
+			 */
+			queue->queue_reg->flags &= ~KBASE_REG_NO_USER_FREE;
+		}
+		kbase_gpu_vm_unlock(kctx);
+
+		/* Remove any pending command queue fatal from
+		 * the per-context list.
+		 */
+		list_del_init(&queue->error.link);
+
+		release_queue(queue);
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+}
+
+int kbase_csf_queue_bind(struct kbase_context *kctx, union kbase_ioctl_cs_queue_bind *bind)
+{
+	struct kbase_queue *queue;
+	struct kbase_queue_group *group;
+	u8 max_streams;
+	int ret = -EINVAL;
+
+	mutex_lock(&kctx->csf.lock);
+
+	group = find_queue_group(kctx, bind->in.group_handle);
+	queue = find_queue(kctx, bind->in.buffer_gpu_addr);
+
+	if (!group || !queue)
+		goto out;
+
+	/* For the time being, all CSGs have the same number of CSs
+	 * so we check CSG 0 for this number
+	 */
+	max_streams = kctx->kbdev->csf.global_iface.groups[0].stream_num;
+
+	if (bind->in.csi_index >= max_streams)
+		goto out;
+
+	if (group->run_state == KBASE_CSF_GROUP_TERMINATED)
+		goto out;
+
+	if (queue->group || group->bound_queues[bind->in.csi_index])
+		goto out;
+
+	ret = get_user_pages_mmap_handle(kctx, queue);
+	if (ret)
+		goto out;
+
+	bind->out.mmap_handle = queue->handle;
+	group->bound_queues[bind->in.csi_index] = queue;
+	queue->group = group;
+	queue->csi_index = bind->in.csi_index;
+	queue->bind_state = KBASE_CSF_QUEUE_BIND_IN_PROGRESS;
+
+out:
+	mutex_unlock(&kctx->csf.lock);
+
+	return ret;
+}
+
+static struct kbase_queue_group *get_bound_queue_group(
+					struct kbase_queue *queue)
+{
+	struct kbase_context *kctx = queue->kctx;
+	struct kbase_queue_group *group;
+
+	if (queue->bind_state == KBASE_CSF_QUEUE_UNBOUND)
+		return NULL;
+
+	if (!queue->group)
+		return NULL;
+
+	if (queue->csi_index == KBASEP_IF_NR_INVALID) {
+		dev_warn(kctx->kbdev->dev, "CS interface index is incorrect\n");
+		return NULL;
+	}
+
+	group = queue->group;
+
+	if (group->bound_queues[queue->csi_index] != queue) {
+		dev_warn(kctx->kbdev->dev, "Incorrect mapping between queues & queue groups\n");
+		return NULL;
+	}
+
+	return group;
+}
+
+void kbase_csf_ring_csg_doorbell(struct kbase_device *kbdev, int slot)
+{
+	if (WARN_ON(slot < 0))
+		return;
+
+	kbase_csf_ring_csg_slots_doorbell(kbdev, (u32) (1 << slot));
+}
+
+void kbase_csf_ring_csg_slots_doorbell(struct kbase_device *kbdev,
+				       u32 slot_bitmap)
+{
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	const u32 allowed_bitmap =
+		(u32) ((1U << kbdev->csf.global_iface.group_num) - 1);
+	u32 value;
+
+	if (WARN_ON(slot_bitmap > allowed_bitmap))
+		return;
+
+	value = kbase_csf_firmware_global_output(global_iface, GLB_DB_ACK);
+	value ^= slot_bitmap;
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_DB_REQ, value,
+					     slot_bitmap);
+
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+}
+
+void kbase_csf_ring_cs_user_doorbell(struct kbase_device *kbdev,
+			struct kbase_queue *queue)
+{
+	mutex_lock(&kbdev->csf.reg_lock);
+
+	if (queue->doorbell_nr != KBASEP_USER_DB_NR_INVALID)
+		kbase_csf_ring_doorbell(kbdev, queue->doorbell_nr);
+
+	mutex_unlock(&kbdev->csf.reg_lock);
+}
+
+void kbase_csf_ring_cs_kernel_doorbell(struct kbase_device *kbdev,
+			struct kbase_queue *queue)
+{
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	struct kbase_queue_group *group = get_bound_queue_group(queue);
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	u32 value;
+	int slot;
+
+	if (WARN_ON(!group))
+		return;
+
+	slot = kbase_csf_scheduler_group_get_slot(group);
+
+	if (WARN_ON(slot < 0))
+		return;
+
+	ginfo = &global_iface->groups[slot];
+
+	value = kbase_csf_firmware_csg_output(ginfo, CSG_DB_ACK);
+	value ^= (1 << queue->csi_index);
+	kbase_csf_firmware_csg_input_mask(ginfo, CSG_DB_REQ, value,
+					  1 << queue->csi_index);
+
+	kbase_csf_ring_csg_doorbell(kbdev, slot);
+}
+
+int kbase_csf_queue_kick(struct kbase_context *kctx,
+			 struct kbase_ioctl_cs_queue_kick *kick)
+{
+	struct kbase_queue_group *group;
+	struct kbase_queue *queue;
+	int err = 0;
+
+	mutex_lock(&kctx->csf.lock);
+
+	queue = find_queue(kctx, kick->buffer_gpu_addr);
+	if (!queue)
+		err = -EINVAL;
+
+	if (!err) {
+		group = get_bound_queue_group(queue);
+		if (!group) {
+			dev_err(kctx->kbdev->dev, "queue not bound\n");
+			err = -EINVAL;
+		}
+	}
+
+	if (!err)
+		err = kbase_csf_scheduler_queue_start(queue);
+
+	mutex_unlock(&kctx->csf.lock);
+	return err;
+}
+
+static void unbind_stopped_queue(struct kbase_context *kctx,
+			struct kbase_queue *queue)
+{
+	lockdep_assert_held(&kctx->csf.lock);
+
+	if (queue->bind_state != KBASE_CSF_QUEUE_UNBOUND) {
+		unsigned long flags;
+
+		kbase_csf_scheduler_spin_lock(kctx->kbdev, &flags);
+		bitmap_clear(queue->group->protm_pending_bitmap,
+				queue->csi_index, 1);
+		queue->group->bound_queues[queue->csi_index] = NULL;
+		queue->group = NULL;
+		kbase_csf_scheduler_spin_unlock(kctx->kbdev, flags);
+
+		put_user_pages_mmap_handle(kctx, queue);
+		queue->bind_state = KBASE_CSF_QUEUE_UNBOUND;
+	}
+}
+/**
+ * unbind_queue() - Remove the linkage between a GPU command queue and the group
+ *		    to which it was bound or being bound.
+ *
+ * @kctx:	Address of the kbase context within which the queue was created.
+ * @queue:	Pointer to the queue to be unlinked.
+ *
+ * This function will also send the stop request to firmware for the command
+ * stream if the group to which the GPU command queue was bound is scheduled.
+ *
+ * This function would be called when :-
+ * - queue is being unbound. This would happen when the IO mapping
+ *   created on bind is removed explicitly by userspace or the process
+ *   is getting exited.
+ * - queue group is being terminated which still has queues bound
+ *   to it. This could happen on an explicit terminate request from userspace
+ *   or when the kbase context is being terminated.
+ * - queue is being terminated without completing the bind operation.
+ *   This could happen if either the queue group is terminated
+ *   after the CS_QUEUE_BIND ioctl but before the 2nd part of bind operation
+ *   to create the IO mapping is initiated.
+ * - There is a failure in executing the 2nd part of bind operation, inside the
+ *   mmap handler, which creates the IO mapping for queue.
+ */
+
+static void unbind_queue(struct kbase_context *kctx, struct kbase_queue *queue)
+{
+	lockdep_assert_held(&kctx->csf.lock);
+
+	if (queue->bind_state != KBASE_CSF_QUEUE_UNBOUND) {
+		if (queue->bind_state == KBASE_CSF_QUEUE_BOUND)
+			kbase_csf_scheduler_queue_stop(queue);
+
+		unbind_stopped_queue(kctx, queue);
+	}
+}
+
+void kbase_csf_queue_unbind(struct kbase_queue *queue)
+{
+	struct kbase_context *kctx = queue->kctx;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	unbind_queue(kctx, queue);
+
+	/* Free the resources, if allocated for this queue. */
+	if (queue->reg)
+		kbase_csf_free_command_stream_user_pages(kctx, queue);
+}
+
+/**
+ * find_free_group_handle() - Find a free handle for a queue group
+ *
+ * @kctx: Address of the kbase context within which the queue group
+ *        is to be created.
+ *
+ * Return: a queue group handle on success, or a negative error code on failure.
+ */
+static int find_free_group_handle(struct kbase_context *const kctx)
+{
+	/* find the available index in the array of CSGs per this context */
+	int idx, group_handle = -ENOMEM;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	for (idx = 0;
+		(idx != MAX_QUEUE_GROUP_NUM) && (group_handle < 0);
+		idx++) {
+		if (!kctx->csf.queue_groups[idx])
+			group_handle = idx;
+	}
+
+	return group_handle;
+}
+
+/**
+ * iface_has_enough_streams() - Check that at least one command stream
+ *				group supports a given number of streams
+ *
+ * @kbdev:	Instance of a GPU platform device that implements a command
+ *		stream front-end interface.
+ * @cs_min:	Minimum number of command streams required.
+ *
+ * Return: true if at least one command stream group supports the given number
+ *         of command streams (or more); otherwise false.
+ */
+static bool iface_has_enough_streams(struct kbase_device *const kbdev,
+	u32 const cs_min)
+{
+	bool has_enough = false;
+	struct kbase_csf_cmd_stream_group_info *const groups =
+		kbdev->csf.global_iface.groups;
+	const u32 group_num = kbdev->csf.global_iface.group_num;
+	u32 i;
+
+	for (i = 0; (i < group_num) && !has_enough; i++) {
+		if (groups[i].stream_num >= cs_min)
+			has_enough = true;
+	}
+
+	return has_enough;
+}
+
+/**
+ * create_normal_suspend_buffer() - Create normal-mode suspend buffer per
+ *					queue group
+ *
+ * @kctx:	Pointer to kbase context where the queue group is created at
+ * @s_buf:	Pointer to suspend buffer that is attached to queue group
+ *
+ * Return: 0 if suspend buffer is successfully allocated and reflected to GPU
+ *         MMU page table. Otherwise -ENOMEM.
+ */
+static int create_normal_suspend_buffer(struct kbase_context *const kctx,
+		struct kbase_normal_suspend_buffer *s_buf)
+{
+	struct kbase_va_region *reg = NULL;
+	const unsigned long mem_flags = KBASE_REG_GPU_RD | KBASE_REG_GPU_WR;
+	const size_t nr_pages =
+		PFN_UP(kctx->kbdev->csf.global_iface.groups[0].suspend_size);
+	int err = 0;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	/* Allocate and initialize Region Object */
+	reg = kbase_alloc_free_region(&kctx->kbdev->csf.shared_reg_rbtree, 0,
+			nr_pages, KBASE_REG_ZONE_MCU_SHARED);
+
+	if (!reg)
+		return -ENOMEM;
+
+	s_buf->phy = kcalloc(nr_pages, sizeof(*s_buf->phy), GFP_KERNEL);
+
+	if (!s_buf->phy) {
+		err = -ENOMEM;
+		goto phy_alloc_failed;
+	}
+
+	/* Get physical page for a normal suspend buffer */
+	err = kbase_mem_pool_alloc_pages(
+			&kctx->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+			nr_pages, &s_buf->phy[0], false);
+
+	if (err < 0)
+		goto phy_pages_alloc_failed;
+
+	/* Insert Region Object into rbtree and make virtual address available
+	 * to map it to physical page
+	 */
+	mutex_lock(&kctx->kbdev->csf.reg_lock);
+	err = kbase_add_va_region_rbtree(kctx->kbdev, reg, 0, nr_pages, 1);
+	reg->flags &= ~KBASE_REG_FREE;
+	mutex_unlock(&kctx->kbdev->csf.reg_lock);
+
+	if (err)
+		goto add_va_region_failed;
+
+	/* Update MMU table */
+	err = kbase_mmu_insert_pages(kctx->kbdev, &kctx->kbdev->csf.mcu_mmu,
+				     reg->start_pfn, &s_buf->phy[0],
+				     nr_pages, mem_flags,
+				     MCU_AS_NR, KBASE_MEM_GROUP_CSF_FW);
+	if (err)
+		goto mmu_insert_failed;
+
+	s_buf->reg = reg;
+
+	return 0;
+
+mmu_insert_failed:
+	mutex_lock(&kctx->kbdev->csf.reg_lock);
+	WARN_ON(kbase_remove_va_region(reg));
+	mutex_unlock(&kctx->kbdev->csf.reg_lock);
+
+add_va_region_failed:
+	kbase_mem_pool_free_pages(
+		&kctx->mem_pools.small[KBASE_MEM_GROUP_CSF_FW], nr_pages,
+		&s_buf->phy[0], false, false);
+
+phy_pages_alloc_failed:
+	kfree(s_buf->phy);
+phy_alloc_failed:
+	kfree(reg);
+
+	return err;
+}
+
+/**
+ * create_protected_suspend_buffer() - Create protected-mode suspend buffer
+ *					per queue group
+ *
+ * @kbdev:	Instance of a GPU platform device that implements a command
+ *		stream front-end interface.
+ * @s_buf:	Pointer to suspend buffer that is attached to queue group
+ *
+ * Return: 0 if suspend buffer is successfully allocated and reflected to GPU
+ *         MMU page table. Otherwise -ENOMEM.
+ */
+static int create_protected_suspend_buffer(struct kbase_device *const kbdev,
+		struct kbase_protected_suspend_buffer *s_buf)
+{
+	struct kbase_va_region *reg = NULL;
+	struct tagged_addr *phys = NULL;
+	const unsigned long mem_flags = KBASE_REG_GPU_RD | KBASE_REG_GPU_WR;
+	const size_t nr_pages =
+		PFN_UP(kbdev->csf.global_iface.groups[0].suspend_size);
+	int err = 0;
+
+	/* Allocate and initialize Region Object */
+	reg = kbase_alloc_free_region(&kbdev->csf.shared_reg_rbtree, 0,
+			nr_pages, KBASE_REG_ZONE_MCU_SHARED);
+
+	if (!reg)
+		return -ENOMEM;
+
+	phys = kcalloc(nr_pages, sizeof(*phys), GFP_KERNEL);
+	if (!phys) {
+		err = -ENOMEM;
+		goto phy_alloc_failed;
+	}
+
+	s_buf->pma = kbase_csf_protected_memory_alloc(kbdev, phys,
+			nr_pages);
+	if (s_buf->pma == NULL) {
+		err = -ENOMEM;
+		goto pma_alloc_failed;
+	}
+
+	/* Insert Region Object into rbtree and make virtual address available
+	 * to map it to physical page
+	 */
+	mutex_lock(&kbdev->csf.reg_lock);
+	err = kbase_add_va_region_rbtree(kbdev, reg, 0, nr_pages, 1);
+	reg->flags &= ~KBASE_REG_FREE;
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+	if (err)
+		goto add_va_region_failed;
+
+	/* Update MMU table */
+	err = kbase_mmu_insert_pages(kbdev, &kbdev->csf.mcu_mmu,
+				     reg->start_pfn, phys,
+				     nr_pages, mem_flags, MCU_AS_NR,
+				     KBASE_MEM_GROUP_CSF_FW);
+	if (err)
+		goto mmu_insert_failed;
+
+	s_buf->reg = reg;
+	kfree(phys);
+	return 0;
+
+mmu_insert_failed:
+	mutex_lock(&kbdev->csf.reg_lock);
+	WARN_ON(kbase_remove_va_region(reg));
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+add_va_region_failed:
+	kbase_csf_protected_memory_free(kbdev, s_buf->pma, nr_pages);
+pma_alloc_failed:
+	kfree(phys);
+phy_alloc_failed:
+	kfree(reg);
+
+	return err;
+}
+
+static void timer_event_worker(struct work_struct *data);
+static void protm_event_worker(struct work_struct *data);
+static void term_normal_suspend_buffer(struct kbase_context *const kctx,
+		struct kbase_normal_suspend_buffer *s_buf);
+
+/**
+ * create_suspend_buffers - Setup normal and protected mode
+ *				suspend buffers.
+ *
+ * @kctx:	Address of the kbase context within which the queue group
+ *		is to be created.
+ * @group:	Pointer to GPU command queue group data.
+ *
+ * Return: 0 if suspend buffers are successfully allocated. Otherwise -ENOMEM.
+ */
+static int create_suspend_buffers(struct kbase_context *const kctx,
+		struct kbase_queue_group * const group)
+{
+	int err = 0;
+
+	if (create_normal_suspend_buffer(kctx, &group->normal_suspend_buf)) {
+		dev_err(kctx->kbdev->dev, "Failed to create normal suspend buffer\n");
+		return -ENOMEM;
+	}
+
+	if (kctx->kbdev->csf.pma_dev) {
+		err = create_protected_suspend_buffer(kctx->kbdev,
+				&group->protected_suspend_buf);
+		if (err) {
+			term_normal_suspend_buffer(kctx,
+					&group->normal_suspend_buf);
+			dev_err(kctx->kbdev->dev, "Failed to create protected suspend buffer\n");
+		}
+	} else {
+		group->protected_suspend_buf.reg = NULL;
+	}
+
+	return err;
+}
+
+/**
+ * create_queue_group() - Create a queue group
+ *
+ * @kctx:	Address of the kbase context within which the queue group
+ *		is to be created.
+ * @create:	Address of a structure which contains details of the
+ *		queue group which is to be created.
+ *
+ * Return: a queue group handle on success, or a negative error code on failure.
+ */
+static int create_queue_group(struct kbase_context *const kctx,
+	const union kbase_ioctl_cs_queue_group_create *const create)
+{
+	int group_handle = find_free_group_handle(kctx);
+
+	if (group_handle < 0) {
+		dev_err(kctx->kbdev->dev,
+			"All queue group handles are already in use\n");
+	} else {
+		struct kbase_queue_group * const group =
+			kmalloc(sizeof(struct kbase_queue_group),
+					GFP_KERNEL);
+
+		lockdep_assert_held(&kctx->csf.lock);
+
+		if (!group) {
+			dev_err(kctx->kbdev->dev, "Failed to allocate a queue\n");
+			group_handle = -ENOMEM;
+		} else {
+			int err = 0;
+
+			group->kctx = kctx;
+			group->handle = group_handle;
+			group->csg_nr = KBASEP_CSG_NR_INVALID;
+
+			group->tiler_mask = create->in.tiler_mask;
+			group->fragment_mask = create->in.fragment_mask;
+			group->compute_mask = create->in.compute_mask;
+
+			group->tiler_max = create->in.tiler_max;
+			group->fragment_max = create->in.fragment_max;
+			group->compute_max = create->in.compute_max;
+			group->priority = create->in.priority;
+			group->doorbell_nr = KBASEP_USER_DB_NR_INVALID;
+			group->faulted = false;
+
+			INIT_LIST_HEAD(&group->link);
+			INIT_LIST_HEAD(&group->link_to_schedule);
+			INIT_LIST_HEAD(&group->error_fatal.link);
+			INIT_LIST_HEAD(&group->error_timeout.link);
+			INIT_LIST_HEAD(&group->error_tiler_oom.link);
+			INIT_WORK(&group->timer_event_work, timer_event_worker);
+			INIT_WORK(&group->protm_event_work, protm_event_worker);
+			bitmap_zero(group->protm_pending_bitmap,
+					MAX_SUPPORTED_STREAMS_PER_GROUP);
+
+			group->run_state = KBASE_CSF_GROUP_INACTIVE;
+			err = create_suspend_buffers(kctx, group);
+
+			if (err < 0) {
+				kfree(group);
+				group_handle = err;
+			} else {
+				int j;
+
+				kctx->csf.queue_groups[group_handle] = group;
+				for (j = 0; j < MAX_SUPPORTED_STREAMS_PER_GROUP;
+						j++)
+					group->bound_queues[j] = NULL;
+			}
+		}
+	}
+
+	return group_handle;
+}
+
+int kbase_csf_queue_group_create(struct kbase_context *const kctx,
+			union kbase_ioctl_cs_queue_group_create *const create)
+{
+	int err = 0;
+	const u32 tiler_count = hweight64(create->in.tiler_mask);
+	const u32 fragment_count = hweight64(create->in.fragment_mask);
+	const u32 compute_count = hweight64(create->in.compute_mask);
+
+	mutex_lock(&kctx->csf.lock);
+
+	if ((create->in.tiler_max > tiler_count) ||
+	    (create->in.fragment_max > fragment_count) ||
+	    (create->in.compute_max > compute_count)) {
+		dev_err(kctx->kbdev->dev,
+			"Invalid maximum number of endpoints for a queue group\n");
+		err = -EINVAL;
+	} else if (create->in.priority >= BASE_QUEUE_GROUP_PRIORITY_COUNT) {
+		dev_err(kctx->kbdev->dev, "Invalid queue group priority %u\n",
+			(unsigned int)create->in.priority);
+		err = -EINVAL;
+	} else if (!iface_has_enough_streams(kctx->kbdev, create->in.cs_min)) {
+		dev_err(kctx->kbdev->dev,
+			"No CSG has at least %d streams\n",
+			create->in.cs_min);
+		err = -EINVAL;
+	} else {
+		/* For the CSG which satisfies the condition for having
+		 * the needed number of CSs, check whether it also conforms
+		 * with the requirements for at least one of its CSs having
+		 * the iterator of the needed type
+		 * (note: for CSF v1.0 all CSs in a CSG will have access to
+		 * the same iterators)
+		 */
+		const int group_handle = create_queue_group(kctx, create);
+
+		if (group_handle >= 0)
+			create->out.group_handle = group_handle;
+		else
+			err = group_handle;
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+
+	return err;
+}
+
+/**
+ * term_normal_suspend_buffer() - Free normal-mode suspend buffer of queue group
+ *
+ * @kctx:	Pointer to kbase context where queue group belongs to
+ * @s_buf:	Pointer to queue group suspend buffer to be freed
+ */
+static void term_normal_suspend_buffer(struct kbase_context *const kctx,
+		struct kbase_normal_suspend_buffer *s_buf)
+{
+	const size_t nr_pages =
+		PFN_UP(kctx->kbdev->csf.global_iface.groups[0].suspend_size);
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	WARN_ON(kbase_mmu_teardown_pages(
+				kctx->kbdev, &kctx->kbdev->csf.mcu_mmu,
+				s_buf->reg->start_pfn, nr_pages, MCU_AS_NR));
+
+	WARN_ON(s_buf->reg->flags & KBASE_REG_FREE);
+
+	mutex_lock(&kctx->kbdev->csf.reg_lock);
+	WARN_ON(kbase_remove_va_region(s_buf->reg));
+	mutex_unlock(&kctx->kbdev->csf.reg_lock);
+
+	kbase_mem_pool_free_pages(
+			&kctx->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+			nr_pages, &s_buf->phy[0], false, false);
+
+	kfree(s_buf->phy);
+	s_buf->phy = NULL;
+	kfree(s_buf->reg);
+	s_buf->reg = NULL;
+}
+
+/**
+ * term_protected_suspend_buffer() - Free normal-mode suspend buffer of
+ *					queue group
+ *
+ * @kbdev:	Instance of a GPU platform device that implements a command
+ *		stream front-end interface.
+ * @s_buf:	Pointer to queue group suspend buffer to be freed
+ */
+static void term_protected_suspend_buffer(struct kbase_device *const kbdev,
+		struct kbase_protected_suspend_buffer *s_buf)
+{
+	const size_t nr_pages =
+		PFN_UP(kbdev->csf.global_iface.groups[0].suspend_size);
+
+	WARN_ON(kbase_mmu_teardown_pages(
+			kbdev, &kbdev->csf.mcu_mmu,
+			s_buf->reg->start_pfn, nr_pages, MCU_AS_NR));
+
+	WARN_ON(s_buf->reg->flags & KBASE_REG_FREE);
+
+	mutex_lock(&kbdev->csf.reg_lock);
+	WARN_ON(kbase_remove_va_region(s_buf->reg));
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+	kbase_csf_protected_memory_free(kbdev, s_buf->pma, nr_pages);
+	s_buf->pma = NULL;
+	kfree(s_buf->reg);
+	s_buf->reg = NULL;
+}
+
+void kbase_csf_term_descheduled_queue_group(struct kbase_queue_group *group)
+{
+	struct kbase_context *kctx = group->kctx;
+
+	/* Currently each group supports the same number of streams */
+	u32 max_streams =
+		kctx->kbdev->csf.global_iface.groups[0].stream_num;
+	u32 i;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	WARN_ON(group->run_state != KBASE_CSF_GROUP_INACTIVE &&
+		group->run_state != KBASE_CSF_GROUP_FAULT_EVICTED);
+
+	for (i = 0; i < max_streams; i++) {
+		struct kbase_queue *queue =
+				group->bound_queues[i];
+
+		/* The group is already being evicted from the scheduler */
+		if (queue)
+			unbind_stopped_queue(kctx, queue);
+	}
+
+	term_normal_suspend_buffer(kctx, &group->normal_suspend_buf);
+	if (kctx->kbdev->csf.pma_dev)
+		term_protected_suspend_buffer(kctx->kbdev,
+			&group->protected_suspend_buf);
+
+	group->run_state = KBASE_CSF_GROUP_TERMINATED;
+}
+
+/**
+ * term_queue_group - Terminate a GPU command queue group.
+ *
+ * @group: Pointer to GPU command queue group data.
+ *
+ * Terminates a GPU command queue group. From the userspace perspective the
+ * group will still exist but it can't bind new queues to it. Userspace can
+ * still add work in queues bound to the group but it won't be executed. (This
+ * is because the IO mapping created upon binding such queues is still intact.)
+ */
+static void term_queue_group(struct kbase_queue_group *group)
+{
+	struct kbase_context *kctx = group->kctx;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	/* Stop the group and evict it from the scheduler */
+	kbase_csf_scheduler_group_deschedule(group);
+
+	if (group->run_state == KBASE_CSF_GROUP_TERMINATED)
+		return;
+
+	dev_dbg(kctx->kbdev->dev, "group %d terminating", group->handle);
+
+	kbase_csf_term_descheduled_queue_group(group);
+}
+
+static void cancel_queue_group_events(struct kbase_queue_group *group)
+{
+	cancel_work_sync(&group->timer_event_work);
+	cancel_work_sync(&group->protm_event_work);
+}
+
+void kbase_csf_queue_group_terminate(struct kbase_context *kctx,
+				     u8 group_handle)
+{
+	struct kbase_queue_group *group;
+
+	mutex_lock(&kctx->csf.lock);
+
+	group = find_queue_group(kctx, group_handle);
+
+	if (group) {
+		/* Remove any pending group fatal error from the per-context list. */
+		list_del_init(&group->error_tiler_oom.link);
+		list_del_init(&group->error_timeout.link);
+		list_del_init(&group->error_fatal.link);
+
+		term_queue_group(group);
+		kctx->csf.queue_groups[group_handle] = NULL;
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+
+	if (!group)
+		return;
+
+	/* Cancel any pending event callbacks. If one is in progress
+	 * then this thread waits synchronously for it to complete (which
+	 * is why we must unlock the context first). We already ensured
+	 * that no more callbacks can be enqueued by terminating the group.
+	 */
+	cancel_queue_group_events(group);
+	kfree(group);
+}
+
+int kbase_csf_queue_group_suspend(struct kbase_context *kctx,
+				  struct kbase_suspend_copy_buffer *sus_buf,
+				  u8 group_handle)
+{
+	int err = -EINVAL;
+	struct kbase_queue_group *group;
+
+	mutex_lock(&kctx->csf.lock);
+
+	group = find_queue_group(kctx, group_handle);
+	if (group)
+		err = kbase_csf_scheduler_group_copy_suspend_buf(group,
+								 sus_buf);
+
+	mutex_unlock(&kctx->csf.lock);
+	return err;
+}
+
+/**
+ * kbase_csf_add_fatal_error_to_kctx - Add a fatal error to per-ctx error list.
+ *
+ * @group:       GPU command queue group.
+ * @err_payload: Error payload to report.
+ */
+static void kbase_csf_add_fatal_error_to_kctx(
+		struct kbase_queue_group *const group,
+		const struct base_gpu_queue_group_error *const err_payload)
+{
+	struct base_csf_notification error;
+
+	if (WARN_ON(!group))
+		return;
+
+	if (WARN_ON(!err_payload))
+		return;
+
+	error = (struct base_csf_notification) {
+		.type = BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+		.payload = {
+			.csg_error = {
+				.handle = group->handle,
+				.error = *err_payload
+			}
+		}
+	};
+
+	lockdep_assert_held(&group->kctx->csf.lock);
+
+	/* If this group has already been in fatal error status,
+	 * subsequent fatal error on this group should never take place.
+	 */
+	if (!WARN_ON(!list_empty(&group->error_fatal.link))) {
+		group->error_fatal.data = error;
+		list_add_tail(&group->error_fatal.link,
+				&group->kctx->csf.error_list);
+	}
+}
+
+void kbase_csf_active_queue_groups_reset(struct kbase_device *kbdev,
+					 struct kbase_context *kctx)
+{
+	struct list_head evicted_groups;
+	struct kbase_queue_group *group;
+	int i;
+	bool fatal_error_built = false;
+
+	INIT_LIST_HEAD(&evicted_groups);
+
+	mutex_lock(&kctx->csf.lock);
+
+	kbase_csf_scheduler_evict_ctx_slots(kbdev, kctx, &evicted_groups);
+	while (!list_empty(&evicted_groups)) {
+		struct kbase_csf_scheduler *scheduler =
+						&kbdev->csf.scheduler;
+		unsigned long flags;
+
+		group = list_first_entry(&evicted_groups,
+				struct kbase_queue_group, link);
+
+		dev_dbg(kbdev->dev, "Context %d_%d active group %d terminated",
+			    kctx->tgid, kctx->id, group->handle);
+		kbase_csf_term_descheduled_queue_group(group);
+		list_del_init(&group->link);
+
+		kbase_csf_scheduler_spin_lock(kbdev, &flags);
+		if ((group == scheduler->active_protm_grp) &&
+		    group->faulted) {
+			const struct base_gpu_queue_group_error err_payload = {
+				.error_type = BASE_GPU_QUEUE_GROUP_ERROR_FATAL,
+				.payload = {
+					.fatal_group = {
+					.status = GPU_EXCEPTION_TYPE_SW_FAULT_0,
+					}
+				}
+			};
+
+			kbase_csf_add_fatal_error_to_kctx(group, &err_payload);
+			fatal_error_built = true;
+		}
+		kbase_csf_scheduler_spin_unlock(kbdev, flags);
+	}
+
+	if (fatal_error_built)
+		kbase_event_wakeup(kctx);
+
+	/* Acting on the queue groups that are pending to be terminated. */
+	for (i = 0; i < MAX_QUEUE_GROUP_NUM; i++) {
+		group = kctx->csf.queue_groups[i];
+		if (group &&
+		    group->run_state == KBASE_CSF_GROUP_FAULT_EVICTED)
+			kbase_csf_term_descheduled_queue_group(group);
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+}
+
+int kbase_csf_ctx_init(struct kbase_context *kctx)
+{
+	int err = -ENOMEM;
+
+	INIT_LIST_HEAD(&kctx->csf.event_callback_list);
+	INIT_LIST_HEAD(&kctx->csf.queue_list);
+	INIT_LIST_HEAD(&kctx->csf.link);
+	INIT_LIST_HEAD(&kctx->csf.error_list);
+
+	spin_lock_init(&kctx->csf.event_lock);
+	kctx->csf.user_reg_vma = NULL;
+
+	/* Mark all the cookies as 'free' */
+	bitmap_fill(kctx->csf.cookies, KBASE_CSF_NUM_USER_IO_PAGES_HANDLE);
+
+	kctx->csf.wq = alloc_workqueue("mali_kbase_csf_wq",
+					WQ_UNBOUND, 1);
+
+	if (likely(kctx->csf.wq)) {
+		err = kbase_csf_scheduler_context_init(kctx);
+
+		if (likely(!err)) {
+			err = kbase_csf_kcpu_queue_context_init(kctx);
+
+			if (likely(!err)) {
+				err = kbase_csf_tiler_heap_context_init(kctx);
+
+				if (likely(!err))
+					mutex_init(&kctx->csf.lock);
+				else
+					kbase_csf_kcpu_queue_context_term(kctx);
+			}
+
+			if (unlikely(err))
+				kbase_csf_scheduler_context_term(kctx);
+		}
+
+		if (unlikely(err))
+			destroy_workqueue(kctx->csf.wq);
+	}
+
+	return err;
+}
+
+void kbase_csf_ctx_handle_fault(struct kbase_context *kctx,
+		struct kbase_fault *fault)
+{
+	int gr;
+	bool reported = false;
+	struct base_gpu_queue_group_error err_payload;
+
+	if (WARN_ON(!kctx))
+		return;
+
+	if (WARN_ON(!fault))
+		return;
+
+	err_payload = (struct base_gpu_queue_group_error) {
+		.error_type = BASE_GPU_QUEUE_GROUP_ERROR_FATAL,
+		.payload = {
+			.fatal_group = {
+				.sideband = fault->addr,
+				.status = fault->status,
+			}
+		}
+	};
+
+	mutex_lock(&kctx->csf.lock);
+
+	for (gr = 0; gr < MAX_QUEUE_GROUP_NUM; gr++) {
+		struct kbase_queue_group *const group =
+			kctx->csf.queue_groups[gr];
+
+		if (group && group->run_state != KBASE_CSF_GROUP_TERMINATED) {
+			term_queue_group(group);
+			kbase_csf_add_fatal_error_to_kctx(group, &err_payload);
+			reported = true;
+		}
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+
+	if (reported)
+		kbase_event_wakeup(kctx);
+}
+
+void kbase_csf_ctx_term(struct kbase_context *kctx)
+{
+	u32 i;
+
+	/* As the kbase context is terminating, its debugfs sub-directory would
+	 * have been removed already and so would be the debugfs file created
+	 * for queue groups & kcpu queues, hence no need to explicitly remove
+	 * those debugfs files.
+	 */
+	kbase_csf_event_wait_remove_all(kctx);
+
+	mutex_lock(&kctx->csf.lock);
+
+	/* Iterate through the queue groups that were not terminated by
+	 * userspace and issue the term request to firmware for them.
+	 */
+	for (i = 0; i < MAX_QUEUE_GROUP_NUM; i++) {
+		if (kctx->csf.queue_groups[i])
+			term_queue_group(kctx->csf.queue_groups[i]);
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+
+	/* Now that all queue groups have been terminated, there can be no
+	 * more OoM or timer event interrupts but there can be inflight work
+	 * items. Destroying the wq will implicitly flush those work items.
+	 */
+	destroy_workqueue(kctx->csf.wq);
+
+	mutex_lock(&kctx->csf.lock);
+
+	for (i = 0; i < MAX_QUEUE_GROUP_NUM; i++)
+		kfree(kctx->csf.queue_groups[i]);
+
+	/* Iterate through the queues that were not terminated by
+	 * userspace and do the required cleanup for them.
+	 */
+	while (!list_empty(&kctx->csf.queue_list)) {
+		struct kbase_queue *queue;
+
+		queue = list_first_entry(&kctx->csf.queue_list,
+						struct kbase_queue, link);
+
+		/* The reference held when the IO mapping was created on bind
+		 * would have been dropped otherwise the termination of Kbase
+		 * context itself wouldn't have kicked-in. So there shall be
+		 * only one reference left that was taken when queue was
+		 * registered.
+		 */
+		if (atomic_read(&queue->refcount) != 1)
+			dev_warn(kctx->kbdev->dev,
+				 "Releasing queue with incorrect refcounting!\n");
+		list_del_init(&queue->link);
+		release_queue(queue);
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+
+	kbase_csf_tiler_heap_context_term(kctx);
+	kbase_csf_kcpu_queue_context_term(kctx);
+	kbase_csf_scheduler_context_term(kctx);
+
+	mutex_destroy(&kctx->csf.lock);
+}
+
+int kbase_csf_event_wait_add(struct kbase_context *kctx,
+			     kbase_csf_event_callback *callback, void *param)
+{
+	int err = -ENOMEM;
+	struct kbase_csf_event *event =
+		kzalloc(sizeof(struct kbase_csf_event), GFP_KERNEL);
+
+	if (event) {
+		unsigned long flags;
+
+		event->kctx = kctx;
+		event->callback = callback;
+		event->param = param;
+
+		spin_lock_irqsave(&kctx->csf.event_lock, flags);
+		list_add_tail(&event->link, &kctx->csf.event_callback_list);
+		spin_unlock_irqrestore(&kctx->csf.event_lock, flags);
+
+		err = 0;
+	}
+
+	return err;
+}
+
+void kbase_csf_event_wait_remove(struct kbase_context *kctx,
+		kbase_csf_event_callback *callback, void *param)
+{
+	struct kbase_csf_event *event;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kctx->csf.event_lock, flags);
+
+	list_for_each_entry(event, &kctx->csf.event_callback_list, link) {
+		if ((event->callback == callback) && (event->param == param)) {
+			list_del(&event->link);
+			kfree(event);
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&kctx->csf.event_lock, flags);
+}
+
+bool kbase_csf_read_error(struct kbase_context *kctx,
+		struct base_csf_notification *event_data)
+{
+	bool got_event = true;
+	struct kbase_csf_notification *error_data = NULL;
+
+	mutex_lock(&kctx->csf.lock);
+
+	if (likely(!list_empty(&kctx->csf.error_list))) {
+		error_data = list_first_entry(&kctx->csf.error_list,
+			struct kbase_csf_notification, link);
+		list_del_init(&error_data->link);
+		*event_data = error_data->data;
+	} else {
+		got_event = false;
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+
+	return got_event;
+}
+
+bool kbase_csf_error_pending(struct kbase_context *kctx)
+{
+	bool event_pended = false;
+
+	mutex_lock(&kctx->csf.lock);
+	event_pended = !list_empty(&kctx->csf.error_list);
+	mutex_unlock(&kctx->csf.lock);
+
+	return event_pended;
+}
+
+void kbase_csf_event_signal(struct kbase_context *kctx, bool notify_gpu)
+{
+	struct kbase_csf_event *event, *next_event;
+	unsigned long flags;
+
+	/* First increment the signal count and wake up event thread.
+	 */
+	atomic_set(&kctx->event_count, 1);
+	kbase_event_wakeup(kctx);
+
+	/* Signal the CSF firmware. This is to ensure that pending command
+	 * stream synch object wait operations are re-evaluated.
+	 * Write to GLB_DOORBELL would suffice as spec says that all pending
+	 * synch object wait operations are re-evaluated on a write to any
+	 * CS_DOORBELL/GLB_DOORBELL register.
+	 */
+	if (notify_gpu) {
+		spin_lock_irqsave(&kctx->kbdev->hwaccess_lock, flags);
+		if (kctx->kbdev->pm.backend.gpu_powered)
+			kbase_csf_ring_doorbell(kctx->kbdev, CSF_KERNEL_DOORBELL_NR);
+		spin_unlock_irqrestore(&kctx->kbdev->hwaccess_lock, flags);
+	}
+
+	/* Now invoke the callbacks registered on backend side.
+	 * Allow item removal inside the loop, if requested by the callback.
+	 */
+	spin_lock_irqsave(&kctx->csf.event_lock, flags);
+
+	list_for_each_entry_safe(
+		event, next_event, &kctx->csf.event_callback_list, link) {
+		enum kbase_csf_event_callback_action action =
+			event->callback(event->param);
+
+		if (action == KBASE_CSF_EVENT_CALLBACK_REMOVE) {
+			list_del(&event->link);
+			kfree(event);
+		}
+	}
+
+	spin_unlock_irqrestore(&kctx->csf.event_lock, flags);
+}
+
+void kbase_csf_event_wait_remove_all(struct kbase_context *kctx)
+{
+	struct kbase_csf_event *event, *next_event;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kctx->csf.event_lock, flags);
+
+	list_for_each_entry_safe(
+		event, next_event, &kctx->csf.event_callback_list, link) {
+		list_del(&event->link);
+		kfree(event);
+	}
+
+	spin_unlock_irqrestore(&kctx->csf.event_lock, flags);
+}
+
+/**
+ * handle_oom_event - Handle the OoM event generated by the firmware for the
+ *                    command stream interface.
+ *
+ * This function will handle the OoM event request from the firmware for the
+ * command stream. It will retrieve the address of heap context and heap's
+ * statistics (like number of render passes in-flight) from the command
+ * stream's kernel output page and pass them to the tiler heap function
+ * to allocate a new chunk.
+ * It will also update the command stream's kernel input page with the address
+ * of a new chunk that was allocated.
+ *
+ * @kctx: Pointer to the kbase context in which the tiler heap was initialized.
+ * @stream: Pointer to the structure containing info provided by the firmware
+ *          about the command stream interface.
+ *
+ * Return: 0 if successfully handled the request, otherwise a negative error
+ *         code on failure.
+ */
+static int handle_oom_event(struct kbase_context *const kctx,
+		struct kbase_csf_cmd_stream_info const *const stream)
+{
+	u64 gpu_heap_va =
+		kbase_csf_firmware_cs_output(stream, CS_HEAP_ADDRESS_LO) |
+		((u64)kbase_csf_firmware_cs_output(stream, CS_HEAP_ADDRESS_HI) << 32);
+	const u32 vt_start =
+		kbase_csf_firmware_cs_output(stream, CS_HEAP_VT_START);
+	const u32 vt_end =
+		kbase_csf_firmware_cs_output(stream, CS_HEAP_VT_END);
+	const u32 frag_end =
+		kbase_csf_firmware_cs_output(stream, CS_HEAP_FRAG_END);
+	u32 renderpasses_in_flight;
+	u64 new_chunk_ptr;
+	int err;
+
+	if ((frag_end > vt_end) || (vt_end >= vt_start)) {
+		dev_warn(kctx->kbdev->dev, "Invalid Heap statistics provided by firmware: vt_start %d, vt_end %d, frag_end %d\n",
+			 vt_start, vt_end, frag_end);
+		return -EINVAL;
+	}
+
+	renderpasses_in_flight = vt_start - frag_end;
+
+	err = kbase_csf_tiler_heap_alloc_new_chunk(kctx,
+		gpu_heap_va, renderpasses_in_flight, &new_chunk_ptr);
+
+	/* It is okay to acknowledge with a NULL chunk (firmware will then wait
+	 * for the fragment jobs to complete and release chunks)
+	 */
+	if (err == -EBUSY)
+		new_chunk_ptr = 0;
+	else if (err)
+		return err;
+
+	kbase_csf_firmware_cs_input(stream, CS_TILER_HEAP_START_LO,
+				new_chunk_ptr & 0xFFFFFFFF);
+	kbase_csf_firmware_cs_input(stream, CS_TILER_HEAP_START_HI,
+				new_chunk_ptr >> 32);
+
+	kbase_csf_firmware_cs_input(stream, CS_TILER_HEAP_END_LO,
+				new_chunk_ptr & 0xFFFFFFFF);
+	kbase_csf_firmware_cs_input(stream, CS_TILER_HEAP_END_HI,
+				new_chunk_ptr >> 32);
+
+	return 0;
+}
+
+/**
+ * report_tiler_oom_error - Report a CSG error due to a tiler heap OOM event
+ *
+ * @group: Pointer to the GPU command queue group that encountered the error
+ */
+static void report_tiler_oom_error(struct kbase_queue_group *group)
+{
+	struct base_csf_notification const
+		error = { .type = BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+			  .payload = {
+				  .csg_error = {
+					  .handle = group->handle,
+					  .error = {
+						  .error_type =
+							  BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM,
+					  } } } };
+	struct kbase_context *kctx = group->kctx;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	/* Ignore this error if the previous one hasn't been reported */
+	if (!WARN_ON(!list_empty(&group->error_tiler_oom.link))) {
+		group->error_tiler_oom.data = error;
+		list_add_tail(&group->error_tiler_oom.link,
+			      &kctx->csf.error_list);
+		kbase_event_wakeup(kctx);
+	}
+}
+
+/**
+ * kbase_queue_oom_event - Handle tiler out-of-memory for a GPU command queue.
+ *
+ * @queue: Pointer to queue for which out-of-memory event was received.
+ *
+ * Called with the command-stream front-end locked for the affected GPU
+ * virtual address space. Do not call in interrupt context.
+ *
+ * Handles tiler out-of-memory for a GPU command queue and then clears the
+ * notification to allow the firmware to report out-of-memory again in future.
+ * If the out-of-memory condition was successfully handled then this function
+ * rings the relevant doorbell to notify the firmware; otherwise, it terminates
+ * the GPU command queue group to which the queue is bound. See
+ * term_queue_group() for details.
+ */
+static void kbase_queue_oom_event(struct kbase_queue *const queue)
+{
+	struct kbase_context *const kctx = queue->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_queue_group *group;
+	int slot_num, err;
+	struct kbase_csf_cmd_stream_group_info const *ginfo;
+	struct kbase_csf_cmd_stream_info const *stream;
+	u32 cs_oom_ack, cs_oom_req;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	group = get_bound_queue_group(queue);
+	if (!group) {
+		dev_warn(kctx->kbdev->dev, "queue not bound\n");
+		return;
+	}
+
+	kbase_csf_scheduler_lock(kbdev);
+
+	slot_num = kbase_csf_scheduler_group_get_slot(group);
+
+	/* The group could have gone off slot before this work item got
+	 * a chance to execute.
+	 */
+	if (slot_num < 0)
+		goto unlock;
+
+	/* If the bound group is on slot yet the kctx is marked with disabled
+	 * on address-space fault, the group is pending to be killed. So skip
+	 * the inflight oom operation.
+	 */
+	if (kbase_ctx_flag(kctx, KCTX_AS_DISABLED_ON_FAULT))
+		goto unlock;
+
+	ginfo = &kbdev->csf.global_iface.groups[slot_num];
+	stream = &ginfo->streams[queue->csi_index];
+	cs_oom_ack = kbase_csf_firmware_cs_output(stream, CS_ACK) &
+		     CS_ACK_TILER_OOM_MASK;
+	cs_oom_req = kbase_csf_firmware_cs_input_read(stream, CS_REQ) &
+		     CS_REQ_TILER_OOM_MASK;
+
+	/* The group could have already undergone suspend-resume cycle before
+	 * this work item got a chance to execute. On CSG resume the CS_ACK
+	 * register is set by firmware to reflect the CS_REQ register, which
+	 * implies that all events signaled before suspension are implicitly
+	 * acknowledged.
+	 * A new OoM event is expected to be generated after resume.
+	 */
+	if (cs_oom_ack == cs_oom_req)
+		goto unlock;
+
+	err = handle_oom_event(kctx, stream);
+
+	kbase_csf_firmware_cs_input_mask(stream, CS_REQ, cs_oom_ack,
+					 CS_REQ_TILER_OOM_MASK);
+
+	if (err) {
+		dev_warn(
+			kbdev->dev,
+			"Queue group to be terminated, couldn't handle the OoM event\n");
+		kbase_csf_scheduler_unlock(kbdev);
+		term_queue_group(group);
+		report_tiler_oom_error(group);
+		return;
+	}
+
+	kbase_csf_ring_cs_kernel_doorbell(kbdev, queue);
+unlock:
+	kbase_csf_scheduler_unlock(kbdev);
+}
+
+/**
+ * oom_event_worker - Tiler out-of-memory handler called from a workqueue.
+ *
+ * @data: Pointer to a work_struct embedded in GPU command queue data.
+ *
+ * Handles a tiler out-of-memory condition for a GPU command queue and then
+ * releases a reference that was added to prevent the queue being destroyed
+ * while this work item was pending on a workqueue.
+ */
+static void oom_event_worker(struct work_struct *data)
+{
+	struct kbase_queue *queue =
+		container_of(data, struct kbase_queue, oom_event_work);
+	struct kbase_context *kctx = queue->kctx;
+
+	mutex_lock(&kctx->csf.lock);
+
+	kbase_queue_oom_event(queue);
+	release_queue(queue);
+
+	mutex_unlock(&kctx->csf.lock);
+}
+
+/**
+ * timer_event_worker - Timer event handler called from a workqueue.
+ *
+ * @data: Pointer to a work_struct embedded in GPU command queue group data.
+ *
+ * Notify the event notification thread of progress timeout fault
+ * for the GPU command queue group.
+ */
+static void timer_event_worker(struct work_struct *data)
+{
+	struct kbase_queue_group *const group =
+		container_of(data, struct kbase_queue_group, timer_event_work);
+	struct base_csf_notification const
+		error = { .type = BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+			  .payload = {
+				  .csg_error = {
+					  .handle = group->handle,
+					  .error = {
+						  .error_type =
+							  BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT,
+					  } } } };
+	struct kbase_context *const kctx = group->kctx;
+
+	mutex_lock(&kctx->csf.lock);
+
+	/* Ignore this error if the previous one hasn't been reported */
+	if (!WARN_ON(!list_empty(&group->error_timeout.link))) {
+		group->error_timeout.data = error;
+		list_add_tail(&group->error_timeout.link,
+			      &kctx->csf.error_list);
+		kbase_event_wakeup(kctx);
+	}
+
+	mutex_unlock(&kctx->csf.lock);
+}
+
+/**
+ * protm_event_worker - Protected mode switch request event handler
+ *			called from a workqueue.
+ *
+ * @data: Pointer to a work_struct embedded in GPU command queue group data.
+ *
+ * Request to switch to protected mode.
+ */
+static void protm_event_worker(struct work_struct *data)
+{
+	struct kbase_queue_group *const group =
+		container_of(data, struct kbase_queue_group, protm_event_work);
+
+	kbase_csf_scheduler_group_protm_enter(group);
+}
+
+/**
+ * handle_fault_event - Handler for CS fault.
+ *
+ * @queue:  Pointer to queue for which fault event was received.
+ * @stream: Pointer to the structure containing info provided by the
+ *          firmware about the command stream interface.
+ *
+ * Prints meaningful CS fault information.
+ *
+ * Return: 0 on success, otherwise a negative system code.
+ */
+static int handle_fault_event(struct kbase_queue const *const queue,
+		   struct kbase_csf_cmd_stream_info const *const stream)
+{
+	const u32 cs_fault = kbase_csf_firmware_cs_output(stream, CS_FAULT);
+	const u64 cs_fault_info =
+		kbase_csf_firmware_cs_output(stream, CS_FAULT_INFO_LO) |
+		((u64)kbase_csf_firmware_cs_output(stream, CS_FAULT_INFO_HI)
+		 << 32);
+	const u8 cs_fault_exception_type =
+		CS_FAULT_EXCEPTION_TYPE_GET(cs_fault);
+	const u32 cs_fault_exception_data =
+		CS_FAULT_EXCEPTION_DATA_GET(cs_fault);
+	const u64 cs_fault_info_exception_data =
+		CS_FAULT_INFO_EXCEPTION_DATA_GET(cs_fault_info);
+	struct kbase_device *const kbdev = queue->kctx->kbdev;
+
+	dev_warn(kbdev->dev, "CSI: %d\n"
+			"CS_FAULT.EXCEPTION_TYPE: 0x%x (%s)\n"
+			"CS_FAULT.EXCEPTION_DATA: 0x%x\n"
+			"CS_FAULT_INFO.EXCEPTION_DATA: 0x%llx\n",
+			queue->csi_index, cs_fault_exception_type,
+			kbase_gpu_exception_name(cs_fault_exception_type),
+			cs_fault_exception_data, cs_fault_info_exception_data);
+
+	return -EFAULT;
+}
+
+/**
+ * report_queue_fatal_error - Report queue fatal error to user space
+ *
+ * @queue:         Pointer to queue for which fatal event was received.
+ * @cs_fatal:      Fault information
+ * @cs_fatal_info: Additional fault information
+ *
+ * If a queue has already been in fatal error status,
+ * subsequent fatal error on the queue should never take place.
+ */
+static void report_queue_fatal_error(struct kbase_queue *const queue,
+		u32 cs_fatal, u64 cs_fatal_info)
+{
+	struct base_csf_notification error = {
+		.type = BASE_CSF_NOTIFICATION_GPU_QUEUE_GROUP_ERROR,
+		.payload = {
+			.csg_error = {
+				.handle = queue->group->handle,
+				.error = {
+					.error_type =
+					BASE_GPU_QUEUE_GROUP_QUEUE_ERROR_FATAL,
+					.payload = {
+						.fatal_queue = {
+						.sideband = cs_fatal_info,
+						.status = cs_fatal,
+						.csi_index = queue->csi_index,
+						}
+					}
+				}
+			}
+		}
+	};
+
+	lockdep_assert_held(&queue->kctx->csf.lock);
+
+	/* If a queue has already been in fatal error status,
+	 * subsequent fatal error on the queue should never take place.
+	 */
+	if (!WARN_ON(!list_empty(&queue->error.link))) {
+		queue->error.data = error;
+		list_add_tail(&queue->error.link, &queue->kctx->csf.error_list);
+		kbase_event_wakeup(queue->kctx);
+	}
+}
+
+/**
+ * handle_fatal_event - Handler for CS fatal.
+ *
+ * @queue:    Pointer to queue for which fatal event was received.
+ * @stream:   Pointer to the structure containing info provided by the
+ *            firmware about the command stream interface.
+ * @fw_error: Return true if internal firmware fatal is handled
+ *
+ * Prints meaningful CS fatal information.
+ * Report queue fatal error to user space.
+ *
+ * Return: 0 on success otherwise a negative system error.
+ */
+static int handle_fatal_event(struct kbase_queue *const queue,
+	struct kbase_csf_cmd_stream_info const *const stream,
+	bool *fw_error)
+{
+	const u32 cs_fatal = kbase_csf_firmware_cs_output(stream, CS_FATAL);
+	const u64 cs_fatal_info =
+		kbase_csf_firmware_cs_output(stream, CS_FATAL_INFO_LO) |
+		((u64)kbase_csf_firmware_cs_output(stream, CS_FATAL_INFO_HI)
+		 << 32);
+	const u32 cs_fatal_exception_type =
+		CS_FATAL_EXCEPTION_TYPE_GET(cs_fatal);
+	const u32 cs_fatal_exception_data =
+		CS_FATAL_EXCEPTION_DATA_GET(cs_fatal);
+	const u64 cs_fatal_info_exception_data =
+		CS_FATAL_INFO_EXCEPTION_DATA_GET(cs_fatal_info);
+	struct kbase_device *const kbdev = queue->kctx->kbdev;
+
+	lockdep_assert_held(&queue->kctx->csf.lock);
+
+	dev_warn(kbdev->dev,
+		 "CSG: %d, CSI: %d\n"
+		 "CS_FATAL.EXCEPTION_TYPE: 0x%x (%s)\n"
+		 "CS_FATAL.EXCEPTION_DATA: 0x%x\n"
+		 "CS_FATAL_INFO.EXCEPTION_DATA: 0x%llx\n",
+		 queue->group->handle, queue->csi_index,
+		 cs_fatal_exception_type,
+		 kbase_gpu_exception_name(cs_fatal_exception_type),
+		 cs_fatal_exception_data, cs_fatal_info_exception_data);
+
+	if (cs_fatal_exception_type ==
+			CS_FATAL_EXCEPTION_TYPE_FIRMWARE_INTERNAL_ERROR)
+		*fw_error = true;
+	else
+		report_queue_fatal_error(queue, cs_fatal, cs_fatal_info);
+
+	return -EFAULT;
+}
+
+/**
+ * handle_internal_firmware_fatal - Handler for CS internal firmware fault.
+ *
+ * @kbdev:  Pointer to kbase device
+ *
+ * Report group fatal error to user space for all GPU command queue groups
+ * in the device, terminate them and reset GPU.
+ */
+static void handle_internal_firmware_fatal(struct kbase_device *const kbdev)
+{
+	int as;
+
+	for (as = 0; as < kbdev->nr_hw_address_spaces; as++) {
+		struct kbase_context *kctx;
+		struct kbase_fault fault = {
+			.status = GPU_EXCEPTION_TYPE_SW_FAULT_1,
+		};
+
+		if (as == MCU_AS_NR)
+			continue;
+
+		kctx = kbase_ctx_sched_as_to_ctx_refcount(kbdev, as);
+		if (!kctx)
+			continue;
+
+		kbase_csf_ctx_handle_fault(kctx, &fault);
+		kbase_ctx_sched_release_ctx_lock(kctx);
+	}
+
+	if (kbase_prepare_to_reset_gpu(kbdev))
+		kbase_reset_gpu(kbdev);
+}
+
+/**
+ * fault_event_worker - Worker function for CS fault/fatal.
+ *
+ * @data: Pointer to a work_struct embedded in GPU command queue data.
+ *
+ * Handle the fault and fatal exception for a GPU command queue and then
+ * releases a reference that was added to prevent the queue being destroyed
+ * while this work item was pending on a workqueue.
+ * 
+ * Report the fault and fatal exception for a GPU command queue and then
+ * clears the corresponding notification fields to allow the firmware to
+ * report other faults in future.
+ * 
+ * It may also terminate the GPU command queue group(s) and reset GPU
+ * in case internal firmware CS fatal exception occurred.
+ */
+static void fault_event_worker(struct work_struct *const data)
+{
+	struct kbase_queue *const queue =
+		container_of(data, struct kbase_queue, fault_event_work);
+
+	struct kbase_context *const kctx = queue->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_queue_group *group;
+	int slot_num;
+	struct kbase_csf_cmd_stream_group_info const *ginfo;
+	struct kbase_csf_cmd_stream_info const *stream;
+	u32 cs_ack, cs_req;
+	int err = 0;
+	bool internal_fw_error = false;
+
+	mutex_lock(&kctx->csf.lock);
+	kbase_csf_scheduler_lock(kbdev);
+
+	group = get_bound_queue_group(queue);
+	if (!group) {
+		dev_warn(kbdev->dev, "queue not bound\n");
+		goto unlock;
+	}
+
+	slot_num = kbase_csf_scheduler_group_get_slot(group);
+
+	/* The group could have gone off slot before this work item got
+	 * a chance to execute.
+	 */
+	if (slot_num < 0) {
+		dev_warn(kbdev->dev, "invalid slot_num\n");
+		goto unlock;
+	}
+
+	/* If the bound group is on slot yet the kctx is marked with disabled
+	 * on address-space fault, the group is pending to be killed. So skip
+	 * the inflight queue exception event operation.
+	 */
+	if (kbase_ctx_flag(kctx, KCTX_AS_DISABLED_ON_FAULT)) {
+		dev_warn(kbdev->dev, "kctx is already disabled on fault\n");
+		goto unlock;
+	}
+
+	ginfo = &kbdev->csf.global_iface.groups[slot_num];
+	stream = &ginfo->streams[queue->csi_index];
+	cs_ack = kbase_csf_firmware_cs_output(stream, CS_ACK);
+	cs_req = kbase_csf_firmware_cs_input_read(stream, CS_REQ);
+
+	if ((cs_ack & CS_ACK_FATAL_MASK) != (cs_req & CS_REQ_FATAL_MASK)) {
+		err = handle_fatal_event(queue, stream, &internal_fw_error);
+		kbase_csf_firmware_cs_input_mask(stream, CS_REQ, cs_ack,
+						 CS_REQ_FATAL_MASK);
+	}
+
+	if ((cs_ack & CS_ACK_FAULT_MASK) != (cs_req & CS_REQ_FAULT_MASK)) {
+		err |= handle_fault_event(queue, stream);
+		kbase_csf_firmware_cs_input_mask(stream, CS_REQ, cs_ack,
+						 CS_REQ_FAULT_MASK);
+		kbase_csf_ring_cs_kernel_doorbell(kbdev, queue);
+	}
+
+	if (err) {
+		/* From 10.x.5, CS_REQ_ERROR_MODE is removed but TI2 bitfile
+		 * upload not finished. Need to remove on GPUCORE-23972
+		 */
+		kbase_csf_firmware_cs_input_mask(stream, CS_REQ, ~cs_ack,
+						CS_REQ_ERROR_MODE_MASK);
+		dev_dbg(kbdev->dev, "Slot-%d CSI-%d entering error mode\n",
+			slot_num, queue->csi_index);
+	}
+
+unlock:
+	release_queue(queue);
+	kbase_csf_scheduler_unlock(kbdev);
+	mutex_unlock(&kctx->csf.lock);
+
+	if (internal_fw_error)
+		handle_internal_firmware_fatal(kbdev);
+}
+
+/**
+ * process_cs_interrupts - Process interrupts for a command stream.
+ *
+ * @group:  Pointer to GPU command queue group data.
+ * @ginfo:  The command stream group interface provided by the firmware.
+ * @irqreq: CSG's IRQ request bitmask (one bit per stream).
+ * @irqack: CSG's IRQ acknowledge bitmask (one bit per stream).
+ *
+ * If the interrupt request bitmask differs from the acknowledge bitmask
+ * then the firmware is notifying the host of an event concerning those
+ * streams indicated by bits whose value differs. The actions required
+ * are then determined by examining which notification flags differ between
+ * the request and acknowledge registers for the individual stream(s).
+ */
+static void process_cs_interrupts(struct kbase_queue_group *const group,
+		      struct kbase_csf_cmd_stream_group_info const *const ginfo,
+		      u32 const irqreq, u32 const irqack)
+{
+	struct kbase_device *const kbdev = group->kctx->kbdev;
+	u32 remaining = irqreq ^ irqack;
+	bool protm_pend = false;
+
+	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
+
+	while (remaining != 0) {
+		int const i = ffs(remaining) - 1;
+		struct kbase_queue *const queue = group->bound_queues[i];
+
+		/* The queue pointer can be NULL, but if it isn't NULL then it
+		 * cannot disappear since scheduler spinlock is held and before
+		 * freeing a bound queue it has to be first unbound which
+		 * requires scheduler spinlock.
+		 */
+		if (queue && !WARN_ON(queue->csi_index != i)) {
+			struct kbase_csf_cmd_stream_info const *const stream =
+				&ginfo->streams[i];
+			u32 const cs_req = kbase_csf_firmware_cs_input_read(
+				stream, CS_REQ);
+			u32 const cs_ack =
+				kbase_csf_firmware_cs_output(stream, CS_ACK);
+			struct workqueue_struct *wq = group->kctx->csf.wq;
+
+			if ((cs_req & CS_REQ_EXCEPTION_MASK) ^
+			    (cs_ack & CS_ACK_EXCEPTION_MASK)) {
+				get_queue(queue);
+				KBASE_KTRACE_ADD_CSF_GRP_Q(kbdev, CSI_FAULT_INTERRUPT, group, queue, cs_req ^ cs_ack);
+				if (!queue_work(wq, &queue->fault_event_work))
+					release_queue(queue);
+			}
+
+			if (((cs_req & CS_REQ_TILER_OOM_MASK) ^
+			     (cs_ack & CS_ACK_TILER_OOM_MASK))) {
+				get_queue(queue);
+				KBASE_KTRACE_ADD_CSF_GRP_Q(kbdev, CSI_TILER_OOM_INTERRUPT, group, queue, cs_req ^ cs_ack);
+				if (WARN_ON(!queue_work(
+					    wq, &queue->oom_event_work))) {
+					/* The work item shall not have been
+					 * already queued, there can be only
+					 * one pending OoM event for a
+					 * queue.
+					 */
+					release_queue(queue);
+				}
+			}
+
+			if ((cs_req & CS_REQ_PROTM_PEND_MASK) ^
+			    (cs_ack & CS_ACK_PROTM_PEND_MASK)) {
+				dev_dbg(kbdev->dev,
+					"Protected mode entry request for queue on csi %d bound to group-%d on slot %d",
+					queue->csi_index, group->handle,
+					group->csg_nr);
+
+				bitmap_set(group->protm_pending_bitmap, i, 1);
+				protm_pend = true;
+			}
+		}
+
+		remaining &= ~(1 << i);
+	}
+
+	if (protm_pend)
+		queue_work(group->kctx->csf.wq, &group->protm_event_work);
+}
+
+/**
+ * process_csg_interrupts - Process interrupts for a command stream group.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command stream
+ *         front-end interface.
+ * @csg_nr: Command stream group number.
+ *
+ * Handles interrupts for a command stream group and for streams within it.
+ *
+ * If the CSG's request register value differs from its acknowledge register
+ * then the firmware is notifying the host of an event concerning the whole
+ * group. The actions required are then determined by examining which
+ * notification flags differ between those two register values.
+ *
+ * See process_cs_interrupts() for details of per-stream interrupt handling.
+ */
+static void process_csg_interrupts(struct kbase_device *const kbdev,
+	int const csg_nr)
+{
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	struct kbase_queue_group *group;
+	u32 req, ack, irqreq, irqack;
+
+	kbase_csf_scheduler_spin_lock_assert_held(kbdev);
+
+	if (WARN_ON(csg_nr >= kbdev->csf.global_iface.group_num))
+		return;
+
+	ginfo = &kbdev->csf.global_iface.groups[csg_nr];
+	req = kbase_csf_firmware_csg_input_read(ginfo, CSG_REQ);
+	ack = kbase_csf_firmware_csg_output(ginfo, CSG_ACK);
+	irqreq = kbase_csf_firmware_csg_output(ginfo, CSG_IRQ_REQ);
+	irqack = kbase_csf_firmware_csg_input_read(ginfo, CSG_IRQ_ACK);
+
+	/* There may not be any pending CSG/CS interrupts to process */
+	if ((req == ack) && (irqreq == irqack))
+		return;
+
+	/* Immediately set IRQ_ACK bits to be same as the IRQ_REQ bits before
+	 * examining the CS_ACK & CS_REQ bits. This would ensure that Host
+	 * doesn't misses an interrupt for the CS in the race scenario where
+	 * whilst Host is servicing an interrupt for the CS, firmware sends
+	 * another interrupt for that CS.
+	 */
+	kbase_csf_firmware_csg_input(ginfo, CSG_IRQ_ACK, irqreq);
+
+	group = kbase_csf_scheduler_get_group_on_slot(kbdev, csg_nr);
+
+	/* The group pointer can be NULL here if interrupts for the group
+	 * (like SYNC_UPDATE, IDLE notification) were delayed and arrived
+	 * just after the suspension of group completed. However if not NULL
+	 * then the group pointer cannot disappear even if User tries to
+	 * terminate the group whilst this loop is running as scheduler
+	 * spinlock is held and for freeing a group that is resident on a CSG
+	 * slot scheduler spinlock is required.
+	 */
+	if (!group)
+		return;
+
+	if (WARN_ON(kbase_csf_scheduler_group_get_slot_locked(group) != csg_nr))
+		return;
+
+	if ((req ^ ack) & CSG_REQ_SYNC_UPDATE) {
+		kbase_csf_firmware_csg_input_mask(ginfo,
+			CSG_REQ, ack, CSG_REQ_SYNC_UPDATE);
+
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SYNC_UPDATE_INTERRUPT, group, req ^ ack);
+		kbase_csf_event_signal_cpu_only(group->kctx);
+	}
+
+	/* IDLE and TILER_OOM can be safely ignored because they will be
+	 * raised again if the group is assigned a CSG slot in future.
+	 * TILER_OOM and PROGRESS_TIMER_EVENT may terminate the group.
+	 */
+	if (!kbase_csf_scheduler_group_events_enabled(kbdev, group))
+		return;
+
+	if ((req ^ ack) & CSG_REQ_IDLE_MASK) {
+		kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ, ack,
+			CSG_REQ_IDLE_MASK);
+
+		set_bit(csg_nr, kbdev->csf.scheduler.csg_slots_idle_mask);
+
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev,  CSG_IDLE_INTERRUPT, group, req ^ ack);
+		dev_dbg(kbdev->dev, "Idle notification received for Group %u on slot %d\n",
+			 group->handle, csg_nr);
+	}
+
+	if ((req ^ ack) & CSG_REQ_PROGRESS_TIMER_EVENT_MASK) {
+		kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ, ack,
+			CSG_REQ_PROGRESS_TIMER_EVENT_MASK);
+
+		dev_dbg(kbdev->dev, "Timeout notification received for Group %u on slot %d\n",
+			group->handle, csg_nr);
+
+		queue_work(group->kctx->csf.wq, &group->timer_event_work);
+	}
+
+	process_cs_interrupts(group, ginfo, irqreq, irqack);
+}
+
+void kbase_csf_interrupt(struct kbase_device *kbdev, u32 val)
+{
+	unsigned long flags;
+	u32 remaining = val;
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_CLEAR), val);
+
+	if (val & JOB_IRQ_GLOBAL_IF) {
+		const struct kbase_csf_global_iface *const global_iface =
+			&kbdev->csf.global_iface;
+		struct kbase_csf_scheduler *scheduler =	&kbdev->csf.scheduler;
+
+		kbdev->csf.interrupt_received = true;
+		remaining &= ~JOB_IRQ_GLOBAL_IF;
+
+		if (!kbdev->csf.firmware_reloaded)
+			kbase_csf_firmware_reload_completed(kbdev);
+		else if (kbdev->csf.glb_init_request_pending)
+			kbase_pm_update_state(kbdev);
+
+		if (global_iface->output) {
+			u32 glb_req, glb_ack;
+
+			kbase_csf_scheduler_spin_lock(kbdev, &flags);
+			glb_req = kbase_csf_firmware_global_input_read(
+					global_iface, GLB_REQ);
+			glb_ack = kbase_csf_firmware_global_output(
+					global_iface, GLB_ACK);
+
+			if ((glb_req ^ glb_ack) & GLB_REQ_PROTM_EXIT_MASK) {
+				dev_dbg(kbdev->dev, "Protected mode exit interrupt received");
+				kbase_csf_firmware_global_input_mask(
+						global_iface, GLB_REQ, glb_ack,
+						GLB_REQ_PROTM_EXIT_MASK);
+				WARN_ON(!kbase_csf_scheduler_protected_mode_in_use(kbdev));
+				scheduler->active_protm_grp = NULL;
+				KBASE_KTRACE_ADD(kbdev, SCHEDULER_EXIT_PROTM, NULL, 0u);
+			}
+			kbase_csf_scheduler_spin_unlock(kbdev, flags);
+		}
+
+		if (!remaining) {
+			wake_up_all(&kbdev->csf.event_wait);
+			return;
+		}
+	}
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	while (remaining != 0) {
+		int const csg_nr = ffs(remaining) - 1;
+
+		process_csg_interrupts(kbdev, csg_nr);
+		remaining &= ~(1 << csg_nr);
+	}
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	wake_up_all(&kbdev->csf.event_wait);
+}
+
+void kbase_csf_doorbell_mapping_term(struct kbase_device *kbdev)
+{
+	if (kbdev->csf.db_filp) {
+		struct page *page = as_page(kbdev->csf.dummy_db_page);
+
+		kbase_mem_pool_free(
+			&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+			page, false);
+
+		fput(kbdev->csf.db_filp);
+	}
+}
+
+int kbase_csf_doorbell_mapping_init(struct kbase_device *kbdev)
+{
+	struct tagged_addr phys;
+	struct file *filp;
+	int ret;
+
+	filp = shmem_file_setup("mali csf", MAX_LFS_FILESIZE, VM_NORESERVE);
+	if (IS_ERR(filp))
+		return PTR_ERR(filp);
+
+	ret = kbase_mem_pool_alloc_pages(
+		&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+		1, &phys, false);
+
+	if (ret <= 0) {
+		fput(filp);
+		return ret;
+	}
+
+	kbdev->csf.db_filp = filp;
+	kbdev->csf.dummy_db_page = phys;
+	kbdev->csf.db_file_offsets = 0;
+
+	return 0;
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h
new file mode 100644
index 000000000000..c183d0a32302
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf.h
@@ -0,0 +1,444 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_H_
+#define _KBASE_CSF_H_
+
+#include "mali_kbase_csf_kcpu.h"
+#include "mali_kbase_csf_scheduler.h"
+#include "mali_kbase_csf_firmware.h"
+#include "mali_kbase_csf_protected_memory.h"
+
+/* Indicate invalid command stream h/w interface
+ */
+#define KBASEP_IF_NR_INVALID ((s8)-1)
+
+/* Indicate invalid command stream group number for a GPU command queue group
+ */
+#define KBASEP_CSG_NR_INVALID ((s8)-1)
+
+/* Indicate invalid user doorbell number for a GPU command queue
+ */
+#define KBASEP_USER_DB_NR_INVALID ((s8)-1)
+
+/* Waiting timeout for global request completion acknowledgment */
+#define GLB_REQ_WAIT_TIMEOUT_MS (300) /* 300 milliseconds */
+
+#define CSG_REQ_EP_CFG (0x1 << CSG_REQ_EP_CFG_SHIFT)
+#define CSG_REQ_SYNC_UPDATE (0x1 << CSG_REQ_SYNC_UPDATE_SHIFT)
+#define FIRMWARE_PING_INTERVAL_MS (2000) /* 2 seconds */
+
+/**
+ * enum kbase_csf_event_callback_action - return type for CSF event callbacks.
+ *
+ * @KBASE_CSF_EVENT_CALLBACK_FIRST: Never set explicitly.
+ * It doesn't correspond to any action or type of event callback.
+ *
+ * @KBASE_CSF_EVENT_CALLBACK_KEEP: The callback will remain registered.
+ *
+ * @KBASE_CSF_EVENT_CALLBACK_REMOVE: The callback will be removed
+ * immediately upon return.
+ *
+ * @KBASE_CSF_EVENT_CALLBACK_LAST: Never set explicitly.
+ * It doesn't correspond to any action or type of event callback.
+ */
+enum kbase_csf_event_callback_action {
+	KBASE_CSF_EVENT_CALLBACK_FIRST = 0,
+	KBASE_CSF_EVENT_CALLBACK_KEEP,
+	KBASE_CSF_EVENT_CALLBACK_REMOVE,
+	KBASE_CSF_EVENT_CALLBACK_LAST,
+};
+
+/**
+ * kbase_csf_event_callback_action - type for callback functions to be
+ *                                   called upon CSF events.
+ *
+ * This is the type of callback functions that can be registered
+ * for CSF events. These function calls shall be triggered by any call
+ * to kbase_csf_event_signal.
+ *
+ * @param:   Generic parameter to pass to the callback function.
+ *
+ * Return: KBASE_CSF_EVENT_CALLBACK_KEEP if the callback should remain
+ * registered, or KBASE_CSF_EVENT_CALLBACK_REMOVE if it should be removed.
+ */
+typedef enum kbase_csf_event_callback_action kbase_csf_event_callback(void *param);
+
+/**
+ * kbase_csf_event_wait_add - Add a CSF event callback
+ *
+ * This function adds an event callback to the list of CSF event callbacks
+ * belonging to a given Kbase context, to be triggered when a CSF event is
+ * signalled by kbase_csf_event_signal.
+ *
+ * @kctx:      The Kbase context the @callback should be registered to.
+ * @callback:  The callback function to register.
+ * @param:     Custom parameter to be passed to the @callback function.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_event_wait_add(struct kbase_context *kctx,
+		kbase_csf_event_callback *callback, void *param);
+
+/**
+ * kbase_csf_event_wait_remove - Remove a CSF event callback
+ *
+ * This function removes an event callback from the list of CSF event callbacks
+ * belonging to a given Kbase context.
+ *
+ * @kctx:      The kbase context the @callback should be removed from.
+ * @callback:  The callback function to remove.
+ * @param:     Custom parameter that would have been passed to the @p callback
+ *             function.
+ */
+void kbase_csf_event_wait_remove(struct kbase_context *kctx,
+		kbase_csf_event_callback *callback, void *param);
+
+/**
+ * kbase_csf_event_wait_remove_all - Removes all CSF event callbacks
+ *
+ * This function empties the list of CSF event callbacks belonging to a given
+ * Kbase context.
+ *
+ * @kctx:  The kbase context for which CSF event callbacks have to be removed.
+ */
+void kbase_csf_event_wait_remove_all(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_read_error - Read command stream fatal error
+ *
+ * This function takes the command stream fatal error from context's ordered
+ * error_list, copies its contents to @event_data.
+ *
+ * @kctx:       The kbase context to read fatal error from
+ * @event_data: Caller-provided buffer to copy the fatal error to
+ *
+ * Return: true if fatal error is read successfully.
+ */
+bool kbase_csf_read_error(struct kbase_context *kctx,
+		struct base_csf_notification *event_data);
+
+/**
+ * kbase_csf_error_pending - Check whether fatal error is pending
+ *
+ * @kctx:  The kbase context to check fatal error upon.
+ *
+ * Return: true if fatal error is pending.
+ */
+bool kbase_csf_error_pending(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_event_signal - Signal a CSF event
+ *
+ * This function triggers all the CSF event callbacks that are registered to
+ * a given Kbase context, and also signals the thread of userspace driver
+ * (front-end), waiting for the CSF event.
+ *
+ * @kctx:  The kbase context whose CSF event callbacks shall be triggered.
+ * @notify_gpu: Flag to indicate if CSF firmware should be notified of the
+ *              signaling of event that happened on the Driver side, either
+ *              the signal came from userspace or from kcpu queues.
+ */
+void kbase_csf_event_signal(struct kbase_context *kctx, bool notify_gpu);
+
+static inline void kbase_csf_event_signal_notify_gpu(struct kbase_context *kctx)
+{
+	kbase_csf_event_signal(kctx, true);
+}
+
+static inline void kbase_csf_event_signal_cpu_only(struct kbase_context *kctx)
+{
+	kbase_csf_event_signal(kctx, false);
+}
+
+/**
+ * kbase_csf_ctx_init - Initialize the command-stream front-end for a GPU
+ *                      address space.
+ *
+ * @kctx:	Pointer to the kbase context which is being initialized.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+int kbase_csf_ctx_init(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_ctx_handle_fault - Terminate queue groups & notify fault upon
+ *                              GPU bus fault, MMU page fault or similar.
+ *
+ * This function terminates all GPU command queue groups in the context and
+ * notifies the event notification thread of the fault.
+ *
+ * @kctx:       Pointer to faulty kbase context.
+ * @fault:      Pointer to the fault.
+ */
+void kbase_csf_ctx_handle_fault(struct kbase_context *kctx,
+		struct kbase_fault *fault);
+
+/**
+ * kbase_csf_ctx_term - Terminate the command-stream front-end for a GPU
+ *                      address space.
+ *
+ * This function terminates any remaining CSGs and CSs which weren't destroyed
+ * before context termination.
+ *
+ * @kctx:	Pointer to the kbase context which is being terminated.
+ */
+void kbase_csf_ctx_term(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_queue_register - Register a GPU command queue.
+ *
+ * @kctx:	Pointer to the kbase context within which the
+ *		queue is to be registered.
+ * @reg:	Pointer to the structure which contains details of the
+ *		queue to be registered within the provided
+ *		context.
+ *
+ * Return:	0 on success, or negative on failure.
+ */
+int kbase_csf_queue_register(struct kbase_context *kctx,
+			     struct kbase_ioctl_cs_queue_register *reg);
+
+/**
+ * kbase_csf_queue_terminate - Terminate a GPU command queue.
+ *
+ * @kctx:	Pointer to the kbase context within which the
+ *		queue is to be terminated.
+ * @term:	Pointer to the structure which identifies which
+ *		queue is to be terminated.
+ */
+void kbase_csf_queue_terminate(struct kbase_context *kctx,
+			      struct kbase_ioctl_cs_queue_terminate *term);
+
+/**
+ * kbase_csf_alloc_command_stream_user_pages - Allocate resources for a
+ *                                             GPU command queue.
+ *
+ * This function allocates a pair of User mode input/output pages for a
+ * GPU command queue and maps them in the shared interface segment of MCU
+ * firmware address space. Also reserves a hardware doorbell page for the queue.
+ *
+ * @kctx:	Pointer to the kbase context within which the resources
+ *		for the queue are being allocated.
+ * @queue:	Pointer to the queue for which to allocate resources.
+ *
+ * Return:	0 on success, or negative on failure.
+ */
+int kbase_csf_alloc_command_stream_user_pages(struct kbase_context *kctx,
+			struct kbase_queue *queue);
+
+/**
+ * kbase_csf_queue_bind - Bind a GPU command queue to a queue group.
+ *
+ * @kctx:	The kbase context.
+ * @bind:	Pointer to the union which specifies a queue group and a
+ *		queue to be bound to that group.
+ *
+ * Return:	0 on success, or negative on failure.
+ */
+int kbase_csf_queue_bind(struct kbase_context *kctx,
+			 union kbase_ioctl_cs_queue_bind *bind);
+
+/**
+ * kbase_csf_queue_unbind - Unbind a GPU command queue from a queue group
+ *			    to which it has been bound and free
+ *			    resources allocated for this queue if there
+ *			    are any.
+ *
+ * @queue:	Pointer to queue to be unbound.
+ */
+void kbase_csf_queue_unbind(struct kbase_queue *queue);
+
+/**
+ * kbase_csf_queue_kick - Schedule a GPU command queue on the firmware
+ *
+ * @kctx:	The kbase context.
+ * @kick:	Pointer to the struct which specifies the queue
+ *		that needs to be scheduled.
+ *
+ * Return:	0 on success, or negative on failure.
+ */
+int kbase_csf_queue_kick(struct kbase_context *kctx,
+			 struct kbase_ioctl_cs_queue_kick *kick);
+
+/** Find if given the queue group handle is valid.
+ *
+ * This function is used to determine if the queue group handle is valid.
+ *
+ * @kctx:		The kbase context under which the queue group exists.
+ * @group_handle:	Handle for the group which uniquely identifies it within
+ *			the context with which it was created.
+ *
+ * Return:		0 on success, or negative on failure.
+ */
+int kbase_csf_queue_group_handle_is_valid(struct kbase_context *kctx,
+	u8 group_handle);
+
+/**
+ * kbase_csf_queue_group_create - Create a GPU command queue group.
+ *
+ * @kctx:	Pointer to the kbase context within which the
+ *		queue group is to be created.
+ * @create:	Pointer to the structure which contains details of the
+ *		queue group which is to be created within the
+ *		provided kbase context.
+ *
+ * Return:	0 on success, or negative on failure.
+ */
+int kbase_csf_queue_group_create(struct kbase_context *kctx,
+	union kbase_ioctl_cs_queue_group_create *create);
+
+/**
+ * kbase_csf_queue_group_terminate - Terminate a GPU command queue group.
+ *
+ * @kctx:		Pointer to the kbase context within which the
+ *			queue group is to be terminated.
+ * @group_handle:	Pointer to the structure which identifies the queue
+ *			group which is to be terminated.
+ */
+void kbase_csf_queue_group_terminate(struct kbase_context *kctx,
+	u8 group_handle);
+
+/**
+ * kbase_csf_term_descheduled_queue_group - Terminate a GPU command queue
+ *                                          group that is not operational
+ *                                          inside the scheduler.
+ *
+ * @group:	Pointer to the structure which identifies the queue
+ *		group to be terminated. The function assumes that the caller
+ *		is sure that the given group is not operational inside the
+ *		scheduler. If in doubt, use its alternative:
+ *		@ref kbase_csf_queue_group_terminate().
+ */
+void kbase_csf_term_descheduled_queue_group(struct kbase_queue_group *group);
+
+/**
+ * kbase_csf_queue_group_suspend - Suspend a GPU command queue group
+ *
+ * This function is used to suspend a queue group and copy the suspend buffer.
+ *
+ * @kctx:		The kbase context for which the queue group is to be
+ *			suspended.
+ * @sus_buf:		Pointer to the structure which contains details of the
+ *			user buffer and its kernel pinned pages.
+ * @size:		The size in bytes for the user provided buffer.
+ * @group_handle:	Handle for the group which uniquely identifies it within
+ *			the context within which it was created.
+ *
+ * Return:		0 on success or negative value if failed to suspend
+ *			queue group and copy suspend buffer contents.
+ */
+int kbase_csf_queue_group_suspend(struct kbase_context *kctx,
+	struct kbase_suspend_copy_buffer *sus_buf, u8 group_handle);
+
+/**
+ * kbase_csf_interrupt - Handle interrupts issued by CSF firmware.
+ *
+ * @kbdev: The kbase device to handle an IRQ for
+ * @val:   The value of JOB IRQ status register which triggered the interrupt
+ */
+void kbase_csf_interrupt(struct kbase_device *kbdev, u32 val);
+
+/**
+ * kbase_csf_doorbell_mapping_init - Initialize the bitmap of Hw doorbell pages
+ *                           used to track their availability.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+int kbase_csf_doorbell_mapping_init(struct kbase_device *kbdev);
+
+void kbase_csf_doorbell_mapping_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_ring_csg_doorbell - ring the doorbell for a command stream group
+ *                               interface.
+ *
+ * The function kicks a notification on the command stream group interface to
+ * firmware.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ * @slot: Index of command stream group interface for ringing the door-bell.
+ */
+void kbase_csf_ring_csg_doorbell(struct kbase_device *kbdev, int slot);
+
+/**
+ * kbase_csf_ring_csg_slots_doorbell - ring the doorbell for a set of command
+ *                                     stream group interfaces.
+ *
+ * The function kicks a notification on a set of command stream group
+ * interfaces to firmware.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ * @slot_bitmap: bitmap for the given slots, slot-0 on bit-0, etc.
+ */
+void kbase_csf_ring_csg_slots_doorbell(struct kbase_device *kbdev,
+				       u32 slot_bitmap);
+
+/**
+ * kbase_csf_ring_cs_kernel_doorbell - ring the kernel doorbell for a queue
+ *
+ * The function kicks a notification to the firmware for the command stream
+ * interface to which the queue is bound.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ * @queue: Pointer to the queue for ringing the door-bell.
+ */
+void kbase_csf_ring_cs_kernel_doorbell(struct kbase_device *kbdev,
+			struct kbase_queue *queue);
+
+/**
+ * kbase_csf_ring_cs_user_doorbell - ring the user doorbell allocated for a
+ *                                   queue.
+ *
+ * The function kicks a notification to the firmware on the doorbell assigned
+ * to the queue.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ * @queue: Pointer to the queue for ringing the door-bell.
+ */
+void kbase_csf_ring_cs_user_doorbell(struct kbase_device *kbdev,
+			struct kbase_queue *queue);
+
+/**
+ * kbase_csf_active_queue_groups_reset - Reset the state of all active GPU
+ *                            command queue groups associated with the context.
+ *
+ * @kbdev:     Instance of a GPU platform device that implements a command
+ *             stream front-end interface.
+ * @kctx:      The kbase context.
+ *
+ * This function will iterate through all the active/scheduled GPU command
+ * queue groups associated with the context, deschedule and mark them as
+ * terminated (which will then lead to unbinding of all the queues bound to
+ * them) and also no more work would be allowed to execute for them.
+ *
+ * This is similar to the action taken in response to an unexpected OoM event.
+ */
+void kbase_csf_active_queue_groups_reset(struct kbase_device *kbdev,
+			struct kbase_context *kctx);
+
+#endif /* _KBASE_CSF_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c
new file mode 100644
index 000000000000..fd8329ba9422
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.c
@@ -0,0 +1,460 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_csf_csg_debugfs.h"
+#include <mali_kbase.h>
+#include <linux/seq_file.h>
+#include <linux/delay.h>
+#include <csf/mali_kbase_csf_trace_buffer.h>
+
+#ifdef CONFIG_DEBUG_FS
+#include "mali_kbase_csf_tl_reader.h"
+
+static void kbasep_csf_scheduler_dump_active_queue_cs_status_wait(
+		struct seq_file *file,
+		u32 wait_status,
+		u32 wait_sync_value,
+		u64 wait_sync_live_value,
+		u64 wait_sync_pointer)
+{
+#define WAITING "Waiting"
+#define NOT_WAITING "Not waiting"
+
+	seq_printf(file, "SB_MASK: %d\n",
+			CS_STATUS_WAIT_SB_MASK_GET(wait_status));
+	seq_printf(file, "PROGRESS_WAIT: %s\n",
+			CS_STATUS_WAIT_PROGRESS_WAIT_GET(wait_status) ?
+			WAITING : NOT_WAITING);
+	seq_printf(file, "PROTM_PEND: %s\n",
+			CS_STATUS_WAIT_PROTM_PEND_GET(wait_status) ?
+			WAITING : NOT_WAITING);
+	seq_printf(file, "SYNC_WAIT: %s\n",
+			CS_STATUS_WAIT_SYNC_WAIT_GET(wait_status) ?
+			WAITING : NOT_WAITING);
+	seq_printf(file, "WAIT_CONDITION: %s\n",
+			CS_STATUS_WAIT_SYNC_WAIT_CONDITION_GET(wait_status) ?
+			"greater than" : "less or equal");
+	seq_printf(file, "SYNC_POINTER: 0x%llx\n", wait_sync_pointer);
+	seq_printf(file, "SYNC_VALUE: %d\n", wait_sync_value);
+	seq_printf(file, "SYNC_LIVE_VALUE: 0x%016llx\n", wait_sync_live_value);
+}
+
+/**
+ * kbasep_csf_scheduler_dump_active_queue() - Print GPU command queue
+ *                                            debug information
+ *
+ * @file:  seq_file for printing to
+ * @queue: Address of a GPU command queue to examine
+ */
+static void kbasep_csf_scheduler_dump_active_queue(struct seq_file *file,
+		struct kbase_queue *queue)
+{
+	u32 *addr;
+	u64 cs_extract;
+	u64 cs_insert;
+	u32 cs_active;
+	u64 wait_sync_pointer;
+	u32 wait_status, wait_sync_value;
+	struct kbase_vmap_struct *mapping;
+	u64 *evt;
+	u64 wait_sync_live_value;
+
+	if (!queue)
+		return;
+
+	if (WARN_ON(queue->csi_index == KBASEP_IF_NR_INVALID ||
+		    !queue->group))
+		return;
+
+	/* Ring the doorbell to have firmware update CS_EXTRACT */
+	kbase_csf_ring_cs_user_doorbell(queue->kctx->kbdev, queue);
+	msleep(100);
+
+	addr = (u32 *)queue->user_io_addr;
+	cs_insert = addr[CS_INSERT_LO/4] | ((u64)addr[CS_INSERT_HI/4] << 32);
+
+	addr = (u32 *)(queue->user_io_addr + PAGE_SIZE);
+	cs_extract = addr[CS_EXTRACT_LO/4] | ((u64)addr[CS_EXTRACT_HI/4] << 32);
+	cs_active = addr[CS_ACTIVE/4];
+
+#define KBASEP_CSF_DEBUGFS_CS_HEADER_USER_IO \
+	"Bind Idx,     Ringbuf addr, Prio,    Insert offset,   Extract offset, Active, Doorbell\n"
+
+	seq_printf(file, KBASEP_CSF_DEBUGFS_CS_HEADER_USER_IO "%8d, %16llx, %4u, %16llx, %16llx, %6u, %8d\n",
+			queue->csi_index, queue->base_addr, queue->priority,
+			cs_insert, cs_extract, cs_active, queue->doorbell_nr);
+
+	/* Print status information for blocked group waiting for sync object */
+	if (kbase_csf_scheduler_group_get_slot(queue->group) < 0) {
+		if (CS_STATUS_WAIT_SYNC_WAIT_GET(queue->status_wait)) {
+			wait_status = queue->status_wait;
+			wait_sync_value = queue->sync_value;
+			wait_sync_pointer = queue->sync_ptr;
+
+			evt = (u64 *)kbase_phy_alloc_mapping_get(queue->kctx, wait_sync_pointer, &mapping);
+			if (evt) {
+				wait_sync_live_value = evt[0];
+				kbase_phy_alloc_mapping_put(queue->kctx, mapping);
+			} else {
+				wait_sync_live_value = U64_MAX;
+			}
+
+			kbasep_csf_scheduler_dump_active_queue_cs_status_wait(
+				file, wait_status, wait_sync_value,
+				wait_sync_live_value, wait_sync_pointer);
+		}
+	} else {
+		struct kbase_device const *const kbdev =
+			queue->group->kctx->kbdev;
+		struct kbase_csf_cmd_stream_group_info const *const ginfo =
+			&kbdev->csf.global_iface.groups[queue->group->csg_nr];
+		struct kbase_csf_cmd_stream_info const *const stream =
+			&ginfo->streams[queue->csi_index];
+		u64 cmd_ptr;
+		u32 req_res;
+
+		if (WARN_ON(!stream))
+			return;
+
+		cmd_ptr = kbase_csf_firmware_cs_output(stream,
+				CS_STATUS_CMD_PTR_LO);
+		cmd_ptr |= (u64)kbase_csf_firmware_cs_output(stream,
+				CS_STATUS_CMD_PTR_HI) << 32;
+		req_res = kbase_csf_firmware_cs_output(stream,
+					CS_STATUS_REQ_RESOURCE);
+
+		seq_printf(file, "CMD_PTR: 0x%llx\n", cmd_ptr);
+		seq_printf(file, "REQ_RESOURCE [COMPUTE]: %d\n",
+			CS_STATUS_REQ_RESOURCE_COMPUTE_RESOURCES_GET(req_res));
+		seq_printf(file, "REQ_RESOURCE [FRAGMENT]: %d\n",
+			CS_STATUS_REQ_RESOURCE_FRAGMENT_RESOURCES_GET(req_res));
+		seq_printf(file, "REQ_RESOURCE [TILER]: %d\n",
+			CS_STATUS_REQ_RESOURCE_TILER_RESOURCES_GET(req_res));
+		seq_printf(file, "REQ_RESOURCE [IDVS]: %d\n",
+			CS_STATUS_REQ_RESOURCE_IDVS_RESOURCES_GET(req_res));
+
+		wait_status = kbase_csf_firmware_cs_output(stream,
+				CS_STATUS_WAIT);
+		wait_sync_value = kbase_csf_firmware_cs_output(stream,
+					CS_STATUS_WAIT_SYNC_VALUE);
+		wait_sync_pointer = kbase_csf_firmware_cs_output(stream,
+					CS_STATUS_WAIT_SYNC_POINTER_LO);
+		wait_sync_pointer |= (u64)kbase_csf_firmware_cs_output(stream,
+					CS_STATUS_WAIT_SYNC_POINTER_HI) << 32;
+
+		evt = (u64 *)kbase_phy_alloc_mapping_get(queue->kctx, wait_sync_pointer, &mapping);
+		if (evt) {
+			wait_sync_live_value = evt[0];
+			kbase_phy_alloc_mapping_put(queue->kctx, mapping);
+		} else {
+			wait_sync_live_value = U64_MAX;
+		}
+
+		kbasep_csf_scheduler_dump_active_queue_cs_status_wait(
+			file, wait_status, wait_sync_value,
+			wait_sync_live_value, wait_sync_pointer);
+	}
+
+	seq_puts(file, "\n");
+}
+
+/* Waiting timeout for STATUS_UPDATE acknowledgment, in milliseconds */
+#define CSF_STATUS_UPDATE_TO_MS (100)
+
+static void kbasep_csf_scheduler_dump_active_group(struct seq_file *file,
+		struct kbase_queue_group *const group)
+{
+	if (kbase_csf_scheduler_group_get_slot(group) >= 0) {
+		struct kbase_device *const kbdev = group->kctx->kbdev;
+		unsigned long flags;
+		u32 ep_c, ep_r;
+		char exclusive;
+		struct kbase_csf_cmd_stream_group_info const *const ginfo =
+			&kbdev->csf.global_iface.groups[group->csg_nr];
+		long remaining =
+			kbase_csf_timeout_in_jiffies(CSF_STATUS_UPDATE_TO_MS);
+		u8 slot_priority =
+			kbdev->csf.scheduler.csg_slots[group->csg_nr].priority;
+
+		kbase_csf_scheduler_spin_lock(kbdev, &flags);
+		kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ,
+				~kbase_csf_firmware_csg_output(ginfo, CSG_ACK),
+				CSG_REQ_STATUS_UPDATE_MASK);
+		kbase_csf_scheduler_spin_unlock(kbdev, flags);
+		kbase_csf_ring_csg_doorbell(kbdev, group->csg_nr);
+
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+			!((kbase_csf_firmware_csg_input_read(ginfo, CSG_REQ) ^
+			   kbase_csf_firmware_csg_output(ginfo, CSG_ACK)) &
+			   CSG_REQ_STATUS_UPDATE_MASK), remaining);
+
+		ep_c = kbase_csf_firmware_csg_output(ginfo,
+				CSG_STATUS_EP_CURRENT);
+		ep_r = kbase_csf_firmware_csg_output(ginfo, CSG_STATUS_EP_REQ);
+
+		if (CSG_STATUS_EP_REQ_EXCLUSIVE_COMPUTE_GET(ep_r))
+			exclusive = 'C';
+		else if (CSG_STATUS_EP_REQ_EXCLUSIVE_FRAGMENT_GET(ep_r))
+			exclusive = 'F';
+		else
+			exclusive = '0';
+
+		if (!remaining) {
+			dev_err(kbdev->dev,
+				"Timed out for STATUS_UPDATE on group %d on slot %d",
+				group->handle, group->csg_nr);
+
+			seq_printf(file, "*** Warn: Timed out for STATUS_UPDATE on slot %d\n",
+				group->csg_nr);
+			seq_printf(file, "*** The following group-record is likely stale\n");
+		}
+
+		seq_puts(file, "GroupID, CSG NR, CSG Prio, Run State, Priority, C_EP(Alloc/Req), F_EP(Alloc/Req), T_EP(Alloc/Req), Exclusive\n");
+		seq_printf(file, "%7d, %6d, %8d, %9d, %8d, %11d/%3d, %11d/%3d, %11d/%3d, %9c\n",
+			group->handle,
+			group->csg_nr,
+			slot_priority,
+			group->run_state,
+			group->priority,
+			CSG_STATUS_EP_CURRENT_COMPUTE_EP_GET(ep_c),
+			CSG_STATUS_EP_REQ_COMPUTE_EP_GET(ep_r),
+			CSG_STATUS_EP_CURRENT_FRAGMENT_EP_GET(ep_c),
+			CSG_STATUS_EP_REQ_FRAGMENT_EP_GET(ep_r),
+			CSG_STATUS_EP_CURRENT_TILER_EP_GET(ep_c),
+			CSG_STATUS_EP_REQ_TILER_EP_GET(ep_r),
+			exclusive);
+	} else {
+		seq_puts(file, "GroupID, CSG NR, Run State, Priority\n");
+		seq_printf(file, "%7d, %6d, %9d, %8d\n",
+			group->handle,
+			group->csg_nr,
+			group->run_state,
+			group->priority);
+	}
+
+	if (group->run_state != KBASE_CSF_GROUP_TERMINATED) {
+		unsigned int i;
+
+		seq_puts(file, "Bound queues:\n");
+
+		for (i = 0; i < MAX_SUPPORTED_STREAMS_PER_GROUP; i++) {
+			kbasep_csf_scheduler_dump_active_queue(file,
+					group->bound_queues[i]);
+		}
+	}
+
+	seq_puts(file, "\n");
+}
+
+/**
+ * kbasep_csf_queue_group_debugfs_show() - Print per-context GPU command queue
+ *					   group debug information
+ *
+ * @file: The seq_file for printing to
+ * @data: The debugfs dentry private data, a pointer to kbase context
+ *
+ * Return: Negative error code or 0 on success.
+ */
+static int kbasep_csf_queue_group_debugfs_show(struct seq_file *file,
+		void *data)
+{
+	u32 gr;
+	struct kbase_context *const kctx = file->private;
+	struct kbase_device *const kbdev = kctx->kbdev;
+
+	if (WARN_ON(!kctx))
+		return -EINVAL;
+
+	seq_printf(file, "MALI_CSF_CSG_DEBUGFS_VERSION: v%u\n",
+			MALI_CSF_CSG_DEBUGFS_VERSION);
+
+	mutex_lock(&kctx->csf.lock);
+	kbase_csf_scheduler_lock(kbdev);
+	for (gr = 0; gr < MAX_QUEUE_GROUP_NUM; gr++) {
+		struct kbase_queue_group *const group =
+			kctx->csf.queue_groups[gr];
+
+		if (group)
+			kbasep_csf_scheduler_dump_active_group(file, group);
+	}
+	kbase_csf_scheduler_unlock(kbdev);
+	mutex_unlock(&kctx->csf.lock);
+
+	return 0;
+}
+
+/**
+ * kbasep_csf_scheduler_dump_active_groups() - Print debug info for active
+ *                                             GPU command queue groups
+ *
+ * @file: The seq_file for printing to
+ * @data: The debugfs dentry private data, a pointer to kbase_device
+ *
+ * Return: Negative error code or 0 on success.
+ */
+static int kbasep_csf_scheduler_dump_active_groups(struct seq_file *file,
+		void *data)
+{
+	u32 csg_nr;
+	struct kbase_device *kbdev = file->private;
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+
+	seq_printf(file, "MALI_CSF_CSG_DEBUGFS_VERSION: v%u\n",
+			MALI_CSF_CSG_DEBUGFS_VERSION);
+
+	kbase_csf_scheduler_lock(kbdev);
+	for (csg_nr = 0; csg_nr < num_groups; csg_nr++) {
+		struct kbase_queue_group *const group =
+			kbdev->csf.scheduler.csg_slots[csg_nr].resident_group;
+
+		if (!group)
+			continue;
+
+		seq_printf(file, "\nCtx %d_%d\n", group->kctx->tgid,
+				group->kctx->id);
+
+		kbasep_csf_scheduler_dump_active_group(file, group);
+	}
+	kbase_csf_scheduler_unlock(kbdev);
+
+	return 0;
+}
+
+static int kbasep_csf_queue_group_debugfs_open(struct inode *in,
+		struct file *file)
+{
+	return single_open(file, kbasep_csf_queue_group_debugfs_show,
+			in->i_private);
+}
+
+static int kbasep_csf_active_queue_groups_debugfs_open(struct inode *in,
+		struct file *file)
+{
+	return single_open(file, kbasep_csf_scheduler_dump_active_groups,
+			in->i_private);
+}
+
+static const struct file_operations kbasep_csf_queue_group_debugfs_fops = {
+	.open = kbasep_csf_queue_group_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+void kbase_csf_queue_group_debugfs_init(struct kbase_context *kctx)
+{
+	struct dentry *file;
+#if (KERNEL_VERSION(4, 7, 0) <= LINUX_VERSION_CODE)
+	const mode_t mode = 0444;
+#else
+	const mode_t mode = 0400;
+#endif
+
+	if (WARN_ON(!kctx || IS_ERR_OR_NULL(kctx->kctx_dentry)))
+		return;
+
+	file = debugfs_create_file("groups", mode,
+		kctx->kctx_dentry, kctx, &kbasep_csf_queue_group_debugfs_fops);
+
+	if (IS_ERR_OR_NULL(file)) {
+		dev_warn(kctx->kbdev->dev,
+		    "Unable to create per context queue groups debugfs entry");
+	}
+}
+
+static const struct file_operations
+	kbasep_csf_active_queue_groups_debugfs_fops = {
+	.open = kbasep_csf_active_queue_groups_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int kbasep_csf_debugfs_scheduling_timer_enabled_get(
+		void *data, u64 *val)
+{
+	struct kbase_device *const kbdev = data;
+
+	*val = kbase_csf_scheduler_timer_is_enabled(kbdev);
+
+	return 0;
+}
+
+static int kbasep_csf_debugfs_scheduling_timer_enabled_set(
+		void *data, u64 val)
+{
+	struct kbase_device *const kbdev = data;
+
+	kbase_csf_scheduler_timer_set_enabled(kbdev, val != 0);
+
+	return 0;
+}
+
+static int kbasep_csf_debugfs_scheduling_timer_kick_set(
+		void *data, u64 val)
+{
+	struct kbase_device *const kbdev = data;
+
+	kbase_csf_scheduler_kick(kbdev);
+
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(kbasep_csf_debugfs_scheduling_timer_enabled_fops,
+		&kbasep_csf_debugfs_scheduling_timer_enabled_get,
+		&kbasep_csf_debugfs_scheduling_timer_enabled_set,
+		"%llu\n");
+DEFINE_SIMPLE_ATTRIBUTE(kbasep_csf_debugfs_scheduling_timer_kick_fops,
+		NULL,
+		&kbasep_csf_debugfs_scheduling_timer_kick_set,
+		"%llu\n");
+
+void kbase_csf_debugfs_init(struct kbase_device *kbdev)
+{
+	debugfs_create_file("active_groups", 0444,
+		kbdev->mali_debugfs_directory, kbdev,
+		&kbasep_csf_active_queue_groups_debugfs_fops);
+
+	debugfs_create_file("scheduling_timer_enabled", 0644,
+			kbdev->mali_debugfs_directory, kbdev,
+			&kbasep_csf_debugfs_scheduling_timer_enabled_fops);
+	debugfs_create_file("scheduling_timer_kick", 0200,
+			kbdev->mali_debugfs_directory, kbdev,
+			&kbasep_csf_debugfs_scheduling_timer_kick_fops);
+
+	kbase_csf_tl_reader_debugfs_init(kbdev);
+	kbase_csf_firmware_trace_buffer_debugfs_init(kbdev);
+}
+
+#else
+/*
+ * Stub functions for when debugfs is disabled
+ */
+void kbase_csf_queue_group_debugfs_init(struct kbase_context *kctx)
+{
+}
+
+void kbase_csf_debugfs_init(struct kbase_device *kbdev)
+{
+}
+
+#endif /* CONFIG_DEBUG_FS */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.h
new file mode 100644
index 000000000000..c2e99d386f8c
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_csg_debugfs.h
@@ -0,0 +1,48 @@
+/*
+ *
+ * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_CSG_DEBUGFS_H_
+#define _KBASE_CSF_CSG_DEBUGFS_H_
+
+/* Forward declarations */
+struct kbase_device;
+struct kbase_context;
+struct kbase_queue_group;
+
+#define MALI_CSF_CSG_DEBUGFS_VERSION 0
+
+/**
+ * kbase_csf_queue_group_debugfs_init() - Add debugfs entry for queue groups
+ *                                        associated with @kctx.
+ *
+ * @kctx: Pointer to kbase_context
+ */
+void kbase_csf_queue_group_debugfs_init(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_debugfs_init() - Add a global debugfs entry for queue groups
+ *
+ * @kbdev: Pointer to the device
+ */
+void kbase_csf_debugfs_init(struct kbase_device *kbdev);
+
+#endif /* _KBASE_CSF_CSG_DEBUGFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h
new file mode 100644
index 000000000000..3829572a1aeb
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_defs.h
@@ -0,0 +1,883 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/* Definitions (types, defines, etcs) common to the command stream frontend.
+ * They are placed here to allow the hierarchy of header files to work.
+ */
+
+#ifndef _KBASE_CSF_DEFS_H_
+#define _KBASE_CSF_DEFS_H_
+
+#include <linux/types.h>
+#include <linux/wait.h>
+
+#include "mali_kbase_csf_firmware.h"
+
+/* Maximum number of KCPU command queues to be created per GPU address space.
+ */
+#define KBASEP_MAX_KCPU_QUEUES ((size_t)256)
+
+/* Maximum number of GPU command queue groups to be created per GPU address
+ * space.
+ */
+#define MAX_QUEUE_GROUP_NUM (256)
+
+/* Maximum number of GPU tiler heaps to allow to be created per GPU address
+ * space.
+ */
+#define MAX_TILER_HEAPS (128)
+
+/**
+ * enum kbase_csf_bind_state - bind state of the queue
+ *
+ * @KBASE_CSF_QUEUE_UNBOUND: Set when the queue is registered or when the link
+ * between queue and the group to which it was bound or being bound is removed.
+ * @KBASE_CSF_QUEUE_BIND_IN_PROGRESS: Set when the first part of bind operation
+ * has completed i.e. CS_QUEUE_BIND ioctl.
+ * @KBASE_CSF_QUEUE_BOUND: Set when the bind operation has completed i.e. IO
+ * pages have been mapped in the process address space.
+ */
+enum kbase_csf_queue_bind_state {
+	KBASE_CSF_QUEUE_UNBOUND,
+	KBASE_CSF_QUEUE_BIND_IN_PROGRESS,
+	KBASE_CSF_QUEUE_BOUND,
+};
+
+/**
+ * enum kbase_csf_reset_gpu_state - state of the gpu reset
+ *
+ * @KBASE_CSF_RESET_GPU_NOT_PENDING: Set when the GPU reset isn't pending
+ * @KBASE_CSF_RESET_GPU_HAPPENING: Set when the GPU reset process is occurring
+ * @KBASE_CSF_RESET_GPU_SILENT: Set when the GPU reset process is occurring,
+ * used when resetting the GPU as part of normal behavior (e.g. when exiting
+ * protected mode).
+ * @KBASE_CSF_RESET_GPU_FAILED: Set when an error is encountered during the
+ * GPU reset process. No more work could then be executed on GPU, unloading
+ * the Driver module is the only option.
+ */
+enum kbase_csf_reset_gpu_state {
+	KBASE_CSF_RESET_GPU_NOT_PENDING,
+	KBASE_CSF_RESET_GPU_HAPPENING,
+	KBASE_CSF_RESET_GPU_SILENT,
+	KBASE_CSF_RESET_GPU_FAILED,
+};
+
+/**
+ * enum kbase_csf_group_state - state of the GPU command queue group
+ *
+ * @KBASE_CSF_GROUP_INACTIVE:          Group is inactive and won't be
+ *                                     considered by scheduler for running on
+ *                                     command stream group slot.
+ * @KBASE_CSF_GROUP_RUNNABLE:          Group is in the list of runnable groups
+ *                                     and is subjected to time-slice based
+ *                                     scheduling. A start request would be
+ *                                     sent (or already has been sent) if the
+ *                                     group is assigned the command stream
+ *                                     group slot for the fist time.
+ * @KBASE_CSF_GROUP_IDLE:              Group is currently on a command stream
+ *                                     group slot but all the command streams
+ *                                     bound to the group have become either
+ *                                     idle or waiting on sync object.
+ *                                     Group could be evicted from the slot on
+ *                                     the next tick if there are no spare
+ *                                     slots left after scheduling non-idle
+ *                                     queue groups. If the group is kept on
+ *                                     slot then it would be moved to the
+ *                                     RUNNABLE state, also if one of the
+ *                                     queues bound to the group is kicked it
+ *                                     would be moved to the RUNNABLE state.
+ *                                     If the group is evicted from the slot it
+ *                                     would be moved to either
+ *                                     KBASE_CSF_GROUP_SUSPENDED_ON_IDLE or
+ *                                     KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC
+ *                                     state.
+ * @KBASE_CSF_GROUP_SUSPENDED:         Group was evicted from the command
+ *                                     stream group slot and is not running but
+ *                                     is still in the list of runnable groups
+ *                                     and subjected to time-slice based
+ *                                     scheduling. A resume request would be
+ *                                     sent when a command stream group slot is
+ *                                     re-assigned to the group and once the
+ *                                     resume is complete group would be moved
+ *                                     back to the RUNNABLE state.
+ * @KBASE_CSF_GROUP_SUSPENDED_ON_IDLE: Same as KBASE_CSF_GROUP_SUSPENDED except
+ *                                     that queue group also became idle before
+ *                                     the suspension. This state helps
+ *                                     Scheduler avoid scheduling the idle
+ *                                     groups over the non-idle groups in the
+ *                                     subsequent ticks. If one of the queues
+ *                                     bound to the group is kicked it would be
+ *                                     moved to the SUSPENDED state.
+ * @KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC: Same as GROUP_SUSPENDED_ON_IDLE
+ *                                          except that at least one command
+ *                                          stream bound to this group was
+ *                                          waiting for synchronization object
+ *                                          before the suspension.
+ * @KBASE_CSF_GROUP_FAULT_EVICTED:     Group is evicted from the scheduler due
+ *                                     to a fault condition, pending to be
+ *                                     terminated.
+ * @KBASE_CSF_GROUP_TERMINATED:        Group is no longer schedulable and is
+ *                                     pending to be deleted by Client, all the
+ *                                     queues bound to it have been unbound.
+ */
+enum kbase_csf_group_state {
+	KBASE_CSF_GROUP_INACTIVE,
+	KBASE_CSF_GROUP_RUNNABLE,
+	KBASE_CSF_GROUP_IDLE,
+	KBASE_CSF_GROUP_SUSPENDED,
+	KBASE_CSF_GROUP_SUSPENDED_ON_IDLE,
+	KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC,
+	KBASE_CSF_GROUP_FAULT_EVICTED,
+	KBASE_CSF_GROUP_TERMINATED,
+};
+
+/**
+ * enum kbase_csf_csg_slot_state - state of the command queue group slots under
+ *                                 the scheduler control.
+ *
+ * @CSG_SLOT_READY:     The slot is clean and ready to be programmed with a
+ *                      queue group.
+ * @CSG_SLOT_READY2RUN: The slot has been programmed with a queue group, i.e. a
+ *                      start or resume request has been sent to the firmware.
+ * @CSG_SLOT_RUNNING:   The queue group is running on the slot, acknowledgment
+ *                      of a start or resume request has been obtained from the
+ *                      firmware.
+ * @CSG_SLOT_DOWN2STOP: The suspend or terminate request for the queue group on
+ *                      the slot has been sent to the firmware.
+ * @CSG_SLOT_STOPPED:   The queue group is removed from the slot, acknowledgment
+ *                      of suspend or terminate request has been obtained from
+ *                      the firmware.
+ * @CSG_SLOT_READY2RUN_TIMEDOUT: The start or resume request sent on the slot
+ *                               for the queue group timed out.
+ * @CSG_SLOT_DOWN2STOP_TIMEDOUT: The suspend or terminate request for queue
+ *                               group on the slot timed out.
+ */
+enum kbase_csf_csg_slot_state {
+	CSG_SLOT_READY,
+	CSG_SLOT_READY2RUN,
+	CSG_SLOT_RUNNING,
+	CSG_SLOT_DOWN2STOP,
+	CSG_SLOT_STOPPED,
+	CSG_SLOT_READY2RUN_TIMEDOUT,
+	CSG_SLOT_DOWN2STOP_TIMEDOUT,
+};
+
+/**
+ * enum kbase_csf_scheduler_state - state of the scheduler operational phases.
+ *
+ * @SCHED_BUSY:         The scheduler is busy performing on tick schedule
+ *                      operations, the state of command stream group slots
+ *                      can't be changed.
+ * @SCHED_INACTIVE:     The scheduler is inactive, it is allowed to modify the
+ *                      state of command stream group slots by in-cycle
+ *                      priority scheduling.
+ * @SCHED_SUSPENDED:    The scheduler is in low-power mode with scheduling
+ *                      operations suspended and is not holding the power
+ *                      management reference. This can happen if the GPU
+ *                      becomes idle for a duration exceeding a threshold,
+ *                      or due to a system triggered suspend action.
+ */
+enum kbase_csf_scheduler_state {
+	SCHED_BUSY,
+	SCHED_INACTIVE,
+	SCHED_SUSPENDED,
+};
+
+/**
+ * struct kbase_csf_notification - Event or error generated as part of command
+ *                                 queue execution
+ *
+ * @data:      Event or error data returned to userspace
+ * @link:      Link to the linked list, &struct_kbase_csf_context.error_list.
+ */
+struct kbase_csf_notification {
+	struct base_csf_notification data;
+	struct list_head link;
+};
+
+/**
+ * struct kbase_queue - Object representing a GPU command queue.
+ *
+ * @kctx:        Pointer to the base context with which this GPU command queue
+ *               is associated.
+ * @reg:         Pointer to the region allocated from the shared
+ *               interface segment for mapping the User mode
+ *               input/output pages in MCU firmware address space.
+ * @phys:        Pointer to the physical pages allocated for the
+ *               pair or User mode input/output page
+ * @user_io_addr: Pointer to the permanent kernel mapping of User mode
+ *                input/output pages. The pages can be accessed through
+ *                the mapping without any cache maintenance.
+ * @handle:      Handle returned with bind ioctl for creating a
+ *               contiguous User mode mapping of input/output pages &
+ *               the hardware doorbell page.
+ * @doorbell_nr: Index of the hardware doorbell page assigned to the
+ *               queue.
+ * @db_file_offset: File offset value that is assigned to userspace mapping
+ *                  created on bind to access the doorbell page.
+ *                  It is in page units.
+ * @link:        Link to the linked list of GPU command queues created per
+ *               GPU address space.
+ * @refcount:    Reference count, stands for the number of times the queue
+ *               has been referenced. The reference is taken when it is
+ *               created, when it is bound to the group and also when the
+ *               @oom_event_work or @fault_event_work work item is queued
+ *               for it.
+ * @group:       Pointer to the group to which this queue is bound.
+ * @queue_reg:   Pointer to the VA region allocated for command
+ *               stream buffer.
+ * @oom_event_work: Work item corresponding to the out of memory event for
+ *                  chunked tiler heap being used for this queue.
+ * @fault_event_work: Work item corresponding to the firmware fault event.
+ * @base_addr:      Base address of the command stream buffer.
+ * @size:           Size of the command stream buffer.
+ * @priority:       Priority of this queue within the group.
+ * @bind_state:     Bind state of the queue.
+ * @csi_index:      The ID of the assigned command stream hardware interface.
+ * @enabled:        Indicating whether the command stream is running, or not.
+ * @status_wait:    Value of CS_STATUS_WAIT register of the command stream will
+ *                  be kept when the command stream gets blocked by sync wait.
+ *                  CS_STATUS_WAIT provides information on conditions queue is
+ *                  blocking on. This is set when the group, to which queue is
+ *                  bound, is suspended after getting blocked, i.e. in
+ *                  KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC state.
+ * @sync_ptr:       Value of CS_STATUS_WAIT_SYNC_POINTER register of the command
+ *                  stream will be kept when the command stream gets blocked by
+ *                  sync wait. CS_STATUS_WAIT_SYNC_POINTER contains the address
+ *                  of synchronization object being waited on.
+ *                  Valid only when @status_wait is set.
+ * @sync_value:     Value of CS_STATUS_WAIT_SYNC_VALUE register of the command
+ *                  stream will be kept when the command stream gets blocked by
+ *                  sync wait. CS_STATUS_WAIT_SYNC_VALUE contains the value
+ *                  tested against the synchronization object.
+ *                  Valid only when @status_wait is set.
+ * @error:          GPU command queue fatal information to pass to user space.
+ */
+struct kbase_queue {
+	struct kbase_context *kctx;
+	struct kbase_va_region *reg;
+	struct tagged_addr phys[2];
+	char *user_io_addr;
+	u64 handle;
+	int doorbell_nr;
+	unsigned long db_file_offset;
+	struct list_head link;
+	atomic_t refcount;
+	struct kbase_queue_group *group;
+	struct kbase_va_region *queue_reg;
+	struct work_struct oom_event_work;
+	struct work_struct fault_event_work;
+	u64 base_addr;
+	u32 size;
+	u8 priority;
+	u8 bind_state;
+	s8 csi_index;
+	bool enabled;
+	u32 status_wait;
+	u64 sync_ptr;
+	u32 sync_value;
+	struct kbase_csf_notification error;
+};
+
+/**
+ * struct kbase_normal_suspend_buffer - Object representing a normal
+ *		suspend buffer for queue group.
+ * @reg:	Memory region allocated for the normal-mode suspend buffer.
+ * @phy:	Array of physical memory pages allocated for the normal-
+ *		mode suspend buffer.
+ */
+struct kbase_normal_suspend_buffer {
+	struct kbase_va_region *reg;
+	struct tagged_addr *phy;
+};
+
+/**
+ * struct kbase_protected_suspend_buffer - Object representing a protected
+ *		suspend buffer for queue group.
+ * @reg:	Memory region allocated for the protected-mode suspend buffer.
+ * @pma:	Array of pointer to protected mode allocations containing
+ *		information about memory pages allocated for protected mode
+ *		suspend	buffer.
+ */
+struct kbase_protected_suspend_buffer {
+	struct kbase_va_region *reg;
+	struct protected_memory_allocation **pma;
+};
+
+/**
+ * struct kbase_queue_group - Object representing a GPU command queue group.
+ *
+ * @kctx:           Pointer to the kbase context with which this queue group
+ *                  is associated.
+ * @normal_suspend_buf:		Object representing the normal suspend buffer.
+ *				Normal-mode suspend buffer that is used for
+ *				group context switch.
+ * @protected_suspend_buf:	Object representing the protected suspend
+ *				buffer. Protected-mode suspend buffer that is
+ *				used for group context switch.
+ * @handle:         Handle which identifies this queue group.
+ * @csg_nr:         Number/index of the command stream group to
+ *                  which this queue group is mapped; KBASEP_CSG_NR_INVALID
+ *                  indicates that the queue group is not scheduled.
+ * @priority:       Priority of the queue group, 0 being the highest,
+ *                  BASE_QUEUE_GROUP_PRIORITY_COUNT - 1 being the lowest.
+ * @tiler_max:      Maximum number of tiler endpoints the group is allowed
+ *                  to use.
+ * @fragment_max:   Maximum number of fragment endpoints the group is
+ *                  allowed to use.
+ * @compute_max:    Maximum number of compute endpoints the group is
+ *                  allowed to use.
+ * @tiler_mask:     Mask of tiler endpoints the group is allowed to use.
+ * @fragment_mask:  Mask of fragment endpoints the group is allowed to use.
+ * @compute_mask:   Mask of compute endpoints the group is allowed to use.
+ * @link:           Link to this queue group in the 'runnable_groups' list of
+ *                  the corresponding kctx.
+ * @link_to_schedule: Link to this queue group in the list of prepared groups
+ *                    to be scheduled, if the group is runnable/suspended.
+ *                    If the group is idle or waiting for CQS, it would be a
+ *                    link to the list of idle/blocked groups list.
+ * @timer_event_work: Work item corresponding to the event generated when a task
+ *                    started by a queue in this group takes too long to execute
+ *                    on an endpoint.
+ * @run_state:      Current state of the queue group.
+ * @prepared_seq_num: Indicates the position of queue group in the list of
+ *                    prepared groups to be scheduled.
+ * @faulted:          Indicates that a GPU fault occurred for the queue group.
+ *                    This flag persists until the fault has been queued to be
+ *                    reported to userspace.
+ * @bound_queues:   Array of registered queues bound to this queue group.
+ * @doorbell_nr:    Index of the hardware doorbell page assigned to the
+ *                  group.
+ * @protm_event_work:   Work item corresponding to the protected mode entry
+ *                      event for this queue.
+ * @protm_pending_bitmap:  Bit array to keep a track of command streams that
+ *                         have pending protected mode entry requests.
+ * @error_fatal: An error of type BASE_GPU_QUEUE_GROUP_ERROR_FATAL to be
+ *               returned to userspace if such an error has occurred.
+ * @error_timeout: An error of type BASE_GPU_QUEUE_GROUP_ERROR_TIMEOUT
+ *                 to be returned to userspace if such an error has occurred.
+ * @error_tiler_oom: An error of type BASE_GPU_QUEUE_GROUP_ERROR_TILER_HEAP_OOM
+ *                   to be returned to userspace if such an error has occurred.
+ */
+struct kbase_queue_group {
+	struct kbase_context *kctx;
+	struct kbase_normal_suspend_buffer normal_suspend_buf;
+	struct kbase_protected_suspend_buffer protected_suspend_buf;
+	u8 handle;
+	s8 csg_nr;
+	u8 priority;
+
+	u8 tiler_max;
+	u8 fragment_max;
+	u8 compute_max;
+
+	u64 tiler_mask;
+	u64 fragment_mask;
+	u64 compute_mask;
+
+	struct list_head link;
+	struct list_head link_to_schedule;
+	struct work_struct timer_event_work;
+	enum kbase_csf_group_state run_state;
+	u32 prepared_seq_num;
+	bool faulted;
+
+	struct kbase_queue *bound_queues[MAX_SUPPORTED_STREAMS_PER_GROUP];
+
+	int doorbell_nr;
+	struct work_struct protm_event_work;
+	DECLARE_BITMAP(protm_pending_bitmap, MAX_SUPPORTED_STREAMS_PER_GROUP);
+
+	struct kbase_csf_notification error_fatal;
+	struct kbase_csf_notification error_timeout;
+	struct kbase_csf_notification error_tiler_oom;
+};
+
+/**
+ * struct kbase_csf_kcpu_queue_context - Object representing the kernel CPU
+ *                                       queues for a GPU address space.
+ *
+ * @lock:   Lock preventing concurrent access to @array and the @in_use bitmap.
+ * @array:  Array of pointers to kernel CPU command queues.
+ * @in_use: Bitmap which indicates which kernel CPU command queues are in use.
+ * @wq:     Dedicated workqueue for processing kernel CPU command queues.
+ * @num_cmds:           The number of commands that have been enqueued across
+ *                      all the KCPU command queues. This could be used as a
+ *                      timestamp to determine the command's enqueueing time.
+ * @jit_cmds_head:      A list of the just-in-time memory commands, both
+ *                      allocate & free, in submission order, protected
+ *                      by kbase_csf_kcpu_queue_context.lock.
+ * @jit_blocked_queues: A list of KCPU command queues blocked by a pending
+ *                      just-in-time memory allocation command which will be
+ *                      reattempted after the impending free of other active
+ *                      allocations.
+ */
+struct kbase_csf_kcpu_queue_context {
+	struct mutex lock;
+	struct kbase_kcpu_command_queue *array[KBASEP_MAX_KCPU_QUEUES];
+	DECLARE_BITMAP(in_use, KBASEP_MAX_KCPU_QUEUES);
+	struct workqueue_struct *wq;
+	u64 num_cmds;
+
+	struct list_head jit_cmds_head;
+	struct list_head jit_blocked_queues;
+};
+
+/**
+ * struct kbase_csf_heap_context_allocator - Allocator of heap contexts
+ *
+ * Heap context structures are allocated by the kernel for use by the firmware.
+ * The current implementation subdivides a single GPU memory region for use as
+ * a sparse array.
+ *
+ * @kctx:     Pointer to the kbase context with which this allocator is
+ *            associated.
+ * @region:   Pointer to a GPU memory region from which heap context structures
+ *            are allocated. NULL if no heap contexts have been allocated.
+ * @gpu_va:   GPU virtual address of the start of the region from which heap
+ *            context structures are allocated. 0 if no heap contexts have been
+ *            allocated.
+ * @lock:     Lock preventing concurrent access to the @in_use bitmap.
+ * @in_use:   Bitmap that indicates which heap context structures are currently
+ *            allocated (in @region).
+ */
+struct kbase_csf_heap_context_allocator {
+	struct kbase_context *kctx;
+	struct kbase_va_region *region;
+	u64 gpu_va;
+	struct mutex lock;
+	DECLARE_BITMAP(in_use, MAX_TILER_HEAPS);
+};
+
+/**
+ * struct kbase_csf_tiler_heap_context - Object representing the tiler heaps
+ *                                       context for a GPU address space.
+ *
+ * This contains all of the command-stream front-end state relating to chunked
+ * tiler heaps for one @kbase_context. It is not the same as a heap context
+ * structure allocated by the kernel for use by the firmware.
+ *
+ * @lock:      Lock preventing concurrent access to the tiler heaps.
+ * @list:      List of tiler heaps.
+ * @ctx_alloc: Allocator for heap context structures.
+ */
+struct kbase_csf_tiler_heap_context {
+	struct mutex lock;
+	struct list_head list;
+	struct kbase_csf_heap_context_allocator ctx_alloc;
+};
+
+/**
+ * struct kbase_csf_scheduler_context - Object representing the scheduler's
+ *                                      context for a GPU address space.
+ *
+ * @runnable_groups:    Lists of runnable GPU command queue groups in the kctx,
+ *                      one per queue group priority level.
+ * @num_runnable_grps:  Total number of runnable groups across all priority
+ *                      levels in @runnable_groups.
+ * @idle_wait_groups:   A list of GPU command queue groups in which all enabled
+ *                      GPU command queues are idle and at least one of them
+ *                      is blocked on a sync wait operation.
+ * @num_idle_wait_grps: Length of the @idle_wait_groups list.
+ * @sync_update_wq:     Dedicated workqueue to process work items corresponding
+ *                      to the sync_update events by sync_set/sync_add
+ *                      instruction execution on command streams bound to groups
+ *                      of @idle_wait_groups list.
+ * @sync_update_work:   work item to process the sync_update events by
+ *                      sync_set / sync_add instruction execution on command
+ *                      streams bound to groups of @idle_wait_groups list.
+ * @ngrp_to_schedule:	Number of groups added for the context to the
+ *                      'groups_to_schedule' list of scheduler instance.
+ */
+struct kbase_csf_scheduler_context {
+	struct list_head runnable_groups[BASE_QUEUE_GROUP_PRIORITY_COUNT];
+	u32 num_runnable_grps;
+	struct list_head idle_wait_groups;
+	u32 num_idle_wait_grps;
+	struct workqueue_struct *sync_update_wq;
+	struct work_struct sync_update_work;
+	u32 ngrp_to_schedule;
+};
+
+/**
+ * struct kbase_csf_context - Object representing command-stream front-end
+ *                            for a GPU address space.
+ *
+ * @event_pages_head: A list of pages allocated for the event memory used by
+ *                    the synchronization objects. A separate list would help
+ *                    in the fast lookup, since the list is expected to be short
+ *                    as one page would provide the memory for up to 1K
+ *                    synchronization objects.
+ *                    KBASE_PERMANENTLY_MAPPED_MEM_LIMIT_PAGES is the upper
+ *                    bound on the size of event memory.
+ * @cookies:          Bitmask containing of KBASE_CSF_NUM_USER_IO_PAGES_HANDLE
+ *                    bits, used for creating the User mode CPU mapping in a
+ *                    deferred manner of a pair of User mode input/output pages
+ *                    & a hardware doorbell page.
+ *                    The pages are allocated when a GPU command queue is
+ *                    bound to a command stream group in kbase_csf_queue_bind.
+ *                    This helps returning unique handles to Userspace from
+ *                    kbase_csf_queue_bind and later retrieving the pointer to
+ *                    queue in the mmap handler.
+ * @user_pages_info:  Array containing pointers to queue
+ *                    structures, used in conjunction with cookies bitmask for
+ *                    providing a mechansim to create a CPU mapping of
+ *                    input/output pages & hardware doorbell page.
+ * @lock:             Serializes accesses to all members, except for ones that
+ *                    have their own locks.
+ * @queue_groups:     Array of registered GPU command queue groups.
+ * @queue_list:       Linked list of GPU command queues not yet deregistered.
+ *                    Note that queues can persist after deregistration if the
+ *                    userspace mapping created for them on bind operation
+ *                    hasn't been removed.
+ * @kcpu_queues:      Kernel CPU command queues.
+ * @event_lock:       Lock protecting access to @event_callback_list
+ * @event_callback_list: List of callbacks which are registered to serve CSF
+ *                       events.
+ * @tiler_heaps:      Chunked tiler memory heaps.
+ * @wq:               Dedicated workqueue to process work items corresponding
+ *                    to the OoM events raised for chunked tiler heaps being
+ *                    used by GPU command queues, and progress timeout events.
+ * @link:             Link to this csf context in the 'runnable_kctxs' list of
+ *                    the scheduler instance
+ * @user_reg_vma:     Pointer to the vma corresponding to the virtual mapping
+ *                    of the USER register page. Currently used only for sanity
+ *                    checking.
+ * @sched:            Object representing the scheduler's context
+ * @error_list:       List for command stream fatal errors in this context.
+ *                    Link of fatal error is
+ *                    &struct_kbase_csf_notification.link.
+ *                    @lock needs to be held to access to this list.
+ */
+struct kbase_csf_context {
+	struct list_head event_pages_head;
+	DECLARE_BITMAP(cookies, KBASE_CSF_NUM_USER_IO_PAGES_HANDLE);
+	struct kbase_queue *user_pages_info[
+		KBASE_CSF_NUM_USER_IO_PAGES_HANDLE];
+	struct mutex lock;
+	struct kbase_queue_group *queue_groups[MAX_QUEUE_GROUP_NUM];
+	struct list_head queue_list;
+	struct kbase_csf_kcpu_queue_context kcpu_queues;
+	spinlock_t event_lock;
+	struct list_head event_callback_list;
+	struct kbase_csf_tiler_heap_context tiler_heaps;
+	struct workqueue_struct *wq;
+	struct list_head link;
+	struct vm_area_struct *user_reg_vma;
+	struct kbase_csf_scheduler_context sched;
+	struct list_head error_list;
+};
+
+/**
+ * struct kbase_csf_reset_gpu - Object containing the members required for
+ *                            GPU reset handling.
+ * @workq:         Workqueue to execute the GPU reset work item @work.
+ * @work:          Work item for performing the GPU reset.
+ * @wait:          Wait queue used to wait for the GPU reset completion.
+ * @state:         Tracks if the GPU reset is in progress or not.
+ */
+struct kbase_csf_reset_gpu {
+	struct workqueue_struct *workq;
+	struct work_struct work;
+	wait_queue_head_t wait;
+	atomic_t state;
+};
+
+/**
+ * struct kbase_csf_csg_slot - Object containing members for tracking the state
+ *                             of command stream group slots.
+ * @resident_group:   pointer to the queue group that is resident on the
+ *                    command stream group slot.
+ * @state:            state of the slot as per enum kbase_csf_csg_slot_state.
+ * @trigger_jiffies:  value of jiffies when change in slot state is recorded.
+ * @priority:         dynamic priority assigned to command stream group slot.
+ */
+struct kbase_csf_csg_slot {
+	struct kbase_queue_group *resident_group;
+	atomic_t state;
+	unsigned long trigger_jiffies;
+	u8 priority;
+};
+
+/**
+ * struct kbase_csf_scheduler - Object representing the scheduler used for
+ *                              command-stream front-end for an instance of
+ *                              GPU platform device.
+ * @lock:                  Lock to serialize the scheduler operations and
+ *                         access to the data members.
+ * @interrupt_lock:        Lock to protect members accessed by interrupt
+ *                         handler.
+ * @state:                 The operational phase the scheduler is in. Primarily
+ *                         used for indicating what in-cycle schedule actions
+ *                         are allowed.
+ * @doorbell_inuse_bitmap: Bitmap of hardware doorbell pages keeping track of
+ *                         which pages are currently available for assignment
+ *                         to clients.
+ * @csg_inuse_bitmap:      Bitmap to keep a track of command stream group slots
+ *                         that are currently in use.
+ * @csg_slots:             The array for tracking the state of command stream
+ *                         group slots.
+ * @runnable_kctxs:        List of Kbase contexts that have runnable command
+ *                         queue groups.
+ * @groups_to_schedule:    List of runnable queue groups prepared on every
+ *                         scheduler tick. The dynamic priority of the command
+ *                         stream group slot assigned to a group will depend
+ *                         upon the position of group in the list.
+ * @ngrp_to_schedule:      Number of groups in the @groups_to_schedule list,
+ *                         incremented when a group is added to the list, used
+ *                         to record the position of group in the list.
+ * @num_active_address_spaces: Number of GPU address space slots that would get
+ *                             used to program the groups in @groups_to_schedule
+ *                             list on all the available command stream group
+ *                             slots.
+ * @num_csg_slots_for_tick:  Number of command stream group slots that can be
+ *                           active in the given tick/tock. This depends on the
+ *                           value of @num_active_address_spaces.
+ * @idle_groups_to_schedule: List of runnable queue groups, in which all GPU
+ *                           command queues became idle or are waiting for
+ *                           synchronization object, prepared on every
+ *                           scheduler tick. The groups in this list are
+ *                           appended to the tail of @groups_to_schedule list
+ *                           after the scan out so that the idle groups aren't
+ *                           preferred for scheduling over the non-idle ones.
+ * @total_runnable_grps:     Total number of runnable groups across all KCTXs.
+ * @csgs_events_enable_mask: Use for temporary masking off asynchronous events
+ *                           from firmware (such as OoM events) before a group
+ *                           is suspended.
+ * @csg_slots_idle_mask:     Bit array for storing the mask of command stream
+ *                           group slots for which idle notification was
+ *                           received.
+ * @csg_slots_prio_update:  Bit array for tracking slots that have an on-slot
+ *                          priority update operation.
+ * @last_schedule:          Time in jiffies recorded when the last "tick" or
+ *                          "tock" schedule operation concluded. Used for
+ *                          evaluating the exclusion window for in-cycle
+ *                          schedule operation.
+ * @timer_enabled:          Whether the CSF scheduler wakes itself up for
+ *                          periodic scheduling tasks. If this value is 0
+ *                          then it will only perform scheduling under the
+ *                          influence of external factors e.g., IRQs, IOCTLs.
+ * @wq:                     Dedicated workqueue to execute the @tick_work.
+ * @tick_work:              Work item that would perform the schedule on tick
+ *                          operation to implement the time slice based
+ *                          scheduling.
+ * @tock_work:              Work item that would perform the schedule on tock
+ *                          operation to implement the asynchronous scheduling.
+ * @ping_work:              Work item that would ping the firmware at regular
+ *                          intervals, only if there is a single active command
+ *                          stream group slot, to check if firmware is alive
+ *                          and would initiate a reset if the ping request
+ *                          isn't acknowledged.
+ * @top_ctx:                Pointer to the Kbase context corresponding to the
+ *                          @top_grp.
+ * @top_grp:                Pointer to queue group inside @groups_to_schedule
+ *                          list that was assigned the highest slot priority.
+ * @head_slot_priority:     The dynamic slot priority to be used for the
+ *                          queue group at the head of @groups_to_schedule
+ *                          list. Once the queue group is assigned a command
+ *                          stream group slot, it is removed from the list and
+ *                          priority is decremented.
+ * @tock_pending_request:   A "tock" request is pending: a group that is not
+ *                          currently on the GPU demands to be scheduled.
+ * @active_protm_grp:       Indicates if firmware has been permitted to let GPU
+ *                          enter protected mode with the given group. On exit
+ *                          from protected mode the pointer is reset to NULL.
+ * @gpu_idle_work:          Work item for facilitating the scheduler to bring
+ *                          the GPU to a low-power mode on becoming idle.
+ * @non_idle_suspended_grps: Count of suspended queue groups not idle.
+ * @pm_active_count:        Count indicating if the scheduler is owning a power
+ *                          management reference count. Reference is taken when
+ *                          the count becomes 1 and is dropped when the count
+ *                          becomes 0. It is used to enable the power up of MCU
+ *                          after GPU and L2 cache have been powered up. So when
+ *                          this count is zero, MCU will not be powered up.
+ */
+struct kbase_csf_scheduler {
+	struct mutex lock;
+	spinlock_t interrupt_lock;
+	enum kbase_csf_scheduler_state state;
+	DECLARE_BITMAP(doorbell_inuse_bitmap, CSF_NUM_DOORBELL);
+	DECLARE_BITMAP(csg_inuse_bitmap, MAX_SUPPORTED_CSGS);
+	struct kbase_csf_csg_slot *csg_slots;
+	struct list_head runnable_kctxs;
+	struct list_head groups_to_schedule;
+	u32 ngrp_to_schedule;
+	u32 num_active_address_spaces;
+	u32 num_csg_slots_for_tick;
+	struct list_head idle_groups_to_schedule;
+	u32 total_runnable_grps;
+	DECLARE_BITMAP(csgs_events_enable_mask, MAX_SUPPORTED_CSGS);
+	DECLARE_BITMAP(csg_slots_idle_mask, MAX_SUPPORTED_CSGS);
+	DECLARE_BITMAP(csg_slots_prio_update, MAX_SUPPORTED_CSGS);
+	unsigned long last_schedule;
+	bool timer_enabled;
+	struct workqueue_struct *wq;
+	struct delayed_work tick_work;
+	struct delayed_work tock_work;
+	struct delayed_work ping_work;
+	struct kbase_context *top_ctx;
+	struct kbase_queue_group *top_grp;
+	u8 head_slot_priority;
+	bool tock_pending_request;
+	struct kbase_queue_group *active_protm_grp;
+	struct delayed_work gpu_idle_work;
+	atomic_t non_idle_suspended_grps;
+	u32 pm_active_count;
+};
+
+/**
+ * Number of GPU cycles per unit of the global progress timeout.
+ */
+#define GLB_PROGRESS_TIMER_TIMEOUT_SCALE ((u64)1024)
+
+/**
+ * Maximum value of the global progress timeout.
+ */
+#define GLB_PROGRESS_TIMER_TIMEOUT_MAX \
+	((GLB_PROGRESS_TIMER_TIMEOUT_MASK >> \
+		GLB_PROGRESS_TIMER_TIMEOUT_SHIFT) * \
+	GLB_PROGRESS_TIMER_TIMEOUT_SCALE)
+
+/**
+ * struct kbase_csf      -  Object representing command-stream front-end for an
+ *                          instance of GPU platform device.
+ *
+ * @mcu_mmu:                MMU page tables for the MCU firmware
+ * @firmware_interfaces:    List of interfaces defined in the firmware image
+ * @firmware_config:        List of configuration options within the firmware
+ *                          image
+ * @firmware_timeline_metadata: List of timeline meta-data within the firmware
+ *                          image
+ * @fw_cfg_kobj:            Pointer to the kobject corresponding to the sysf
+ *                          directory that contains a sub-directory for each
+ *                          of the configuration option present in the
+ *                          firmware image.
+ * @firmware_trace_buffers: List of trace buffers described in the firmware
+ *                          image.
+ * @shared_interface:       Pointer to the interface object containing info for
+ *                          the memory area shared between firmware & host.
+ * @shared_reg_rbtree:      RB tree of the memory regions allocated from the
+ *                          shared interface segment in MCU firmware address
+ *                          space.
+ * @db_filp:                Pointer to a dummy file, that alongwith
+ *                          @db_file_offsets, facilitates the use of unqiue
+ *                          file offset for the userspace mapping created
+ *                          for Hw Doorbell pages. The userspace mapping
+ *                          is made to point to this file inside the mmap
+ *                          handler.
+ * @db_file_offsets:        Counter that is incremented every time a GPU
+ *                          command queue is bound to provide a unique file
+ *                          offset range for @db_filp file, so that pte of
+ *                          Doorbell page can be zapped through the kernel
+ *                          function unmap_mapping_range(). It is incremented
+ *                          in page units.
+ * @dummy_db_page:          Address of the dummy page that is mapped in place
+ *                          of the real Hw doorbell page for the active GPU
+ *                          command queues after they are stopped or after the
+ *                          GPU is powered down.
+ * @reg_lock:               Lock to serialize the MCU firmware related actions
+ *                          that affect all contexts such as allocation of
+ *                          regions from shared interface area, assignment of
+ *                          of hardware doorbell pages, assignment of CSGs,
+ *                          sending global requests.
+ * @event_wait:             Wait queue to wait for receiving csf events, i.e.
+ *                          the interrupt from CSF firmware, or scheduler state
+ *                          changes.
+ * @interrupt_received:     Flag set when the interrupt is received from CSF fw
+ * @global_iface:           The result of parsing the global interface
+ *                          structure set up by the firmware, including the
+ *                          CSGs, CSs, and their properties
+ * @scheduler:              The command stream scheduler instance.
+ * @reset:                  Contain members required for GPU reset handling.
+ * @progress_timeout:       Maximum number of GPU clock cycles without forward
+ *                          progress to allow, for all tasks running on
+ *                          hardware endpoints (e.g. shader cores), before
+ *                          terminating a GPU command queue group.
+ *                          Must not exceed @GLB_PROGRESS_TIMER_TIMEOUT_MAX.
+ * @pma_dev:                Pointer to protected memory allocator device.
+ * @firmware_inited:        Flag for indicating that the cold-boot stage of
+ *                          the MCU has completed.
+ * @firmware_reloaded:      Flag for indicating a firmware reload operation
+ *                          in GPU reset has completed.
+ * @firmware_reload_needed: Flag for indicating that the firmware needs to be
+ *                          reloaded as part of the GPU reset action.
+ * @firmware_reload_work:   Work item for facilitating the procedural actions
+ *                          on reloading the firmware.
+ * @glb_init_request_pending: Flag to indicate that Global requests have been
+ *                            sent to the FW after MCU was re-enabled and their
+ *                            acknowledgement is pending.
+ */
+struct kbase_csf_device {
+	struct kbase_mmu_table mcu_mmu;
+	struct list_head firmware_interfaces;
+	struct list_head firmware_config;
+	struct list_head firmware_timeline_metadata;
+	struct kobject *fw_cfg_kobj;
+	struct kbase_csf_trace_buffers firmware_trace_buffers;
+	void *shared_interface;
+	struct rb_root shared_reg_rbtree;
+	struct file *db_filp;
+	u32 db_file_offsets;
+	struct tagged_addr dummy_db_page;
+	struct mutex reg_lock;
+	wait_queue_head_t event_wait;
+	bool interrupt_received;
+	struct kbase_csf_global_iface global_iface;
+	struct kbase_csf_scheduler scheduler;
+	struct kbase_csf_reset_gpu reset;
+	atomic64_t progress_timeout;
+	struct protected_memory_allocator_device *pma_dev;
+	bool firmware_inited;
+	bool firmware_reloaded;
+	bool firmware_reload_needed;
+	struct work_struct firmware_reload_work;
+	bool glb_init_request_pending;
+};
+
+/**
+ * struct kbase_as   - Object representing an address space of GPU.
+ * @number:            Index at which this address space structure is present
+ *                     in an array of address space structures embedded inside
+ *                     the &struct kbase_device.
+ * @pf_wq:             Workqueue for processing work items related to
+ *                     Page fault, Bus fault and GPU fault handling.
+ * @work_pagefault:    Work item for the Page fault handling.
+ * @work_busfault:     Work item for the Bus fault handling.
+ * @work_gpufault:     Work item for the GPU fault handling.
+ * @pf_data:           Data relating to Page fault.
+ * @bf_data:           Data relating to Bus fault.
+ * @gf_data:           Data relating to GPU fault.
+ * @current_setup:     Stores the MMU configuration for this address space.
+ */
+struct kbase_as {
+	int number;
+	struct workqueue_struct *pf_wq;
+	struct work_struct work_pagefault;
+	struct work_struct work_busfault;
+	struct work_struct work_gpufault;
+	struct kbase_fault pf_data;
+	struct kbase_fault bf_data;
+	struct kbase_fault gf_data;
+	struct kbase_mmu_setup current_setup;
+};
+
+#endif /* _KBASE_CSF_DEFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c
new file mode 100644
index 000000000000..4a924f346685
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.c
@@ -0,0 +1,1993 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase.h"
+#include "mali_kbase_csf_firmware_cfg.h"
+#include "mali_kbase_csf_trace_buffer.h"
+#include "mali_kbase_csf_timeout.h"
+#include "mali_kbase_mem.h"
+#include <mali_kbase_reset_gpu.h>
+#include "mali_kbase_csf_scheduler.h"
+#include "device/mali_kbase_device.h"
+#include "backend/gpu/mali_kbase_pm_internal.h"
+#include "tl/mali_kbase_timeline_priv.h"
+#include "mali_kbase_csf_tl_reader.h"
+
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/firmware.h>
+#include <linux/mman.h>
+#include <linux/string.h>
+#if (KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE)
+#include <linux/set_memory.h>
+#endif
+#include <mmu/mali_kbase_mmu.h>
+
+#define MALI_MAX_FIRMWARE_NAME_LEN ((size_t)20)
+
+static char fw_name[MALI_MAX_FIRMWARE_NAME_LEN] = "mali_csffw.bin";
+module_param_string(fw_name, fw_name, sizeof(fw_name), 0644);
+MODULE_PARM_DESC(fw_name, "firmware image");
+
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+/* Makes Driver wait indefinitely for an acknowledgment for the different
+ * requests it sends to firmware. Otherwise the timeouts interfere with the
+ * use of debugger for source-level debugging of firmware as Driver initiates
+ * a GPU reset when a request times out, which always happen when a debugger
+ * is connected.
+ */
+bool fw_debug; /* Default value of 0/false */
+module_param(fw_debug, bool, 0444);
+MODULE_PARM_DESC(fw_debug,
+	"Enables effective use of a debugger for debugging firmware code.");
+#endif
+
+#define FIRMWARE_HEADER_MAGIC    (0xC3F13A6Eul)
+#define FIRMWARE_HEADER_VERSION  (0ul)
+#define FIRMWARE_HEADER_LENGTH   (0x14ul)
+
+#define CSF_FIRMWARE_ENTRY_READ       (1ul << 0)
+#define CSF_FIRMWARE_ENTRY_WRITE      (1ul << 1)
+#define CSF_FIRMWARE_ENTRY_EXECUTE    (1ul << 2)
+#define CSF_FIRMWARE_ENTRY_CACHE_MODE (3ul << 3)
+#define CSF_FIRMWARE_ENTRY_PROTECTED  (1ul << 5)
+#define CSF_FIRMWARE_ENTRY_SHARED     (1ul << 30)
+#define CSF_FIRMWARE_ENTRY_ZERO       (1ul << 31)
+
+#define CSF_FIRMWARE_ENTRY_SUPPORTED_FLAGS \
+	(CSF_FIRMWARE_ENTRY_READ | \
+	 CSF_FIRMWARE_ENTRY_WRITE | \
+	 CSF_FIRMWARE_ENTRY_EXECUTE | \
+	 CSF_FIRMWARE_ENTRY_PROTECTED | \
+	 CSF_FIRMWARE_ENTRY_SHARED | \
+	 CSF_FIRMWARE_ENTRY_ZERO | \
+	 CSF_FIRMWARE_ENTRY_CACHE_MODE)
+
+#define CSF_FIRMWARE_ENTRY_TYPE_INTERFACE     (0)
+#define CSF_FIRMWARE_ENTRY_TYPE_CONFIGURATION (1)
+#define CSF_FIRMWARE_ENTRY_TYPE_FUTF_TEST     (2)
+#define CSF_FIRMWARE_ENTRY_TYPE_TRACE_BUFFER  (3)
+#define CSF_FIRMWARE_ENTRY_TYPE_TIMELINE_METADATA (4)
+
+#define CSF_FIRMWARE_CACHE_MODE_NONE              (0ul << 3)
+#define CSF_FIRMWARE_CACHE_MODE_CACHED            (1ul << 3)
+#define CSF_FIRMWARE_CACHE_MODE_UNCACHED_COHERENT (2ul << 3)
+#define CSF_FIRMWARE_CACHE_MODE_CACHED_COHERENT   (3ul << 3)
+
+#define INTERFACE_ENTRY_NAME_OFFSET (0x14)
+
+#define TL_METADATA_ENTRY_NAME_OFFSET (0x8)
+
+#define CSF_FIRMWARE_BOOT_TIMEOUT_MS     (500)
+#define CSF_MAX_FW_STOP_LOOPS            (100000)
+
+#define CSF_GLB_REQ_CFG_MASK \
+	(GLB_REQ_CFG_ALLOC_EN_MASK | GLB_REQ_CFG_PROGRESS_TIMER_MASK)
+
+static inline u32 input_page_read(const u32 *const input, const u32 offset)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	return input[offset / sizeof(u32)];
+}
+
+static inline void input_page_write(u32 *const input, const u32 offset,
+			const u32 value)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	input[offset / sizeof(u32)] = value;
+}
+
+static inline void input_page_partial_write(u32 *const input, const u32 offset,
+			u32 value, u32 mask)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	input[offset / sizeof(u32)] =
+		(input_page_read(input, offset) & ~mask) | (value & mask);
+}
+
+static inline u32 output_page_read(const u32 *const output, const u32 offset)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	return output[offset / sizeof(u32)];
+}
+
+static unsigned int entry_type(u32 header)
+{
+	return header & 0xFF;
+}
+static unsigned int entry_size(u32 header)
+{
+	return (header >> 8) & 0xFF;
+}
+static bool entry_optional(u32 header)
+{
+	return (header >> 31) & 0x1;
+}
+
+/**
+ * struct firmware_interface - Represents an interface in the MCU firmware
+ *
+ * @node:  Interface objects are on the kbase_device:csf.firmware_interfaces
+ *         list using this list_head to link them
+ * @phys:  Array of the physical (tagged) addresses making up this interface
+ * @name:  NUL-terminated string naming the interface
+ * @num_pages: Number of entries in @phys (and length of the interface)
+ * @virtual: Virtual address that this interface is mapped at for the GPU
+ * @flags: bitmask of CSF_FIRMWARE_ENTRY_* conveying the interface attributes
+ * @data_start: Offset into firmware image at which the interface data starts
+ * @data_end: Offset into firmware image at which the interface data ends
+ * @kernel_map: A kernel mapping of the memory or NULL if not required to be
+ *              mapped in the kernel
+ * @pma: Array of pointers to protected memory allocations.
+ */
+struct firmware_interface {
+	struct list_head node;
+	struct tagged_addr *phys;
+	char *name;
+	u32 num_pages;
+	u32 virtual;
+	u32 flags;
+	u32 data_start;
+	u32 data_end;
+	void *kernel_map;
+	struct protected_memory_allocation **pma;
+};
+
+/**
+ * Timeline metadata item within the MCU firmware
+ *
+ * @node: List head linking all timeline metadata to
+ *        kbase_device:csf.firmware_timeline_metadata.
+ * @name: NUL-terminated string naming the metadata.
+ * @data: Metadata content.
+ * @size: Metadata size.
+ */
+struct firmware_timeline_metadata {
+	struct list_head node;
+	char *name;
+	char *data;
+	size_t size;
+};
+
+/* The shared interface area, used for communicating with firmware, is managed
+ * like a virtual memory zone. Reserve the virtual space from that zone
+ * corresponding to shared interface entry parsed from the firmware image.
+ * The shared_reg_rbtree should have been initialized before calling this
+ * function.
+ */
+static int setup_shared_iface_static_region(struct kbase_device *kbdev)
+{
+	struct firmware_interface *interface = kbdev->csf.shared_interface;
+	struct kbase_va_region *reg;
+	int ret = -ENOMEM;
+
+	if (!interface)
+		return -EINVAL;
+
+	reg = kbase_alloc_free_region(&kbdev->csf.shared_reg_rbtree, 0,
+			interface->num_pages, KBASE_REG_ZONE_MCU_SHARED);
+	if (reg) {
+		ret = kbase_add_va_region_rbtree(kbdev, reg,
+				interface->virtual, interface->num_pages, 1);
+		if (ret)
+			kfree(reg);
+		else
+			reg->flags &= ~KBASE_REG_FREE;
+	}
+
+	return ret;
+}
+
+static int wait_mcu_status_value(struct kbase_device *kbdev, u32 val)
+{
+	u32 max_loops = CSF_MAX_FW_STOP_LOOPS;
+
+	/* wait for the MCU_STATUS register to reach the given status value */
+	while (--max_loops &&
+	       (kbase_reg_read(kbdev, GPU_CONTROL_REG(MCU_STATUS)) != val)) {
+	}
+
+	return (max_loops == 0) ? -1 : 0;
+}
+
+void kbase_csf_firmware_disable_mcu_wait(struct kbase_device *kbdev)
+{
+	if (wait_mcu_status_value(kbdev, MCU_CNTRL_DISABLE) < 0)
+		dev_err(kbdev->dev, "MCU failed to get disabled");
+}
+
+static void wait_for_firmware_stop(struct kbase_device *kbdev)
+{
+	if (wait_mcu_status_value(kbdev, MCU_CNTRL_DISABLE) < 0) {
+		/* This error shall go away once MIDJM-2371 is closed */
+		dev_err(kbdev->dev, "Firmware failed to stop");
+	}
+}
+
+static void stop_csf_firmware(struct kbase_device *kbdev)
+{
+	/* Stop the MCU firmware */
+	kbase_csf_firmware_disable_mcu(kbdev);
+
+	wait_for_firmware_stop(kbdev);
+}
+
+static void wait_for_firmware_boot(struct kbase_device *kbdev)
+{
+	const long wait_timeout =
+		kbase_csf_timeout_in_jiffies(CSF_FIRMWARE_BOOT_TIMEOUT_MS);
+	long remaining;
+
+	/* Firmware will generate a global interface interrupt once booting
+	 * is complete
+	 */
+	remaining = wait_event_timeout(kbdev->csf.event_wait,
+			kbdev->csf.interrupt_received == true, wait_timeout);
+
+	if (!remaining)
+		dev_err(kbdev->dev, "Timed out waiting for fw boot completion");
+
+	kbdev->csf.interrupt_received = false;
+}
+
+static void boot_csf_firmware(struct kbase_device *kbdev)
+{
+	kbase_csf_firmware_enable_mcu(kbdev);
+
+	wait_for_firmware_boot(kbdev);
+}
+
+static void wait_ready(struct kbase_device *kbdev)
+{
+	u32 max_loops = KBASE_AS_INACTIVE_MAX_LOOPS;
+	u32 val;
+
+	val = kbase_reg_read(kbdev, MMU_AS_REG(MCU_AS_NR, AS_STATUS));
+
+	/* Wait for a while for the update command to take effect */
+	while (--max_loops && (val & AS_STATUS_AS_ACTIVE))
+		val = kbase_reg_read(kbdev, MMU_AS_REG(MCU_AS_NR, AS_STATUS));
+
+	if (max_loops == 0)
+		dev_err(kbdev->dev, "AS_ACTIVE bit stuck, might be caused by slow/unstable GPU clock or possible faulty FPGA connector\n");
+}
+
+static void unload_mmu_tables(struct kbase_device *kbdev)
+{
+	unsigned long irq_flags;
+
+	mutex_lock(&kbdev->mmu_hw_mutex);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
+	if (kbdev->pm.backend.gpu_powered)
+		kbase_mmu_disable_as(kbdev, MCU_AS_NR);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
+}
+
+static void load_mmu_tables(struct kbase_device *kbdev)
+{
+	unsigned long irq_flags;
+
+	mutex_lock(&kbdev->mmu_hw_mutex);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, irq_flags);
+	kbase_mmu_update(kbdev, &kbdev->csf.mcu_mmu, MCU_AS_NR);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, irq_flags);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
+
+	/* Wait for a while for the update command to take effect */
+	wait_ready(kbdev);
+}
+
+/**
+ * convert_mem_flags() - Convert firmware memory flags to GPU region flags
+ *
+ * Return: GPU memory region flags
+ *
+ * @kbdev: Instance of GPU platform device (used to determine system coherency)
+ * @flags: Flags of an "interface memory setup" section in a firmware image
+ * @cm:    appropriate cache mode chosen for the "interface memory setup"
+ *         section, which could be different from the cache mode requested by
+ *         firmware.
+ */
+static unsigned long convert_mem_flags(const struct kbase_device * const kbdev,
+	const u32 flags, u32 *cm)
+{
+	unsigned long mem_flags = 0;
+	u32 cache_mode = flags & CSF_FIRMWARE_ENTRY_CACHE_MODE;
+	bool is_shared = (flags & CSF_FIRMWARE_ENTRY_SHARED) ? true : false;
+
+	/* The memory flags control the access permissions for the MCU, the
+	 * shader cores/tiler are not expected to access this memory
+	 */
+	if (flags & CSF_FIRMWARE_ENTRY_READ)
+		mem_flags |= KBASE_REG_GPU_RD;
+
+	if (flags & CSF_FIRMWARE_ENTRY_WRITE)
+		mem_flags |= KBASE_REG_GPU_WR;
+
+	if ((flags & CSF_FIRMWARE_ENTRY_EXECUTE) == 0)
+		mem_flags |= KBASE_REG_GPU_NX;
+
+	if (flags & CSF_FIRMWARE_ENTRY_PROTECTED)
+		mem_flags |= KBASE_REG_PROTECTED;
+
+	/* Substitute uncached coherent memory for cached coherent memory if
+	 * the system does not support ACE coherency.
+	 */
+	if ((cache_mode == CSF_FIRMWARE_CACHE_MODE_CACHED_COHERENT) &&
+		(kbdev->system_coherency != COHERENCY_ACE))
+		cache_mode = CSF_FIRMWARE_CACHE_MODE_UNCACHED_COHERENT;
+
+	/* Substitute uncached incoherent memory for uncached coherent memory
+	 * if the system does not support ACE-Lite coherency.
+	 */
+	if ((cache_mode == CSF_FIRMWARE_CACHE_MODE_UNCACHED_COHERENT) &&
+		(kbdev->system_coherency == COHERENCY_NONE))
+		cache_mode = CSF_FIRMWARE_CACHE_MODE_NONE;
+
+	*cm = cache_mode;
+
+	switch (cache_mode) {
+	case CSF_FIRMWARE_CACHE_MODE_NONE:
+		mem_flags |=
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_NON_CACHEABLE);
+		break;
+	case CSF_FIRMWARE_CACHE_MODE_CACHED:
+		mem_flags |=
+			KBASE_REG_MEMATTR_INDEX(
+			AS_MEMATTR_INDEX_IMPL_DEF_CACHE_POLICY);
+		break;
+	case CSF_FIRMWARE_CACHE_MODE_UNCACHED_COHERENT:
+	case CSF_FIRMWARE_CACHE_MODE_CACHED_COHERENT:
+		WARN_ON(!is_shared);
+		mem_flags |= KBASE_REG_SHARE_BOTH |
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_SHARED);
+		break;
+	default:
+		dev_err(kbdev->dev,
+			"Firmware contains interface with unsupported cache mode\n");
+		break;
+	}
+	return mem_flags;
+}
+
+static void load_fw_image_section(struct kbase_device *kbdev, const u8 *data,
+		struct tagged_addr *phys, u32 num_pages, u32 flags,
+		u32 data_start, u32 data_end)
+{
+	u32 data_pos = data_start;
+	u32 data_len = data_end - data_start;
+	u32 page_num;
+	u32 page_limit;
+
+	if (flags & CSF_FIRMWARE_ENTRY_ZERO)
+		page_limit = num_pages;
+	else
+		page_limit = (data_len + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	for (page_num = 0; page_num < page_limit; ++page_num) {
+		struct page *const page = as_page(phys[page_num]);
+		char *const p = kmap_atomic(page);
+		u32 const copy_len = min_t(u32, PAGE_SIZE, data_len);
+
+		if (copy_len > 0) {
+			memcpy(p, data + data_pos, copy_len);
+			data_pos += copy_len;
+			data_len -= copy_len;
+		}
+
+		if (flags & CSF_FIRMWARE_ENTRY_ZERO) {
+			u32 const zi_len = PAGE_SIZE - copy_len;
+
+			memset(p + copy_len, 0, zi_len);
+		}
+
+		kbase_sync_single_for_device(kbdev, kbase_dma_addr(page),
+				PAGE_SIZE, DMA_TO_DEVICE);
+		kunmap_atomic(p);
+	}
+}
+
+static int reload_fw_data_sections(struct kbase_device *kbdev)
+{
+	const u32 magic = FIRMWARE_HEADER_MAGIC;
+	struct firmware_interface *interface;
+	const struct firmware *firmware;
+	int ret = 0;
+
+	if (request_firmware(&firmware, fw_name, kbdev->dev) != 0) {
+		dev_err(kbdev->dev,
+			"Failed to reload firmware image '%s'\n",
+			fw_name);
+		return -ENOENT;
+	}
+
+	/* Do couple of basic sanity checks */
+	if (firmware->size < FIRMWARE_HEADER_LENGTH) {
+		dev_err(kbdev->dev, "Firmware image unexpectedly too small\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (memcmp(firmware->data, &magic, sizeof(magic)) != 0) {
+		dev_err(kbdev->dev, "Incorrect magic value, firmware image could have been corrupted\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	list_for_each_entry(interface, &kbdev->csf.firmware_interfaces, node) {
+		/* Skip reload of text & read only data sections */
+		if ((interface->flags & CSF_FIRMWARE_ENTRY_EXECUTE) ||
+		    !(interface->flags & CSF_FIRMWARE_ENTRY_WRITE))
+			continue;
+
+		load_fw_image_section(kbdev, firmware->data, interface->phys,
+			interface->num_pages, interface->flags,
+			interface->data_start, interface->data_end);
+	}
+
+	kbase_csf_firmware_reload_trace_buffers_data(kbdev);
+
+out:
+	release_firmware(firmware);
+	return ret;
+}
+
+/**
+ * parse_memory_setup_entry() - Process an "interface memory setup" section
+ *
+ * Read an "interface memory setup" section from the firmware image and create
+ * the necessary memory region including the MMU page tables. If successful
+ * the interface will be added to the kbase_device:csf.firmware_interfaces list.
+ *
+ * Return: 0 if successful, negative error code on failure
+ *
+ * @kbdev: Kbase device structure
+ * @fw: The firmware image containing the section
+ * @entry: Pointer to the start of the section
+ * @size: Size (in bytes) of the section
+ */
+static int parse_memory_setup_entry(struct kbase_device *kbdev,
+		const struct firmware *fw,
+		const u32 *entry, unsigned int size)
+{
+	int ret = 0;
+	const u32 flags = entry[0];
+	const u32 virtual_start = entry[1];
+	const u32 virtual_end = entry[2];
+	const u32 data_start = entry[3];
+	const u32 data_end = entry[4];
+	u32 num_pages;
+	char *name;
+	struct tagged_addr *phys = NULL;
+	struct firmware_interface *interface = NULL;
+	bool allocated_pages = false, protected_mode = false;
+	unsigned long mem_flags = 0;
+	u32 cache_mode = 0;
+	struct protected_memory_allocation **pma = NULL;
+
+	if (data_end < data_start) {
+		dev_err(kbdev->dev, "Firmware corrupt, data_end < data_start (0x%x<0x%x)\n",
+				data_end, data_start);
+		return -EINVAL;
+	}
+	if (virtual_end < virtual_start) {
+		dev_err(kbdev->dev, "Firmware corrupt, virtual_end < virtual_start (0x%x<0x%x)\n",
+				virtual_end, virtual_start);
+		return -EINVAL;
+	}
+	if (data_end > fw->size) {
+		dev_err(kbdev->dev, "Firmware corrupt, file truncated? data_end=0x%x > fw->size=0x%zx\n",
+				data_end, fw->size);
+		return -EINVAL;
+	}
+
+	if ((virtual_start & ~PAGE_MASK) != 0 ||
+			(virtual_end & ~PAGE_MASK) != 0) {
+		dev_err(kbdev->dev, "Firmware corrupt: virtual addresses not page aligned: 0x%x-0x%x\n",
+				virtual_start, virtual_end);
+		return -EINVAL;
+	}
+
+	if ((flags & CSF_FIRMWARE_ENTRY_SUPPORTED_FLAGS) != flags) {
+		dev_err(kbdev->dev, "Firmware contains interface with unsupported flags (0x%x)\n",
+				flags);
+		return -EINVAL;
+	}
+
+	if (flags & CSF_FIRMWARE_ENTRY_PROTECTED)
+		protected_mode = true;
+
+	if (protected_mode && kbdev->csf.pma_dev == NULL) {
+		dev_err(kbdev->dev,
+			"Protected memory allocator not found, Firmware protected mode entry will not be supported");
+		return 0;
+	}
+
+	num_pages = (virtual_end - virtual_start)
+		>> PAGE_SHIFT;
+
+	phys = kmalloc_array(num_pages, sizeof(*phys), GFP_KERNEL);
+	if (!phys)
+		return -ENOMEM;
+
+	if (protected_mode) {
+		pma = kbase_csf_protected_memory_alloc(kbdev, phys, num_pages);
+
+		if (pma == NULL) {
+			ret = -ENOMEM;
+			goto out;
+		}
+	} else {
+		ret = kbase_mem_pool_alloc_pages(
+			&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+			num_pages, phys, false);
+		if (ret < 0)
+			goto out;
+	}
+
+	allocated_pages = true;
+	load_fw_image_section(kbdev, fw->data, phys, num_pages, flags,
+			data_start, data_end);
+
+	/* Allocate enough memory for the struct firmware_interface and
+	 * the name of the interface. An extra byte is allocated to place a
+	 * NUL-terminator in. This should already be included according to the
+	 * specification but here we add it anyway to be robust against a
+	 * corrupt firmware image.
+	 */
+	interface = kmalloc(sizeof(*interface) +
+			size - INTERFACE_ENTRY_NAME_OFFSET + 1, GFP_KERNEL);
+	if (!interface) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	name = (void *)(interface + 1);
+	memcpy(name, entry + (INTERFACE_ENTRY_NAME_OFFSET / sizeof(*entry)),
+			size - INTERFACE_ENTRY_NAME_OFFSET);
+	name[size - INTERFACE_ENTRY_NAME_OFFSET] = 0;
+
+	interface->name = name;
+	interface->phys = phys;
+	interface->num_pages = num_pages;
+	interface->virtual = virtual_start;
+	interface->kernel_map = NULL;
+	interface->flags = flags;
+	interface->data_start = data_start;
+	interface->data_end = data_end;
+	interface->pma = pma;
+
+	mem_flags = convert_mem_flags(kbdev, flags, &cache_mode);
+
+	if (flags & CSF_FIRMWARE_ENTRY_SHARED) {
+		struct page **page_list;
+		u32 i;
+		pgprot_t cpu_map_prot;
+		u32 mem_attr_index = KBASE_REG_MEMATTR_VALUE(mem_flags);
+
+		/* Since SHARED memory type was used for mapping shared memory
+		 * on GPU side, it can be mapped as cached on CPU side on both
+		 * types of coherent platforms.
+		 */
+		if ((cache_mode == CSF_FIRMWARE_CACHE_MODE_CACHED_COHERENT) ||
+		    (cache_mode == CSF_FIRMWARE_CACHE_MODE_UNCACHED_COHERENT)) {
+			WARN_ON(mem_attr_index !=
+					AS_MEMATTR_INDEX_SHARED);
+			cpu_map_prot = PAGE_KERNEL;
+		} else {
+			WARN_ON(mem_attr_index !=
+					AS_MEMATTR_INDEX_NON_CACHEABLE);
+			cpu_map_prot = pgprot_writecombine(PAGE_KERNEL);
+		}
+
+		page_list = kmalloc_array(num_pages, sizeof(*page_list),
+				GFP_KERNEL);
+		if (!page_list) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		for (i = 0; i < num_pages; i++)
+			page_list[i] = as_page(phys[i]);
+
+		interface->kernel_map = vmap(page_list, num_pages, VM_MAP,
+				cpu_map_prot);
+
+		kfree(page_list);
+
+		if (!interface->kernel_map) {
+			ret = -ENOMEM;
+			goto out;
+		}
+	}
+
+	/* Start location of the shared interface area is fixed and is
+	 * specified in firmware spec, and so there shall only be a
+	 * single entry with that start address.
+	 */
+	if (virtual_start == (KBASE_REG_ZONE_MCU_SHARED_BASE << PAGE_SHIFT))
+		kbdev->csf.shared_interface = interface;
+
+	list_add(&interface->node, &kbdev->csf.firmware_interfaces);
+
+	ret = kbase_mmu_insert_pages_no_flush(kbdev, &kbdev->csf.mcu_mmu,
+			virtual_start >> PAGE_SHIFT, phys, num_pages, mem_flags,
+			KBASE_MEM_GROUP_CSF_FW);
+
+	if (ret != 0) {
+		dev_err(kbdev->dev, "Failed to insert firmware pages\n");
+		/* The interface has been added to the list, so cleanup will
+		 * be handled by firmware unloading
+		 */
+	}
+
+	dev_dbg(kbdev->dev, "Processed section '%s'", name);
+
+	return ret;
+
+out:
+	if (allocated_pages) {
+		if (protected_mode) {
+			if (interface) {
+				kbase_csf_protected_memory_free(kbdev,
+						interface->pma, num_pages);
+			}
+		} else {
+			kbase_mem_pool_free_pages(
+				&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+				num_pages, phys, false, false);
+		}
+	}
+
+	kfree(phys);
+	kfree(interface);
+	return ret;
+}
+
+/**
+ * parse_timeline_metadata_entry() - Process a "timeline metadata" section
+ *
+ * Return: 0 if successful, negative error code on failure
+ *
+ * @kbdev: Kbase device structure
+ * @fw:    Firmware image containing the section
+ * @entry: Pointer to the section
+ * @size:  Size (in bytes) of the section
+ */
+static int parse_timeline_metadata_entry(struct kbase_device *kbdev,
+	const struct firmware *fw, const u32 *entry, unsigned int size)
+{
+	const u32 data_start = entry[0];
+	const u32 data_size = entry[1];
+	const u32 data_end = data_start + data_size;
+	const char *name = (char *)&entry[2];
+	struct firmware_timeline_metadata *metadata;
+	const unsigned int name_len =
+		size - TL_METADATA_ENTRY_NAME_OFFSET;
+	size_t allocation_size = sizeof(*metadata) + name_len + 1 + data_size;
+
+	if (data_end > fw->size) {
+		dev_err(kbdev->dev,
+			"Firmware corrupt, file truncated? data_end=0x%x > fw->size=0x%zx",
+			data_end, fw->size);
+		return -EINVAL;
+	}
+
+	/* Allocate enough space for firmware_timeline_metadata,
+	 * its name and the content.
+	 */
+	metadata = kmalloc(allocation_size, GFP_KERNEL);
+	if (!metadata)
+		return -ENOMEM;
+
+	metadata->name = (char *)(metadata + 1);
+	metadata->data = (char *)(metadata + 1) + name_len + 1;
+	metadata->size = data_size;
+
+	memcpy(metadata->name, name, name_len);
+	metadata->name[name_len] = 0;
+
+	/* Copy metadata's content. */
+	memcpy(metadata->data, fw->data + data_start, data_size);
+
+	list_add(&metadata->node, &kbdev->csf.firmware_timeline_metadata);
+
+	dev_dbg(kbdev->dev, "Timeline metadata '%s'", metadata->name);
+
+	return 0;
+}
+
+/**
+ * load_firmware_entry() - Process an entry from a firmware image
+ *
+ * Read an entry from a firmware image and do any necessary work (e.g. loading
+ * the data into page accessible to the MCU).
+ *
+ * Unknown entries are ignored if the 'optional' flag is set within the entry,
+ * otherwise the function will fail with -EINVAL
+ *
+ * Return: 0 if successful, negative error code on failure
+ *
+ * @kbdev:  Kbase device
+ * @fw:     Firmware image containing the entry
+ * @offset: Byte offset within the image of the entry to load
+ * @header: Header word of the entry
+ */
+static int load_firmware_entry(struct kbase_device *kbdev,
+		const struct firmware *fw,
+		u32 offset, u32 header)
+{
+	const unsigned int type = entry_type(header);
+	unsigned int size = entry_size(header);
+	const bool optional = entry_optional(header);
+	const u32 *entry = (void *)(fw->data + offset);
+
+	if ((offset % sizeof(*entry)) || (size % sizeof(*entry))) {
+		dev_err(kbdev->dev, "Firmware entry isn't 32 bit aligned, offset=0x%x size=0x%x\n",
+				offset, size);
+		return -EINVAL;
+	}
+
+	if (size < sizeof(*entry)) {
+		dev_err(kbdev->dev, "Size field too small: %u\n", size);
+		return -EINVAL;
+	}
+
+	/* Remove the header */
+	entry++;
+	size -= sizeof(*entry);
+
+	switch (type) {
+	case CSF_FIRMWARE_ENTRY_TYPE_INTERFACE:
+		/* Interface memory setup */
+		if (size < INTERFACE_ENTRY_NAME_OFFSET + sizeof(*entry)) {
+			dev_err(kbdev->dev, "Interface memory setup entry too short (size=%u)\n",
+					size);
+			return -EINVAL;
+		}
+		return parse_memory_setup_entry(kbdev, fw, entry, size);
+	case CSF_FIRMWARE_ENTRY_TYPE_CONFIGURATION:
+		/* Configuration option */
+		if (size < CONFIGURATION_ENTRY_NAME_OFFSET + sizeof(*entry)) {
+			dev_err(kbdev->dev, "Configuration option entry too short (size=%u)\n",
+					size);
+			return -EINVAL;
+		}
+		return kbase_csf_firmware_cfg_option_entry_parse(
+			kbdev, fw, entry, size);
+	case CSF_FIRMWARE_ENTRY_TYPE_FUTF_TEST:
+#ifndef MALI_KBASE_BUILD
+		/* FW UTF option */
+		if (size < 2*sizeof(*entry)) {
+			dev_err(kbdev->dev, "FW UTF entry too short (size=%u)\n",
+					size);
+			return -EINVAL;
+		}
+		return mali_kutf_process_fw_utf_entry(kbdev, fw->data,
+						      fw->size, entry);
+#endif
+		break;
+	case CSF_FIRMWARE_ENTRY_TYPE_TRACE_BUFFER:
+		/* Trace buffer */
+		if (size < TRACE_BUFFER_ENTRY_NAME_OFFSET + sizeof(*entry)) {
+			dev_err(kbdev->dev, "Trace Buffer entry too short (size=%u)\n",
+				size);
+			return -EINVAL;
+		}
+		return kbase_csf_firmware_parse_trace_buffer_entry(
+				kbdev, entry, size);
+	case CSF_FIRMWARE_ENTRY_TYPE_TIMELINE_METADATA:
+		/* Meta data section */
+		if (size < TL_METADATA_ENTRY_NAME_OFFSET + sizeof(*entry)) {
+			dev_err(kbdev->dev, "Timeline metadata entry too short (size=%u)\n",
+				size);
+			return -EINVAL;
+		}
+		return parse_timeline_metadata_entry(kbdev, fw, entry, size);
+	}
+
+	if (!optional) {
+		dev_err(kbdev->dev,
+			"Unsupported non-optional entry type %u in firmware\n",
+			type);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static void free_global_iface(struct kbase_device *kbdev)
+{
+	struct kbase_csf_global_iface *iface = &kbdev->csf.global_iface;
+
+	if (iface->groups) {
+		unsigned int gid;
+
+		for (gid = 0; gid < iface->group_num; ++gid)
+			kfree(iface->groups[gid].streams);
+
+		kfree(iface->groups);
+		iface->groups = NULL;
+	}
+}
+
+/**
+ * iface_gpu_va_to_cpu - Convert a GPU VA address within the shared interface
+ *                       region to a CPU address, using the existing mapping.
+ * @kbdev: Device pointer
+ * @gpu_va: GPU VA to convert
+ *
+ * Return: A CPU pointer to the location within the shared interface region, or
+ *         NULL on failure.
+ */
+static inline void *iface_gpu_va_to_cpu(struct kbase_device *kbdev, u32 gpu_va)
+{
+	struct firmware_interface *interface = kbdev->csf.shared_interface;
+	u8 *kernel_base = interface->kernel_map;
+
+	if (gpu_va < interface->virtual ||
+	    gpu_va >= interface->virtual + interface->num_pages * PAGE_SIZE) {
+		dev_err(kbdev->dev,
+				"Interface address 0x%x not within %u-page region at 0x%x",
+				gpu_va, interface->num_pages,
+				interface->virtual);
+		return NULL;
+	}
+
+	return (void *)(kernel_base + (gpu_va - interface->virtual));
+}
+
+static int parse_cmd_stream_info(struct kbase_device *kbdev,
+		struct kbase_csf_cmd_stream_info *sinfo,
+		u32 *stream_base)
+{
+	sinfo->kbdev = kbdev;
+	sinfo->features = stream_base[STREAM_FEATURES/4];
+	sinfo->input = iface_gpu_va_to_cpu(kbdev,
+			stream_base[STREAM_INPUT_VA/4]);
+	sinfo->output = iface_gpu_va_to_cpu(kbdev,
+			stream_base[STREAM_OUTPUT_VA/4]);
+
+	if (sinfo->input == NULL || sinfo->output == NULL)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int parse_cmd_stream_group_info(struct kbase_device *kbdev,
+		struct kbase_csf_cmd_stream_group_info *ginfo,
+		u32 *group_base, u32 group_stride)
+{
+	unsigned int sid;
+
+	ginfo->kbdev = kbdev;
+	ginfo->features = group_base[GROUP_FEATURES/4];
+	ginfo->input = iface_gpu_va_to_cpu(kbdev,
+			group_base[GROUP_INPUT_VA/4]);
+	ginfo->output = iface_gpu_va_to_cpu(kbdev,
+			group_base[GROUP_OUTPUT_VA/4]);
+
+	if (ginfo->input == NULL || ginfo->output == NULL)
+		return -ENOMEM;
+
+	ginfo->suspend_size = group_base[GROUP_SUSPEND_SIZE/4];
+	ginfo->protm_suspend_size = group_base[GROUP_PROTM_SUSPEND_SIZE/4];
+	ginfo->stream_num = group_base[GROUP_STREAM_NUM/4];
+
+	if (ginfo->stream_num < MIN_SUPPORTED_STREAMS_PER_GROUP ||
+			ginfo->stream_num > MAX_SUPPORTED_STREAMS_PER_GROUP) {
+		dev_err(kbdev->dev, "CSG with %u streams out of range %u-%u",
+				ginfo->stream_num,
+				MIN_SUPPORTED_STREAMS_PER_GROUP,
+				MAX_SUPPORTED_STREAMS_PER_GROUP);
+		return -EINVAL;
+	}
+
+	ginfo->stream_stride = group_base[GROUP_STREAM_STRIDE/4];
+
+	if (ginfo->stream_num * ginfo->stream_stride > group_stride) {
+		dev_err(kbdev->dev,
+				"group stride of 0x%x exceeded by %u streams with stride 0x%x",
+				group_stride, ginfo->stream_num,
+				ginfo->stream_stride);
+		return -EINVAL;
+	}
+
+	ginfo->streams = kmalloc_array(ginfo->stream_num,
+			sizeof(*ginfo->streams), GFP_KERNEL);
+
+	if (!ginfo->streams)
+		return -ENOMEM;
+
+	for (sid = 0; sid < ginfo->stream_num; sid++) {
+		int err;
+		u32 *stream_base = group_base + (STREAM_CONTROL_0 +
+				ginfo->stream_stride * sid) / 4;
+
+		err = parse_cmd_stream_info(kbdev, &ginfo->streams[sid],
+				stream_base);
+		if (err < 0) {
+			/* caller will free the memory for streams array */
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static u32 get_firmware_version(struct kbase_device *kbdev)
+{
+	struct firmware_interface *interface = kbdev->csf.shared_interface;
+	u32 *shared_info = interface->kernel_map;
+
+	return shared_info[GLB_VERSION/4];
+}
+
+static int parse_capabilities(struct kbase_device *kbdev)
+{
+	struct firmware_interface *interface = kbdev->csf.shared_interface;
+	u32 *shared_info = interface->kernel_map;
+	struct kbase_csf_global_iface *iface = &kbdev->csf.global_iface;
+	unsigned int gid;
+
+	/* All offsets are in bytes, so divide by 4 for access via a u32 pointer
+	 */
+
+	/* The version number of the global interface is expected to be a
+	 * non-zero value. If it's not, the firmware may not have booted.
+	 */
+	iface->version = get_firmware_version(kbdev);
+	if (!iface->version) {
+		dev_err(kbdev->dev, "Version check failed. Firmware may have failed to boot.");
+		return -EINVAL;
+	}
+
+
+	iface->kbdev = kbdev;
+	iface->features = shared_info[GLB_FEATURES/4];
+	iface->input = iface_gpu_va_to_cpu(kbdev, shared_info[GLB_INPUT_VA/4]);
+	iface->output = iface_gpu_va_to_cpu(kbdev,
+			shared_info[GLB_OUTPUT_VA/4]);
+
+	if (iface->input == NULL || iface->output == NULL)
+		return -ENOMEM;
+
+	iface->group_num = shared_info[GLB_GROUP_NUM/4];
+
+	if (iface->group_num < MIN_SUPPORTED_CSGS ||
+			iface->group_num > MAX_SUPPORTED_CSGS) {
+		dev_err(kbdev->dev,
+				"Interface containing %u CSGs outside of range %u-%u",
+				iface->group_num, MIN_SUPPORTED_CSGS,
+				MAX_SUPPORTED_CSGS);
+		return -EINVAL;
+	}
+
+	iface->group_stride = shared_info[GLB_GROUP_STRIDE/4];
+	iface->prfcnt_size = shared_info[GLB_PRFCNT_SIZE/4];
+
+	if ((GROUP_CONTROL_0 +
+		(unsigned long)iface->group_num * iface->group_stride) >
+			(interface->num_pages * PAGE_SIZE)) {
+		dev_err(kbdev->dev,
+				"interface size of %u pages exceeded by %u CSGs with stride 0x%x",
+				interface->num_pages, iface->group_num,
+				iface->group_stride);
+		return -EINVAL;
+	}
+
+	WARN_ON(iface->groups);
+
+	iface->groups = kcalloc(iface->group_num, sizeof(*iface->groups),
+				GFP_KERNEL);
+	if (!iface->groups)
+		return -ENOMEM;
+
+	for (gid = 0; gid < iface->group_num; gid++) {
+		int err;
+		u32 *group_base = shared_info + (GROUP_CONTROL_0 +
+				iface->group_stride * gid) / 4;
+
+		err = parse_cmd_stream_group_info(kbdev, &iface->groups[gid],
+				group_base, iface->group_stride);
+		if (err < 0) {
+			free_global_iface(kbdev);
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static inline void access_firmware_memory(struct kbase_device *kbdev,
+	u32 gpu_addr, u32 *value, const bool read)
+{
+	struct firmware_interface *interface;
+
+	list_for_each_entry(interface, &kbdev->csf.firmware_interfaces, node) {
+		if ((gpu_addr >= interface->virtual) &&
+			(gpu_addr < interface->virtual + (interface->num_pages << PAGE_SHIFT))) {
+			u32 offset_bytes = gpu_addr - interface->virtual;
+			u32 page_num = offset_bytes >> PAGE_SHIFT;
+			u32 offset_in_page = offset_bytes & ~PAGE_MASK;
+			struct page *target_page = as_page(
+				interface->phys[page_num]);
+			u32 *cpu_addr = kmap_atomic(target_page);
+
+			if (read) {
+				kbase_sync_single_for_device(kbdev,
+					kbase_dma_addr(target_page) + offset_in_page,
+					sizeof(u32), DMA_BIDIRECTIONAL);
+
+				*value = cpu_addr[offset_in_page >> 2];
+			} else {
+				cpu_addr[offset_in_page >> 2] = *value;
+
+				kbase_sync_single_for_device(kbdev,
+					kbase_dma_addr(target_page) + offset_in_page,
+					sizeof(u32), DMA_BIDIRECTIONAL);
+			}
+
+			kunmap_atomic(cpu_addr);
+			return;
+		}
+	}
+	dev_warn(kbdev->dev, "Invalid GPU VA %x passed\n", gpu_addr);
+}
+
+void kbase_csf_read_firmware_memory(struct kbase_device *kbdev,
+	u32 gpu_addr, u32 *value)
+{
+	access_firmware_memory(kbdev, gpu_addr, value, true);
+}
+
+void kbase_csf_update_firmware_memory(struct kbase_device *kbdev,
+	u32 gpu_addr, u32 value)
+{
+	access_firmware_memory(kbdev, gpu_addr, &value, false);
+}
+
+void kbase_csf_firmware_cs_input(
+	const struct kbase_csf_cmd_stream_info *const info, const u32 offset,
+	const u32 value)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x\n", offset, value);
+	input_page_write(info->input, offset, value);
+}
+
+u32 kbase_csf_firmware_cs_input_read(
+	const struct kbase_csf_cmd_stream_info *const info,
+	const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = input_page_read(info->input, offset);
+
+	dev_dbg(kbdev->dev, "cs input r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_cs_input_mask(
+	const struct kbase_csf_cmd_stream_info *const info, const u32 offset,
+	const u32 value, const u32 mask)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x mask %08x\n",
+			offset, value, mask);
+	input_page_partial_write(info->input, offset, value, mask);
+}
+
+u32 kbase_csf_firmware_cs_output(
+	const struct kbase_csf_cmd_stream_info *const info, const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = output_page_read(info->output, offset);
+
+	dev_dbg(kbdev->dev, "cs output r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_csg_input(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset, const u32 value)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x\n",
+			offset, value);
+	input_page_write(info->input, offset, value);
+}
+
+u32 kbase_csf_firmware_csg_input_read(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = input_page_read(info->input, offset);
+
+	dev_dbg(kbdev->dev, "csg input r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_csg_input_mask(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset, const u32 value, const u32 mask)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x mask %08x\n",
+			offset, value, mask);
+	input_page_partial_write(info->input, offset, value, mask);
+}
+
+u32 kbase_csf_firmware_csg_output(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = output_page_read(info->output, offset);
+
+	dev_dbg(kbdev->dev, "csg output r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_global_input(
+	const struct kbase_csf_global_iface *const iface, const u32 offset,
+	const u32 value)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+
+	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x\n", offset, value);
+	input_page_write(iface->input, offset, value);
+}
+
+void kbase_csf_firmware_global_input_mask(
+	const struct kbase_csf_global_iface *const iface, const u32 offset,
+	const u32 value, const u32 mask)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+
+	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x mask %08x\n",
+			offset, value, mask);
+	input_page_partial_write(iface->input, offset, value, mask);
+}
+
+u32 kbase_csf_firmware_global_input_read(
+	const struct kbase_csf_global_iface *const iface, const u32 offset)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+	u32 const val = input_page_read(iface->input, offset);
+
+	dev_dbg(kbdev->dev, "glob input r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+u32 kbase_csf_firmware_global_output(
+	const struct kbase_csf_global_iface *const iface, const u32 offset)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+	u32 const val = output_page_read(iface->output, offset);
+
+	dev_dbg(kbdev->dev, "glob output r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+static bool global_request_complete(struct kbase_device *const kbdev,
+				    u32 const req_mask)
+{
+	struct kbase_csf_global_iface *global_iface =
+				&kbdev->csf.global_iface;
+	bool complete = false;
+	unsigned long flags;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+
+	if ((kbase_csf_firmware_global_output(global_iface, GLB_ACK) &
+	     req_mask) ==
+	    (kbase_csf_firmware_global_input_read(global_iface, GLB_REQ) &
+	     req_mask))
+		complete = true;
+
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	return complete;
+}
+
+static int wait_for_global_request(struct kbase_device *const kbdev,
+				   u32 const req_mask)
+{
+	const long wait_timeout =
+		kbase_csf_timeout_in_jiffies(GLB_REQ_WAIT_TIMEOUT_MS);
+	long remaining;
+	int err = 0;
+
+	remaining = wait_event_timeout(kbdev->csf.event_wait,
+				       global_request_complete(kbdev, req_mask),
+				       wait_timeout);
+
+	if (!remaining) {
+		dev_warn(kbdev->dev, "Timed out waiting for global request %x to complete",
+			 req_mask);
+		err = -ETIMEDOUT;
+	}
+
+	return err;
+}
+
+static void set_global_request(
+	const struct kbase_csf_global_iface *const global_iface,
+	u32 const req_mask)
+{
+	u32 glb_req;
+
+	kbase_csf_scheduler_spin_lock_assert_held(global_iface->kbdev);
+
+	glb_req = kbase_csf_firmware_global_output(global_iface, GLB_ACK);
+	glb_req ^= req_mask;
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, glb_req,
+					     req_mask);
+}
+
+static void enable_endpoints_global(
+	const struct kbase_csf_global_iface *const global_iface,
+	u64 const shader_core_mask)
+{
+	kbase_csf_firmware_global_input(global_iface, GLB_ALLOC_EN_LO,
+		shader_core_mask & U32_MAX);
+	kbase_csf_firmware_global_input(global_iface, GLB_ALLOC_EN_HI,
+		shader_core_mask >> 32);
+
+	set_global_request(global_iface, GLB_REQ_CFG_ALLOC_EN_MASK);
+}
+
+static void set_timeout_global(
+	const struct kbase_csf_global_iface *const global_iface,
+	u64 const timeout)
+{
+	kbase_csf_firmware_global_input(global_iface, GLB_PROGRESS_TIMER,
+		timeout / GLB_PROGRESS_TIMER_TIMEOUT_SCALE);
+
+	set_global_request(global_iface, GLB_REQ_CFG_PROGRESS_TIMER_MASK);
+}
+
+static void set_coherency_mode(struct kbase_device *const kbdev)
+{
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	u32 protected_mode_coherency = kbdev->system_coherency;
+
+	/* GPU is supposed to use ACE-Lite coherency mode on a fully coherent
+	 * system during protected mode execution.
+	 */
+	if (kbdev->system_coherency == COHERENCY_ACE)
+		protected_mode_coherency = COHERENCY_ACE_LITE;
+
+	kbase_csf_firmware_global_input(global_iface, GLB_PROTM_COHERENCY,
+					protected_mode_coherency);
+}
+
+static void global_init(struct kbase_device *const kbdev, u32 req_mask)
+{
+	u32 const ack_irq_mask = GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_MASK  |
+			GLB_ACK_IRQ_MASK_PING_MASK |
+			GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_MASK |
+			GLB_ACK_IRQ_MASK_PROTM_ENTER_MASK |
+			GLB_ACK_IRQ_MASK_PROTM_EXIT_MASK;
+
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	unsigned long flags;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+
+	/* Set the cohereny mode for protected mode execution */
+	set_coherency_mode(kbdev);
+
+	/* Enable endpoints on all present shader cores */
+	enable_endpoints_global(global_iface,
+		kbase_pm_get_present_cores(kbdev, KBASE_PM_CORE_SHADER));
+
+	set_timeout_global(global_iface, kbase_csf_timeout_get(kbdev));
+
+	/* Unmask the interrupts */
+	kbase_csf_firmware_global_input(global_iface,
+		GLB_ACK_IRQ_MASK, ack_irq_mask);
+
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+}
+
+/**
+ * global_init_on_boot - Sends a global request to control various features.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * Currently only the request to enable endpoints and timeout for GPU progress
+ * timer is sent.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+static int global_init_on_boot(struct kbase_device *const kbdev)
+{
+	u32 const req_mask = CSF_GLB_REQ_CFG_MASK;
+
+	global_init(kbdev, req_mask);
+
+	return wait_for_global_request(kbdev, req_mask);
+}
+
+void kbase_csf_firmware_global_reinit(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbdev->csf.glb_init_request_pending = true;
+	global_init(kbdev, CSF_GLB_REQ_CFG_MASK);
+}
+
+bool kbase_csf_firmware_global_reinit_complete(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+	WARN_ON(!kbdev->csf.glb_init_request_pending);
+
+	if (global_request_complete(kbdev, CSF_GLB_REQ_CFG_MASK))
+		kbdev->csf.glb_init_request_pending = false;
+
+	return !kbdev->csf.glb_init_request_pending;
+}
+
+/**
+ * This helper function will reload the firmware image and re-enable the MCU.
+ * It is supposed to be called after MCU(GPU) has been reset.
+ * Unlike the initial boot the firmware binary image is not parsed completely.
+ * Only the data sections, which were loaded in memory during the initial boot,
+ * are re-initialized either by zeroing them or copying their data from the
+ * firmware binary image. The memory allocation for the firmware pages and
+ * MMU programming is not needed for the reboot, presuming the firmware binary
+ * file on the filesystem would not change.
+ */
+static void kbase_csf_firmware_reload_worker(struct work_struct *work)
+{
+	struct kbase_device *kbdev = container_of(work, struct kbase_device,
+						  csf.firmware_reload_work);
+	int err;
+
+	dev_info(kbdev->dev, "reloading firmware");
+
+	/* Reload just the data sections from firmware binary image */
+	err = reload_fw_data_sections(kbdev);
+	if (err)
+		return;
+
+	kbase_csf_tl_reader_reset(&kbdev->timeline->csf_tl_reader);
+
+	/* Reboot the firmware */
+	kbase_csf_firmware_enable_mcu(kbdev);
+}
+
+void kbase_csf_firmware_trigger_reload(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbdev->csf.firmware_reloaded = false;
+
+	if (kbdev->csf.firmware_reload_needed) {
+		kbdev->csf.firmware_reload_needed = false;
+		queue_work(system_wq, &kbdev->csf.firmware_reload_work);
+	} else {
+		kbase_csf_firmware_enable_mcu(kbdev);
+	}
+}
+
+void kbase_csf_firmware_reload_completed(struct kbase_device *kbdev)
+{
+	u32 version;
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (unlikely(!kbdev->csf.firmware_inited))
+		return;
+
+	/* Check firmware rebooted properly: we do not expect
+	 * the version number to change with a running reboot.
+	 */
+	version = get_firmware_version(kbdev);
+
+	if (version != kbdev->csf.global_iface.version)
+		dev_err(kbdev->dev, "Version check failed in firmware reboot.");
+
+	KBASE_KTRACE_ADD(kbdev, FIRMWARE_REBOOT, NULL, 0u);
+
+	/* Tell MCU state machine to transit to next state */
+	kbdev->csf.firmware_reloaded = true;
+	kbase_pm_update_state(kbdev);
+}
+
+int kbase_csf_firmware_init(struct kbase_device *kbdev)
+{
+	const struct firmware *firmware;
+	const u32 magic = FIRMWARE_HEADER_MAGIC;
+	u8 version_major, version_minor;
+	u32 version_hash;
+	u32 entry_end_offset;
+	u32 entry_offset;
+	int ret;
+
+	if (WARN_ON((kbdev->as_free & MCU_AS_BITMASK) == 0))
+		return -EINVAL;
+	kbdev->as_free &= ~MCU_AS_BITMASK;
+
+	ret = kbase_mmu_init(kbdev, &kbdev->csf.mcu_mmu, NULL,
+		BASE_MEM_GROUP_DEFAULT);
+
+	if (ret != 0) {
+		/* Release the address space */
+		kbdev->as_free |= MCU_AS_BITMASK;
+		return ret;
+	}
+
+	init_waitqueue_head(&kbdev->csf.event_wait);
+	kbdev->csf.interrupt_received = false;
+
+	INIT_LIST_HEAD(&kbdev->csf.firmware_interfaces);
+	INIT_LIST_HEAD(&kbdev->csf.firmware_config);
+	INIT_LIST_HEAD(&kbdev->csf.firmware_timeline_metadata);
+	INIT_LIST_HEAD(&kbdev->csf.firmware_trace_buffers.list);
+	INIT_WORK(&kbdev->csf.firmware_reload_work,
+		  kbase_csf_firmware_reload_worker);
+
+	mutex_init(&kbdev->csf.reg_lock);
+
+	ret = kbase_mcu_shared_interface_region_tracker_init(kbdev);
+	if (ret != 0) {
+		dev_err(kbdev->dev, "Failed to setup the rb tree for managing shared interface segment\n");
+		goto error;
+	}
+
+	if (request_firmware(&firmware, fw_name, kbdev->dev) != 0) {
+		dev_err(kbdev->dev,
+				"Failed to load firmware image '%s'\n",
+				fw_name);
+		ret = -ENOENT;
+		goto error;
+	}
+
+	if (firmware->size < FIRMWARE_HEADER_LENGTH) {
+		dev_err(kbdev->dev, "Firmware too small\n");
+		ret = -EINVAL;
+		goto error;
+	}
+
+	if (memcmp(firmware->data, &magic, sizeof(magic)) != 0) {
+		dev_err(kbdev->dev, "Incorrect firmware magic\n");
+		ret = -EINVAL;
+		goto error;
+	}
+
+	version_major = firmware->data[4];
+	version_minor = firmware->data[5];
+
+	if (version_major != FIRMWARE_HEADER_VERSION) {
+		dev_err(kbdev->dev,
+				"Firmware header version %d.%d not understood\n",
+				version_major, version_minor);
+		ret = -EINVAL;
+		goto error;
+	}
+
+	memcpy(&version_hash, &firmware->data[8], sizeof(version_hash));
+
+	dev_notice(kbdev->dev, "Loading Mali firmware 0x%x", version_hash);
+
+	memcpy(&entry_end_offset, &firmware->data[0x10],
+			sizeof(entry_end_offset));
+
+	if (entry_end_offset > firmware->size) {
+		dev_err(kbdev->dev, "Firmware image is truncated\n");
+		ret = -EINVAL;
+		goto error;
+	}
+
+	entry_offset = FIRMWARE_HEADER_LENGTH;
+	while (entry_offset < entry_end_offset) {
+		u32 header;
+		unsigned int size;
+
+		memcpy(&header, &firmware->data[entry_offset], sizeof(header));
+
+		size = entry_size(header);
+
+		ret = load_firmware_entry(kbdev, firmware, entry_offset,
+				header);
+		if (ret != 0) {
+			dev_err(kbdev->dev, "Failed to load firmware image\n");
+			goto error;
+		}
+		entry_offset += size;
+	}
+
+	if (!kbdev->csf.shared_interface) {
+		dev_err(kbdev->dev, "Shared interface region not found\n");
+		ret = -EINVAL;
+		goto error;
+	} else {
+		ret = setup_shared_iface_static_region(kbdev);
+		if (ret != 0) {
+			dev_err(kbdev->dev, "Failed to insert a region for shared iface entry parsed from fw image\n");
+			goto error;
+		}
+	}
+
+	ret = kbase_csf_firmware_trace_buffers_init(kbdev);
+	if (ret != 0) {
+		dev_err(kbdev->dev, "Failed to initialize trace buffers\n");
+		goto error;
+	}
+
+	/* Make sure L2 cache is powered up */
+	kbase_pm_wait_for_l2_powered(kbdev);
+
+	/* Load the MMU tables into the selected address space */
+	load_mmu_tables(kbdev);
+
+	boot_csf_firmware(kbdev);
+
+	ret = parse_capabilities(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = kbase_csf_doorbell_mapping_init(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = kbase_csf_scheduler_init(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = kbase_csf_timeout_init(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = global_init_on_boot(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = kbase_csf_firmware_cfg_init(kbdev);
+	if (ret != 0)
+		goto error;
+
+	/* Firmware loaded successfully */
+	release_firmware(firmware);
+	KBASE_KTRACE_ADD(kbdev, FIRMWARE_BOOT, NULL,
+			(((u64)version_hash) << 32) |
+			(((u64)version_major) << 8) | version_minor);
+	return 0;
+
+error:
+	kbase_csf_firmware_term(kbdev);
+	release_firmware(firmware);
+	return ret;
+}
+
+void kbase_csf_firmware_term(struct kbase_device *kbdev)
+{
+	unsigned long flags;
+	int ret = 0;
+
+	while (kbase_reset_gpu_is_active(kbdev) && !ret)
+		ret = kbase_reset_gpu_wait(kbdev);
+
+	WARN(ret, "failed to wait for GPU reset");
+
+	/* Make sure ongoing transitions have completed */
+	kbase_pm_wait_for_desired_state(kbdev);
+
+	kbase_csf_firmware_cfg_term(kbdev);
+
+	kbase_csf_timeout_term(kbdev);
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbdev->csf.firmware_inited = false;
+	if (kbdev->pm.backend.mcu_state != KBASE_MCU_OFF) {
+		kbdev->pm.backend.mcu_state = KBASE_MCU_OFF;
+		stop_csf_firmware(kbdev);
+	}
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	unload_mmu_tables(kbdev);
+
+	kbase_mmu_term(kbdev, &kbdev->csf.mcu_mmu);
+
+	kbase_csf_scheduler_term(kbdev);
+
+	kbase_csf_doorbell_mapping_term(kbdev);
+
+	free_global_iface(kbdev);
+
+	/* Release the address space */
+	kbdev->as_free |= MCU_AS_BITMASK;
+
+	while (!list_empty(&kbdev->csf.firmware_interfaces)) {
+		struct firmware_interface *interface;
+
+		interface = list_first_entry(&kbdev->csf.firmware_interfaces,
+				struct firmware_interface, node);
+		list_del(&interface->node);
+
+		vunmap(interface->kernel_map);
+		if (interface->flags & CSF_FIRMWARE_ENTRY_PROTECTED) {
+			kbase_csf_protected_memory_free(kbdev, interface->pma,
+				interface->num_pages);
+		} else {
+			kbase_mem_pool_free_pages(
+				&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+				interface->num_pages, interface->phys,
+				true, false);
+		}
+
+		kfree(interface->phys);
+		kfree(interface);
+	}
+
+	while (!list_empty(&kbdev->csf.firmware_timeline_metadata)) {
+		struct firmware_timeline_metadata *metadata;
+
+		metadata = list_first_entry(
+			&kbdev->csf.firmware_timeline_metadata,
+			struct firmware_timeline_metadata,
+			node);
+		list_del(&metadata->node);
+
+		kfree(metadata);
+	}
+
+	kbase_csf_firmware_trace_buffers_term(kbdev);
+
+#ifndef MALI_KBASE_BUILD
+	mali_kutf_fw_utf_entry_cleanup(kbdev);
+#endif
+
+	mutex_destroy(&kbdev->csf.reg_lock);
+
+	/* This will also free up the region allocated for the shared interface
+	 * entry parsed from the firmware image.
+	 */
+	kbase_mcu_shared_interface_region_tracker_term(kbdev);
+}
+
+int kbase_csf_firmware_ping(struct kbase_device *const kbdev)
+{
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	unsigned long flags;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	set_global_request(global_iface, GLB_REQ_PING_MASK);
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	return wait_for_global_request(kbdev, GLB_REQ_PING_MASK);
+}
+
+int kbase_csf_firmware_set_timeout(struct kbase_device *const kbdev,
+	u64 const timeout)
+{
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	unsigned long flags;
+	int err;
+
+	/* The 'reg_lock' is also taken and is held till the update is not
+	 * complete, to ensure the update of timeout value by multiple Users
+	 * gets serialized.
+	 */
+	mutex_lock(&kbdev->csf.reg_lock);
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	set_timeout_global(global_iface, timeout);
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	err = wait_for_global_request(kbdev, GLB_REQ_CFG_PROGRESS_TIMER_MASK);
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+	return err;
+}
+
+void kbase_csf_enter_protected_mode(struct kbase_device *kbdev)
+{
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	unsigned long flags;
+	unsigned int value;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	value = kbase_csf_firmware_global_output(global_iface, GLB_ACK);
+	value ^= GLB_REQ_PROTM_ENTER_MASK;
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, value,
+					     GLB_REQ_PROTM_ENTER_MASK);
+	dev_dbg(kbdev->dev, "Sending request to enter protected mode");
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	wait_for_global_request(kbdev, GLB_REQ_PROTM_ENTER_MASK);
+}
+
+void kbase_csf_firmware_trigger_mcu_halt(struct kbase_device *kbdev)
+{
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	unsigned long flags;
+	unsigned int value;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	value = kbase_csf_firmware_global_output(global_iface, GLB_ACK);
+	value ^= GLB_REQ_HALT_MASK;
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, value,
+					     GLB_REQ_HALT_MASK);
+	dev_dbg(kbdev->dev, "Sending request to HALT MCU");
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+}
+
+/**
+ * copy_grp_and_stm - Copy command stream and/or group data
+ *
+ * @iface:                Global command stream front-end interface provided by
+ *                        the firmware.
+ * @group_data:           Pointer where to store all the group data
+ *                        (sequentially).
+ * @max_group_num:        The maximum number of groups to be read. Can be 0, in
+ *                        which case group_data is unused.
+ * @stream_data:          Pointer where to store all the stream data
+ *                        (sequentially).
+ * @max_total_stream_num: The maximum number of streams to be read.
+ *                        Can be 0, in which case stream_data is unused.
+ *
+ * Return: Total number of command streams, summed across all groups.
+ */
+static u32 copy_grp_and_stm(
+	const struct kbase_csf_global_iface * const iface,
+	struct basep_cs_group_control * const group_data,
+	u32 max_group_num,
+	struct basep_cs_stream_control * const stream_data,
+	u32 max_total_stream_num)
+{
+	u32 i, total_stream_num = 0;
+
+	if (WARN_ON((max_group_num > 0) && !group_data))
+		max_group_num = 0;
+
+	if (WARN_ON((max_total_stream_num > 0) && !stream_data))
+		max_total_stream_num = 0;
+
+	for (i = 0; i < iface->group_num; i++) {
+		u32 j;
+
+		if (i < max_group_num) {
+			group_data[i].features = iface->groups[i].features;
+			group_data[i].stream_num = iface->groups[i].stream_num;
+			group_data[i].suspend_size =
+				iface->groups[i].suspend_size;
+		}
+		for (j = 0; j < iface->groups[i].stream_num; j++) {
+			if (total_stream_num < max_total_stream_num)
+				stream_data[total_stream_num].features =
+					iface->groups[i].streams[j].features;
+			total_stream_num++;
+		}
+	}
+
+	return total_stream_num;
+}
+
+u32 kbase_csf_firmware_get_glb_iface(struct kbase_device *kbdev,
+	struct basep_cs_group_control *const group_data,
+	u32 const max_group_num,
+	struct basep_cs_stream_control *const stream_data,
+	u32 const max_total_stream_num, u32 *const glb_version,
+	u32 *const features, u32 *const group_num, u32 *const prfcnt_size)
+{
+	const struct kbase_csf_global_iface * const iface =
+		&kbdev->csf.global_iface;
+
+	if (WARN_ON(!glb_version) ||
+		WARN_ON(!features) ||
+		WARN_ON(!group_num) ||
+		WARN_ON(!prfcnt_size))
+		return 0;
+
+	*glb_version = iface->version;
+	*features = iface->features;
+	*group_num = iface->group_num;
+	*prfcnt_size = iface->prfcnt_size;
+
+	return copy_grp_and_stm(iface, group_data, max_group_num,
+		stream_data, max_total_stream_num);
+}
+
+const char *kbase_csf_firmware_get_timeline_metadata(
+	struct kbase_device *kbdev, const char *name, size_t *size)
+{
+	struct firmware_timeline_metadata *metadata;
+
+	list_for_each_entry(
+		metadata, &kbdev->csf.firmware_timeline_metadata, node) {
+		if (!strcmp(metadata->name, name)) {
+			*size = metadata->size;
+			return metadata->data;
+		}
+	}
+
+	*size = 0;
+	return NULL;
+}
+
+int kbase_csf_firmware_mcu_shared_mapping_init(
+		struct kbase_device *kbdev,
+		unsigned int num_pages,
+		unsigned long cpu_map_properties,
+		unsigned long gpu_map_properties,
+		struct kbase_csf_mapping *csf_mapping)
+{
+	struct tagged_addr *phys;
+	struct kbase_va_region *va_reg;
+	struct page **page_list;
+	void *cpu_addr;
+	int i, ret = 0;
+	pgprot_t cpu_map_prot = PAGE_KERNEL;
+	unsigned long gpu_map_prot;
+
+	if (cpu_map_properties & PROT_READ)
+		cpu_map_prot = PAGE_KERNEL_RO;
+
+	if (kbdev->system_coherency == COHERENCY_ACE) {
+		gpu_map_prot =
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_DEFAULT_ACE);
+	} else {
+		gpu_map_prot =
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_NON_CACHEABLE);
+		cpu_map_prot = pgprot_writecombine(cpu_map_prot);
+	};
+
+	phys = kmalloc_array(num_pages, sizeof(*phys), GFP_KERNEL);
+	if (!phys)
+		goto out;
+
+	page_list = kmalloc_array(num_pages, sizeof(*page_list), GFP_KERNEL);
+	if (!page_list)
+		goto page_list_alloc_error;
+
+	ret = kbase_mem_pool_alloc_pages(
+		&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+		num_pages, phys, false);
+	if (ret <= 0)
+		goto phys_mem_pool_alloc_error;
+
+	for (i = 0; i < num_pages; i++)
+		page_list[i] = as_page(phys[i]);
+
+	cpu_addr = vmap(page_list, num_pages, VM_MAP, cpu_map_prot);
+	if (!cpu_addr)
+		goto vmap_error;
+
+	va_reg = kbase_alloc_free_region(&kbdev->csf.shared_reg_rbtree, 0,
+			num_pages, KBASE_REG_ZONE_MCU_SHARED);
+	if (!va_reg)
+		goto va_region_alloc_error;
+
+	mutex_lock(&kbdev->csf.reg_lock);
+	ret = kbase_add_va_region_rbtree(kbdev, va_reg, 0, num_pages, 1);
+	va_reg->flags &= ~KBASE_REG_FREE;
+	mutex_unlock(&kbdev->csf.reg_lock);
+	if (ret)
+		goto va_region_add_error;
+
+	gpu_map_properties &= (KBASE_REG_GPU_RD | KBASE_REG_GPU_WR);
+	gpu_map_properties |= gpu_map_prot;
+
+	ret = kbase_mmu_insert_pages_no_flush(kbdev, &kbdev->csf.mcu_mmu,
+			va_reg->start_pfn, &phys[0], num_pages,
+			gpu_map_properties, KBASE_MEM_GROUP_CSF_FW);
+	if (ret)
+		goto mmu_insert_pages_error;
+
+	kfree(page_list);
+	csf_mapping->phys = phys;
+	csf_mapping->cpu_addr = cpu_addr;
+	csf_mapping->va_reg = va_reg;
+	csf_mapping->num_pages = num_pages;
+
+	return 0;
+
+mmu_insert_pages_error:
+	mutex_lock(&kbdev->csf.reg_lock);
+	kbase_remove_va_region(va_reg);
+	mutex_unlock(&kbdev->csf.reg_lock);
+va_region_add_error:
+	kbase_free_alloced_region(va_reg);
+va_region_alloc_error:
+	vunmap(cpu_addr);
+vmap_error:
+	kbase_mem_pool_free_pages(
+		&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+		num_pages, phys, false, false);
+
+phys_mem_pool_alloc_error:
+	kfree(page_list);
+page_list_alloc_error:
+	kfree(phys);
+out:
+	/* Zero-initialize the mapping to make sure that the termination
+	 * function doesn't try to unmap or free random addresses. */
+	csf_mapping->phys = NULL;
+	csf_mapping->cpu_addr = NULL;
+	csf_mapping->va_reg = NULL;
+	csf_mapping->num_pages = 0;
+
+	return -ENOMEM;
+}
+
+void kbase_csf_firmware_mcu_shared_mapping_term(
+		struct kbase_device *kbdev, struct kbase_csf_mapping *csf_mapping)
+{
+	if (csf_mapping->va_reg) {
+		mutex_lock(&kbdev->csf.reg_lock);
+		kbase_remove_va_region(csf_mapping->va_reg);
+		mutex_unlock(&kbdev->csf.reg_lock);
+		kbase_free_alloced_region(csf_mapping->va_reg);
+	}
+
+	if (csf_mapping->phys) {
+		kbase_mem_pool_free_pages(
+			&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+			csf_mapping->num_pages, csf_mapping->phys, false,
+			false);
+	}
+
+	vunmap(csf_mapping->cpu_addr);
+	kfree(csf_mapping->phys);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h
new file mode 100644
index 000000000000..03a5217cffb0
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware.h
@@ -0,0 +1,663 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_FIRMWARE_H_
+#define _KBASE_CSF_FIRMWARE_H_
+
+#include "device/mali_kbase_device.h"
+#include "mali_gpu_csf_registers.h"
+
+/*
+ * PAGE_KERNEL_RO was only defined on 32bit ARM in 4.19 in:
+ * Commit a3266bd49c721e2e0a71f352d83713fbd60caadb
+ * Author: Luis R. Rodriguez <mcgrof@kernel.org>
+ * Date:   Fri Aug 17 15:46:29 2018 -0700
+ *
+ * mm: provide a fallback for PAGE_KERNEL_RO for architectures
+ *
+ * Some architectures do not define certain PAGE_KERNEL_* flags, this is
+ * either because:
+ *
+ * a) The way to implement some of these flags is *not yet ported*, or
+ * b) The architecture *has no way* to describe them
+ *
+ * [snip]
+ *
+ * This can be removed once support of 32bit ARM kernels predating 4.19 is no
+ * longer required.
+ */
+#ifndef PAGE_KERNEL_RO
+#define PAGE_KERNEL_RO PAGE_KERNEL
+#endif
+
+/* Address space number to claim for the firmware. */
+#define MCU_AS_NR 0
+#define MCU_AS_BITMASK (1 << MCU_AS_NR)
+
+/* Number of available Doorbells */
+#define CSF_NUM_DOORBELL ((u8)24)
+
+/* Offset to the first HW doorbell page */
+#define CSF_HW_DOORBELL_PAGE_OFFSET ((u32)0x80000)
+
+/* Size of HW Doorbell page, used to calculate the offset to subsequent pages */
+#define CSF_HW_DOORBELL_PAGE_SIZE ((u32)0x10000)
+
+/* Doorbell 0 is used by the driver. */
+#define CSF_KERNEL_DOORBELL_NR ((u32)0)
+
+/* Offset of name inside a trace buffer entry in the firmware image */
+#define TRACE_BUFFER_ENTRY_NAME_OFFSET (0x1C)
+
+/* All implementations of the host interface with major version 0 must comply
+ * with these restrictions:
+ */
+/* GLB_GROUP_NUM: At least 3 command stream groups, but no more than 31 */
+#define MIN_SUPPORTED_CSGS 3
+#define MAX_SUPPORTED_CSGS 31
+/* GROUP_STREAM_NUM: At least 8 command streams per CSG, but no more than 32 */
+#define MIN_SUPPORTED_STREAMS_PER_GROUP 8
+/* Maximum command streams per csg. */
+#define MAX_SUPPORTED_STREAMS_PER_GROUP 32
+
+struct kbase_device;
+
+
+/**
+ * struct kbase_csf_mapping - Memory mapping for CSF memory.
+ * @phys:      Physical memory allocation used by the mapping.
+ * @cpu_addr:  Starting CPU address for the mapping.
+ * @va_reg:    GPU virtual address region for the mapping.
+ * @num_pages: Size of the mapping, in memory pages.
+ */
+struct kbase_csf_mapping {
+	struct tagged_addr *phys;
+	void *cpu_addr;
+	struct kbase_va_region *va_reg;
+	unsigned int num_pages;
+};
+
+/**
+ * struct kbase_csf_trace_buffers - List and state of firmware trace buffers.
+ * @list:       List of trace buffers descriptors.
+ * @mcu_rw:     Metadata for the MCU shared memory mapping used for
+ *              GPU-readable,writable/CPU-writable variables.
+ * @mcu_write:  Metadata for the MCU shared memory mapping used for
+ *              GPU-writable/CPU-readable variables.
+ */
+struct kbase_csf_trace_buffers {
+	struct list_head list;
+	struct kbase_csf_mapping mcu_rw;
+	struct kbase_csf_mapping mcu_write;
+};
+
+/**
+ * struct kbase_csf_cmd_stream_info - Command stream interface provided by the
+ *                                    firmware.
+ *
+ * @kbdev: Address of the instance of a GPU platform device that implements
+ *         this interface.
+ * @features: Bit field of command stream features (e.g. which types of jobs
+ *            are supported). Bits 7:0 specify the number of work registers(-1).
+ *            Bits 11:8 specify the number of scoreboard entries(-1).
+ * @input: Address of command stream interface input page.
+ * @output: Address of command stream interface output page.
+ */
+struct kbase_csf_cmd_stream_info {
+	struct kbase_device *kbdev;
+	u32 features;
+	void *input;
+	void *output;
+};
+
+/**
+ * kbase_csf_firmware_cs_input() - Set a word in a command stream's input page
+ *
+ * @info: Command stream interface provided by the firmware.
+ * @offset: Offset of the word to be written, in bytes.
+ * @value: Value to be written.
+ */
+void kbase_csf_firmware_cs_input(
+	const struct kbase_csf_cmd_stream_info *info, u32 offset, u32 value);
+
+/**
+ * kbase_csf_firmware_cs_input_read() - Read a word in a command stream's input
+ *                                      page
+ *
+ * Return: Value of the word read from the command stream's input page.
+ *
+ * @info: Command stream interface provided by the firmware.
+ * @offset: Offset of the word to be read, in bytes.
+ */
+u32 kbase_csf_firmware_cs_input_read(
+	const struct kbase_csf_cmd_stream_info *const info, const u32 offset);
+
+/**
+ * kbase_csf_firmware_cs_input_mask() - Set part of a word in a command stream's
+ *                                      input page
+ *
+ * @info: Command stream interface provided by the firmware.
+ * @offset: Offset of the word to be modified, in bytes.
+ * @value: Value to be written.
+ * @mask: Bitmask with the bits to be modified set.
+ */
+void kbase_csf_firmware_cs_input_mask(
+	const struct kbase_csf_cmd_stream_info *info, u32 offset,
+	u32 value, u32 mask);
+
+/**
+ * kbase_csf_firmware_cs_output() - Read a word in a command stream's output
+ *                                  page
+ *
+ * Return: Value of the word read from the command stream's output page.
+ *
+ * @info: Command stream interface provided by the firmware.
+ * @offset: Offset of the word to be read, in bytes.
+ */
+u32 kbase_csf_firmware_cs_output(
+	const struct kbase_csf_cmd_stream_info *info, u32 offset);
+/**
+ * struct kbase_csf_cmd_stream_group_info - Command stream group interface
+ *                                          provided by the firmware.
+ *
+ * @kbdev: Address of the instance of a GPU platform device that implements
+ *         this interface.
+ * @features: Bit mask of features. Reserved bits should be 0, and should
+ *            be ignored.
+ * @input: Address of global interface input page.
+ * @output: Address of global interface output page.
+ * @suspend_size: Size in bytes for normal suspend buffer for the command
+ *                stream group.
+ * @protm_suspend_size: Size in bytes for protected mode suspend buffer
+ *                      for the command stream group.
+ * @stream_num: Number of command streams in the command stream group.
+ * @stream_stride: Stride in bytes in JASID0 virtual address between
+ *                 command stream capability structures.
+ * @streams: Address of an array of command stream capability structures.
+ */
+struct kbase_csf_cmd_stream_group_info {
+	struct kbase_device *kbdev;
+	u32 features;
+	void *input;
+	void *output;
+	u32 suspend_size;
+	u32 protm_suspend_size;
+	u32 stream_num;
+	u32 stream_stride;
+	struct kbase_csf_cmd_stream_info *streams;
+};
+
+/**
+ * kbase_csf_firmware_csg_input() - Set a word in a command stream group's
+ *                                  input page
+ *
+ * @info: Command stream group interface provided by the firmware.
+ * @offset: Offset of the word to be written, in bytes.
+ * @value: Value to be written.
+ */
+void kbase_csf_firmware_csg_input(
+	const struct kbase_csf_cmd_stream_group_info *info, u32 offset,
+	u32 value);
+
+/**
+ * kbase_csf_firmware_csg_input_read() - Read a word in a command stream group's
+ *                                       input page
+ *
+ * Return: Value of the word read from the command stream group's input page.
+ *
+ * @info: Command stream group interface provided by the firmware.
+ * @offset: Offset of the word to be read, in bytes.
+ */
+u32 kbase_csf_firmware_csg_input_read(
+	const struct kbase_csf_cmd_stream_group_info *info, u32 offset);
+
+/**
+ * kbase_csf_firmware_csg_input_mask() - Set part of a word in a command stream
+ *                                       group's input page
+ *
+ * @info: Command stream group interface provided by the firmware.
+ * @offset: Offset of the word to be modified, in bytes.
+ * @value: Value to be written.
+ * @mask: Bitmask with the bits to be modified set.
+ */
+void kbase_csf_firmware_csg_input_mask(
+	const struct kbase_csf_cmd_stream_group_info *info, u32 offset,
+	u32 value, u32 mask);
+
+/**
+ * kbase_csf_firmware_csg_output()- Read a word in a command stream group's
+ *                                  output page
+ *
+ * Return: Value of the word read from the command stream group's output page.
+ *
+ * @info: Command stream group interface provided by the firmware.
+ * @offset: Offset of the word to be read, in bytes.
+ */
+u32 kbase_csf_firmware_csg_output(
+	const struct kbase_csf_cmd_stream_group_info *info, u32 offset);
+
+/**
+ * struct kbase_csf_global_iface - Global command stream front-end interface
+ *                                 provided by the firmware.
+ *
+ * @kbdev: Address of the instance of a GPU platform device that implements
+ *         this interface.
+ * @version: Bits 31:16 hold the major version number and 15:0 hold the minor
+ *           version number. A higher minor version is backwards-compatible
+ *           with a lower minor version for the same major version.
+ * @features: Bit mask of features (e.g. whether certain types of job can
+ *            be suspended). Reserved bits should be 0, and should be ignored.
+ * @input: Address of global interface input page.
+ * @output: Address of global interface output page.
+ * @group_num: Number of command stream groups supported.
+ * @group_stride: Stride in bytes in JASID0 virtual address between
+ *                command stream group capability structures.
+ * @prfcnt_size: Performance counters size.
+ * @groups: Address of an array of command stream group capability structures.
+ */
+struct kbase_csf_global_iface {
+	struct kbase_device *kbdev;
+	u32 version;
+	u32 features;
+	void *input;
+	void *output;
+	u32 group_num;
+	u32 group_stride;
+	u32 prfcnt_size;
+	struct kbase_csf_cmd_stream_group_info *groups;
+};
+
+/**
+ * kbase_csf_firmware_global_input() - Set a word in the global input page
+ *
+ * @iface: Command stream front-end interface provided by the firmware.
+ * @offset: Offset of the word to be written, in bytes.
+ * @value: Value to be written.
+ */
+void kbase_csf_firmware_global_input(
+	const struct kbase_csf_global_iface *iface, u32 offset, u32 value);
+
+/**
+ * kbase_csf_firmware_global_input_mask() - Set part of a word in the global
+ *                                          input page
+ *
+ * @iface: Command stream front-end interface provided by the firmware.
+ * @offset: Offset of the word to be modified, in bytes.
+ * @value: Value to be written.
+ * @mask: Bitmask with the bits to be modified set.
+ */
+void kbase_csf_firmware_global_input_mask(
+	const struct kbase_csf_global_iface *iface, u32 offset,
+	u32 value, u32 mask);
+
+/**
+ * kbase_csf_firmware_global_input_read() - Read a word in a global input page
+ *
+ * Return: Value of the word read from the global input page.
+ *
+ * @info: Command stream group interface provided by the firmware.
+ * @offset: Offset of the word to be read, in bytes.
+ */
+u32 kbase_csf_firmware_global_input_read(
+	const struct kbase_csf_global_iface *info, u32 offset);
+
+/**
+ * kbase_csf_firmware_global_output() - Read a word in the global output page
+ *
+ * Return: Value of the word read from the global output page.
+ *
+ * @iface: Command stream front-end interface provided by the firmware.
+ * @offset: Offset of the word to be read, in bytes.
+ */
+u32 kbase_csf_firmware_global_output(
+	const struct kbase_csf_global_iface *iface, u32 offset);
+
+/* Calculate the offset to the Hw doorbell page corresponding to the
+ * doorbell number.
+ */
+static u32 csf_doorbell_offset(int doorbell_nr)
+{
+	WARN_ON(doorbell_nr >= CSF_NUM_DOORBELL);
+
+	return CSF_HW_DOORBELL_PAGE_OFFSET +
+		(doorbell_nr * CSF_HW_DOORBELL_PAGE_SIZE);
+}
+
+static inline void kbase_csf_ring_doorbell(struct kbase_device *kbdev,
+					   int doorbell_nr)
+{
+	WARN_ON(doorbell_nr >= CSF_NUM_DOORBELL);
+
+	kbase_reg_write(kbdev, csf_doorbell_offset(doorbell_nr), (u32)1);
+}
+
+/**
+ * kbase_csf_read_firmware_memory - Read a value in a GPU address
+ *
+ * This function read a value in a GPU address that belongs to
+ * a private firmware memory region. The function assumes that the location
+ * is not permanently mapped on the CPU address space, therefore it maps it
+ * and then unmaps it to access it independently.
+ *
+ * @kbdev:     Device pointer
+ * @gpu_addr:  GPU address to read
+ * @value:     output pointer to which the read value will be written.
+ */
+void kbase_csf_read_firmware_memory(struct kbase_device *kbdev,
+	u32 gpu_addr, u32 *value);
+
+/**
+ * kbase_csf_update_firmware_memory - Write a value in a GPU address
+ *
+ * This function writes a given value in a GPU address that belongs to
+ * a private firmware memory region. The function assumes that the destination
+ * is not permanently mapped on the CPU address space, therefore it maps it
+ * and then unmaps it to access it independently.
+ *
+ * @kbdev:     Device pointer
+ * @gpu_addr:  GPU address to write
+ * @value:     Value to write
+ */
+void kbase_csf_update_firmware_memory(struct kbase_device *kbdev,
+	u32 gpu_addr, u32 value);
+
+/**
+ * kbase_csf_firmware_init() - Load the firmware for the CSF MCU
+ *
+ * Request the firmware from user space and load it into memory.
+ *
+ * Return: 0 if successful, negative error code on failure
+ *
+ * @kbdev: Kbase device
+ */
+int kbase_csf_firmware_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_term() - Unload the firmware
+ *
+ * Frees the memory allocated by kbase_csf_firmware_init()
+ *
+ * @kbdev: Kbase device
+ */
+void kbase_csf_firmware_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_ping - Send the ping request to firmware.
+ *
+ * The function sends the ping request to firmware to confirm it is alive.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_firmware_ping(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_set_timeout - Set a hardware endpoint progress timeout.
+ *
+ * @kbdev:   Instance of a GPU platform device that implements a command
+ *           stream front-end interface.
+ * @timeout: The maximum number of GPU cycles that is allowed to elapse
+ *           without forward progress before the driver terminates a GPU
+ *           command queue group.
+ *
+ * Configures the progress timeout value used by the firmware to decide
+ * when to report that a task is not making progress on an endpoint.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_firmware_set_timeout(struct kbase_device *kbdev, u64 timeout);
+
+/**
+ * kbase_csf_enter_protected_mode - Send the Global request to firmware to
+ *                                  enter protected mode and wait for its
+ *                                  completion.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_enter_protected_mode(struct kbase_device *kbdev);
+
+static inline bool kbase_csf_firmware_mcu_halted(struct kbase_device *kbdev)
+{
+#ifndef CONFIG_MALI_BIFROST_NO_MALI
+	return (kbase_reg_read(kbdev, GPU_CONTROL_REG(MCU_STATUS)) ==
+		MCU_STATUS_HALTED);
+#else
+	return true;
+#endif
+}
+
+/**
+ * kbase_csf_firmware_trigger_mcu_halt - Send the Global request to firmware to
+ *                                       halt its operation and bring itself
+ *                                       into a known internal state for warm
+ *                                       boot later.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_firmware_trigger_mcu_halt(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_enable_mcu - Send the command to enable MCU
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+static inline void kbase_csf_firmware_enable_mcu(struct kbase_device *kbdev)
+{
+	/* Trigger the boot of MCU firmware, Use the AUTO mode as
+	 * otherwise on fast reset, to exit protected mode, MCU will
+	 * not reboot by itself to enter normal mode.
+	 */
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(MCU_CONTROL), MCU_CNTRL_AUTO);
+}
+
+/**
+ * kbase_csf_firmware_disable_mcu - Send the command to disable MCU
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+static inline void kbase_csf_firmware_disable_mcu(struct kbase_device *kbdev)
+{
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(MCU_CONTROL), MCU_CNTRL_DISABLE);
+}
+
+/**
+ * kbase_csf_firmware_disable_mcu_wait - Wait for the MCU to reach disabled
+ *                                       status.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_firmware_disable_mcu_wait(struct kbase_device *kbdev);
+
+/**
+ * kbase_trigger_firmware_reload - Trigger the reboot of MCU firmware, for the
+ *                                 cold boot case firmware image would be
+ *                                 reloaded from filesystem into memory.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_firmware_trigger_reload(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_reload_completed - The reboot of MCU firmware has
+ *                                       completed.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_firmware_reload_completed(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_global_reinit - Send the Global configuration requests
+ *                                    after the reboot of MCU firmware.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_firmware_global_reinit(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_global_reinit_complete - Check the Global configuration
+ *                      requests, sent after the reboot of MCU firmware, have
+ *                      completed or not.
+ *
+ * Return: true if the Global configuration requests completed otherwise false.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+bool kbase_csf_firmware_global_reinit_complete(struct kbase_device *kbdev);
+
+/**
+ * Request the global control block of CSF interface capabilities
+ *
+ * Return: Total number of command streams, summed across all groups.
+ *
+ * @kbdev:                 Kbase device.
+ * @group_data:            Pointer where to store all the group data
+ *                         (sequentially).
+ * @max_group_num:         The maximum number of groups to be read.
+ *                         Can be 0, in which case group_data is unused.
+ * @stream_data:           Pointer where to store all the stream data
+ *                         (sequentially).
+ * @max_total_stream_num:  The maximum number of streams to be read.
+ *                         Can be 0, in which case stream_data is unused.
+ * @glb_version:           Where to store the global interface version.
+ *                         Bits 31:16 hold the major version number and
+ *                         15:0 hold the minor version number.
+ *                         A higher minor version is backwards-compatible
+ *                         with a lower minor version for the same major
+ *                         version.
+ * @features:              Where to store a bit mask of features (e.g.
+ *                         whether certain types of job can be suspended).
+ * @group_num:             Where to store the number of command stream groups
+ *                         supported.
+ * @prfcnt_size:           Where to store the size of CSF performance counters,
+ *                         in bytes. Bits 31:16 hold the size of firmware
+ *                         performance counter data and 15:0 hold the size of
+ *                         hardware performance counter data.
+ */
+u32 kbase_csf_firmware_get_glb_iface(struct kbase_device *kbdev,
+	struct basep_cs_group_control *group_data, u32 max_group_num,
+	struct basep_cs_stream_control *stream_data, u32 max_total_stream_num,
+	u32 *glb_version, u32 *features, u32 *group_num, u32 *prfcnt_size);
+
+
+/**
+ * Get CSF firmware header timeline metadata content
+ *
+ * Return: The firmware timeline metadata content which match @p name.
+ *
+ * @kbdev:        Kbase device.
+ * @name:         Name of the metadata which metadata content to be returned.
+ * @size:         Metadata size if specified metadata found.
+ */
+const char *kbase_csf_firmware_get_timeline_metadata(struct kbase_device *kbdev,
+	const char *name, size_t *size);
+
+/**
+ * kbase_csf_firmware_mcu_shared_mapping_init -
+ * Allocate and map MCU shared memory.
+ *
+ * This helper function allocates memory and maps it on both the CPU
+ * and the GPU address spaces. Most of the properties of the mapping
+ * are implicit and will be automatically determined by the function,
+ * e.g. whether memory is cacheable.
+ *
+ * The client is only expected to specify whether the mapping is readable
+ * or writable in the CPU and the GPU address spaces; any other flag
+ * will be ignored by the function.
+ *
+ * Return: 0 if success, or an error code on failure.
+ *
+ * @kbdev:              Kbase device the memory mapping shall belong to.
+ * @num_pages:          Number of memory pages to map.
+ * @cpu_map_properties: Either PROT_READ or PROT_WRITE.
+ * @gpu_map_properties: Either KBASE_REG_GPU_RD or KBASE_REG_GPU_WR.
+ * @csf_mapping:        Object where to write metadata for the memory mapping.
+ */
+int kbase_csf_firmware_mcu_shared_mapping_init(
+		struct kbase_device *kbdev,
+		unsigned int num_pages,
+		unsigned long cpu_map_properties,
+		unsigned long gpu_map_properties,
+		struct kbase_csf_mapping *csf_mapping);
+
+/**
+ * kbase_csf_firmware_mcu_shared_mapping_term - Unmap and free MCU shared memory.
+ *
+ * @kbdev:       Device pointer.
+ * @csf_mapping: Metadata of the memory mapping to terminate.
+ */
+void kbase_csf_firmware_mcu_shared_mapping_term(
+		struct kbase_device *kbdev, struct kbase_csf_mapping *csf_mapping);
+
+#ifndef MALI_KBASE_BUILD
+/**
+ * mali_kutf_process_fw_utf_entry() - Process the "Firmware UTF tests" section
+ *
+ * Read "Firmware UTF tests" section from the firmware image and create
+ * necessary kutf app+suite+tests.
+ *
+ * Return: 0 if successful, negative error code on failure. In both cases
+ * caller will have to invoke mali_kutf_fw_utf_entry_cleanup for the cleanup
+ *
+ * @kbdev: Kbase device structure
+ * @fw_data: Pointer to the start of firmware binary image loaded from disk
+ * @fw_size: Size (in bytes) of the firmware image
+ * @entry: Pointer to the start of the section
+ */
+int mali_kutf_process_fw_utf_entry(struct kbase_device *kbdev,
+	const void *fw_data, size_t fw_size, const u32 *entry);
+
+/**
+ * mali_kutf_fw_utf_entry_cleanup() - Remove the Fw UTF tests debugfs entries
+ *
+ * Destroy the kutf apps+suites+tests created on parsing "Firmware UTF tests"
+ * section from the firmware image.
+ *
+ * @kbdev: Kbase device structure
+ */
+void mali_kutf_fw_utf_entry_cleanup(struct kbase_device *kbdev);
+#endif
+
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+extern bool fw_debug;
+#endif
+
+static inline long kbase_csf_timeout_in_jiffies(const unsigned int msecs)
+{
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	return (fw_debug ? MAX_SCHEDULE_TIMEOUT : msecs_to_jiffies(msecs));
+#else
+	return msecs_to_jiffies(msecs);
+#endif
+}
+
+#endif
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c
new file mode 100644
index 000000000000..d282d5ca7fc2
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.c
@@ -0,0 +1,306 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include "mali_kbase_csf_firmware_cfg.h"
+#include <mali_kbase_reset_gpu.h>
+
+#if CONFIG_SYSFS
+#define CSF_FIRMWARE_CFG_SYSFS_DIR_NAME "firmware_config"
+
+/**
+ * struct firmware_config - Configuration item within the MCU firmware
+ *
+ * The firmware may expose configuration options. Each option has a name, the
+ * address where the option is controlled and the minimum and maximum values
+ * that the option can take.
+ *
+ * @node:        List head linking all options to
+ *               kbase_device:csf.firmware_config
+ * @kbdev:       Pointer to the Kbase device
+ * @kobj:        Kobject corresponding to the sysfs sub-directory,
+ *               inside CSF_FIRMWARE_CFG_SYSFS_DIR_NAME directory,
+ *               representing the configuration option @name.
+ * @kobj_inited: kobject initialization state
+ * @name:        NUL-terminated string naming the option
+ * @address:     The address in the firmware image of the configuration option
+ * @min:         The lowest legal value of the configuration option
+ * @max:         The maximum legal value of the configuration option
+ * @cur_val:     The current value of the configuration option
+ */
+struct firmware_config {
+	struct list_head node;
+	struct kbase_device *kbdev;
+	struct kobject kobj;
+	bool kobj_inited;
+	char *name;
+	u32 address;
+	u32 min;
+	u32 max;
+	u32 cur_val;
+};
+
+#define FW_CFG_ATTR(_name, _mode)					\
+	struct attribute fw_cfg_attr_##_name = {			\
+			.name = __stringify(_name),			\
+			.mode = VERIFY_OCTAL_PERMISSIONS(_mode),	\
+	}
+
+static FW_CFG_ATTR(min, S_IRUGO);
+static FW_CFG_ATTR(max, S_IRUGO);
+static FW_CFG_ATTR(cur, S_IRUGO | S_IWUSR);
+
+static void fw_cfg_kobj_release(struct kobject *kobj)
+{
+	struct firmware_config *config =
+		container_of(kobj, struct firmware_config, kobj);
+
+	kfree(config);
+}
+
+static ssize_t show_fw_cfg(struct kobject *kobj,
+	struct attribute *attr, char *buf)
+{
+	struct firmware_config *config =
+		container_of(kobj, struct firmware_config, kobj);
+	struct kbase_device *kbdev = config->kbdev;
+	u32 val = 0;
+
+	if (!kbdev)
+		return -ENODEV;
+
+	if (attr == &fw_cfg_attr_max)
+		val = config->max;
+	else if (attr == &fw_cfg_attr_min)
+		val = config->min;
+	else if (attr == &fw_cfg_attr_cur) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		val = config->cur_val;
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	} else {
+		dev_warn(kbdev->dev,
+			"Unexpected read from entry %s/%s",
+			config->name, attr->name);
+		return -EINVAL;
+	}
+
+	return snprintf(buf, PAGE_SIZE, "%u\n", val);
+}
+
+static ssize_t store_fw_cfg(struct kobject *kobj,
+	struct attribute *attr,
+	const char *buf,
+	size_t count)
+{
+	struct firmware_config *config =
+		container_of(kobj, struct firmware_config, kobj);
+	struct kbase_device *kbdev = config->kbdev;
+
+	if (!kbdev)
+		return -ENODEV;
+
+	if (attr == &fw_cfg_attr_cur) {
+		unsigned long flags;
+		u32 val;
+		int ret = kstrtouint(buf, 0, &val);
+
+		if (ret) {
+			dev_err(kbdev->dev,
+				"Couldn't process %s/%s write operation.\n"
+				"Use format <value>\n",
+				config->name, attr->name);
+			return -EINVAL;
+		}
+
+		if ((val < config->min) || (val > config->max))
+			return -EINVAL;
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		if (config->cur_val == val) {
+			spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+			return count;
+		}
+
+		/*
+		 * If there is already a GPU reset pending then inform
+		 * the User to retry the write.
+		 */
+		if (kbase_reset_gpu_silent(kbdev)) {
+			spin_unlock_irqrestore(
+				&kbdev->hwaccess_lock, flags);
+			return -EAGAIN;
+		}
+
+		/*
+		 * GPU reset request has been placed, now update the
+		 * firmware image. GPU reset will take place only after
+		 * hwaccess_lock is released.
+		 * Update made to firmware image in memory would not
+		 * be lost on GPU reset as configuration entries reside
+		 * in the RONLY section of firmware image, which is not
+		 * reloaded on firmware reboot due to GPU reset.
+		 */
+		kbase_csf_update_firmware_memory(
+			kbdev, config->address, val);
+
+		config->cur_val = val;
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+		/* Wait for the config update to take effect */
+		kbase_reset_gpu_wait(kbdev);
+	} else {
+		dev_warn(kbdev->dev,
+			"Unexpected write to entry %s/%s",
+			config->name, attr->name);
+		return -EINVAL;
+	}
+
+	return count;
+}
+
+static const struct sysfs_ops fw_cfg_ops = {
+	.show = &show_fw_cfg,
+	.store = &store_fw_cfg,
+};
+
+static struct attribute *fw_cfg_attrs[] = {
+	&fw_cfg_attr_min,
+	&fw_cfg_attr_max,
+	&fw_cfg_attr_cur,
+	NULL,
+};
+
+static struct kobj_type fw_cfg_kobj_type = {
+	.release = &fw_cfg_kobj_release,
+	.sysfs_ops = &fw_cfg_ops,
+	.default_attrs = fw_cfg_attrs,
+};
+
+int kbase_csf_firmware_cfg_init(struct kbase_device *kbdev)
+{
+	struct firmware_config *config;
+
+	kbdev->csf.fw_cfg_kobj = kobject_create_and_add(
+		CSF_FIRMWARE_CFG_SYSFS_DIR_NAME, &kbdev->dev->kobj);
+	if (!kbdev->csf.fw_cfg_kobj) {
+		kobject_put(kbdev->csf.fw_cfg_kobj);
+		dev_err(kbdev->dev,
+			"Creation of %s sysfs sub-directory failed\n",
+			CSF_FIRMWARE_CFG_SYSFS_DIR_NAME);
+		return -ENOMEM;
+	}
+
+	list_for_each_entry(config, &kbdev->csf.firmware_config, node) {
+		int err;
+
+		kbase_csf_read_firmware_memory(kbdev, config->address,
+			&config->cur_val);
+
+		err = kobject_init_and_add(&config->kobj, &fw_cfg_kobj_type,
+				kbdev->csf.fw_cfg_kobj, "%s", config->name);
+		if (err) {
+			kobject_put(&config->kobj);
+			dev_err(kbdev->dev,
+				"Creation of %s sysfs sub-directory failed\n",
+				config->name);
+			return err;
+		}
+
+		config->kobj_inited = true;
+	}
+
+	return 0;
+}
+
+void kbase_csf_firmware_cfg_term(struct kbase_device *kbdev)
+{
+	while (!list_empty(&kbdev->csf.firmware_config)) {
+		struct firmware_config *config;
+
+		config = list_first_entry(&kbdev->csf.firmware_config,
+				struct firmware_config, node);
+		list_del(&config->node);
+
+		if (config->kobj_inited) {
+			kobject_del(&config->kobj);
+			kobject_put(&config->kobj);
+		} else
+			kfree(config);
+	}
+
+	kobject_del(kbdev->csf.fw_cfg_kobj);
+	kobject_put(kbdev->csf.fw_cfg_kobj);
+}
+
+int kbase_csf_firmware_cfg_option_entry_parse(struct kbase_device *kbdev,
+		const struct firmware *fw,
+		const u32 *entry, unsigned int size)
+{
+	const char *name = (char *)&entry[3];
+	struct firmware_config *config;
+	const unsigned int name_len = size - CONFIGURATION_ENTRY_NAME_OFFSET;
+
+	/* Allocate enough space for struct firmware_config and the
+	 * configuration option name (with NULL termination)
+	 */
+	config = kzalloc(sizeof(*config) + name_len + 1, GFP_KERNEL);
+
+	if (!config)
+		return -ENOMEM;
+
+	config->kbdev = kbdev;
+	config->name = (char *)(config+1);
+	config->address = entry[0];
+	config->min = entry[1];
+	config->max = entry[2];
+
+	memcpy(config->name, name, name_len);
+	config->name[name_len] = 0;
+
+	list_add(&config->node, &kbdev->csf.firmware_config);
+
+	dev_dbg(kbdev->dev, "Configuration option '%s' at 0x%x range %u-%u",
+			config->name, config->address,
+			config->min, config->max);
+
+	return 0;
+}
+#else
+int kbase_csf_firmware_cfg_init(struct kbase_device *kbdev)
+{
+	return 0;
+}
+
+void kbase_csf_firmware_cfg_term(struct kbase_device *kbdev)
+{
+	/* !CONFIG_SYSFS: Nothing to do here */
+}
+
+int kbase_csf_firmware_cfg_option_entry_parse(struct kbase_device *kbdev,
+		const struct firmware *fw,
+		const u32 *entry, unsigned int size)
+{
+	return 0;
+}
+#endif /* CONFIG_SYSFS */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.h
new file mode 100644
index 000000000000..ab4b6ebc5296
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_cfg.h
@@ -0,0 +1,72 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_FIRMWARE_CFG_H_
+#define _KBASE_CSF_FIRMWARE_CFG_H_
+
+#include <mali_kbase.h>
+#include "mali_kbase_csf_firmware.h"
+#include <linux/firmware.h>
+
+#define CONFIGURATION_ENTRY_NAME_OFFSET (0xC)
+
+/**
+ * kbase_csf_firmware_cfg_init - Create the sysfs directory for configuration
+ *                               options present in firmware image.
+ *
+ * This function would create a sysfs directory and populate it with a
+ * sub-directory, that would contain a file per attribute, for every
+ * configuration option parsed from firmware image.
+ *
+ * @kbdev: Pointer to the Kbase device
+ *
+ * Return: The initialization error code.
+ */
+int kbase_csf_firmware_cfg_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_cfg_term - Delete the sysfs directory that was created
+ *                               for firmware configuration options.
+ *
+ * @kbdev: Pointer to the Kbase device
+ *
+ */
+void kbase_csf_firmware_cfg_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_cfg_option_entry_parse() - Process a
+ *                                               "configuration option" section.
+ *
+ * Read a "configuration option" section adding it to the
+ * kbase_device:csf.firmware_config list.
+ *
+ * Return: 0 if successful, negative error code on failure
+ *
+ * @kbdev: Kbase device structure
+ * @fw:    Firmware image containing the section
+ * @entry: Pointer to the section
+ * @size:  Size (in bytes) of the section
+ */
+int kbase_csf_firmware_cfg_option_entry_parse(struct kbase_device *kbdev,
+		const struct firmware *fw,
+		const u32 *entry, unsigned int size);
+#endif /* _KBASE_CSF_FIRMWARE_CFG_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c
new file mode 100644
index 000000000000..7401113c5d6a
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_firmware_no_mali.c
@@ -0,0 +1,1012 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase.h"
+#include "mali_kbase_csf_firmware.h"
+#include "mali_kbase_csf_trace_buffer.h"
+#include "mali_kbase_csf_timeout.h"
+#include "mali_kbase_mem.h"
+#include "mali_kbase_reset_gpu.h"
+#include "device/mali_kbase_device.h"
+#include "backend/gpu/mali_kbase_pm_internal.h"
+#include "mali_kbase_csf_scheduler.h"
+#include "mmu/mali_kbase_mmu.h"
+
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/firmware.h>
+#include <linux/mman.h>
+#include <linux/string.h>
+#if (KERNEL_VERSION(4, 13, 0) <= LINUX_VERSION_CODE)
+#include <linux/set_memory.h>
+#endif
+
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+/* Makes Driver wait indefinitely for an acknowledgment for the different
+ * requests it sends to firmware. Otherwise the timeouts interfere with the
+ * use of debugger for source-level debugging of firmware as Driver initiates
+ * a GPU reset when a request times out, which always happen when a debugger
+ * is connected.
+ */
+bool fw_debug; /* Default value of 0/false */
+module_param(fw_debug, bool, 0444);
+MODULE_PARM_DESC(fw_debug,
+	"Enables effective use of a debugger for debugging firmware code.");
+#endif
+
+#define DUMMY_FW_PAGE_SIZE SZ_4K
+
+/**
+ * struct dummy_firmware_csi - Represents a dummy interface for MCU firmware streams
+ *
+ * @cs_kernel_input:  CS kernel input memory region
+ * @cs_kernel_output: CS kernel output memory region
+ */
+struct dummy_firmware_csi {
+	u8 cs_kernel_input[DUMMY_FW_PAGE_SIZE];
+	u8 cs_kernel_output[DUMMY_FW_PAGE_SIZE];
+};
+
+/**
+ * struct dummy_firmware_csg - Represents a dummy interface for MCU firmware stream groups
+ *
+ * @csg_input:  CSG kernel input memory region
+ * @csg_output: CSG kernel output memory region
+ * @csi:               Dummy firmware CSIs
+ */
+struct dummy_firmware_csg {
+	u8 csg_input[DUMMY_FW_PAGE_SIZE];
+	u8 csg_output[DUMMY_FW_PAGE_SIZE];
+	struct dummy_firmware_csi csi[8];
+} dummy_firmware_csg;
+
+/**
+ * struct dummy_firmware_interface - Represents a dummy interface in the MCU firmware
+ *
+ * @global_input:  Global input memory region
+ * @global_output: Global output memory region
+ * @csg:   Dummy firmware CSGs
+ * @node:  Interface objects are on the kbase_device:csf.firmware_interfaces
+ *         list using this list_head to link them
+ */
+struct dummy_firmware_interface {
+	u8 global_input[DUMMY_FW_PAGE_SIZE];
+	u8 global_output[DUMMY_FW_PAGE_SIZE];
+	struct dummy_firmware_csg csg[8];
+	struct list_head node;
+} dummy_firmware_interface;
+
+#define CSF_GLB_REQ_CFG_MASK \
+	(GLB_REQ_CFG_ALLOC_EN_MASK | GLB_REQ_CFG_PROGRESS_TIMER_MASK)
+
+static inline u32 input_page_read(const u32 *const input, const u32 offset)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	return input[offset / sizeof(u32)];
+}
+
+static inline void input_page_write(u32 *const input, const u32 offset,
+			const u32 value)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	input[offset / sizeof(u32)] = value;
+}
+
+static inline void input_page_partial_write(u32 *const input, const u32 offset,
+			u32 value, u32 mask)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	input[offset / sizeof(u32)] =
+		(input_page_read(input, offset) & ~mask) | (value & mask);
+}
+
+static inline u32 output_page_read(const u32 *const output, const u32 offset)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	return output[offset / sizeof(u32)];
+}
+
+static inline void output_page_write(u32 *const output, const u32 offset,
+			const u32 value)
+{
+	WARN_ON(offset % sizeof(u32));
+
+	output[offset / sizeof(u32)] = value;
+}
+
+/**
+ * invent_memory_setup_entry() - Invent an "interface memory setup" section
+ *
+ * Invent an "interface memory setup" section similar to one from a firmware
+ * image. If successful the interface will be added to the
+ * kbase_device:csf.firmware_interfaces list.
+ *
+ * Return: 0 if successful, negative error code on failure
+ *
+ * @kbdev: Kbase device structure
+ */
+static int invent_memory_setup_entry(struct kbase_device *kbdev)
+{
+	struct dummy_firmware_interface *interface = NULL;
+
+	/* Allocate enough memory for the struct dummy_firmware_interface.
+	 */
+	interface = kmalloc(sizeof(*interface), GFP_KERNEL);
+	if (!interface)
+		return -ENOMEM;
+
+	kbdev->csf.shared_interface = interface;
+	list_add(&interface->node, &kbdev->csf.firmware_interfaces);
+
+	/* NO_MALI: Don't insert any firmware pages */
+	return 0;
+}
+
+static void free_global_iface(struct kbase_device *kbdev)
+{
+	struct kbase_csf_global_iface *iface = &kbdev->csf.global_iface;
+
+	if (iface->groups) {
+		unsigned int gid;
+
+		for (gid = 0; gid < iface->group_num; ++gid)
+			kfree(iface->groups[gid].streams);
+
+		kfree(iface->groups);
+		iface->groups = NULL;
+	}
+}
+
+static int invent_cmd_stream_group_info(struct kbase_device *kbdev,
+		struct kbase_csf_cmd_stream_group_info *ginfo,
+		struct dummy_firmware_csg *csg)
+{
+	unsigned int sid;
+
+	ginfo->input = csg->csg_input;
+	ginfo->output = csg->csg_output;
+
+	ginfo->kbdev = kbdev;
+	ginfo->features = 0;
+	ginfo->suspend_size = 64;
+	ginfo->protm_suspend_size = 64;
+	ginfo->stream_num = ARRAY_SIZE(csg->csi);
+	ginfo->stream_stride = 0;
+
+	ginfo->streams = kcalloc(ginfo->stream_num, sizeof(*ginfo->streams), GFP_KERNEL);
+	if (ginfo->streams == NULL) {
+		return -ENOMEM;
+	}
+
+	for (sid = 0; sid < ginfo->stream_num; ++sid) {
+		struct kbase_csf_cmd_stream_info *stream = &ginfo->streams[sid];
+		struct dummy_firmware_csi *csi = &csg->csi[sid];
+
+		stream->input = csi->cs_kernel_input;
+		stream->output = csi->cs_kernel_output;
+
+		stream->kbdev = kbdev;
+		stream->features =
+			STREAM_FEATURES_WORK_REGISTERS_SET(0, 80) |
+			STREAM_FEATURES_SCOREBOARDS_SET(0, 8) |
+			STREAM_FEATURES_COMPUTE_SET(0, 1) |
+			STREAM_FEATURES_FRAGMENT_SET(0, 1) |
+			STREAM_FEATURES_TILER_SET(0, 1);
+	}
+
+	return 0;
+}
+
+static int invent_capabilities(struct kbase_device *kbdev)
+{
+	struct dummy_firmware_interface *interface = kbdev->csf.shared_interface;
+	struct kbase_csf_global_iface *iface = &kbdev->csf.global_iface;
+	unsigned int gid;
+
+	iface->input = interface->global_input;
+	iface->output = interface->global_output;
+
+	iface->version = 1;
+	iface->kbdev = kbdev;
+	iface->features = 0;
+	iface->prfcnt_size = 64;
+	iface->group_num = ARRAY_SIZE(interface->csg);
+	iface->group_stride = 0;
+
+	iface->groups = kcalloc(iface->group_num, sizeof(*iface->groups), GFP_KERNEL);
+	if (iface->groups == NULL) {
+		return -ENOMEM;
+	}
+
+	for (gid = 0; gid < iface->group_num; ++gid) {
+		int err;
+
+		err = invent_cmd_stream_group_info(kbdev, &iface->groups[gid],
+			&interface->csg[gid]);
+		if (err < 0) {
+			free_global_iface(kbdev);
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+void kbase_csf_read_firmware_memory(struct kbase_device *kbdev,
+	u32 gpu_addr, u32 *value)
+{
+	/* NO_MALI: Nothing to do here */
+}
+
+
+void kbase_csf_update_firmware_memory(struct kbase_device *kbdev,
+	u32 gpu_addr, u32 value)
+{
+	/* NO_MALI: Nothing to do here */
+}
+
+void kbase_csf_firmware_cs_input(
+	const struct kbase_csf_cmd_stream_info *const info, const u32 offset,
+	const u32 value)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x\n", offset, value);
+	input_page_write(info->input, offset, value);
+
+	if (offset == CS_REQ) {
+		/* NO_MALI: Immediately acknowledge requests */
+		output_page_write(info->output, CS_ACK, value);
+	}
+}
+
+u32 kbase_csf_firmware_cs_input_read(
+	const struct kbase_csf_cmd_stream_info *const info,
+	const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = input_page_read(info->input, offset);
+
+	dev_dbg(kbdev->dev, "cs input r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_cs_input_mask(
+	const struct kbase_csf_cmd_stream_info *const info, const u32 offset,
+	const u32 value, const u32 mask)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "cs input w: reg %08x val %08x mask %08x\n",
+			offset, value, mask);
+
+	/* NO_MALI: Go through kbase_csf_firmware_cs_input to capture writes */
+	kbase_csf_firmware_cs_input(info, offset, (input_page_read(info->input, offset) & ~mask) | (value & mask));
+}
+
+u32 kbase_csf_firmware_cs_output(
+	const struct kbase_csf_cmd_stream_info *const info, const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = output_page_read(info->output, offset);
+
+	dev_dbg(kbdev->dev, "cs output r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_csg_input(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset, const u32 value)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x\n",
+			offset, value);
+	input_page_write(info->input, offset, value);
+
+	if (offset == CSG_REQ) {
+		/* NO_MALI: Immediately acknowledge requests */
+		output_page_write(info->output, CSG_ACK, value);
+	}
+}
+
+u32 kbase_csf_firmware_csg_input_read(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = input_page_read(info->input, offset);
+
+	dev_dbg(kbdev->dev, "csg input r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_csg_input_mask(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset, const u32 value, const u32 mask)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+
+	dev_dbg(kbdev->dev, "csg input w: reg %08x val %08x mask %08x\n",
+			offset, value, mask);
+
+	/* NO_MALI: Go through kbase_csf_firmware_csg_input to capture writes */
+	kbase_csf_firmware_csg_input(info, offset, (input_page_read(info->input, offset) & ~mask) | (value & mask));
+}
+
+u32 kbase_csf_firmware_csg_output(
+	const struct kbase_csf_cmd_stream_group_info *const info,
+	const u32 offset)
+{
+	const struct kbase_device * const kbdev = info->kbdev;
+	u32 const val = output_page_read(info->output, offset);
+
+	dev_dbg(kbdev->dev, "csg output r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+void kbase_csf_firmware_global_input(
+	const struct kbase_csf_global_iface *const iface, const u32 offset,
+	const u32 value)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+
+	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x\n", offset, value);
+	input_page_write(iface->input, offset, value);
+
+	if (offset == GLB_REQ) {
+		/* NO_MALI: Immediately acknowledge requests */
+		output_page_write(iface->output, GLB_ACK, value);
+	}
+}
+
+void kbase_csf_firmware_global_input_mask(
+	const struct kbase_csf_global_iface *const iface, const u32 offset,
+	const u32 value, const u32 mask)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+
+	dev_dbg(kbdev->dev, "glob input w: reg %08x val %08x mask %08x\n",
+			offset, value, mask);
+
+	/* NO_MALI: Go through kbase_csf_firmware_global_input to capture writes */
+	kbase_csf_firmware_global_input(iface, offset, (input_page_read(iface->input, offset) & ~mask) | (value & mask));
+}
+
+u32 kbase_csf_firmware_global_input_read(
+	const struct kbase_csf_global_iface *const iface, const u32 offset)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+	u32 const val = input_page_read(iface->input, offset);
+
+	dev_dbg(kbdev->dev, "glob input r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+u32 kbase_csf_firmware_global_output(
+	const struct kbase_csf_global_iface *const iface, const u32 offset)
+{
+	const struct kbase_device * const kbdev = iface->kbdev;
+	u32 const val = output_page_read(iface->output, offset);
+
+	dev_dbg(kbdev->dev, "glob output r: reg %08x val %08x\n", offset, val);
+	return val;
+}
+
+static bool global_request_complete(struct kbase_device *const kbdev,
+				    u32 const req_mask)
+{
+	struct kbase_csf_global_iface *global_iface =
+				&kbdev->csf.global_iface;
+	bool complete = false;
+	unsigned long flags;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+
+	if ((kbase_csf_firmware_global_output(global_iface, GLB_ACK) &
+	     req_mask) ==
+	    (kbase_csf_firmware_global_input_read(global_iface, GLB_REQ) &
+	     req_mask))
+		complete = true;
+
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	return complete;
+}
+
+static int wait_for_global_request(struct kbase_device *const kbdev,
+				   u32 const req_mask)
+{
+	const long wait_timeout =
+		kbase_csf_timeout_in_jiffies(GLB_REQ_WAIT_TIMEOUT_MS);
+	long remaining;
+	int err = 0;
+
+	remaining = wait_event_timeout(kbdev->csf.event_wait,
+				       global_request_complete(kbdev, req_mask),
+				       wait_timeout);
+
+	if (!remaining) {
+		dev_warn(kbdev->dev, "Timed out waiting for global request %x to complete",
+			 req_mask);
+		err = -ETIMEDOUT;
+	}
+
+	return err;
+}
+
+static void set_global_request(
+	const struct kbase_csf_global_iface *const global_iface,
+	u32 const req_mask)
+{
+	u32 glb_req;
+
+	lockdep_assert_held(&global_iface->kbdev->csf.reg_lock);
+
+	glb_req = kbase_csf_firmware_global_output(global_iface, GLB_ACK);
+	glb_req ^= req_mask;
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, glb_req,
+					     req_mask);
+}
+
+static void enable_endpoints_global(
+	const struct kbase_csf_global_iface *const global_iface,
+	u64 const shader_core_mask)
+{
+	kbase_csf_firmware_global_input(global_iface, GLB_ALLOC_EN_LO,
+		shader_core_mask & U32_MAX);
+	kbase_csf_firmware_global_input(global_iface, GLB_ALLOC_EN_HI,
+		shader_core_mask >> 32);
+
+	set_global_request(global_iface, GLB_REQ_CFG_ALLOC_EN_MASK);
+}
+
+static void set_timeout_global(
+	const struct kbase_csf_global_iface *const global_iface,
+	u64 const timeout)
+{
+	kbase_csf_firmware_global_input(global_iface, GLB_PROGRESS_TIMER,
+		timeout / GLB_PROGRESS_TIMER_TIMEOUT_SCALE);
+
+	set_global_request(global_iface, GLB_REQ_CFG_PROGRESS_TIMER_MASK);
+}
+
+static void global_init(struct kbase_device *const kbdev, u32 req_mask)
+{
+	u32 const ack_irq_mask = GLB_ACK_IRQ_MASK_CFG_ALLOC_EN_MASK  |
+			GLB_ACK_IRQ_MASK_PING_MASK |
+			GLB_ACK_IRQ_MASK_CFG_PROGRESS_TIMER_MASK |
+			GLB_ACK_IRQ_MASK_PROTM_ENTER_MASK |
+			GLB_ACK_IRQ_MASK_PROTM_EXIT_MASK;
+
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	unsigned long flags;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+
+	/* Enable endpoints on all present shader cores */
+	enable_endpoints_global(global_iface,
+		kbase_pm_get_present_cores(kbdev, KBASE_PM_CORE_SHADER));
+
+	set_timeout_global(global_iface, kbase_csf_timeout_get(kbdev));
+
+	/* Unmask the interrupts */
+	kbase_csf_firmware_global_input(global_iface,
+		GLB_ACK_IRQ_MASK, ack_irq_mask);
+
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+}
+
+/**
+ * global_init_on_boot - Sends a global request to control various features.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * Currently only the request to enable endpoints and cycle counter is sent.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+static int global_init_on_boot(struct kbase_device *const kbdev)
+{
+	u32 const req_mask = CSF_GLB_REQ_CFG_MASK;
+
+	global_init(kbdev, req_mask);
+
+	return wait_for_global_request(kbdev, req_mask);
+}
+
+void kbase_csf_firmware_global_reinit(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbdev->csf.glb_init_request_pending = true;
+	global_init(kbdev, CSF_GLB_REQ_CFG_MASK);
+}
+
+bool kbase_csf_firmware_global_reinit_complete(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+	WARN_ON(!kbdev->csf.glb_init_request_pending);
+
+	if (global_request_complete(kbdev, CSF_GLB_REQ_CFG_MASK))
+		kbdev->csf.glb_init_request_pending = false;
+
+	return !kbdev->csf.glb_init_request_pending;
+}
+
+static void kbase_csf_firmware_reload_worker(struct work_struct *work)
+{
+	struct kbase_device *kbdev = container_of(work, struct kbase_device,
+						  csf.firmware_reload_work);
+	unsigned long flags;
+
+	/* Reboot the firmware */
+	kbase_csf_firmware_enable_mcu(kbdev);
+
+	/* Tell MCU state machine to transit to next state */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbdev->csf.firmware_reloaded = true;
+	kbase_pm_update_state(kbdev);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+}
+
+void kbase_csf_firmware_trigger_reload(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbdev->csf.firmware_reloaded = false;
+
+	if (kbdev->csf.firmware_reload_needed) {
+		kbdev->csf.firmware_reload_needed = false;
+		queue_work(system_wq, &kbdev->csf.firmware_reload_work);
+	} else {
+		kbase_csf_firmware_enable_mcu(kbdev);
+		kbdev->csf.firmware_reloaded = true;
+	}
+}
+
+void kbase_csf_firmware_reload_completed(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (unlikely(!kbdev->csf.firmware_inited))
+		return;
+
+	/* Tell MCU state machine to transit to next state */
+	kbdev->csf.firmware_reloaded = true;
+	kbase_pm_update_state(kbdev);
+}
+
+int kbase_csf_firmware_init(struct kbase_device *kbdev)
+{
+	int ret;
+
+	if (WARN_ON((kbdev->as_free & MCU_AS_BITMASK) == 0))
+		return -EINVAL;
+	kbdev->as_free &= ~MCU_AS_BITMASK;
+
+	ret = kbase_mmu_init(kbdev, &kbdev->csf.mcu_mmu, NULL,
+		BASE_MEM_GROUP_DEFAULT);
+
+	if (ret != 0) {
+		/* Release the address space */
+		kbdev->as_free |= MCU_AS_BITMASK;
+		return ret;
+	}
+
+	init_waitqueue_head(&kbdev->csf.event_wait);
+	kbdev->csf.interrupt_received = false;
+
+	INIT_LIST_HEAD(&kbdev->csf.firmware_interfaces);
+	INIT_LIST_HEAD(&kbdev->csf.firmware_config);
+	INIT_LIST_HEAD(&kbdev->csf.firmware_trace_buffers.list);
+	INIT_WORK(&kbdev->csf.firmware_reload_work,
+		  kbase_csf_firmware_reload_worker);
+
+	mutex_init(&kbdev->csf.reg_lock);
+
+	ret = kbase_mcu_shared_interface_region_tracker_init(kbdev);
+	if (ret != 0) {
+		dev_err(kbdev->dev, "Failed to setup the rb tree for managing shared interface segment\n");
+		goto error;
+	}
+
+	ret = invent_memory_setup_entry(kbdev);
+	if (ret != 0) {
+		dev_err(kbdev->dev, "Failed to load firmware entry\n");
+		goto error;
+	}
+
+	/* Make sure L2 cache is powered up */
+	kbase_pm_wait_for_l2_powered(kbdev);
+
+	/* NO_MALI: Don't init trace buffers */
+
+	/* NO_MALI: Don't load the MMU tables or boot CSF firmware */
+
+	ret = invent_capabilities(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = kbase_csf_doorbell_mapping_init(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = kbase_csf_scheduler_init(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = kbase_csf_timeout_init(kbdev);
+	if (ret != 0)
+		goto error;
+
+	ret = global_init_on_boot(kbdev);
+	if (ret != 0)
+		goto error;
+
+	return 0;
+
+error:
+	kbase_csf_firmware_term(kbdev);
+	return ret;
+}
+
+void kbase_csf_firmware_term(struct kbase_device *kbdev)
+{
+	kbase_csf_timeout_term(kbdev);
+
+	/* NO_MALI: Don't stop firmware or unload MMU tables */
+
+	kbase_mmu_term(kbdev, &kbdev->csf.mcu_mmu);
+
+	kbase_csf_scheduler_term(kbdev);
+
+	kbase_csf_doorbell_mapping_term(kbdev);
+
+	free_global_iface(kbdev);
+
+	/* Release the address space */
+	kbdev->as_free |= MCU_AS_BITMASK;
+
+	while (!list_empty(&kbdev->csf.firmware_interfaces)) {
+		struct dummy_firmware_interface *interface;
+
+		interface = list_first_entry(&kbdev->csf.firmware_interfaces,
+				struct dummy_firmware_interface, node);
+		list_del(&interface->node);
+
+		/* NO_MALI: No cleanup in dummy interface necessary */
+
+		kfree(interface);
+	}
+
+	/* NO_MALI: No trace buffers to terminate */
+
+#ifndef MALI_KBASE_BUILD
+	mali_kutf_fw_utf_entry_cleanup(kbdev);
+#endif
+
+	mutex_destroy(&kbdev->csf.reg_lock);
+
+	/* This will also free up the region allocated for the shared interface
+	 * entry parsed from the firmware image.
+	 */
+	kbase_mcu_shared_interface_region_tracker_term(kbdev);
+}
+
+int kbase_csf_firmware_ping(struct kbase_device *const kbdev)
+{
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	unsigned long flags;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	set_global_request(global_iface, GLB_REQ_PING_MASK);
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	return wait_for_global_request(kbdev, GLB_REQ_PING_MASK);
+}
+
+int kbase_csf_firmware_set_timeout(struct kbase_device *const kbdev,
+	u64 const timeout)
+{
+	const struct kbase_csf_global_iface *const global_iface =
+		&kbdev->csf.global_iface;
+	unsigned long flags;
+	int err;
+
+	/* The 'reg_lock' is also taken and is held till the update is not
+	 * complete, to ensure the update of timeout value by multiple Users
+	 * gets serialized.
+	 */
+	mutex_lock(&kbdev->csf.reg_lock);
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	set_timeout_global(global_iface, timeout);
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	err = wait_for_global_request(kbdev, GLB_REQ_CFG_PROGRESS_TIMER_MASK);
+	mutex_unlock(&kbdev->csf.reg_lock);
+
+	return err;
+}
+
+void kbase_csf_enter_protected_mode(struct kbase_device *kbdev)
+{
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	unsigned long flags;
+	unsigned int value;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	value = kbase_csf_firmware_global_output(global_iface, GLB_ACK);
+	value ^= GLB_REQ_PROTM_ENTER_MASK;
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, value,
+					     GLB_REQ_PROTM_ENTER_MASK);
+	dev_dbg(kbdev->dev, "Sending request to enter protected mode");
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+	wait_for_global_request(kbdev, GLB_REQ_PROTM_ENTER_MASK);
+}
+
+void kbase_csf_firmware_trigger_mcu_halt(struct kbase_device *kbdev)
+{
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	unsigned long flags;
+	unsigned int value;
+
+	kbase_csf_scheduler_spin_lock(kbdev, &flags);
+	value = kbase_csf_firmware_global_output(global_iface, GLB_ACK);
+	value ^= GLB_REQ_HALT_MASK;
+	kbase_csf_firmware_global_input_mask(global_iface, GLB_REQ, value,
+					     GLB_REQ_HALT_MASK);
+	dev_dbg(kbdev->dev, "Sending request to HALT MCU");
+	kbase_csf_ring_doorbell(kbdev, CSF_KERNEL_DOORBELL_NR);
+	kbase_csf_scheduler_spin_unlock(kbdev, flags);
+}
+
+/**
+ * copy_grp_and_stm - Copy command stream and/or group data
+ *
+ * @iface:                Global command stream front-end interface provided by
+ *                        the firmware.
+ * @group_data:           Pointer where to store all the group data
+ *                        (sequentially).
+ * @max_group_num:        The maximum number of groups to be read. Can be 0, in
+ *                        which case group_data is unused.
+ * @stream_data:          Pointer where to store all the stream data
+ *                        (sequentially).
+ * @max_total_stream_num: The maximum number of streams to be read.
+ *                        Can be 0, in which case stream_data is unused.
+ *
+ * Return: Total number of command streams, summed across all groups.
+ */
+static u32 copy_grp_and_stm(
+	const struct kbase_csf_global_iface * const iface,
+	struct basep_cs_group_control * const group_data,
+	u32 max_group_num,
+	struct basep_cs_stream_control * const stream_data,
+	u32 max_total_stream_num)
+{
+	u32 i, total_stream_num = 0;
+
+	if (WARN_ON((max_group_num > 0) && !group_data))
+		max_group_num = 0;
+
+	if (WARN_ON((max_total_stream_num > 0) && !stream_data))
+		max_total_stream_num = 0;
+
+	for (i = 0; i < iface->group_num; i++) {
+		u32 j;
+
+		if (i < max_group_num) {
+			group_data[i].features = iface->groups[i].features;
+			group_data[i].stream_num = iface->groups[i].stream_num;
+		}
+		for (j = 0; j < iface->groups[i].stream_num; j++) {
+			if (total_stream_num < max_total_stream_num)
+				stream_data[total_stream_num].features =
+					iface->groups[i].streams[j].features;
+			total_stream_num++;
+		}
+	}
+
+	return total_stream_num;
+}
+
+u32 kbase_csf_firmware_get_glb_iface(struct kbase_device *kbdev,
+	struct basep_cs_group_control *const group_data,
+	u32 const max_group_num,
+	struct basep_cs_stream_control *const stream_data,
+	u32 const max_total_stream_num, u32 *const glb_version,
+	u32 *const features, u32 *const group_num, u32 *const prfcnt_size)
+{
+	const struct kbase_csf_global_iface * const iface =
+		&kbdev->csf.global_iface;
+
+	if (WARN_ON(!glb_version) ||
+		WARN_ON(!features) ||
+		WARN_ON(!group_num) ||
+		WARN_ON(!prfcnt_size))
+		return 0;
+
+	*glb_version = iface->version;
+	*features = iface->features;
+	*group_num = iface->group_num;
+	*prfcnt_size = iface->prfcnt_size;
+
+	return copy_grp_and_stm(iface, group_data, max_group_num,
+		stream_data, max_total_stream_num);
+}
+
+const char *kbase_csf_firmware_get_timeline_metadata(
+	struct kbase_device *kbdev, const char *name, size_t *size)
+{
+	if (WARN_ON(!kbdev) ||
+		WARN_ON(!name) ||
+		WARN_ON(!size)) {
+		return NULL;
+	}
+
+	*size = 0;
+	return NULL;
+}
+
+void kbase_csf_firmware_disable_mcu_wait(struct kbase_device *kbdev)
+{
+	/* NO_MALI: Nothing to do here */
+}
+
+int kbase_csf_firmware_mcu_shared_mapping_init(
+		struct kbase_device *kbdev,
+		unsigned int num_pages,
+		unsigned long cpu_map_properties,
+		unsigned long gpu_map_properties,
+		struct kbase_csf_mapping *csf_mapping)
+{
+	struct tagged_addr *phys;
+	struct kbase_va_region *va_reg;
+	struct page **page_list;
+	void *cpu_addr;
+	int i, ret = 0;
+	pgprot_t cpu_map_prot = PAGE_KERNEL;
+	unsigned long gpu_map_prot;
+
+	if (cpu_map_properties & PROT_READ)
+		cpu_map_prot = PAGE_KERNEL_RO;
+
+	if (kbdev->system_coherency == COHERENCY_ACE) {
+		gpu_map_prot =
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_DEFAULT_ACE);
+	} else {
+		gpu_map_prot =
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_NON_CACHEABLE);
+		cpu_map_prot = pgprot_writecombine(cpu_map_prot);
+	};
+
+	phys = kmalloc_array(num_pages, sizeof(*phys), GFP_KERNEL);
+	if (!phys)
+		goto out;
+
+	page_list = kmalloc_array(num_pages, sizeof(*page_list), GFP_KERNEL);
+	if (!page_list)
+		goto page_list_alloc_error;
+
+	ret = kbase_mem_pool_alloc_pages(
+		&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+		num_pages, phys, false);
+	if (ret <= 0)
+		goto phys_mem_pool_alloc_error;
+
+	for (i = 0; i < num_pages; i++)
+		page_list[i] = as_page(phys[i]);
+
+	cpu_addr = vmap(page_list, num_pages, VM_MAP, cpu_map_prot);
+	if (!cpu_addr)
+		goto vmap_error;
+
+	va_reg = kbase_alloc_free_region(&kbdev->csf.shared_reg_rbtree, 0,
+			num_pages, KBASE_REG_ZONE_MCU_SHARED);
+	if (!va_reg)
+		goto va_region_alloc_error;
+
+	mutex_lock(&kbdev->csf.reg_lock);
+	ret = kbase_add_va_region_rbtree(kbdev, va_reg, 0, num_pages, 1);
+	va_reg->flags &= ~KBASE_REG_FREE;
+	mutex_unlock(&kbdev->csf.reg_lock);
+	if (ret)
+		goto va_region_add_error;
+
+	gpu_map_properties &= (KBASE_REG_GPU_RD | KBASE_REG_GPU_WR);
+	gpu_map_properties |= gpu_map_prot;
+
+	ret = kbase_mmu_insert_pages_no_flush(kbdev, &kbdev->csf.mcu_mmu,
+			va_reg->start_pfn, &phys[0], num_pages,
+			gpu_map_properties, KBASE_MEM_GROUP_CSF_FW);
+	if (ret)
+		goto mmu_insert_pages_error;
+
+	kfree(page_list);
+	csf_mapping->phys = phys;
+	csf_mapping->cpu_addr = cpu_addr;
+	csf_mapping->va_reg = va_reg;
+	csf_mapping->num_pages = num_pages;
+
+	return 0;
+
+mmu_insert_pages_error:
+	mutex_lock(&kbdev->csf.reg_lock);
+	kbase_remove_va_region(va_reg);
+	mutex_unlock(&kbdev->csf.reg_lock);
+va_region_add_error:
+	kbase_free_alloced_region(va_reg);
+va_region_alloc_error:
+	vunmap(cpu_addr);
+vmap_error:
+	kbase_mem_pool_free_pages(
+		&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+		num_pages, phys, false, false);
+
+phys_mem_pool_alloc_error:
+	kfree(page_list);
+page_list_alloc_error:
+	kfree(phys);
+out:
+	/* Zero-initialize the mapping to make sure that the termination
+	 * function doesn't try to unmap or free random addresses. */
+	csf_mapping->phys = NULL;
+	csf_mapping->cpu_addr = NULL;
+	csf_mapping->va_reg = NULL;
+	csf_mapping->num_pages = 0;
+
+	return -ENOMEM;
+}
+
+void kbase_csf_firmware_mcu_shared_mapping_term(
+		struct kbase_device *kbdev, struct kbase_csf_mapping *csf_mapping)
+{
+	if (csf_mapping->va_reg) {
+		mutex_lock(&kbdev->csf.reg_lock);
+		kbase_remove_va_region(csf_mapping->va_reg);
+		mutex_unlock(&kbdev->csf.reg_lock);
+		kbase_free_alloced_region(csf_mapping->va_reg);
+	}
+
+	if (csf_mapping->phys) {
+		kbase_mem_pool_free_pages(
+			&kbdev->mem_pools.small[KBASE_MEM_GROUP_CSF_FW],
+			csf_mapping->num_pages, csf_mapping->phys, false,
+			false);
+	}
+
+	vunmap(csf_mapping->cpu_addr);
+	kfree(csf_mapping->phys);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c
new file mode 100644
index 000000000000..087cc858c2b8
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.c
@@ -0,0 +1,196 @@
+/*
+ *
+ * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include "mali_kbase_csf_heap_context_alloc.h"
+
+/* Size of one heap context structure, in bytes. */
+#define HEAP_CTX_SIZE ((size_t)32)
+
+/* Total size of the GPU memory region allocated for heap contexts, in bytes. */
+#define HEAP_CTX_REGION_SIZE (MAX_TILER_HEAPS * HEAP_CTX_SIZE)
+
+/**
+ * sub_alloc - Sub-allocate a heap context from a GPU memory region
+ *
+ * @ctx_alloc: Pointer to the heap context allocator.
+ *
+ * Return: GPU virtual address of the allocated heap context or 0 on failure.
+ */
+static u64 sub_alloc(struct kbase_csf_heap_context_allocator *const ctx_alloc)
+{
+	struct kbase_context *const kctx = ctx_alloc->kctx;
+	int heap_nr = 0;
+	size_t ctx_offset = 0;
+	u64 heap_gpu_va = 0;
+	struct kbase_vmap_struct mapping;
+	void *ctx_ptr = NULL;
+
+	lockdep_assert_held(&ctx_alloc->lock);
+
+	heap_nr = find_first_zero_bit(ctx_alloc->in_use,
+		MAX_TILER_HEAPS);
+
+	if (unlikely(heap_nr >= MAX_TILER_HEAPS)) {
+		dev_err(kctx->kbdev->dev,
+			"No free tiler heap contexts in the pool\n");
+		return 0;
+	}
+
+	ctx_offset = heap_nr * HEAP_CTX_SIZE;
+	heap_gpu_va = ctx_alloc->gpu_va + ctx_offset;
+	ctx_ptr = kbase_vmap_prot(kctx, heap_gpu_va,
+		HEAP_CTX_SIZE, KBASE_REG_CPU_WR, &mapping);
+
+	if (unlikely(!ctx_ptr)) {
+		dev_err(kctx->kbdev->dev,
+			"Failed to map tiler heap context %d (0x%llX)\n",
+			heap_nr, heap_gpu_va);
+		return 0;
+	}
+
+	memset(ctx_ptr, 0, HEAP_CTX_SIZE);
+	kbase_vunmap(ctx_ptr, &mapping);
+
+	bitmap_set(ctx_alloc->in_use, heap_nr, 1);
+
+	dev_dbg(kctx->kbdev->dev, "Allocated tiler heap context %d (0x%llX)\n",
+		heap_nr, heap_gpu_va);
+
+	return heap_gpu_va;
+}
+
+/**
+ * sub_free - Free a heap context sub-allocated from a GPU memory region
+ *
+ * @ctx_alloc:   Pointer to the heap context allocator.
+ * @heap_gpu_va: The GPU virtual address of a heap context structure to free.
+ */
+static void sub_free(struct kbase_csf_heap_context_allocator *const ctx_alloc,
+	u64 const heap_gpu_va)
+{
+	struct kbase_context *const kctx = ctx_alloc->kctx;
+	u64 ctx_offset = 0;
+	unsigned int heap_nr = 0;
+
+	lockdep_assert_held(&ctx_alloc->lock);
+
+	if (WARN_ON(!ctx_alloc->region))
+		return;
+
+	if (WARN_ON(heap_gpu_va < ctx_alloc->gpu_va))
+		return;
+
+	ctx_offset = heap_gpu_va - ctx_alloc->gpu_va;
+
+	if (WARN_ON(ctx_offset >= HEAP_CTX_REGION_SIZE) ||
+		WARN_ON(ctx_offset % HEAP_CTX_SIZE))
+		return;
+
+	heap_nr = ctx_offset / HEAP_CTX_SIZE;
+	dev_dbg(kctx->kbdev->dev,
+		"Freed tiler heap context %d (0x%llX)\n", heap_nr, heap_gpu_va);
+
+	bitmap_clear(ctx_alloc->in_use, heap_nr, 1);
+}
+
+int kbase_csf_heap_context_allocator_init(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc,
+	struct kbase_context *const kctx)
+{
+	/* We cannot pre-allocate GPU memory here because the
+	 * custom VA zone may not have been created yet.
+	 */
+	ctx_alloc->kctx = kctx;
+	ctx_alloc->region = NULL;
+	ctx_alloc->gpu_va = 0;
+
+	mutex_init(&ctx_alloc->lock);
+	bitmap_zero(ctx_alloc->in_use, MAX_TILER_HEAPS);
+
+	dev_dbg(kctx->kbdev->dev,
+		"Initialized a tiler heap context allocator\n");
+
+	return 0;
+}
+
+void kbase_csf_heap_context_allocator_term(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc)
+{
+	struct kbase_context *const kctx = ctx_alloc->kctx;
+
+	dev_dbg(kctx->kbdev->dev,
+		"Terminating tiler heap context allocator\n");
+
+	if (ctx_alloc->region) {
+		kbase_gpu_vm_lock(kctx);
+		ctx_alloc->region->flags &= ~KBASE_REG_NO_USER_FREE;
+		kbase_mem_free_region(kctx, ctx_alloc->region);
+		kbase_gpu_vm_unlock(kctx);
+	}
+
+	mutex_destroy(&ctx_alloc->lock);
+}
+
+u64 kbase_csf_heap_context_allocator_alloc(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc)
+{
+	struct kbase_context *const kctx = ctx_alloc->kctx;
+	u64 flags = BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR |
+		BASE_MEM_PROT_CPU_WR | BASEP_MEM_NO_USER_FREE;
+	u64 nr_pages = PFN_UP(HEAP_CTX_REGION_SIZE);
+	u64 heap_gpu_va = 0;
+
+#ifdef CONFIG_MALI_VECTOR_DUMP
+	flags |= BASE_MEM_PROT_CPU_RD;
+#endif
+
+	mutex_lock(&ctx_alloc->lock);
+
+	/* If the pool of heap contexts wasn't already allocated then
+	 * allocate it.
+	 */
+	if (!ctx_alloc->region) {
+		ctx_alloc->region = kbase_mem_alloc(kctx, nr_pages, nr_pages,
+					0, &flags, &ctx_alloc->gpu_va);
+	}
+
+	/* If the pool still isn't allocated then an error occurred. */
+	if (unlikely(!ctx_alloc->region)) {
+		dev_err(kctx->kbdev->dev, "Failed to allocate a pool of tiler heap contexts\n");
+	} else {
+		heap_gpu_va = sub_alloc(ctx_alloc);
+	}
+
+	mutex_unlock(&ctx_alloc->lock);
+
+	return heap_gpu_va;
+}
+
+void kbase_csf_heap_context_allocator_free(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc,
+	u64 const heap_gpu_va)
+{
+	mutex_lock(&ctx_alloc->lock);
+	sub_free(ctx_alloc, heap_gpu_va);
+	mutex_unlock(&ctx_alloc->lock);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.h
new file mode 100644
index 000000000000..f71ea01ed8c0
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_heap_context_alloc.h
@@ -0,0 +1,76 @@
+/*
+ *
+ * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+
+#ifndef _KBASE_CSF_HEAP_CONTEXT_ALLOC_H_
+#define _KBASE_CSF_HEAP_CONTEXT_ALLOC_H_
+
+/**
+ * kbase_csf_heap_context_allocator_init - Initialize an allocator for heap
+ *                                         contexts
+ * @ctx_alloc: Pointer to the heap context allocator to initialize.
+ * @kctx:      Pointer to the kbase context.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+int kbase_csf_heap_context_allocator_init(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc,
+	struct kbase_context *const kctx);
+
+/**
+ * kbase_csf_heap_context_allocator_term - Terminate an allocator for heap
+ *                                         contexts
+ * @ctx_alloc: Pointer to the heap context allocator to terminate.
+ */
+void kbase_csf_heap_context_allocator_term(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc);
+
+/**
+ * kbase_csf_heap_context_allocator_alloc - Allocate a heap context structure
+ *
+ * If this function is successful then it returns the address of a
+ * zero-initialized heap context structure for use by the firmware.
+ *
+ * @ctx_alloc: Pointer to the heap context allocator.
+ *
+ * Return: GPU virtual address of the allocated heap context or 0 on failure.
+ */
+u64 kbase_csf_heap_context_allocator_alloc(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc);
+
+/**
+ * kbase_csf_heap_context_allocator_free - Free a heap context structure
+ *
+ * This function returns a heap context structure to the free pool of unused
+ * contexts for possible reuse by a future call to
+ * @kbase_csf_heap_context_allocator_alloc.
+ *
+ * @ctx_alloc:   Pointer to the heap context allocator.
+ * @heap_gpu_va: The GPU virtual address of a heap context structure that
+ *               was allocated for the firmware.
+ */
+void kbase_csf_heap_context_allocator_free(
+	struct kbase_csf_heap_context_allocator *const ctx_alloc,
+	u64 const heap_gpu_va);
+
+#endif /* _KBASE_CSF_HEAP_CONTEXT_ALLOC_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h
new file mode 100644
index 000000000000..e9bb8d299754
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_ioctl.h
@@ -0,0 +1,379 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_IOCTL_H_
+#define _KBASE_CSF_IOCTL_H_
+
+#include <asm-generic/ioctl.h>
+#include <linux/types.h>
+
+/*
+ * 1.0:
+ * - CSF IOCTL header separated from JM
+ */
+
+#define BASE_UK_VERSION_MAJOR 1
+#define BASE_UK_VERSION_MINOR 0
+
+/**
+ * struct kbase_ioctl_version_check - Check version compatibility between
+ * kernel and userspace
+ *
+ * @major: Major version number
+ * @minor: Minor version number
+ */
+struct kbase_ioctl_version_check {
+	__u16 major;
+	__u16 minor;
+};
+
+#define KBASE_IOCTL_VERSION_CHECK \
+	_IOWR(KBASE_IOCTL_TYPE, 52, struct kbase_ioctl_version_check)
+
+#define KBASE_IOCTL_VERSION_CHECK_RESERVED \
+	_IOWR(KBASE_IOCTL_TYPE, 0, struct kbase_ioctl_version_check)
+
+
+/**
+ * struct kbase_ioctl_cs_queue_register - Register a GPU command queue with the
+ *                                        base back-end
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @buffer_size: Size of the buffer in bytes
+ * @priority: Priority of the queue within a group when run within a process
+ * @padding: Currently unused, must be zero
+ */
+struct kbase_ioctl_cs_queue_register {
+	__u64 buffer_gpu_addr;
+	__u32 buffer_size;
+	__u8 priority;
+	__u8 padding[3];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_REGISTER \
+	_IOW(KBASE_IOCTL_TYPE, 36, struct kbase_ioctl_cs_queue_register)
+
+/**
+ * struct kbase_ioctl_cs_queue_kick - Kick the GPU command queue group scheduler
+ *                                    to notify that a queue has been updated
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ */
+struct kbase_ioctl_cs_queue_kick {
+	__u64 buffer_gpu_addr;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_KICK \
+	_IOW(KBASE_IOCTL_TYPE, 37, struct kbase_ioctl_cs_queue_kick)
+
+/**
+ * union kbase_ioctl_cs_queue_bind - Bind a GPU command queue to a group
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ * @group_handle: Handle of the group to which the queue should be bound
+ * @csi_index: Index of the CSF interface the queue should be bound to
+ * @padding: Currently unused, must be zero
+ * @mmap_handle: Handle to be used for creating the mapping of command stream
+ *               input/output pages
+ *
+ * @in: Input parameters
+ * @out: Output parameters
+ *
+ */
+union kbase_ioctl_cs_queue_bind {
+	struct {
+		__u64 buffer_gpu_addr;
+		__u8 group_handle;
+		__u8 csi_index;
+		__u8 padding[6];
+	} in;
+	struct {
+		__u64 mmap_handle;
+	} out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_BIND \
+	_IOWR(KBASE_IOCTL_TYPE, 39, union kbase_ioctl_cs_queue_bind)
+
+/* ioctl 40 is free to use */
+
+/**
+ * struct kbase_ioctl_cs_queue_terminate - Terminate a GPU command queue
+ *
+ * @buffer_gpu_addr: GPU address of the buffer backing the queue
+ */
+struct kbase_ioctl_cs_queue_terminate {
+	__u64 buffer_gpu_addr;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_TERMINATE \
+	_IOW(KBASE_IOCTL_TYPE, 41, struct kbase_ioctl_cs_queue_terminate)
+
+/**
+ * union kbase_ioctl_cs_queue_group_create - Create a GPU command queue group
+ *
+ * @tiler_mask:		Mask of tiler endpoints the group is allowed to use.
+ * @fragment_mask:	Mask of fragment endpoints the group is allowed to use.
+ * @compute_mask:	Mask of compute endpoints the group is allowed to use.
+ * @cs_min:		Minimum number of command streams required.
+ * @priority:		Queue group's priority within a process.
+ * @tiler_max:		Maximum number of tiler endpoints the group is allowed
+ *			to use.
+ * @fragment_max:	Maximum number of fragment endpoints the group is
+ *			allowed to use.
+ * @compute_max:	Maximum number of compute endpoints the group is allowed
+ *			to use.
+ * @padding:		Currently unused, must be zero
+ * @group_handle:	Handle of a newly created queue group.
+ *
+ * @in: Input parameters
+ * @out: Output parameters
+ *
+ */
+union kbase_ioctl_cs_queue_group_create {
+	struct {
+		__u64 tiler_mask;
+		__u64 fragment_mask;
+		__u64 compute_mask;
+		__u8 cs_min;
+		__u8 priority;
+		__u8 tiler_max;
+		__u8 fragment_max;
+		__u8 compute_max;
+		__u8 padding[3];
+
+	} in;
+	struct {
+		__u8 group_handle;
+		__u8 padding[7];
+	} out;
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_CREATE \
+	_IOWR(KBASE_IOCTL_TYPE, 42, union kbase_ioctl_cs_queue_group_create)
+
+/**
+ * struct kbase_ioctl_cs_queue_group_term - Terminate a GPU command queue group
+ *
+ * @group_handle: Handle of the queue group to be terminated
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_cs_queue_group_term {
+	__u8 group_handle;
+	__u8 padding[7];
+};
+
+#define KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE \
+	_IOW(KBASE_IOCTL_TYPE, 43, struct kbase_ioctl_cs_queue_group_term)
+
+#define KBASE_IOCTL_CS_EVENT_SIGNAL \
+	_IO(KBASE_IOCTL_TYPE, 44)
+
+typedef __u8 base_kcpu_queue_id; /* We support up to 256 active KCPU queues */
+
+/**
+ * struct kbase_ioctl_kcpu_queue_new - Create a KCPU command queue
+ *
+ * @id: ID of the new command queue returned by the kernel
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_new {
+	base_kcpu_queue_id id;
+	__u8 padding[7];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_CREATE \
+	_IOR(KBASE_IOCTL_TYPE, 45, struct kbase_ioctl_kcpu_queue_new)
+
+/**
+ * struct kbase_ioctl_kcpu_queue_delete - Destroy a KCPU command queue
+ *
+ * @id: ID of the command queue to be destroyed
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_delete {
+	base_kcpu_queue_id id;
+	__u8 padding[7];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_DELETE \
+	_IOW(KBASE_IOCTL_TYPE, 46, struct kbase_ioctl_kcpu_queue_delete)
+
+/**
+ * struct kbase_ioctl_kcpu_queue_enqueue - Enqueue commands into the KCPU queue
+ *
+ * @addr: Memory address of an array of struct base_kcpu_queue_command
+ * @nr_commands: Number of commands in the array
+ * @id: kcpu queue identifier, returned by KBASE_IOCTL_KCPU_QUEUE_CREATE ioctl
+ * @padding: Padding to round up to a multiple of 8 bytes, must be zero
+ */
+struct kbase_ioctl_kcpu_queue_enqueue {
+	__u64 addr;
+	__u32 nr_commands;
+	base_kcpu_queue_id id;
+	__u8 padding[3];
+};
+
+#define KBASE_IOCTL_KCPU_QUEUE_ENQUEUE \
+	_IOW(KBASE_IOCTL_TYPE, 47, struct kbase_ioctl_kcpu_queue_enqueue)
+
+/**
+ * union kbase_ioctl_cs_tiler_heap_init - Initialize chunked tiler memory heap
+ *
+ * @chunk_size: Size of each chunk.
+ * @initial_chunks: Initial number of chunks that heap will be created with.
+ * @max_chunks: Maximum number of chunks that the heap is allowed to use.
+ * @target_in_flight: Number of render-passes that the driver should attempt to
+ *                    keep in flight for which allocation of new chunks is
+ *                    allowed.
+ * @group_id: Group ID to be used for physical allocations.
+ * @gpu_heap_va: GPU VA (virtual address) of Heap context that was set up for
+ *               the heap.
+ * @first_chunk_va: GPU VA of the first chunk allocated for the heap, actually
+ *                  points to the header of heap chunk and not to the low
+ *                  address of free memory in the chunk.
+ *
+ * @in: Input parameters
+ * @out: Output parameters
+ *
+ */
+union kbase_ioctl_cs_tiler_heap_init {
+	struct {
+		__u32 chunk_size;
+		__u32 initial_chunks;
+		__u32 max_chunks;
+		__u16 target_in_flight;
+		__u8 group_id;
+		__u8 padding;
+	} in;
+	struct {
+		__u64 gpu_heap_va;
+		__u64 first_chunk_va;
+	} out;
+};
+
+#define KBASE_IOCTL_CS_TILER_HEAP_INIT \
+	_IOWR(KBASE_IOCTL_TYPE, 48, union kbase_ioctl_cs_tiler_heap_init)
+
+/**
+ * struct kbase_ioctl_cs_tiler_heap_term - Terminate a chunked tiler heap
+ *                                         instance
+ *
+ * @gpu_heap_va: GPU VA of Heap context that was set up for the heap.
+ */
+struct kbase_ioctl_cs_tiler_heap_term {
+	__u64 gpu_heap_va;
+};
+
+#define KBASE_IOCTL_CS_TILER_HEAP_TERM \
+	_IOW(KBASE_IOCTL_TYPE, 49, struct kbase_ioctl_cs_tiler_heap_term)
+
+/**
+ * union kbase_ioctl_cs_get_glb_iface - Request the global control block
+ *                                        of CSF interface capabilities
+ *
+ * @max_group_num:        The maximum number of groups to be read. Can be 0, in
+ *                        which case groups_ptr is unused.
+ * @max_total_stream_num: The maximum number of streams to be read. Can be 0, in
+ *                        which case streams_ptr is unused.
+ * @groups_ptr:       Pointer where to store all the group data (sequentially).
+ * @streams_ptr:      Pointer where to store all the stream data (sequentially).
+ * @glb_version:      Global interface version. Bits 31:16 hold the major
+ *                    version number and 15:0 hold the minor version number.
+ *                    A higher minor version is backwards-compatible with a
+ *                    lower minor version for the same major version.
+ * @features:         Bit mask of features (e.g. whether certain types of job
+ *                    can be suspended).
+ * @group_num:        Number of command stream groups supported.
+ * @prfcnt_size:      Size of CSF performance counters, in bytes. Bits 31:16
+ *                    hold the size of firmware performance counter data
+ *                    and 15:0 hold the size of hardware performance counter
+ *                    data.
+ * @total_stream_num: Total number of command streams, summed across all groups.
+ * @padding:          Will be zeroed.
+ *
+ * @in: Input parameters
+ * @out: Output parameters
+ *
+ */
+union kbase_ioctl_cs_get_glb_iface {
+	struct {
+		__u32 max_group_num;
+		__u32 max_total_stream_num;
+		__u64 groups_ptr;
+		__u64 streams_ptr;
+	} in;
+	struct {
+		__u32 glb_version;
+		__u32 features;
+		__u32 group_num;
+		__u32 prfcnt_size;
+		__u32 total_stream_num;
+		__u32 padding;
+	} out;
+};
+
+#define KBASE_IOCTL_CS_GET_GLB_IFACE \
+	_IOWR(KBASE_IOCTL_TYPE, 51, union kbase_ioctl_cs_get_glb_iface)
+
+/***************
+ * test ioctls *
+ ***************/
+#if MALI_UNIT_TEST
+/* These ioctls are purely for test purposes and are not used in the production
+ * driver, they therefore may change without notice
+ */
+
+/**
+ * struct kbase_ioctl_cs_event_memory_write - Write an event memory address
+ * @cpu_addr: Memory address to write
+ * @value: Value to write
+ * @padding: Currently unused, must be zero
+ */
+struct kbase_ioctl_cs_event_memory_write {
+	__u64 cpu_addr;
+	__u8 value;
+	__u8 padding[7];
+};
+
+/**
+ * union kbase_ioctl_cs_event_memory_read - Read an event memory address
+ * @cpu_addr: Memory address to read
+ * @value: Value read
+ * @padding: Currently unused, must be zero
+ *
+ * @in: Input parameters
+ * @out: Output parameters
+ */
+union kbase_ioctl_cs_event_memory_read {
+	struct {
+		__u64 cpu_addr;
+	} in;
+	struct {
+		__u8 value;
+		__u8 padding[7];
+	} out;
+};
+
+#endif /* MALI_UNIT_TEST */
+
+#endif /* _KBASE_CSF_IOCTL_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c
new file mode 100644
index 000000000000..e1263d535918
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.c
@@ -0,0 +1,1737 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <tl/mali_kbase_tracepoints.h>
+#include <mali_kbase_ctx_sched.h>
+#include "device/mali_kbase_device.h"
+#include "mali_kbase_csf.h"
+#include <linux/export.h>
+
+#ifdef CONFIG_SYNC_FILE
+#include "mali_kbase_fence.h"
+#include "mali_kbase_sync.h"
+
+static DEFINE_SPINLOCK(kbase_csf_fence_lock);
+#endif
+
+static void kcpu_queue_process(struct kbase_kcpu_command_queue *kcpu_queue,
+			bool ignore_waits);
+
+static void kcpu_queue_process_worker(struct work_struct *data);
+
+static int kbase_kcpu_map_import_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_import_info *import_info,
+		struct kbase_kcpu_command *current_command)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+	struct kbase_va_region *reg;
+	int ret = 0;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	/* Take the processes mmap lock */
+	down_read(kbase_mem_get_process_mmap_lock());
+	kbase_gpu_vm_lock(kctx);
+
+	reg = kbase_region_tracker_find_region_enclosing_address(kctx,
+					import_info->handle);
+
+	if (kbase_is_region_invalid_or_free(reg) ||
+	    !kbase_mem_is_imported(reg->gpu_alloc->type)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (reg->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_USER_BUF) {
+		/* Pin the physical pages backing the user buffer while
+		 * we are in the process context and holding the mmap lock.
+		 * The dma mapping & GPU mapping of the pages would be done
+		 * when the MAP_IMPORT operation is executed.
+		 *
+		 * Though the pages would be pinned, no reference is taken
+		 * on the physical pages tracking object. When the last
+		 * reference to the tracking object is dropped the pages
+		 * would be unpinned if they weren't unpinned before.
+		 */
+		ret = kbase_jd_user_buf_pin_pages(kctx, reg);
+		if (ret)
+			goto out;
+	}
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_MAP_IMPORT;
+	current_command->info.import.gpu_va = import_info->handle;
+
+out:
+	kbase_gpu_vm_unlock(kctx);
+	/* Release the processes mmap lock */
+	up_read(kbase_mem_get_process_mmap_lock());
+
+	return ret;
+}
+
+static int kbase_kcpu_unmap_import_prepare_internal(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_import_info *import_info,
+		struct kbase_kcpu_command *current_command,
+		enum base_kcpu_command_type type)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+	struct kbase_va_region *reg;
+	int ret = 0;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	kbase_gpu_vm_lock(kctx);
+
+	reg = kbase_region_tracker_find_region_enclosing_address(kctx,
+					import_info->handle);
+
+	if (kbase_is_region_invalid_or_free(reg) ||
+	    !kbase_mem_is_imported(reg->gpu_alloc->type)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (reg->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_USER_BUF) {
+		/* The pages should have been pinned when MAP_IMPORT
+		 * was enqueued previously.
+		 */
+		if (reg->gpu_alloc->nents !=
+		    reg->gpu_alloc->imported.user_buf.nr_pages) {
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	current_command->type = type;
+	current_command->info.import.gpu_va = import_info->handle;
+
+out:
+	kbase_gpu_vm_unlock(kctx);
+
+	return ret;
+}
+
+static int kbase_kcpu_unmap_import_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_import_info *import_info,
+		struct kbase_kcpu_command *current_command)
+{
+	return kbase_kcpu_unmap_import_prepare_internal(kcpu_queue,
+			import_info, current_command,
+			BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT);
+}
+
+static int kbase_kcpu_unmap_import_force_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_import_info *import_info,
+		struct kbase_kcpu_command *current_command)
+{
+	return kbase_kcpu_unmap_import_prepare_internal(kcpu_queue,
+			import_info, current_command,
+			BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE);
+}
+
+/**
+ * kbase_jit_add_to_pending_alloc_list() - Pend JIT allocation
+ *
+ * @queue: The queue containing this JIT allocation
+ * @cmd:   The JIT allocation that is blocking this queue
+ */
+static void kbase_jit_add_to_pending_alloc_list(
+		struct kbase_kcpu_command_queue *queue,
+		struct kbase_kcpu_command *cmd)
+{
+	struct kbase_context *const kctx = queue->kctx;
+	struct list_head *target_list_head =
+			&kctx->csf.kcpu_queues.jit_blocked_queues;
+	struct kbase_kcpu_command_queue *blocked_queue;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	list_for_each_entry(blocked_queue,
+			&kctx->csf.kcpu_queues.jit_blocked_queues,
+			jit_blocked) {
+		struct kbase_kcpu_command const*const jit_alloc_cmd =
+				&blocked_queue->commands[blocked_queue->start_offset];
+
+		WARN_ON(jit_alloc_cmd->type != BASE_KCPU_COMMAND_TYPE_JIT_ALLOC);
+		if (cmd->enqueue_ts < jit_alloc_cmd->enqueue_ts) {
+			target_list_head = &blocked_queue->jit_blocked;
+			break;
+		}
+	}
+
+	list_add_tail(&queue->jit_blocked, target_list_head);
+}
+
+/**
+ * kbase_kcpu_jit_allocate_process() - Process JIT allocation
+ *
+ * @queue: The queue containing this JIT allocation
+ * @cmd:   The JIT allocation command
+ */
+static int kbase_kcpu_jit_allocate_process(
+		struct kbase_kcpu_command_queue *queue,
+		struct kbase_kcpu_command *cmd)
+{
+	struct kbase_context *const kctx = queue->kctx;
+	struct kbase_kcpu_command_jit_alloc_info *alloc_info =
+			&cmd->info.jit_alloc;
+	struct base_jit_alloc_info *info = alloc_info->info;
+	struct kbase_vmap_struct mapping;
+	struct kbase_va_region *reg;
+	u32 count = alloc_info->count;
+	u64 *ptr, new_addr;
+	u32 i;
+	int ret;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	if (alloc_info->blocked) {
+		list_del(&queue->jit_blocked);
+		alloc_info->blocked = false;
+	}
+
+	if (WARN_ON(!info))
+		return -EINVAL;
+
+	/* Check if all JIT IDs are not in use */
+	for (i = 0; i < count; i++, info++) {
+		/* The JIT ID is still in use so fail the allocation */
+		if (kctx->jit_alloc[info->id]) {
+			dev_warn(kctx->kbdev->dev, "JIT ID still in use\n");
+			return -EINVAL;
+		}
+	}
+
+	/* Now start the allocation loop */
+	for (i = 0, info = alloc_info->info; i < count; i++, info++) {
+		if (kctx->jit_alloc[info->id]) {
+			/* The JIT ID is duplicated in this command. Roll back
+			 * previous allocations and fail.
+			 */
+			dev_warn(kctx->kbdev->dev, "JIT ID is duplicated\n");
+			ret = -EINVAL;
+			goto fail;
+		}
+
+		/* Create a JIT allocation */
+		reg = kbase_jit_allocate(kctx, info, true);
+		if (!reg) {
+			bool can_block = false;
+			struct kbase_kcpu_command const *jit_cmd;
+
+			list_for_each_entry(jit_cmd, &kctx->csf.kcpu_queues.jit_cmds_head, info.jit_alloc.node) {
+				if (jit_cmd == cmd)
+					break;
+
+				if (jit_cmd->type == BASE_KCPU_COMMAND_TYPE_JIT_FREE) {
+					u8 const*const free_ids = jit_cmd->info.jit_free.ids;
+
+					if (free_ids && *free_ids && kctx->jit_alloc[*free_ids]) {
+						/**
+						 * A JIT free which is active
+						 * and submitted before this
+						 * command.
+						 */
+						can_block = true;
+						break;
+					}
+				}
+			}
+
+			if (!can_block) {
+				/**
+				 * No prior JIT_FREE command is active. Roll
+				 * back previous allocations and fail.
+				 */
+				dev_warn_ratelimited(kctx->kbdev->dev, "JIT alloc command failed: %p\n", cmd);
+				ret = -ENOMEM;
+				goto fail;
+			}
+
+			/* There are pending frees for an active allocation
+			 * so we should wait to see whether they free the
+			 * memory. Add to the list of atoms for which JIT
+			 * allocation is pending.
+			 */
+			kbase_jit_add_to_pending_alloc_list(queue, cmd);
+			alloc_info->blocked = true;
+
+			/* Rollback, the whole set will be re-attempted */
+			while (i-- > 0) {
+				info--;
+				kbase_jit_free(kctx, kctx->jit_alloc[info->id]);
+				kctx->jit_alloc[info->id] = NULL;
+			}
+
+			return -EAGAIN;
+		}
+
+		/* Bind it to the user provided ID. */
+		kctx->jit_alloc[info->id] = reg;
+	}
+
+	for (i = 0, info = alloc_info->info; i < count; i++, info++) {
+		/*
+		 * Write the address of the JIT allocation to the user provided
+		 * GPU allocation.
+		 */
+		ptr = kbase_vmap(kctx, info->gpu_alloc_addr, sizeof(*ptr),
+				&mapping);
+		if (!ptr) {
+			ret = -ENOMEM;
+			goto fail;
+		}
+
+		reg = kctx->jit_alloc[info->id];
+		new_addr = reg->start_pfn << PAGE_SHIFT;
+		*ptr = new_addr;
+		kbase_vunmap(kctx, &mapping);
+	}
+
+	return 0;
+
+fail:
+	/* Roll back completely */
+	for (i = 0, info = alloc_info->info; i < count; i++, info++) {
+		/* Free the allocations that were successful.
+		 * Mark all the allocations including the failed one and the
+		 * other un-attempted allocations in the set, so we know they
+		 * are in use.
+		 */
+		if (kctx->jit_alloc[info->id])
+			kbase_jit_free(kctx, kctx->jit_alloc[info->id]);
+
+		kctx->jit_alloc[info->id] = KBASE_RESERVED_REG_JIT_ALLOC;
+	}
+
+	return ret;
+}
+
+static int kbase_kcpu_jit_allocate_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_jit_alloc_info *alloc_info,
+		struct kbase_kcpu_command *current_command)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+	void __user *data = u64_to_user_ptr(alloc_info->info);
+	struct base_jit_alloc_info *info;
+	u32 count = alloc_info->count;
+	int ret = 0;
+	u32 i;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	if (!data || count > kcpu_queue->kctx->jit_max_allocations ||
+			count > ARRAY_SIZE(kctx->jit_alloc)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	info = kmalloc_array(count, sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (copy_from_user(info, data, sizeof(*info) * count) != 0) {
+		ret = -EINVAL;
+		goto out_free;
+	}
+
+	for (i = 0; i < count; i++) {
+		ret = kbasep_jit_alloc_validate(kctx, &info[i]);
+		if (ret)
+			goto out_free;
+	}
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_JIT_ALLOC;
+	list_add_tail(&current_command->info.jit_alloc.node,
+			&kctx->csf.kcpu_queues.jit_cmds_head);
+	current_command->info.jit_alloc.info = info;
+	current_command->info.jit_alloc.count = count;
+	current_command->info.jit_alloc.blocked = false;
+
+	return 0;
+out_free:
+	kfree(info);
+out:
+	return ret;
+}
+
+/**
+ * kbase_kcpu_jit_allocate_finish() - Finish handling the JIT_ALLOC command
+ *
+ * @queue: The queue containing this JIT allocation
+ * @cmd:  The JIT allocation command
+ */
+static void kbase_kcpu_jit_allocate_finish(
+		struct kbase_kcpu_command_queue *queue,
+		struct kbase_kcpu_command *cmd)
+{
+	lockdep_assert_held(&queue->kctx->csf.kcpu_queues.lock);
+
+	/* Remove this command from the jit_cmds_head list */
+	list_del(&cmd->info.jit_alloc.node);
+
+	/**
+	 * If we get to this point we must have already cleared the blocked
+	 * flag, otherwise it'd be a bug.
+	 */
+	if (WARN_ON(cmd->info.jit_alloc.blocked)) {
+		list_del(&queue->jit_blocked);
+		cmd->info.jit_alloc.blocked = false;
+	}
+
+	kfree(cmd->info.jit_alloc.info);
+}
+
+/**
+ * kbase_kcpu_jit_retry_pending_allocs() - Retry blocked JIT_ALLOC commands
+ *
+ * @kctx: The context containing the blocked JIT_ALLOC commands
+ */
+static void kbase_kcpu_jit_retry_pending_allocs(struct kbase_context *kctx)
+{
+	struct kbase_kcpu_command_queue *blocked_queue;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	/**
+	 * Reschedule all queues blocked by JIT_ALLOC commands.
+	 * NOTE: This code traverses the list of blocked queues directly. It
+	 * only works as long as the queued works are not executed at the same
+	 * time. This precondition is true since we're holding the
+	 * kbase_csf_kcpu_queue_context.lock .
+	 */
+	list_for_each_entry(blocked_queue,
+			&kctx->csf.kcpu_queues.jit_blocked_queues, jit_blocked)
+		queue_work(kctx->csf.kcpu_queues.wq, &blocked_queue->work);
+}
+
+static int kbase_kcpu_jit_free_process(struct kbase_context *kctx,
+		struct kbase_kcpu_command *const cmd)
+{
+	struct kbase_kcpu_command_jit_free_info *const free_info =
+			&cmd->info.jit_free;
+	u8 *ids = free_info->ids;
+	u32 count = free_info->count;
+	u32 i;
+
+	if (WARN_ON(!ids))
+		return -EINVAL;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	for (i = 0; i < count; i++, ids++) {
+		if ((*ids == 0) || (kctx->jit_alloc[*ids] == NULL)) {
+			dev_warn(kctx->kbdev->dev, "invalid JIT free ID\n");
+		} else {
+			/* If the ID is valid but the allocation request
+			 * failed, still succeed this command but don't
+			 * try and free the allocation.
+			 */
+			if (kctx->jit_alloc[*ids] !=
+					KBASE_RESERVED_REG_JIT_ALLOC)
+				kbase_jit_free(kctx, kctx->jit_alloc[*ids]);
+
+			kctx->jit_alloc[*ids] = NULL;
+		}
+	}
+
+	/* Free the list of ids */
+	kfree(free_info->ids);
+
+	/**
+	 * Remove this command from the jit_cmds_head list and retry pending
+	 * allocations.
+	 */
+	list_del(&cmd->info.jit_free.node);
+	kbase_kcpu_jit_retry_pending_allocs(kctx);
+
+	return 0;
+}
+
+static int kbase_kcpu_jit_free_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_jit_free_info *free_info,
+		struct kbase_kcpu_command *current_command)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+	void __user *data = u64_to_user_ptr(free_info->ids);
+	u8 *ids;
+	u32 count = free_info->count;
+	int ret;
+	u32 i;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	/* Sanity checks */
+	if (!count || count > ARRAY_SIZE(kctx->jit_alloc)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* Copy the information for safe access and future storage */
+	ids = kmalloc_array(count, sizeof(*ids), GFP_KERNEL);
+	if (!ids) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (!data) {
+		ret = -EINVAL;
+		goto out_free;
+	}
+
+	if (copy_from_user(ids, data, sizeof(*ids) * count)) {
+		ret = -EINVAL;
+		goto out_free;
+	}
+
+	for (i = 0; i < count; i++) {
+		/* Fail the command if ID sent is zero */
+		if (!ids[i]) {
+			ret = -EINVAL;
+			goto out_free;
+		}
+	}
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_JIT_FREE;
+	list_add_tail(&current_command->info.jit_free.node,
+			&kctx->csf.kcpu_queues.jit_cmds_head);
+	current_command->info.jit_free.ids = ids;
+	current_command->info.jit_free.count = count;
+
+	return 0;
+out_free:
+	kfree(ids);
+out:
+	return ret;
+}
+
+static int kbase_csf_queue_group_suspend_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_group_suspend_info *suspend_buf,
+		struct kbase_kcpu_command *current_command)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+	struct kbase_suspend_copy_buffer *sus_buf = NULL;
+	u64 addr = suspend_buf->buffer;
+	u64 page_addr = addr & PAGE_MASK;
+	u64 end_addr = addr + suspend_buf->size - 1;
+	u64 last_page_addr = end_addr & PAGE_MASK;
+	int nr_pages = (last_page_addr - page_addr) / PAGE_SIZE + 1;
+	int pinned_pages;
+	int ret = 0;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	if (suspend_buf->size <
+			kctx->kbdev->csf.global_iface.groups[0].suspend_size)
+		return -EINVAL;
+
+	ret = kbase_csf_queue_group_handle_is_valid(kctx,
+			suspend_buf->group_handle);
+	if (ret)
+		return ret;
+
+	sus_buf = kzalloc(sizeof(*sus_buf), GFP_KERNEL);
+	if (!sus_buf)
+		return -ENOMEM;
+
+	sus_buf->size = suspend_buf->size;
+	sus_buf->nr_pages = nr_pages;
+	sus_buf->offset = addr & ~PAGE_MASK;
+
+	sus_buf->pages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);
+	if (!sus_buf->pages) {
+		ret = -ENOMEM;
+		goto out_clean_sus_buf;
+	}
+
+	pinned_pages = get_user_pages_fast(page_addr, nr_pages, 1,
+			sus_buf->pages);
+	if (pinned_pages < 0) {
+		ret = pinned_pages;
+		goto out_clean_pages;
+	}
+	if (pinned_pages != nr_pages) {
+		ret = -EINVAL;
+		goto out_clean_pages;
+	}
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND;
+	current_command->info.suspend_buf_copy.sus_buf = sus_buf;
+	current_command->info.suspend_buf_copy.group_handle =
+				suspend_buf->group_handle;
+	return ret;
+
+out_clean_pages:
+	kfree(sus_buf->pages);
+out_clean_sus_buf:
+	kfree(sus_buf);
+	return ret;
+}
+
+static int kbase_csf_queue_group_suspend_process(struct kbase_context *kctx,
+		struct kbase_suspend_copy_buffer *sus_buf,
+		u8 group_handle)
+{
+	return kbase_csf_queue_group_suspend(kctx, sus_buf, group_handle);
+}
+
+static enum kbase_csf_event_callback_action event_cqs_callback(void *param)
+{
+	struct kbase_kcpu_command_queue *kcpu_queue =
+		(struct kbase_kcpu_command_queue *)param;
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+
+	queue_work(kctx->csf.kcpu_queues.wq, &kcpu_queue->work);
+
+	return KBASE_CSF_EVENT_CALLBACK_KEEP;
+}
+
+static void cleanup_cqs_wait(struct kbase_kcpu_command_queue *queue,
+		struct kbase_kcpu_command_cqs_wait_info *cqs_wait)
+{
+	WARN_ON(!cqs_wait->nr_objs);
+	WARN_ON(!cqs_wait->objs);
+	WARN_ON(!cqs_wait->signaled);
+	WARN_ON(!queue->cqs_wait_count);
+
+	if (--queue->cqs_wait_count == 0) {
+		kbase_csf_event_wait_remove(queue->kctx,
+				event_cqs_callback, queue);
+	}
+
+	kfree(cqs_wait->signaled);
+	kfree(cqs_wait->objs);
+	cqs_wait->signaled = NULL;
+	cqs_wait->objs = NULL;
+}
+
+static int kbase_kcpu_cqs_wait_process(struct kbase_device *kbdev,
+		struct kbase_kcpu_command_queue *queue,
+		struct kbase_kcpu_command_cqs_wait_info *cqs_wait)
+{
+	u32 i;
+
+	lockdep_assert_held(&queue->kctx->csf.kcpu_queues.lock);
+
+	if (WARN_ON(!cqs_wait->nr_objs))
+		return -EINVAL;
+
+	if (WARN_ON(!cqs_wait->objs))
+		return -EINVAL;
+
+
+	/* Skip the CQS waits that have already been signaled when processing */
+	for (i = find_first_zero_bit(cqs_wait->signaled, cqs_wait->nr_objs); i < cqs_wait->nr_objs; i++) {
+		if (!test_bit(i, cqs_wait->signaled)) {
+			struct kbase_vmap_struct *mapping;
+			bool sig_set;
+			u32 *evt = (u32 *)kbase_phy_alloc_mapping_get(queue->kctx,
+						cqs_wait->objs[i].addr, &mapping);
+
+			if (!queue->command_started) {
+				KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_START(
+					kbdev, queue);
+				queue->command_started = true;
+			}
+
+			if (WARN_ON(!evt)) {
+				queue->has_error = true;
+				return -EINVAL;
+			}
+
+			sig_set = evt[BASEP_EVENT_VAL_INDEX] > cqs_wait->objs[i].val;
+			if (sig_set) {
+				bitmap_set(cqs_wait->signaled, i, 1);
+				if ((cqs_wait->inherit_err_flags & (1U << i)) &&
+				    evt[BASEP_EVENT_ERR_INDEX] > 0)
+					queue->has_error = true;
+
+				KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_END(
+					kbdev, queue);
+				queue->command_started = false;
+			}
+
+			kbase_phy_alloc_mapping_put(queue->kctx, mapping);
+
+			if (!sig_set)
+				break;
+		}
+	}
+
+	/* For the queue to progress further, all cqs objects should get
+	 * signaled.
+	 */
+	return bitmap_full(cqs_wait->signaled, cqs_wait->nr_objs);
+}
+
+static int kbase_kcpu_cqs_wait_prepare(struct kbase_kcpu_command_queue *queue,
+		struct base_kcpu_command_cqs_wait_info *cqs_wait_info,
+		struct kbase_kcpu_command *current_command)
+{
+	struct base_cqs_wait *objs;
+	unsigned int nr_objs = cqs_wait_info->nr_objs;
+
+	lockdep_assert_held(&queue->kctx->csf.kcpu_queues.lock);
+
+	if (cqs_wait_info->nr_objs > BASEP_KCPU_CQS_MAX_NUM_OBJS)
+		return -EINVAL;
+
+	objs = kcalloc(nr_objs, sizeof(*objs), GFP_KERNEL);
+	if (!objs)
+		return -ENOMEM;
+
+	if (copy_from_user(objs, u64_to_user_ptr(cqs_wait_info->objs),
+			nr_objs * sizeof(*objs))) {
+		kfree(objs);
+		return -ENOMEM;
+	}
+
+	if (++queue->cqs_wait_count == 1) {
+		if (kbase_csf_event_wait_add(queue->kctx,
+				event_cqs_callback, queue)) {
+			kfree(objs);
+			return -ENOMEM;
+		}
+	}
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_CQS_WAIT;
+	current_command->info.cqs_wait.nr_objs = nr_objs;
+	current_command->info.cqs_wait.objs = objs;
+	current_command->info.cqs_wait.inherit_err_flags =
+					cqs_wait_info->inherit_err_flags;
+
+	current_command->info.cqs_wait.signaled = kcalloc(BITS_TO_LONGS(nr_objs),
+		sizeof(*current_command->info.cqs_wait.signaled), GFP_KERNEL);
+	if (!current_command->info.cqs_wait.signaled)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void kbase_kcpu_cqs_set_process(struct kbase_device *kbdev,
+		struct kbase_kcpu_command_queue *queue,
+		struct kbase_kcpu_command_cqs_set_info *cqs_set)
+{
+	unsigned int i;
+
+	lockdep_assert_held(&queue->kctx->csf.kcpu_queues.lock);
+
+	WARN_ON(!cqs_set->nr_objs);
+	WARN_ON(!cqs_set->objs);
+
+	for (i = 0; i < cqs_set->nr_objs; i++) {
+		struct kbase_vmap_struct *mapping;
+		u32 *evt = (u32 *)kbase_phy_alloc_mapping_get(queue->kctx,
+					cqs_set->objs[i].addr, &mapping);
+		KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_SET(kbdev, queue);
+		if (WARN_ON(!evt))
+			queue->has_error = true;
+		else {
+			if (cqs_set->propagate_flags & (1 << i))
+				evt[BASEP_EVENT_ERR_INDEX] = queue->has_error;
+			else
+				evt[BASEP_EVENT_ERR_INDEX] = false;
+			/* Set to signaled */
+			evt[BASEP_EVENT_VAL_INDEX]++;
+			kbase_phy_alloc_mapping_put(queue->kctx, mapping);
+		}
+	}
+
+	kbase_csf_event_signal_notify_gpu(queue->kctx);
+
+	kfree(cqs_set->objs);
+	cqs_set->objs = NULL;
+}
+
+static int kbase_kcpu_cqs_set_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_cqs_set_info *cqs_set_info,
+		struct kbase_kcpu_command *current_command)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+	struct base_cqs_set *objs;
+	unsigned int nr_objs = cqs_set_info->nr_objs;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	if (cqs_set_info->nr_objs > BASEP_KCPU_CQS_MAX_NUM_OBJS)
+		return -EINVAL;
+
+	objs = kcalloc(nr_objs, sizeof(*objs), GFP_KERNEL);
+	if (!objs)
+		return -ENOMEM;
+
+	if (copy_from_user(objs, u64_to_user_ptr(cqs_set_info->objs),
+			nr_objs * sizeof(*objs))) {
+		kfree(objs);
+		return -ENOMEM;
+	}
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_CQS_SET;
+	current_command->info.cqs_set.nr_objs = nr_objs;
+	current_command->info.cqs_set.objs = objs;
+	current_command->info.cqs_set.propagate_flags =
+					cqs_set_info->propagate_flags;
+
+	return 0;
+}
+
+#ifdef CONFIG_SYNC_FILE
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+static void kbase_csf_fence_wait_callback(struct fence *fence,
+			struct fence_cb *cb)
+#else
+static void kbase_csf_fence_wait_callback(struct dma_fence *fence,
+			struct dma_fence_cb *cb)
+#endif
+{
+	struct kbase_kcpu_command_fence_info *fence_info = container_of(cb,
+			struct kbase_kcpu_command_fence_info, fence_cb);
+	struct kbase_kcpu_command_queue *kcpu_queue = fence_info->kcpu_queue;
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+
+	/* Resume kcpu command queue processing. */
+	queue_work(kctx->csf.kcpu_queues.wq, &kcpu_queue->work);
+}
+
+static void kbase_kcpu_fence_wait_cancel(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct kbase_kcpu_command_fence_info *fence_info)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	if (WARN_ON(!fence_info->fence))
+		return;
+
+	if (kcpu_queue->fence_wait_processed) {
+		dma_fence_remove_callback(fence_info->fence,
+				&fence_info->fence_cb);
+	}
+
+	/* Release the reference which is kept by the kcpu_queue */
+	kbase_fence_put(fence_info->fence);
+	kcpu_queue->fence_wait_processed = false;
+
+	fence_info->fence = NULL;
+}
+
+/**
+ * kbase_kcpu_fence_wait_process() - Process the kcpu fence wait command
+ *
+ * @kcpu_queue: The queue containing the fence wait command
+ * @fence_info: Reference to a fence for which the command is waiting
+ *
+ * Return: 0 if fence wait is blocked, 1 if it is unblocked, negative error if
+ *         an error has occurred and fence should no longer be waited on.
+ */
+static int kbase_kcpu_fence_wait_process(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct kbase_kcpu_command_fence_info *fence_info)
+{
+	int fence_status = 0;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence;
+#else
+	struct dma_fence *fence;
+#endif
+
+	lockdep_assert_held(&kcpu_queue->kctx->csf.kcpu_queues.lock);
+
+	if (WARN_ON(!fence_info->fence))
+		return -EINVAL;
+
+	fence = fence_info->fence;
+
+	if (kcpu_queue->fence_wait_processed) {
+		fence_status = dma_fence_get_status(fence);
+	} else {
+		int cb_err = dma_fence_add_callback(fence,
+			&fence_info->fence_cb,
+			kbase_csf_fence_wait_callback);
+
+		fence_status = cb_err;
+		if (cb_err == 0)
+			kcpu_queue->fence_wait_processed = true;
+		else if (cb_err == -ENOENT)
+			fence_status = dma_fence_get_status(fence);
+	}
+
+	/*
+	 * At this point fence status can contain 3 types of values:
+	 * - Value 0 to represent that fence in question is not signalled yet
+	 * - Value 1 to represent that fence in question is signalled without
+	 *   errors
+	 * - Negative error code to represent that some error has occurred such
+	 *   that waiting on it is no longer valid.
+	 */
+
+	if (fence_status)
+		kbase_kcpu_fence_wait_cancel(kcpu_queue, fence_info);
+
+	return fence_status;
+}
+
+static int kbase_kcpu_fence_wait_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_fence_info *fence_info,
+		struct kbase_kcpu_command *current_command)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence_in;
+#else
+	struct dma_fence *fence_in;
+#endif
+	struct base_fence fence;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	if (copy_from_user(&fence, u64_to_user_ptr(fence_info->fence),
+			sizeof(fence)))
+		return -ENOMEM;
+
+	fence_in = sync_file_get_fence(fence.basep.fd);
+
+	if (!fence_in)
+		return -ENOENT;
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_FENCE_WAIT;
+	current_command->info.fence.fence = fence_in;
+	current_command->info.fence.kcpu_queue = kcpu_queue;
+
+	return 0;
+}
+
+static int kbase_kcpu_fence_signal_process(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct kbase_kcpu_command_fence_info *fence_info)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+	int ret;
+
+	if (WARN_ON(!fence_info->fence))
+		return -EINVAL;
+
+	ret = dma_fence_signal(fence_info->fence);
+
+	if (unlikely(ret < 0)) {
+		dev_warn(kctx->kbdev->dev,
+			"fence_signal() failed with %d\n", ret);
+	}
+
+	dma_fence_put(fence_info->fence);
+	fence_info->fence = NULL;
+
+	return ret;
+}
+
+static int kbase_kcpu_fence_signal_prepare(
+		struct kbase_kcpu_command_queue *kcpu_queue,
+		struct base_kcpu_command_fence_info *fence_info,
+		struct kbase_kcpu_command *current_command)
+{
+	struct kbase_context *const kctx = kcpu_queue->kctx;
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence *fence_out;
+#else
+	struct dma_fence *fence_out;
+#endif
+	struct base_fence fence;
+	struct sync_file *sync_file;
+	int ret = 0;
+	int fd;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	if (copy_from_user(&fence, u64_to_user_ptr(fence_info->fence),
+			sizeof(fence)))
+		return -EFAULT;
+
+	fence_out = kzalloc(sizeof(*fence_out), GFP_KERNEL);
+	if (!fence_out)
+		return -ENOMEM;
+
+	dma_fence_init(fence_out,
+		       &kbase_fence_ops,
+		       &kbase_csf_fence_lock,
+		       kcpu_queue->fence_context,
+		       ++kcpu_queue->fence_seqno);
+
+#if (KERNEL_VERSION(4, 9, 67) >= LINUX_VERSION_CODE)
+	/* Take an extra reference to the fence on behalf of the sync file.
+	 * This is only needded on older kernels where sync_file_create()
+	 * does not take its own reference. This was changed in v4.9.68
+	 * where sync_file_create() now takes its own reference.
+	 */
+	dma_fence_get(fence_out);
+#endif
+
+	/* create a sync_file fd representing the fence */
+	sync_file = sync_file_create(fence_out);
+	if (!sync_file) {
+#if (KERNEL_VERSION(4, 9, 67) >= LINUX_VERSION_CODE)
+		dma_fence_put(fence_out);
+#endif
+		ret = -ENOMEM;
+		goto file_create_fail;
+	}
+
+	fd = get_unused_fd_flags(O_CLOEXEC);
+	if (fd < 0) {
+		ret = fd;
+		goto fd_flags_fail;
+	}
+
+	fd_install(fd, sync_file->file);
+
+	fence.basep.fd = fd;
+
+	current_command->type = BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL;
+	current_command->info.fence.fence = fence_out;
+
+	if (copy_to_user(u64_to_user_ptr(fence_info->fence), &fence,
+			sizeof(fence))) {
+		ret = -EFAULT;
+		goto fd_flags_fail;
+	}
+
+	return 0;
+
+fd_flags_fail:
+	fput(sync_file->file);
+file_create_fail:
+	dma_fence_put(fence_out);
+
+	return ret;
+}
+#endif /* CONFIG_SYNC_FILE */
+
+static void kcpu_queue_process_worker(struct work_struct *data)
+{
+	struct kbase_kcpu_command_queue *queue = container_of(data,
+				struct kbase_kcpu_command_queue, work);
+
+	mutex_lock(&queue->kctx->csf.kcpu_queues.lock);
+
+	kcpu_queue_process(queue, false);
+
+	mutex_unlock(&queue->kctx->csf.kcpu_queues.lock);
+}
+
+static int delete_queue(struct kbase_context *kctx, u32 id)
+{
+	int err = 0;
+
+	mutex_lock(&kctx->csf.kcpu_queues.lock);
+
+	if ((id < KBASEP_MAX_KCPU_QUEUES) && kctx->csf.kcpu_queues.array[id]) {
+		struct kbase_kcpu_command_queue *queue =
+					kctx->csf.kcpu_queues.array[id];
+
+		/* Drain the remaining work for this queue first and go past
+		 * all the waits.
+		 */
+		kcpu_queue_process(queue, true);
+
+		/* All commands should have been processed */
+		WARN_ON(queue->num_pending_cmds);
+
+		/* All CQS wait commands should have been cleaned up */
+		WARN_ON(queue->cqs_wait_count);
+
+		kctx->csf.kcpu_queues.array[id] = NULL;
+		bitmap_clear(kctx->csf.kcpu_queues.in_use, id, 1);
+
+		/* Fire the tracepoint with the mutex held to enforce correct
+		 * ordering with the summary stream.
+		 */
+		KBASE_TLSTREAM_TL_KBASE_DEL_KCPUQUEUE(kctx->kbdev, queue);
+
+		mutex_unlock(&kctx->csf.kcpu_queues.lock);
+
+		cancel_work_sync(&queue->work);
+
+		kfree(queue);
+	} else {
+		dev_warn(kctx->kbdev->dev,
+			"Attempt to delete a non-existent KCPU queue\n");
+		mutex_unlock(&kctx->csf.kcpu_queues.lock);
+		err = -EINVAL;
+	}
+	return err;
+}
+
+static void KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_INFO(
+	struct kbase_device *kbdev,
+	const struct kbase_kcpu_command_queue *queue,
+	const struct kbase_kcpu_command_jit_alloc_info *jit_alloc,
+	bool alloc_success)
+{
+	u8 i;
+
+	KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(
+		kbdev, queue);
+	for (i = 0; i < jit_alloc->count; i++) {
+		const u8 id = jit_alloc->info[i].id;
+		const struct kbase_va_region *reg = queue->kctx->jit_alloc[id];
+		u64 gpu_alloc_addr = 0;
+		u64 mmu_flags = 0;
+
+		if (alloc_success && !WARN_ON(!reg) &&
+			!WARN_ON(reg == KBASE_RESERVED_REG_JIT_ALLOC)) {
+#ifdef CONFIG_MALI_VECTOR_DUMP
+			struct tagged_addr phy = {0};
+#endif /* CONFIG_MALI_VECTOR_DUMP */
+
+			gpu_alloc_addr = reg->start_pfn << PAGE_SHIFT;
+#ifdef CONFIG_MALI_VECTOR_DUMP
+			mmu_flags = kbase_mmu_create_ate(kbdev,
+				phy, reg->flags,
+				MIDGARD_MMU_BOTTOMLEVEL,
+				queue->kctx->jit_group_id);
+#endif /* CONFIG_MALI_VECTOR_DUMP */
+		}
+		KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(
+			kbdev, queue, gpu_alloc_addr, mmu_flags);
+	}
+}
+
+static void KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(
+	struct kbase_device *kbdev,
+	const struct kbase_kcpu_command_queue *queue)
+{
+	KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(
+		kbdev, queue);
+}
+
+static void KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_INFO(
+	struct kbase_device *kbdev,
+	const struct kbase_kcpu_command_queue *queue,
+	const struct kbase_kcpu_command_jit_free_info *jit_free)
+{
+	u8 i;
+
+	KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_FREE_END(
+		kbdev, queue);
+	for (i = 0; i < jit_free->count; i++) {
+		const u8 id = jit_free->ids[i];
+		u64 pages_used = 0;
+
+		if (id != 0) {
+			const struct kbase_va_region *reg =
+				queue->kctx->jit_alloc[id];
+			if (reg && (reg != KBASE_RESERVED_REG_JIT_ALLOC))
+				pages_used = reg->gpu_alloc->nents;
+		}
+		KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_FREE_END(
+			kbdev, queue, pages_used);
+	}
+}
+
+static void KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_END(
+	struct kbase_device *kbdev,
+	const struct kbase_kcpu_command_queue *queue)
+{
+	KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_FREE_END(
+		kbdev, queue);
+}
+
+static void kcpu_queue_process(struct kbase_kcpu_command_queue *queue,
+			bool ignore_waits)
+{
+	struct kbase_device *kbdev = queue->kctx->kbdev;
+	bool process_next = true;
+	size_t i;
+
+	lockdep_assert_held(&queue->kctx->csf.kcpu_queues.lock);
+
+	for (i = 0; i != queue->num_pending_cmds; ++i) {
+		struct kbase_kcpu_command *cmd =
+			&queue->commands[(u8)(queue->start_offset + i)];
+		int status;
+
+		switch (cmd->type) {
+		case BASE_KCPU_COMMAND_TYPE_FENCE_WAIT:
+			if (!queue->command_started) {
+				KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_START(
+					kbdev, queue);
+				queue->command_started = true;
+			}
+
+#ifdef CONFIG_SYNC_FILE
+			status = 0;
+
+
+			if (ignore_waits) {
+				kbase_kcpu_fence_wait_cancel(queue,
+					&cmd->info.fence);
+			} else {
+				status = kbase_kcpu_fence_wait_process(queue,
+					&cmd->info.fence);
+
+				if (status == 0)
+					process_next = false;
+				else if (status < 0)
+					queue->has_error = true;
+			}
+#else
+			dev_warn(kbdev->dev,
+				"unexpected fence wait command found\n");
+#endif
+
+			if (process_next) {
+				KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_END(
+					kbdev, queue);
+				queue->command_started = false;
+			}
+			break;
+		case BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL:
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_START(
+				kbdev, queue);
+
+#ifdef CONFIG_SYNC_FILE
+			kbase_kcpu_fence_signal_process(queue,
+						&cmd->info.fence);
+#else
+			dev_warn(kbdev->dev,
+				"unexpected fence signal command found\n");
+#endif
+
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_END(
+				kbdev, queue);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_CQS_WAIT:
+			status = kbase_kcpu_cqs_wait_process(kbdev, queue,
+						&cmd->info.cqs_wait);
+
+			if (!status && !ignore_waits) {
+				process_next = false;
+			} else {
+				/* Either all CQS objects were signaled or
+				 * there was an error or the queue itself is
+				 * being deleted.
+				 * In all cases can move to the next command.
+				 * TBD: handle the error
+				 */
+				cleanup_cqs_wait(queue,	&cmd->info.cqs_wait);
+			}
+
+			break;
+		case BASE_KCPU_COMMAND_TYPE_CQS_SET:
+			kbase_kcpu_cqs_set_process(kbdev, queue,
+				&cmd->info.cqs_set);
+
+			/* CQS sets are only traced before execution */
+			break;
+		case BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER:
+			/* Clear the queue's error state */
+			queue->has_error = false;
+			break;
+		case BASE_KCPU_COMMAND_TYPE_MAP_IMPORT:
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START(
+				kbdev, queue);
+
+			kbase_gpu_vm_lock(queue->kctx);
+			kbase_sticky_resource_acquire(queue->kctx,
+						cmd->info.import.gpu_va);
+			kbase_gpu_vm_unlock(queue->kctx);
+
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_END(
+				kbdev, queue);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT:
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_START(
+				kbdev, queue);
+
+			kbase_gpu_vm_lock(queue->kctx);
+			kbase_sticky_resource_release(queue->kctx, NULL,
+						cmd->info.import.gpu_va);
+			kbase_gpu_vm_unlock(queue->kctx);
+
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_END(
+				kbdev, queue);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE:
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_START(
+					kbdev, queue);
+
+			kbase_gpu_vm_lock(queue->kctx);
+			kbase_sticky_resource_release_force(queue->kctx, NULL,
+						cmd->info.import.gpu_va);
+			kbase_gpu_vm_unlock(queue->kctx);
+
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_END(
+					kbdev, queue);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_JIT_ALLOC:
+		{
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_START(
+				kbdev, queue);
+
+			status = kbase_kcpu_jit_allocate_process(queue, cmd);
+			if (status == -EAGAIN) {
+				process_next = false;
+			} else {
+				if (status != 0)
+					queue->has_error = true;
+
+				KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_INFO(
+						kbdev, queue, &cmd->info.jit_alloc, (status == 0));
+
+				kbase_kcpu_jit_allocate_finish(queue, cmd);
+				KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(
+						kbdev, queue);
+			}
+			break;
+		}
+		case BASE_KCPU_COMMAND_TYPE_JIT_FREE:
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_START(
+				kbdev, queue);
+
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_INFO(
+				kbdev, queue, &cmd->info.jit_free);
+
+			status = kbase_kcpu_jit_free_process(queue->kctx, cmd);
+			if (status)
+				queue->has_error = true;
+
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_END(
+				kbdev, queue);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND:
+			status = kbase_csf_queue_group_suspend_process(
+					queue->kctx,
+					cmd->info.suspend_buf_copy.sus_buf,
+					cmd->info.suspend_buf_copy.group_handle);
+			if (status)
+				queue->has_error = true;
+
+			kfree(cmd->info.suspend_buf_copy.sus_buf->pages);
+			kfree(cmd->info.suspend_buf_copy.sus_buf);
+			break;
+		default:
+			dev_warn(kbdev->dev,
+				"Unrecognized command type\n");
+			break;
+		} /* switch */
+
+		/*TBD: error handling */
+
+		if (!process_next)
+			break;
+	}
+
+	if (i > 0) {
+		queue->start_offset += i;
+		queue->num_pending_cmds -= i;
+
+		/* If an attempt to enqueue commands failed then we must raise
+		 * an event in case the client wants to retry now that there is
+		 * free space in the buffer.
+		 */
+		if (queue->enqueue_failed) {
+			queue->enqueue_failed = false;
+			kbase_csf_event_signal_cpu_only(queue->kctx);
+		}
+	}
+}
+
+static size_t kcpu_queue_get_space(struct kbase_kcpu_command_queue *queue)
+{
+	return KBASEP_KCPU_QUEUE_SIZE - queue->num_pending_cmds;
+}
+
+static void KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_COMMAND(
+	const struct kbase_kcpu_command_queue *queue,
+	const struct kbase_kcpu_command *cmd)
+{
+	struct kbase_device *kbdev = queue->kctx->kbdev;
+
+	switch (cmd->type) {
+	case BASE_KCPU_COMMAND_TYPE_FENCE_WAIT:
+		KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_WAIT(
+			kbdev, queue, cmd->info.fence.fence);
+		break;
+	case BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL:
+		KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_SIGNAL(
+			kbdev, queue, cmd->info.fence.fence);
+		break;
+	case BASE_KCPU_COMMAND_TYPE_CQS_WAIT:
+	{
+		const struct base_cqs_wait *waits = cmd->info.cqs_wait.objs;
+		unsigned int i;
+
+		for (i = 0; i < cmd->info.cqs_wait.nr_objs; i++) {
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_WAIT(
+				kbdev, queue, waits[i].addr, waits[i].val);
+		}
+		break;
+	}
+	case BASE_KCPU_COMMAND_TYPE_CQS_SET:
+	{
+		const struct base_cqs_set *sets = cmd->info.cqs_set.objs;
+		unsigned int i;
+
+		for (i = 0; i < cmd->info.cqs_set.nr_objs; i++) {
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_SET(
+				kbdev, queue, sets[i].addr);
+		}
+		break;
+	}
+	case BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER:
+		/* No implemented tracepoint */
+		break;
+	case BASE_KCPU_COMMAND_TYPE_MAP_IMPORT:
+		KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_MAP_IMPORT(
+			kbdev, queue, cmd->info.import.gpu_va);
+		break;
+	case BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT:
+		KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT(
+			kbdev, queue, cmd->info.import.gpu_va);
+		break;
+	case BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE:
+		KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT_FORCE(
+			kbdev, queue, cmd->info.import.gpu_va);
+		break;
+	case BASE_KCPU_COMMAND_TYPE_JIT_ALLOC:
+	{
+		u8 i;
+
+		KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_ALLOC(
+			kbdev, queue);
+		for (i = 0; i < cmd->info.jit_alloc.count; i++) {
+			const struct base_jit_alloc_info *info =
+				&cmd->info.jit_alloc.info[i];
+
+			KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_ALLOC(
+				kbdev, queue,
+				info->gpu_alloc_addr, info->va_pages,
+				info->commit_pages, info->extent, info->id,
+				info->bin_id, info->max_allocations,
+				info->flags, info->usage_id);
+		}
+		KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_ALLOC(
+			kbdev, queue);
+		break;
+	}
+	case BASE_KCPU_COMMAND_TYPE_JIT_FREE:
+	{
+		u8 i;
+
+		KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_FREE(
+			kbdev, queue);
+		for (i = 0; i < cmd->info.jit_free.count; i++) {
+			KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_FREE(
+				kbdev, queue, cmd->info.jit_free.ids[i]);
+		}
+		KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_FREE(
+			kbdev, queue);
+		break;
+	}
+	case BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND:
+		/* No implemented tracepoint */
+		break;
+	}
+}
+
+int kbase_csf_kcpu_queue_enqueue(struct kbase_context *kctx,
+			struct kbase_ioctl_kcpu_queue_enqueue *enq)
+{
+	struct kbase_kcpu_command_queue *queue = NULL;
+	void __user *user_cmds = u64_to_user_ptr(enq->addr);
+	int ret = 0;
+	u32 i;
+
+	/* The offset to the first command that is being processed or yet to
+	 * be processed is of u8 type, so the number of commands inside the
+	 * queue cannot be more than 256.
+	 */
+	BUILD_BUG_ON(KBASEP_KCPU_QUEUE_SIZE > 256);
+
+	/* Whilst the backend interface allows enqueueing multiple commands in
+	 * a single operation, the Base interface does not expose any mechanism
+	 * to do so. And also right now the handling is missing for the case
+	 * where multiple commands are submitted and the enqueue of one of the
+	 * command in the set fails after successfully enqueuing other commands
+	 * in the set.
+	 */
+	if (enq->nr_commands != 1) {
+		dev_err(kctx->kbdev->dev,
+			"More than one commands enqueued\n");
+		return -EINVAL;
+	}
+
+	mutex_lock(&kctx->csf.kcpu_queues.lock);
+
+	if (!kctx->csf.kcpu_queues.array[enq->id]) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	queue = kctx->csf.kcpu_queues.array[enq->id];
+
+	if (kcpu_queue_get_space(queue) < enq->nr_commands) {
+		ret = -EBUSY;
+		queue->enqueue_failed = true;
+		goto out;
+	}
+
+	/* Copy all command's info to the command buffer.
+	 * Note: it would be more efficient to process all commands in-line
+	 * until we encounter an unresolved CQS_ / FENCE_WAIT, however, the
+	 * interface allows multiple commands to be enqueued so we must account
+	 * for the possibility to roll back.
+	 */
+
+	for (i = 0; (i != enq->nr_commands) && !ret; ++i, ++kctx->csf.kcpu_queues.num_cmds) {
+		struct kbase_kcpu_command *kcpu_cmd =
+			&queue->commands[(u8)(queue->start_offset + queue->num_pending_cmds + i)];
+		struct base_kcpu_command command;
+		unsigned int j;
+
+		if (copy_from_user(&command, user_cmds, sizeof(command))) {
+			ret = -EFAULT;
+			goto out;
+		}
+
+		user_cmds = (void __user *)((uintptr_t)user_cmds +
+				sizeof(struct base_kcpu_command));
+
+		for (j = 0; j < sizeof(command.padding); j++) {
+			if (command.padding[j] != 0) {
+				dev_dbg(kctx->kbdev->dev,
+					"base_kcpu_command padding not 0\n");
+				ret = -EINVAL;
+				goto out;
+			}
+		}
+
+		kcpu_cmd->enqueue_ts = kctx->csf.kcpu_queues.num_cmds;
+		switch (command.type) {
+		case BASE_KCPU_COMMAND_TYPE_FENCE_WAIT:
+#ifdef CONFIG_SYNC_FILE
+			ret = kbase_kcpu_fence_wait_prepare(queue,
+						&command.info.fence, kcpu_cmd);
+#else
+			ret = -EINVAL;
+			dev_warn(kctx->kbdev->dev, "fence wait command unsupported\n");
+#endif
+			break;
+		case BASE_KCPU_COMMAND_TYPE_FENCE_SIGNAL:
+#ifdef CONFIG_SYNC_FILE
+			ret = kbase_kcpu_fence_signal_prepare(queue,
+						&command.info.fence, kcpu_cmd);
+#else
+			ret = -EINVAL;
+			dev_warn(kctx->kbdev->dev, "fence signal command unsupported\n");
+#endif
+			break;
+		case BASE_KCPU_COMMAND_TYPE_CQS_WAIT:
+			ret = kbase_kcpu_cqs_wait_prepare(queue,
+					&command.info.cqs_wait, kcpu_cmd);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_CQS_SET:
+			ret = kbase_kcpu_cqs_set_prepare(queue,
+					&command.info.cqs_set, kcpu_cmd);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER:
+			kcpu_cmd->type = BASE_KCPU_COMMAND_TYPE_ERROR_BARRIER;
+			ret = 0;
+			break;
+		case BASE_KCPU_COMMAND_TYPE_MAP_IMPORT:
+			ret = kbase_kcpu_map_import_prepare(queue,
+					&command.info.import, kcpu_cmd);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT:
+			ret = kbase_kcpu_unmap_import_prepare(queue,
+					&command.info.import, kcpu_cmd);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_UNMAP_IMPORT_FORCE:
+			ret = kbase_kcpu_unmap_import_force_prepare(queue,
+					&command.info.import, kcpu_cmd);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_JIT_ALLOC:
+			ret = kbase_kcpu_jit_allocate_prepare(queue,
+					&command.info.jit_alloc, kcpu_cmd);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_JIT_FREE:
+			ret = kbase_kcpu_jit_free_prepare(queue,
+					&command.info.jit_free, kcpu_cmd);
+			break;
+		case BASE_KCPU_COMMAND_TYPE_GROUP_SUSPEND:
+			ret = kbase_csf_queue_group_suspend_prepare(queue,
+					&command.info.suspend_buf_copy,
+					kcpu_cmd);
+			break;
+
+		default:
+			dev_warn(queue->kctx->kbdev->dev,
+				"Unknown command type %u\n", command.type);
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	if (!ret) {
+		/* We only instrument the enqueues after all commands have been
+		 * successfully enqueued, as if we do them during the enqueue
+		 * and there is an error, we won't be able to roll them back
+		 * like is done for the command enqueues themselves.
+		 */
+		for (i = 0; i != enq->nr_commands; ++i) {
+			u8 cmd_idx = (u8)(queue->start_offset + queue->num_pending_cmds + i);
+
+			KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_COMMAND(
+				queue, &queue->commands[cmd_idx]);
+		}
+
+		queue->num_pending_cmds += enq->nr_commands;
+		kcpu_queue_process(queue, false);
+	} else {
+		/* Roll back the number of enqueued commands */
+		kctx->csf.kcpu_queues.num_cmds -= i;
+	}
+
+out:
+	mutex_unlock(&kctx->csf.kcpu_queues.lock);
+
+	return ret;
+}
+
+int kbase_csf_kcpu_queue_context_init(struct kbase_context *kctx)
+{
+	int idx;
+
+	bitmap_zero(kctx->csf.kcpu_queues.in_use, KBASEP_MAX_KCPU_QUEUES);
+
+	for (idx = 0; idx < KBASEP_MAX_KCPU_QUEUES; ++idx)
+		kctx->csf.kcpu_queues.array[idx] = NULL;
+
+	kctx->csf.kcpu_queues.wq = alloc_workqueue("mali_kbase_csf_kcpu",
+					WQ_UNBOUND | WQ_HIGHPRI, 0);
+	if (!kctx->csf.kcpu_queues.wq)
+		return -ENOMEM;
+
+	mutex_init(&kctx->csf.kcpu_queues.lock);
+
+	kctx->csf.kcpu_queues.num_cmds = 0;
+
+	return 0;
+}
+
+void kbase_csf_kcpu_queue_context_term(struct kbase_context *kctx)
+{
+	while (!bitmap_empty(kctx->csf.kcpu_queues.in_use,
+			KBASEP_MAX_KCPU_QUEUES)) {
+		int id = find_first_bit(kctx->csf.kcpu_queues.in_use,
+				KBASEP_MAX_KCPU_QUEUES);
+
+		if (WARN_ON(!kctx->csf.kcpu_queues.array[id]))
+			clear_bit(id, kctx->csf.kcpu_queues.in_use);
+		else
+			(void)delete_queue(kctx, id);
+	}
+
+	destroy_workqueue(kctx->csf.kcpu_queues.wq);
+	mutex_destroy(&kctx->csf.kcpu_queues.lock);
+}
+
+int kbase_csf_kcpu_queue_delete(struct kbase_context *kctx,
+			struct kbase_ioctl_kcpu_queue_delete *del)
+{
+	return delete_queue(kctx, (u32)del->id);
+}
+
+int kbase_csf_kcpu_queue_new(struct kbase_context *kctx,
+			struct kbase_ioctl_kcpu_queue_new *newq)
+{
+	struct kbase_kcpu_command_queue *queue;
+	int idx;
+	int ret = 0;
+
+	/* The queue id is of u8 type and we use the index of the kcpu_queues
+	 * array as an id, so the number of elements in the array can't be
+	 * more than 256.
+	 */
+	BUILD_BUG_ON(KBASEP_MAX_KCPU_QUEUES > 256);
+
+	mutex_lock(&kctx->csf.kcpu_queues.lock);
+
+	idx = find_first_zero_bit(kctx->csf.kcpu_queues.in_use,
+			KBASEP_MAX_KCPU_QUEUES);
+	if (idx >= (int)KBASEP_MAX_KCPU_QUEUES) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (WARN_ON(kctx->csf.kcpu_queues.array[idx])) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	queue = kzalloc(sizeof(*queue), GFP_KERNEL);
+
+	if (!queue) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	bitmap_set(kctx->csf.kcpu_queues.in_use, idx, 1);
+	kctx->csf.kcpu_queues.array[idx] = queue;
+	queue->kctx = kctx;
+	queue->start_offset = 0;
+	queue->num_pending_cmds = 0;
+#ifdef CONFIG_SYNC_FILE
+	queue->fence_context = dma_fence_context_alloc(1);
+	queue->fence_seqno = 0;
+	queue->fence_wait_processed = false;
+#endif
+	queue->enqueue_failed = false;
+	queue->command_started = false;
+	INIT_LIST_HEAD(&queue->jit_blocked);
+	queue->has_error = false;
+	INIT_WORK(&queue->work, kcpu_queue_process_worker);
+
+	newq->id = idx;
+
+	/* Fire the tracepoint with the mutex held to enforce correct ordering
+	 * with the summary stream.
+	 */
+	KBASE_TLSTREAM_TL_KBASE_NEW_KCPUQUEUE(
+		kctx->kbdev, queue, kctx->id, queue->num_pending_cmds);
+out:
+	mutex_unlock(&kctx->csf.kcpu_queues.lock);
+
+	return ret;
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h
new file mode 100644
index 000000000000..45c76af04c0f
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu.h
@@ -0,0 +1,305 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_KCPU_H_
+#define _KBASE_CSF_KCPU_H_
+
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+#include <linux/fence.h>
+#else
+#include <linux/dma-fence.h>
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0) */
+
+/* The maximum number of KCPU commands in flight, enqueueing more commands
+ * than this value shall block.
+ */
+#define KBASEP_KCPU_QUEUE_SIZE ((size_t)256)
+
+/**
+ * struct kbase_kcpu_command_import_info - Structure which holds information
+ *				about the buffer to be imported
+ *
+ * @gpu_va:	Address of the buffer to be imported.
+ */
+struct kbase_kcpu_command_import_info {
+	u64 gpu_va;
+};
+
+/**
+ * struct kbase_kcpu_command_fence_info - Structure which holds information
+ *		about the fence object enqueued in the kcpu command queue
+ *
+ * @fence_cb:
+ * @fence:
+ * @kcpu_queue:
+ */
+struct kbase_kcpu_command_fence_info {
+#if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
+	struct fence_cb fence_cb;
+	struct fence *fence;
+#else
+	struct dma_fence_cb fence_cb;
+	struct dma_fence *fence;
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0) */
+	struct kbase_kcpu_command_queue *kcpu_queue;
+};
+
+/**
+ * struct kbase_kcpu_command_cqs_set_info - Structure which holds information
+ *				about CQS objects for the kcpu CQS set command
+ *
+ * @objs:	Array of structures which define CQS objects to be used by
+ *		the kcpu command.
+ * @nr_objs:	Number of CQS objects in the array.
+ * @propagate_flags:  Bit-pattern for the CQSs in the array that are set
+ *		      to propagate queue error-state to the flagged CQSs.
+ */
+struct kbase_kcpu_command_cqs_set_info {
+	struct base_cqs_set *objs;
+	unsigned int nr_objs;
+	u32 propagate_flags;
+};
+
+/**
+ * struct kbase_kcpu_command_cqs_wait_info - Structure which holds information
+ *				about CQS objects for the kcpu CQS wait command
+ *
+ * @objs:	Array of structures which define CQS objects to be used by
+ *		the kcpu command.
+ * @signaled:	Bit array used to report the status of the CQS wait objects.
+ *              1 is signaled, 0 otherwise.
+ * @nr_objs:	Number of CQS objects in the array.
+ * @inherit_err_flags:	Bit-pattern for the CQSs in the array who's error field
+ *			to be served as the source for importing into the
+ *			queue's error-state.
+ */
+struct kbase_kcpu_command_cqs_wait_info {
+	struct base_cqs_wait *objs;
+	unsigned long *signaled;
+	unsigned int nr_objs;
+	u32 inherit_err_flags;
+};
+
+/**
+ * struct kbase_kcpu_command_jit_alloc_info - Structure which holds information
+ *				needed for the kcpu command for jit allocations
+ *
+ * @node	Used to keep track of all JIT free/alloc commands in submission
+ *		order. This must be located in the front of this struct to
+ *		match that of kbase_kcpu_command_jit_free_info.
+ * @info:	Array of objects of the struct base_jit_alloc_info type which
+ *		specify jit allocations to be made by the kcpu command.
+ * @count:	Number of jit alloc objects in the array.
+ * @blocked:	Whether this allocation has been put into the pending list to
+ *		be retried later.
+ */
+struct kbase_kcpu_command_jit_alloc_info {
+	struct list_head node;
+	struct base_jit_alloc_info *info;
+	u8 count;
+	bool blocked;
+};
+
+/**
+ * struct kbase_kcpu_command_jit_free_info - Structure which holds information
+ *				needed for the kcpu jit free command
+ *
+ * @node:	Used to keep track of all JIT free/alloc commands in submission
+ *		order. This must be located in the front of this struct to
+ *		match that of kbase_kcpu_command_jit_alloc_info.
+ * @ids:	Array of identifiers of jit allocations which are to be freed
+ *		by the kcpu command.
+ * @count:	Number of elements in the array.
+ */
+struct kbase_kcpu_command_jit_free_info {
+	struct list_head node;
+	u8 *ids;
+	u8 count;
+};
+
+/**
+ * struct kbase_suspend_copy_buffer - information about the suspend buffer
+ *		to be copied.
+ *
+ * @size:	size of the suspend buffer in bytes.
+ * @pages:	pointer to an array of pointers to the pages which contain
+ *		the user buffer.
+ * @nr_pages:	number of pages.
+ * @offset:	offset into the pages
+ */
+struct kbase_suspend_copy_buffer {
+	size_t size;
+	struct page **pages;
+	int nr_pages;
+	size_t offset;
+};
+
+/**
+ * struct base_kcpu_command_group_suspend - structure which contains
+ *		suspend buffer data captured for a suspended queue group.
+ *
+ * @sus_buf:		Pointer to the structure which contains details of the
+ *			user buffer and its kernel pinned pages.
+ * @group_handle:	Handle to the mapping of command stream group.
+ */
+struct kbase_kcpu_command_group_suspend_info {
+	struct kbase_suspend_copy_buffer *sus_buf;
+	u8 group_handle;
+};
+
+/**
+ * struct kbase_cpu_command - Command which is to be part of the kernel
+ *                            command queue
+ *
+ * @type:	Type of the command.
+ * @enqueue_ts:	Denotes the relative time of enqueueing, a smaller value
+ *		indicates that it has been enqueued earlier.
+ * @info:	Structure which holds information about the command
+ *		dependent on the command type.
+ */
+struct kbase_kcpu_command {
+	enum base_kcpu_command_type type;
+	u64 enqueue_ts;
+	union {
+		struct kbase_kcpu_command_fence_info fence;
+		struct kbase_kcpu_command_cqs_wait_info cqs_wait;
+		struct kbase_kcpu_command_cqs_set_info cqs_set;
+		struct kbase_kcpu_command_import_info import;
+		struct kbase_kcpu_command_jit_alloc_info jit_alloc;
+		struct kbase_kcpu_command_jit_free_info jit_free;
+		struct kbase_kcpu_command_group_suspend_info suspend_buf_copy;
+	} info;
+};
+
+/**
+ * struct kbase_kcpu_command_queue - a command queue executed by the kernel
+ *
+ * @kctx:			The context to which this command queue belongs.
+ * @commands:			Array of commands which have been successfully
+ *				enqueued to this command queue.
+ * @work:			struct work_struct which contains a pointer to
+ *				the function which handles processing of kcpu
+ *				commands enqueued into a kcpu command queue;
+ *				part of kernel API for processing workqueues
+ * @start_offset:		Index of the command to be executed next
+ * @num_pending_cmds:		The number of commands enqueued but not yet
+ *				executed or pending
+ * @cqs_wait_count:		Tracks the number of CQS wait commands enqueued
+ * @fence_context:		The dma-buf fence context number for this kcpu
+ *				queue. A unique context number is allocated for
+ *				each kcpu queue.
+ * @fence_seqno:		The dma-buf fence sequence number for the fence
+ *				that is returned on the enqueue of fence signal
+ *				command. This is increased every time the
+ *				fence signal command is queued.
+ * @fence_wait_processed:	Used to avoid reprocessing of the fence wait
+ *				command which has blocked the processing of
+ *				commands that follow it.
+ * @enqueue_failed:		Indicates that no space has become available in
+ *				the buffer since an enqueue operation failed
+ *				because of insufficient free space.
+ * @command_started:		Indicates that the command at the front of the
+ *				queue has been started in a previous queue
+ *				process, but was not completed due to some
+ *				unmet dependencies. Ensures that instrumentation
+ *				of the execution start of these commands is only
+ *				fired exactly once.
+ * @has_error:			Indicates that the kcpu queue is in error mode
+ *				or without errors since last cleaned.
+ * @jit_blocked:		Used to keep track of command queues blocked
+ *				by a pending JIT allocation command.
+ */
+struct kbase_kcpu_command_queue {
+	struct kbase_context *kctx;
+	struct kbase_kcpu_command commands[KBASEP_KCPU_QUEUE_SIZE];
+	struct work_struct work;
+	u8 start_offset;
+	u16 num_pending_cmds;
+	u32 cqs_wait_count;
+	u64 fence_context;
+	unsigned int fence_seqno;
+	bool fence_wait_processed;
+	bool enqueue_failed;
+	bool command_started;
+	struct list_head jit_blocked;
+	bool has_error;
+};
+
+/**
+ * kbase_csf_kcpu_queue_new - Create new KCPU command queue.
+ *
+ * @kctx:	Pointer to the kbase context within which the KCPU command
+ *		queue will be created.
+ * @newq:	Pointer to the structure which contains information about
+ *		the new KCPU command queue to be created.
+ */
+int kbase_csf_kcpu_queue_new(struct kbase_context *kctx,
+			 struct kbase_ioctl_kcpu_queue_new *newq);
+
+/**
+ * kbase_csf_kcpu_queue_delete - Delete KCPU command queue.
+ *
+ * Return: 0 if successful, -EINVAL if the queue ID is invalid.
+ *
+ * @kctx:	Pointer to the kbase context from which the KCPU command
+ *		queue is to be deleted.
+ * @del:	Pointer to the structure which specifies the KCPU command
+ *		queue to be deleted.
+ */
+int kbase_csf_kcpu_queue_delete(struct kbase_context *kctx,
+			    struct kbase_ioctl_kcpu_queue_delete *del);
+
+/**
+ * kbase_csf_kcpu_queue_enqueue - Enqueue a KCPU command into a KCPU command
+ *				  queue.
+ *
+ * @kctx:	Pointer to the kbase context within which the KCPU command
+ *		is to be enqueued into the KCPU command queue.
+ * @enq:	Pointer to the structure which specifies the KCPU command
+ *		as well as the KCPU command queue into which the command
+ *		is to be enqueued.
+ */
+int kbase_csf_kcpu_queue_enqueue(struct kbase_context *kctx,
+				 struct kbase_ioctl_kcpu_queue_enqueue *enq);
+
+/**
+ * kbase_csf_kcpu_queue_context_init - Initialize the kernel CPU queues context
+ *                                     for a GPU address space
+ *
+ * @kctx: Pointer to the kbase context being initialized.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+int kbase_csf_kcpu_queue_context_init(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_kcpu_queue_context_term - Terminate the kernel CPU queues context
+ *                                     for a GPU address space
+ *
+ * This function deletes any kernel CPU queues that weren't deleted before
+ * context termination.
+ *
+ * @kctx: Pointer to the kbase context being terminated.
+ */
+void kbase_csf_kcpu_queue_context_term(struct kbase_context *kctx);
+
+#endif /* _KBASE_CSF_KCPU_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.c
new file mode 100644
index 000000000000..55e3b64cbe71
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.c
@@ -0,0 +1,199 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_csf_kcpu_debugfs.h"
+#include <mali_kbase.h>
+#include <linux/seq_file.h>
+
+#ifdef CONFIG_SYNC_FILE
+#include "mali_kbase_sync.h"
+#endif
+
+#ifdef CONFIG_DEBUG_FS
+
+/**
+ * kbasep_csf_kcpu_debugfs_print_queue() - Print additional info for KCPU
+ *					queues blocked on CQS wait commands.
+ *
+ * @file:  The seq_file to print to
+ * @kctx:  The context of the KCPU queue
+ * @waits: Pointer to the KCPU CQS wait command info
+ */
+static void kbasep_csf_kcpu_debugfs_print_cqs_waits(struct seq_file *file,
+		struct kbase_context *kctx,
+		struct kbase_kcpu_command_cqs_wait_info *waits)
+{
+	unsigned int i;
+
+	for (i = 0; i < waits->nr_objs; i++) {
+		struct kbase_vmap_struct *mapping;
+		u32 val;
+		char const *msg;
+		u32 *const cpu_ptr = (u32 *)kbase_phy_alloc_mapping_get(kctx,
+					waits->objs[i].addr, &mapping);
+
+		if (!cpu_ptr)
+			return;
+
+		val = *cpu_ptr;
+
+		kbase_phy_alloc_mapping_put(kctx, mapping);
+
+		msg = (waits->inherit_err_flags && (1U << i)) ? "true" :
+								"false";
+		seq_printf(file, "   %llx(%u > %u, inherit_err: %s), ",
+			   waits->objs[i].addr, val, waits->objs[i].val, msg);
+	}
+}
+
+/**
+ * kbasep_csf_kcpu_debugfs_print_queue() - Print debug data for a KCPU queue
+ *
+ * @file:  The seq_file to print to
+ * @kctx:  The context of the KCPU queue
+ * @queue: Pointer to the KCPU queue
+ */
+static void kbasep_csf_kcpu_debugfs_print_queue(struct seq_file *file,
+		struct kbase_context *kctx,
+		struct kbase_kcpu_command_queue *queue)
+{
+	if (WARN_ON(!queue))
+		return;
+
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+
+	seq_printf(file, "%16u, %11u, %7u, %13llu  %8u",
+			queue->num_pending_cmds, queue->enqueue_failed,
+			queue->command_started ? 1 : 0,
+			queue->fence_context, queue->fence_seqno);
+
+	if (queue->command_started) {
+		struct kbase_kcpu_command *cmd =
+				&queue->commands[queue->start_offset];
+		switch (cmd->type) {
+#ifdef CONFIG_SYNC_FILE
+		case BASE_KCPU_COMMAND_TYPE_FENCE_WAIT:
+		{
+			struct kbase_sync_fence_info info;
+
+			kbase_sync_fence_info_get(cmd->info.fence.fence, &info);
+			seq_printf(file, ",  Fence      %p %s %s",
+				   info.fence, info.name,
+				   kbase_sync_status_string(info.status));
+			break;
+		}
+#endif
+		case BASE_KCPU_COMMAND_TYPE_CQS_WAIT:
+			seq_puts(file, ",  CQS     ");
+			kbasep_csf_kcpu_debugfs_print_cqs_waits(file, kctx,
+					&cmd->info.cqs_wait);
+			break;
+		default:
+			seq_puts(file, ", U, Unknown blocking command");
+			break;
+		}
+	}
+
+	seq_puts(file, "\n");
+}
+
+/**
+ * kbasep_csf_kcpu_debugfs_show() - Print the KCPU queues debug information
+ *
+ * @file: The seq_file for printing to
+ * @data: The debugfs dentry private data, a pointer to kbase_context
+ *
+ * Return: Negative error code or 0 on success.
+ */
+static int kbasep_csf_kcpu_debugfs_show(struct seq_file *file, void *data)
+{
+	struct kbase_context *kctx = file->private;
+	unsigned long idx;
+
+	seq_printf(file, "MALI_CSF_KCPU_DEBUGFS_VERSION: v%u\n", MALI_CSF_KCPU_DEBUGFS_VERSION);
+	seq_puts(file, "Queue Idx(err-mode), Pending Commands, Enqueue err, Blocked, Fence context  &  seqno, (Wait Type, Additional info)\n");
+	mutex_lock(&kctx->csf.kcpu_queues.lock);
+
+	idx = find_first_bit(kctx->csf.kcpu_queues.in_use,
+			KBASEP_MAX_KCPU_QUEUES);
+
+	while (idx < KBASEP_MAX_KCPU_QUEUES) {
+		struct kbase_kcpu_command_queue *queue =
+					kctx->csf.kcpu_queues.array[idx];
+
+		seq_printf(file, "%9lu( %s ), ", idx,
+				 queue->has_error ? "InErr" : "NoErr");
+		kbasep_csf_kcpu_debugfs_print_queue(file, kctx,
+				kctx->csf.kcpu_queues.array[idx]);
+
+		idx = find_next_bit(kctx->csf.kcpu_queues.in_use,
+				KBASEP_MAX_KCPU_QUEUES, idx + 1);
+	}
+
+	mutex_unlock(&kctx->csf.kcpu_queues.lock);
+	return 0;
+}
+
+static int kbasep_csf_kcpu_debugfs_open(struct inode *in, struct file *file)
+{
+	return single_open(file, kbasep_csf_kcpu_debugfs_show, in->i_private);
+}
+
+static const struct file_operations kbasep_csf_kcpu_debugfs_fops = {
+	.open = kbasep_csf_kcpu_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+void kbase_csf_kcpu_debugfs_init(struct kbase_context *kctx)
+{
+	struct dentry *file;
+#if (KERNEL_VERSION(4, 7, 0) <= LINUX_VERSION_CODE)
+	const mode_t mode = 0444;
+#else
+	const mode_t mode = 0400;
+#endif
+
+	if (WARN_ON(!kctx || IS_ERR_OR_NULL(kctx->kctx_dentry)))
+		return;
+
+	file = debugfs_create_file("kcpu_queues", mode, kctx->kctx_dentry,
+			kctx, &kbasep_csf_kcpu_debugfs_fops);
+
+	if (IS_ERR_OR_NULL(file)) {
+		dev_warn(kctx->kbdev->dev,
+				"Unable to create KCPU debugfs entry");
+	}
+}
+
+
+#else
+/*
+ * Stub functions for when debugfs is disabled
+ */
+void kbase_csf_kcpu_debugfs_init(struct kbase_context *kctx)
+{
+}
+
+#endif /* CONFIG_DEBUG_FS */
+
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.h
new file mode 100644
index 000000000000..359fe2cb0168
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_kcpu_debugfs.h
@@ -0,0 +1,38 @@
+/*
+ *
+ * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_KCPU_DEBUGFS_H_
+#define _KBASE_CSF_KCPU_DEBUGFS_H_
+
+/* Forward declaration */
+struct kbase_context;
+
+#define MALI_CSF_KCPU_DEBUGFS_VERSION 0
+
+/**
+ * kbase_csf_kcpu_debugfs_init() - Create a debugfs entry for KCPU queues
+ *
+ * @kctx: The kbase_context for which to create the debugfs entry
+ */
+void kbase_csf_kcpu_debugfs_init(struct kbase_context *kctx);
+
+#endif /* _KBASE_CSF_KCPU_DEBUGFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c
new file mode 100644
index 000000000000..987cbc2fc201
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.c
@@ -0,0 +1,120 @@
+/*
+ *
+ * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_csf_protected_memory.h"
+#include <linux/protected_memory_allocator.h>
+
+#ifdef CONFIG_OF
+#include <linux/of_platform.h>
+#endif
+
+int kbase_csf_protected_memory_init(struct kbase_device *const kbdev)
+{
+	int err = 0;
+
+#if CONFIG_OF
+	struct device_node *pma_node = of_parse_phandle(kbdev->dev->of_node,
+					"protected-memory-allocator", 0);
+	if (!pma_node) {
+		dev_info(kbdev->dev, "Protected memory allocator not available\n");
+	} else {
+		struct platform_device *const pdev =
+				of_find_device_by_node(pma_node);
+
+		kbdev->csf.pma_dev = NULL;
+		if (!pdev) {
+			dev_err(kbdev->dev, "Platform device for Protected memory allocator not found\n");
+		} else {
+			kbdev->csf.pma_dev = platform_get_drvdata(pdev);
+			if (!kbdev->csf.pma_dev) {
+				dev_info(kbdev->dev, "Protected memory allocator is not ready\n");
+				err = -EPROBE_DEFER;
+			} else if (!try_module_get(kbdev->csf.pma_dev->owner)) {
+				dev_err(kbdev->dev, "Failed to get Protected memory allocator module\n");
+				err = -ENODEV;
+			} else {
+				dev_info(kbdev->dev, "Protected memory allocator successfully loaded\n");
+			}
+		}
+		of_node_put(pma_node);
+	}
+#endif
+
+	return err;
+}
+
+void kbase_csf_protected_memory_term(struct kbase_device *const kbdev)
+{
+	if (kbdev->csf.pma_dev)
+		module_put(kbdev->csf.pma_dev->owner);
+}
+
+struct protected_memory_allocation **
+		kbase_csf_protected_memory_alloc(
+		struct kbase_device *const kbdev,
+		struct tagged_addr *phys,
+		size_t num_pages)
+{
+	size_t i;
+	struct protected_memory_allocator_device *pma_dev =
+		kbdev->csf.pma_dev;
+	struct protected_memory_allocation **pma =
+		kmalloc_array(num_pages, sizeof(*pma), GFP_KERNEL);
+
+	if (WARN_ON(!pma_dev) || WARN_ON(!phys) || !pma)
+		return NULL;
+
+	for (i = 0; i < num_pages; i++) {
+		pma[i] = pma_dev->ops.pma_alloc_page(pma_dev,
+				KBASE_MEM_POOL_4KB_PAGE_TABLE_ORDER);
+		if (!pma[i])
+			break;
+
+		phys[i] = as_tagged(pma_dev->ops.pma_get_phys_addr(pma_dev,
+					pma[i]));
+	}
+
+	if (i != num_pages) {
+		kbase_csf_protected_memory_free(kbdev, pma, i);
+		return NULL;
+	}
+
+	return pma;
+}
+
+void kbase_csf_protected_memory_free(
+		struct kbase_device *const kbdev,
+		struct protected_memory_allocation **pma,
+		size_t num_pages)
+{
+	size_t i;
+	struct protected_memory_allocator_device *pma_dev =
+		kbdev->csf.pma_dev;
+
+	if (WARN_ON(!pma_dev) || WARN_ON(!pma))
+		return;
+
+	for (i = 0; i < num_pages; i++)
+		pma_dev->ops.pma_free_page(pma_dev, pma[i]);
+
+	kfree(pma);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.h
new file mode 100644
index 000000000000..2b459911d834
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_protected_memory.h
@@ -0,0 +1,72 @@
+/*
+ *
+ * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_PROTECTED_MEMORY_H_
+#define _KBASE_CSF_PROTECTED_MEMORY_H_
+
+#include "mali_kbase.h"
+/**
+ * kbase_csf_protected_memory_init - Initilaise protected memory allocator.
+ *
+ * @kbdev:	Device pointer.
+ *
+ * Return: 0 if success, or an error code on failure.
+ */
+int kbase_csf_protected_memory_init(struct kbase_device *const kbdev);
+
+/**
+ * kbase_csf_protected_memory_term - Terminate prtotected memory allocator.
+ *
+ * @kbdev:	Device pointer.
+ */
+void kbase_csf_protected_memory_term(struct kbase_device *const kbdev);
+
+/**
+ * kbase_csf_protected_memory_alloc - Allocate protected memory pages.
+ *
+ * @kbdev:	Device pointer.
+ * @phys:	Array of physical addresses to be filled in by the protected
+ *		memory allocator.
+ * @num_pages:	Number of pages requested to be allocated.
+ *
+ * Return: Pointer to an array of protected memory allocations on success,
+ *		or NULL on failure.
+ */
+struct protected_memory_allocation **
+	kbase_csf_protected_memory_alloc(
+		struct kbase_device *const kbdev,
+		struct tagged_addr *phys,
+		size_t num_pages);
+
+/**
+ * kbase_csf_protected_memory_free - Free the allocated
+ *					protected memory pages
+ *
+ * @kbdev:	Device pointer.
+ * @pma:	Array of pointer to protected memory allocations.
+ * @num_pages:	Number of pages to be freed.
+ */
+void kbase_csf_protected_memory_free(
+		struct kbase_device *const kbdev,
+		struct protected_memory_allocation **pma,
+		size_t num_pages);
+#endif
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c
new file mode 100644
index 000000000000..f1a318d26f43
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_reset_gpu.c
@@ -0,0 +1,355 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_ctx_sched.h>
+#include <mali_kbase_hwcnt_context.h>
+#include <device/mali_kbase_device.h>
+#include <backend/gpu/mali_kbase_irq_internal.h>
+#include <backend/gpu/mali_kbase_pm_internal.h>
+#include <mali_kbase_regs_history_debugfs.h>
+#include <csf/mali_kbase_csf_trace_buffer.h>
+
+/* Waiting timeout for GPU reset to complete */
+#define GPU_RESET_TIMEOUT_MS (5000) /* 5 seconds */
+#define DUMP_DWORDS_PER_LINE (4)
+/* 16 characters needed for a 8 byte value in hex & 1 character for space */
+#define DUMP_HEX_CHARS_PER_DWORD ((2 * 8) + 1)
+#define DUMP_HEX_CHARS_PER_LINE  \
+	(DUMP_DWORDS_PER_LINE * DUMP_HEX_CHARS_PER_DWORD)
+
+static void kbase_csf_debug_dump_registers(struct kbase_device *kbdev)
+{
+	kbase_io_history_dump(kbdev);
+
+	dev_err(kbdev->dev, "Register state:");
+	dev_err(kbdev->dev, "  GPU_IRQ_RAWSTAT=0x%08x   GPU_STATUS=0x%08x  MCU_STATUS=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT)),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_STATUS)),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(MCU_STATUS)));
+	dev_err(kbdev->dev, "  JOB_IRQ_RAWSTAT=0x%08x   MMU_IRQ_RAWSTAT=0x%08x   GPU_FAULTSTATUS=0x%08x",
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_RAWSTAT)),
+		kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_RAWSTAT)),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_FAULTSTATUS)));
+	dev_err(kbdev->dev, "  GPU_IRQ_MASK=0x%08x   JOB_IRQ_MASK=0x%08x   MMU_IRQ_MASK=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK)),
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_MASK)),
+		kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK)));
+	dev_err(kbdev->dev, "  PWR_OVERRIDE0=0x%08x   PWR_OVERRIDE1=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(PWR_OVERRIDE0)),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(PWR_OVERRIDE1)));
+	dev_err(kbdev->dev, "  SHADER_CONFIG=0x%08x   L2_MMU_CONFIG=0x%08x   TILER_CONFIG=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(SHADER_CONFIG)),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG)),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(TILER_CONFIG)));
+}
+
+static void kbase_csf_dump_firmware_trace_buffer(struct kbase_device *kbdev)
+{
+	u8 *buf, *line_str;
+	unsigned int read_size;
+	struct firmware_trace_buffer *tb =
+		kbase_csf_firmware_get_trace_buffer(kbdev, FW_TRACE_BUF_NAME);
+
+	if (tb == NULL) {
+		dev_dbg(kbdev->dev, "Can't get the trace buffer, firmware trace dump skipped");
+		return;
+	}
+
+	buf = kmalloc(PAGE_SIZE + DUMP_HEX_CHARS_PER_LINE + 1, GFP_KERNEL);
+	if (buf == NULL) {
+		dev_err(kbdev->dev, "Short of memory, firmware trace dump skipped");
+		return;
+	}
+	line_str = &buf[PAGE_SIZE];
+
+	dev_err(kbdev->dev, "Firmware trace buffer dump:");
+	while ((read_size = kbase_csf_firmware_trace_buffer_read_data(tb, buf,
+								PAGE_SIZE))) {
+		u64 *ptr = (u64 *)buf;
+		u32 num_dwords;
+
+		for (num_dwords = read_size / sizeof(u64);
+		     num_dwords >= DUMP_DWORDS_PER_LINE;
+		     num_dwords -= DUMP_DWORDS_PER_LINE) {
+			dev_err(kbdev->dev, "%016llx %016llx %016llx %016llx",
+				ptr[0], ptr[1], ptr[2], ptr[3]);
+			ptr += DUMP_DWORDS_PER_LINE;
+		}
+
+		if (num_dwords) {
+			int pos = 0;
+
+			while (num_dwords--) {
+				pos += snprintf(line_str + pos,
+						DUMP_HEX_CHARS_PER_DWORD + 1,
+						"%016llx ", ptr[0]);
+				ptr++;
+			}
+
+			dev_err(kbdev->dev, "%s", line_str);
+		}
+	}
+
+	kfree(buf);
+}
+
+static int kbase_csf_reset_gpu_now(struct kbase_device *kbdev,
+				   bool firmware_inited)
+{
+	unsigned long flags;
+	bool silent = false;
+	int err;
+
+	if (atomic_read(&kbdev->csf.reset.state) == KBASE_CSF_RESET_GPU_SILENT)
+		silent = true;
+
+	WARN_ON(kbdev->irq_reset_flush);
+
+	/* Reset the scheduler state before disabling the interrupts as suspend of active
+	 * CSG slots would also be done as a part of reset.
+	 */
+	if (likely(firmware_inited))
+		kbase_csf_scheduler_reset(kbdev);
+	cancel_work_sync(&kbdev->csf.firmware_reload_work);
+
+	/* Disable GPU hardware counters.
+	 * This call will block until counters are disabled.
+	 */
+	kbase_hwcnt_context_disable(kbdev->hwcnt_gpu_ctx);
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	spin_lock(&kbdev->mmu_mask_change);
+	kbase_pm_reset_start_locked(kbdev);
+
+	/* We're about to flush out the IRQs and their bottom halves */
+	kbdev->irq_reset_flush = true;
+
+	/* Disable IRQ to avoid IRQ handlers to kick in after releasing the
+	 * spinlock; this also clears any outstanding interrupts
+	 */
+	kbase_pm_disable_interrupts_nolock(kbdev);
+
+	spin_unlock(&kbdev->mmu_mask_change);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	/* Ensure that any IRQ handlers have finished
+	 * Must be done without any locks IRQ handlers will take.
+	 */
+	kbase_synchronize_irqs(kbdev);
+
+	/* Flush out any in-flight work items */
+	kbase_flush_mmu_wqs(kbdev);
+
+	/* The flush has completed so reset the active indicator */
+	kbdev->irq_reset_flush = false;
+
+	mutex_lock(&kbdev->pm.lock);
+	if (!silent)
+		dev_err(kbdev->dev, "Resetting GPU (allowing up to %d ms)",
+								RESET_TIMEOUT);
+
+	/* Output the state of some interesting registers to help in the
+	 * debugging of GPU resets, and dump the firmware trace buffer
+	 */
+	if (!silent) {
+		kbase_csf_debug_dump_registers(kbdev);
+		if (likely(firmware_inited))
+			kbase_csf_dump_firmware_trace_buffer(kbdev);
+	}
+
+	/* Reset the GPU */
+	err = kbase_pm_init_hw(kbdev, 0);
+
+	mutex_unlock(&kbdev->pm.lock);
+
+	if (WARN_ON(err))
+		return err;
+
+	mutex_lock(&kbdev->mmu_hw_mutex);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_ctx_sched_restore_all_as(kbdev);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
+
+	kbase_pm_enable_interrupts(kbdev);
+
+	mutex_lock(&kbdev->pm.lock);
+	kbase_pm_reset_complete(kbdev);
+	/* Synchronously wait for the reload of firmware to complete */
+	err = kbase_pm_wait_for_desired_state(kbdev);
+	mutex_unlock(&kbdev->pm.lock);
+
+	if (err)
+		return err;
+
+	/* Re-enable GPU hardware counters */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_hwcnt_context_enable(kbdev->hwcnt_gpu_ctx);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	if (!silent)
+		dev_err(kbdev->dev, "Reset complete");
+
+	return 0;
+}
+
+static void kbase_csf_reset_gpu_worker(struct work_struct *data)
+{
+	struct kbase_device *kbdev = container_of(data, struct kbase_device,
+						  csf.reset.work);
+	bool firmware_inited;
+	unsigned long flags;
+	int err = 0;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	firmware_inited = kbdev->csf.firmware_inited;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	if (!kbase_pm_context_active_handle_suspend(kbdev,
+			KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE)) {
+		err = kbase_csf_reset_gpu_now(kbdev, firmware_inited);
+		kbase_pm_context_idle(kbdev);
+	}
+
+	kbase_disjoint_state_down(kbdev);
+
+	if (!err) {
+		atomic_set(&kbdev->csf.reset.state,
+				KBASE_CSF_RESET_GPU_NOT_PENDING);
+		if (likely(firmware_inited))
+			kbase_csf_scheduler_enable_tick_timer(kbdev);
+	} else {
+		dev_err(kbdev->dev, "Reset failed to complete");
+		atomic_set(&kbdev->csf.reset.state,
+				KBASE_CSF_RESET_GPU_FAILED);
+	}
+
+	wake_up(&kbdev->csf.reset.wait);
+}
+
+bool kbase_prepare_to_reset_gpu(struct kbase_device *kbdev)
+{
+	if (atomic_cmpxchg(&kbdev->csf.reset.state,
+			KBASE_CSF_RESET_GPU_NOT_PENDING,
+			KBASE_CSF_RESET_GPU_HAPPENING) !=
+			KBASE_CSF_RESET_GPU_NOT_PENDING) {
+		/* Some other thread is already resetting the GPU */
+		return false;
+	}
+
+	return true;
+}
+KBASE_EXPORT_TEST_API(kbase_prepare_to_reset_gpu);
+
+bool kbase_prepare_to_reset_gpu_locked(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	return kbase_prepare_to_reset_gpu(kbdev);
+}
+
+int kbase_reset_gpu(struct kbase_device *kbdev)
+{
+	dev_err(kbdev->dev, "Preparing to soft-reset GPU\n");
+
+	kbase_disjoint_state_up(kbdev);
+
+	queue_work(kbdev->csf.reset.workq, &kbdev->csf.reset.work);
+
+	return 0;
+}
+KBASE_EXPORT_TEST_API(kbase_reset_gpu);
+
+void kbase_reset_gpu_locked(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbase_reset_gpu(kbdev);
+}
+
+int kbase_reset_gpu_silent(struct kbase_device *kbdev)
+{
+	if (atomic_cmpxchg(&kbdev->csf.reset.state,
+				KBASE_CSF_RESET_GPU_NOT_PENDING,
+				KBASE_CSF_RESET_GPU_SILENT) !=
+				KBASE_CSF_RESET_GPU_NOT_PENDING) {
+		/* Some other thread is already resetting the GPU */
+		return -EAGAIN;
+	}
+
+	kbase_disjoint_state_up(kbdev);
+
+	queue_work(kbdev->csf.reset.workq, &kbdev->csf.reset.work);
+
+	return 0;
+}
+
+bool kbase_reset_gpu_is_active(struct kbase_device *kbdev)
+{
+	if (atomic_read(&kbdev->csf.reset.state) ==
+			KBASE_CSF_RESET_GPU_NOT_PENDING)
+		return false;
+
+	return true;
+}
+
+int kbase_reset_gpu_wait(struct kbase_device *kbdev)
+{
+	const long wait_timeout =
+		kbase_csf_timeout_in_jiffies(GPU_RESET_TIMEOUT_MS);
+	long remaining = wait_event_timeout(kbdev->csf.reset.wait,
+				(atomic_read(&kbdev->csf.reset.state) ==
+					KBASE_CSF_RESET_GPU_NOT_PENDING) ||
+				(atomic_read(&kbdev->csf.reset.state) ==
+					KBASE_CSF_RESET_GPU_FAILED),
+				wait_timeout);
+
+	if (!remaining) {
+		dev_warn(kbdev->dev, "Timed out waiting for the GPU reset to complete");
+		return -ETIMEDOUT;
+	} else if (atomic_read(&kbdev->csf.reset.state) ==
+			KBASE_CSF_RESET_GPU_FAILED) {
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+KBASE_EXPORT_TEST_API(kbase_reset_gpu_wait);
+
+int kbase_reset_gpu_init(struct kbase_device *kbdev)
+{
+	kbdev->csf.reset.workq = alloc_workqueue("Mali reset workqueue", 0, 1);
+	if (kbdev->csf.reset.workq == NULL)
+		return -ENOMEM;
+
+	INIT_WORK(&kbdev->csf.reset.work, kbase_csf_reset_gpu_worker);
+
+	init_waitqueue_head(&kbdev->csf.reset.wait);
+
+	return 0;
+}
+
+void kbase_reset_gpu_term(struct kbase_device *kbdev)
+{
+	destroy_workqueue(kbdev->csf.reset.workq);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c
new file mode 100644
index 000000000000..a3017a7f25ba
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.c
@@ -0,0 +1,4135 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include "mali_kbase_config_defaults.h"
+#include <mali_kbase_ctx_sched.h>
+#include <mali_kbase_reset_gpu.h>
+#include <mali_kbase_as_fault_debugfs.h>
+#include <mali_kbase_bits.h>
+#include "mali_kbase_csf.h"
+#include "../tl/mali_kbase_tracepoints.h"
+#include "backend/gpu/mali_kbase_pm_internal.h"
+#include <linux/export.h>
+#include "mali_gpu_csf_registers.h"
+#include <mali_base_kernel.h>
+
+/* Value to indicate that a queue group is not groups_to_schedule list */
+#define KBASEP_GROUP_PREPARED_SEQ_NUM_INVALID (U32_MAX)
+
+/* Waiting timeout for status change acknowledgment, in milliseconds */
+#define CSF_STATE_WAIT_TIMEOUT_MS (800) /* Relaxed to 800ms from 100ms */
+
+/* Waiting timeout for scheduler state change for descheduling a CSG */
+#define CSG_SCHED_STOP_TIMEOUT_MS (50)
+
+#define CSG_SUSPEND_ON_RESET_WAIT_TIMEOUT_MS DEFAULT_RESET_TIMEOUT_MS
+
+/* Maximum number of endpoints which may run tiler jobs. */
+#define CSG_TILER_MAX ((u8)1)
+
+/* Maximum dynamic CSG slot priority value */
+#define MAX_CSG_SLOT_PRIORITY ((u8)15)
+
+/* CSF scheduler time slice value */
+#define CSF_SCHEDULER_TIME_TICK_MS (100) /* 100 milliseconds */
+#define CSF_SCHEDULER_TIME_TICK_JIFFIES \
+	msecs_to_jiffies(CSF_SCHEDULER_TIME_TICK_MS)
+
+/*
+ * CSF scheduler time threshold for converting "tock" requests into "tick" if
+ * they come too close to the end of a tick interval. This avoids scheduling
+ * twice in a row.
+ */
+#define CSF_SCHEDULER_TIME_TICK_THRESHOLD_MS \
+	CSF_SCHEDULER_TIME_TICK_MS
+
+#define CSF_SCHEDULER_TIME_TICK_THRESHOLD_JIFFIES \
+	msecs_to_jiffies(CSF_SCHEDULER_TIME_TICK_THRESHOLD_MS)
+
+/* Nanoseconds per millisecond */
+#define NS_PER_MS ((u64)1000 * 1000)
+
+/*
+ * CSF minimum time to reschedule for a new "tock" request. Bursts of "tock"
+ * requests are not serviced immediately, but shall wait for a minimum time in
+ * order to reduce load on the CSF scheduler thread.
+ */
+#define CSF_SCHEDULER_TIME_TOCK_JIFFIES 1 /* 1 jiffies-time */
+
+/* Command stream suspended and is idle (empty ring buffer) */
+#define CS_IDLE_FLAG (1 << 0)
+
+/* Command stream suspended and is wait for a CQS condition */
+#define CS_WAIT_SYNC_FLAG (1 << 1)
+
+/* This is to avoid the immediate power down of GPU when then are no groups
+ * left for scheduling. GPUCORE-24250 would add the proper GPU idle detection
+ * logic.
+ */
+#define GPU_IDLE_POWEROFF_HYSTERESIS_DELAY msecs_to_jiffies((u32)10)
+
+static int scheduler_group_schedule(struct kbase_queue_group *group);
+static void remove_group_from_idle_wait(struct kbase_queue_group *const group);
+static
+void insert_group_to_runnable(struct kbase_csf_scheduler *const scheduler,
+		struct kbase_queue_group *const group,
+		enum kbase_csf_group_state run_state);
+static struct kbase_queue_group *scheduler_get_protm_enter_async_group(
+		struct kbase_device *const kbdev,
+		struct kbase_queue_group *const group);
+static struct kbase_queue_group *get_tock_top_group(
+	struct kbase_csf_scheduler *const scheduler);
+static void scheduler_enable_tick_timer_nolock(struct kbase_device *kbdev);
+static int suspend_active_queue_groups(struct kbase_device *kbdev,
+				       unsigned long *slot_mask);
+
+#define kctx_as_enabled(kctx) (!kbase_ctx_flag(kctx, KCTX_AS_DISABLED_ON_FAULT))
+
+static void release_doorbell(struct kbase_device *kbdev, int doorbell_nr)
+{
+	WARN_ON(doorbell_nr >= CSF_NUM_DOORBELL);
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+	clear_bit(doorbell_nr, kbdev->csf.scheduler.doorbell_inuse_bitmap);
+}
+
+static int acquire_doorbell(struct kbase_device *kbdev)
+{
+	int doorbell_nr;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	doorbell_nr = find_first_zero_bit(
+			kbdev->csf.scheduler.doorbell_inuse_bitmap,
+			CSF_NUM_DOORBELL);
+
+	if (doorbell_nr >= CSF_NUM_DOORBELL)
+		return KBASEP_USER_DB_NR_INVALID;
+
+	set_bit(doorbell_nr, kbdev->csf.scheduler.doorbell_inuse_bitmap);
+
+	return doorbell_nr;
+}
+
+static void unassign_user_doorbell_from_group(struct kbase_device *kbdev,
+		struct kbase_queue_group *group)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (group->doorbell_nr != KBASEP_USER_DB_NR_INVALID) {
+		release_doorbell(kbdev, group->doorbell_nr);
+		group->doorbell_nr = KBASEP_USER_DB_NR_INVALID;
+	}
+}
+
+static void unassign_user_doorbell_from_queue(struct kbase_device *kbdev,
+		struct kbase_queue *queue)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	mutex_lock(&kbdev->csf.reg_lock);
+
+	if (queue->doorbell_nr != KBASEP_USER_DB_NR_INVALID) {
+		queue->doorbell_nr = KBASEP_USER_DB_NR_INVALID;
+		/* After this the dummy page would be mapped in */
+		unmap_mapping_range(kbdev->csf.db_filp->f_inode->i_mapping,
+			queue->db_file_offset << PAGE_SHIFT, PAGE_SIZE, 1);
+	}
+
+	mutex_unlock(&kbdev->csf.reg_lock);
+}
+
+static void assign_user_doorbell_to_group(struct kbase_device *kbdev,
+		struct kbase_queue_group *group)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (group->doorbell_nr == KBASEP_USER_DB_NR_INVALID)
+		group->doorbell_nr = acquire_doorbell(kbdev);
+}
+
+static void assign_user_doorbell_to_queue(struct kbase_device *kbdev,
+		struct kbase_queue *const queue)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	mutex_lock(&kbdev->csf.reg_lock);
+
+	/* If bind operation for the queue hasn't completed yet, then the
+	 * the command stream interface can't be programmed for the queue
+	 * (even in stopped state) and so the doorbell also can't be assigned
+	 * to it.
+	 */
+	if ((queue->bind_state == KBASE_CSF_QUEUE_BOUND) &&
+	    (queue->doorbell_nr == KBASEP_USER_DB_NR_INVALID)) {
+		WARN_ON(queue->group->doorbell_nr == KBASEP_USER_DB_NR_INVALID);
+		queue->doorbell_nr = queue->group->doorbell_nr;
+
+		/* After this the real Hw doorbell page would be mapped in */
+		unmap_mapping_range(
+				kbdev->csf.db_filp->f_inode->i_mapping,
+				queue->db_file_offset << PAGE_SHIFT,
+				PAGE_SIZE, 1);
+	}
+
+	mutex_unlock(&kbdev->csf.reg_lock);
+}
+
+static void scheduler_doorbell_init(struct kbase_device *kbdev)
+{
+	int doorbell_nr;
+
+	bitmap_zero(kbdev->csf.scheduler.doorbell_inuse_bitmap,
+		CSF_NUM_DOORBELL);
+
+	mutex_lock(&kbdev->csf.scheduler.lock);
+	/* Reserve doorbell 0 for use by kernel driver */
+	doorbell_nr = acquire_doorbell(kbdev);
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+
+	WARN_ON(doorbell_nr != CSF_KERNEL_DOORBELL_NR);
+}
+
+static u32 get_nr_active_csgs(struct kbase_device *kbdev)
+{
+	u32 nr_active_csgs;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	nr_active_csgs = bitmap_weight(kbdev->csf.scheduler.csg_inuse_bitmap,
+				kbdev->csf.global_iface.group_num);
+
+	return nr_active_csgs;
+}
+
+/**
+ * csgs_active - returns true if any of CSG slots are in use
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * Return: the interface is actively engaged flag.
+ */
+bool csgs_active(struct kbase_device *kbdev)
+{
+	u32 nr_active_csgs;
+
+	mutex_lock(&kbdev->csf.scheduler.lock);
+	nr_active_csgs = get_nr_active_csgs(kbdev);
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+
+	/* Right now if any of the command stream group interfaces are in use
+	 * then we need to assume that there is some work pending.
+	 * In future when we have IDLE notifications from firmware implemented
+	 * then we would have a better idea of the pending work.
+	 */
+	return (nr_active_csgs != 0);
+}
+
+/**
+ * csg_slot_in_use - returns true if a queue group has been programmed on a
+ *                   given CSG slot.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ * @slot:  Index/number of the CSG slot in question.
+ *
+ * Return: the interface is actively engaged flag.
+ *
+ * Note: Caller must hold the scheduler lock.
+ */
+static inline bool csg_slot_in_use(struct kbase_device *kbdev, int slot)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	return (kbdev->csf.scheduler.csg_slots[slot].resident_group != NULL);
+}
+
+static bool queue_group_suspended_locked(struct kbase_queue_group *group)
+{
+	lockdep_assert_held(&group->kctx->kbdev->csf.scheduler.lock);
+
+	return (group->run_state == KBASE_CSF_GROUP_SUSPENDED ||
+		group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_IDLE ||
+		group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC);
+}
+
+static bool queue_group_idle_locked(struct kbase_queue_group *group)
+{
+	lockdep_assert_held(&group->kctx->kbdev->csf.scheduler.lock);
+
+	return (group->run_state == KBASE_CSF_GROUP_IDLE ||
+		group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_IDLE);
+}
+
+static bool queue_group_scheduled(struct kbase_queue_group *group)
+{
+	return (group->run_state != KBASE_CSF_GROUP_INACTIVE &&
+		group->run_state != KBASE_CSF_GROUP_TERMINATED &&
+		group->run_state != KBASE_CSF_GROUP_FAULT_EVICTED);
+}
+
+static bool queue_group_scheduled_locked(struct kbase_queue_group *group)
+{
+	lockdep_assert_held(&group->kctx->kbdev->csf.scheduler.lock);
+
+	return queue_group_scheduled(group);
+}
+
+/**
+ * scheduler_timer_is_enabled_nolock() - Check if the scheduler wakes up
+ * automatically for periodic tasks.
+ *
+ * @kbdev: Pointer to the device
+ *
+ * This is a variant of kbase_csf_scheduler_timer_is_enabled() that assumes the
+ * CSF scheduler lock to already have been held.
+ *
+ * Return: true if the scheduler is configured to wake up periodically
+ */
+static bool scheduler_timer_is_enabled_nolock(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	return kbdev->csf.scheduler.timer_enabled;
+}
+
+static void scheduler_wakeup(struct kbase_device *kbdev, bool kick)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	if (scheduler->state == SCHED_SUSPENDED) {
+		dev_info(kbdev->dev, "Re-activating the Scheduler");
+		kbase_csf_scheduler_pm_active(kbdev);
+		scheduler->state = SCHED_INACTIVE;
+
+		if (kick)
+			scheduler_enable_tick_timer_nolock(kbdev);
+	}
+}
+
+static void scheduler_suspend(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	if (!WARN_ON(scheduler->state == SCHED_SUSPENDED)) {
+		dev_dbg(kbdev->dev, "Suspending the Scheduler");
+		kbase_csf_scheduler_pm_idle(kbdev);
+		scheduler->state = SCHED_SUSPENDED;
+	}
+}
+
+/**
+ * update_idle_suspended_group_state() - Move the queue group to a non-idle
+ *                                       suspended state.
+ * @group: Pointer to the queue group.
+ *
+ * This function is called to change the state of queue group to non-idle
+ * suspended state, if the group was suspended when all the queues bound to it
+ * became empty or when some queues got blocked on a sync wait & others became
+ * empty. The group is also moved to the runnbale list from idle wait list in
+ * the latter case.
+ * So the function gets called when a queue is kicked or sync wait condition
+ * gets satisfied.
+ */
+static void update_idle_suspended_group_state(struct kbase_queue_group *group)
+{
+	struct kbase_csf_scheduler *scheduler =
+		&group->kctx->kbdev->csf.scheduler;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	if (group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC) {
+		remove_group_from_idle_wait(group);
+		insert_group_to_runnable(scheduler, group,
+					 KBASE_CSF_GROUP_SUSPENDED);
+	} else {
+		if (group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_IDLE)
+			group->run_state = KBASE_CSF_GROUP_SUSPENDED;
+		else
+			return;
+	}
+
+	atomic_inc(&scheduler->non_idle_suspended_grps);
+}
+
+int kbase_csf_scheduler_group_get_slot_locked(struct kbase_queue_group *group)
+{
+	struct kbase_csf_scheduler *scheduler =
+			&group->kctx->kbdev->csf.scheduler;
+	int slot_num = group->csg_nr;
+
+	lockdep_assert_held(&scheduler->interrupt_lock);
+
+	if (slot_num >= 0) {
+		if (WARN_ON(scheduler->csg_slots[slot_num].resident_group !=
+			    group))
+			return -1;
+	}
+
+	return slot_num;
+}
+
+int kbase_csf_scheduler_group_get_slot(struct kbase_queue_group *group)
+{
+	struct kbase_csf_scheduler *scheduler =
+			&group->kctx->kbdev->csf.scheduler;
+	unsigned long flags;
+	int slot_num;
+
+	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+	slot_num = kbase_csf_scheduler_group_get_slot_locked(group);
+	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+
+	return slot_num;
+}
+
+static bool kbasep_csf_scheduler_group_is_on_slot_locked(
+				struct kbase_queue_group *group)
+{
+	struct kbase_csf_scheduler *scheduler =
+			&group->kctx->kbdev->csf.scheduler;
+	int slot_num = group->csg_nr;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	if (slot_num >= 0) {
+		if (!WARN_ON(scheduler->csg_slots[slot_num].resident_group !=
+			     group))
+			return true;
+	}
+
+	return false;
+}
+
+bool kbase_csf_scheduler_group_events_enabled(struct kbase_device *kbdev,
+			struct kbase_queue_group *group)
+{
+	struct kbase_csf_scheduler *scheduler =
+			&group->kctx->kbdev->csf.scheduler;
+	int slot_num = group->csg_nr;
+
+	lockdep_assert_held(&scheduler->interrupt_lock);
+
+	if (WARN_ON(slot_num < 0))
+		return false;
+
+	return test_bit(slot_num, scheduler->csgs_events_enable_mask);
+}
+
+struct kbase_queue_group *kbase_csf_scheduler_get_group_on_slot(
+			struct kbase_device *kbdev, int slot)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.interrupt_lock);
+
+	return kbdev->csf.scheduler.csg_slots[slot].resident_group;
+}
+
+static int halt_stream_sync(struct kbase_queue *queue)
+{
+	struct kbase_queue_group *group = queue->group;
+	struct kbase_device *kbdev = queue->kctx->kbdev;
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	struct kbase_csf_cmd_stream_info *stream;
+	long remaining =
+		kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+
+	if (WARN_ON(!group) ||
+	    WARN_ON(!kbasep_csf_scheduler_group_is_on_slot_locked(group)))
+		return -EINVAL;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+	ginfo = &global_iface->groups[group->csg_nr];
+	stream = &ginfo->streams[queue->csi_index];
+
+	if (CS_REQ_STATE_GET(kbase_csf_firmware_cs_input_read(stream, CS_REQ)) ==
+			CS_REQ_STATE_START) {
+
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+			(CS_ACK_STATE_GET(kbase_csf_firmware_cs_output(stream, CS_ACK))
+			 == CS_ACK_STATE_START), remaining);
+
+		if (!remaining) {
+			dev_warn(kbdev->dev, "Timed out waiting for queue to start on csi %d bound to group %d on slot %d",
+				queue->csi_index, group->handle, group->csg_nr);
+			if (kbase_prepare_to_reset_gpu(kbdev))
+				kbase_reset_gpu(kbdev);
+
+			return -ETIMEDOUT;
+		}
+
+		remaining =
+			kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+	}
+
+	/* Set state to STOP */
+	kbase_csf_firmware_cs_input_mask(stream, CS_REQ, CS_REQ_STATE_STOP,
+					 CS_REQ_STATE_MASK);
+
+	KBASE_KTRACE_ADD_CSF_GRP_Q(kbdev, CSI_STOP_REQUESTED, group, queue, 0u);
+	kbase_csf_ring_cs_kernel_doorbell(kbdev, queue);
+
+	/* Timed wait */
+	remaining = wait_event_timeout(kbdev->csf.event_wait,
+		(CS_ACK_STATE_GET(kbase_csf_firmware_cs_output(stream, CS_ACK))
+		 == CS_ACK_STATE_STOP), remaining);
+
+	if (!remaining) {
+		dev_warn(kbdev->dev, "Timed out waiting for queue to stop on csi %d bound to group %d on slot %d",
+			 queue->csi_index, group->handle, group->csg_nr);
+		if (kbase_prepare_to_reset_gpu(kbdev))
+			kbase_reset_gpu(kbdev);
+	}
+	return (remaining) ? 0 : -ETIMEDOUT;
+}
+
+static bool can_halt_stream(struct kbase_device *kbdev,
+		struct kbase_queue_group *group)
+{
+	struct kbase_csf_csg_slot *const csg_slot =
+			kbdev->csf.scheduler.csg_slots;
+	unsigned long flags;
+	bool can_halt;
+	int slot;
+
+	if (!queue_group_scheduled(group))
+		return true;
+
+	spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, flags);
+	slot = kbase_csf_scheduler_group_get_slot_locked(group);
+	can_halt = (slot >= 0) &&
+		   (atomic_read(&csg_slot[slot].state) == CSG_SLOT_RUNNING);
+	spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock,
+				flags);
+
+	return can_halt;
+}
+
+/**
+ * sched_halt_stream() - Stop a GPU queue when its queue group is not running
+ *                       on a CSG slot.
+ * @queue: Pointer to the GPU queue to stop.
+ *
+ * This function handles stopping gpu queues for groups that are either not on
+ * a command stream group slot or are on the slot but undergoing transition to
+ * resume or suspend states.
+ * It waits until the queue group is scheduled on a slot and starts running,
+ * which is needed as groups that were suspended may need to resume all queues
+ * that were enabled and running at the time of suspension.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+static int sched_halt_stream(struct kbase_queue *queue)
+{
+	struct kbase_queue_group *group = queue->group;
+	struct kbase_device *kbdev = queue->kctx->kbdev;
+	struct kbase_csf_scheduler *const scheduler =
+			&kbdev->csf.scheduler;
+	struct kbase_csf_csg_slot *const csg_slot =
+			kbdev->csf.scheduler.csg_slots;
+	bool retry_needed = false;
+	bool retried = false;
+	long remaining;
+	int slot;
+	int err = 0;
+
+	if (WARN_ON(!group))
+		return -EINVAL;
+
+	lockdep_assert_held(&queue->kctx->csf.lock);
+	lockdep_assert_held(&scheduler->lock);
+
+	slot = kbase_csf_scheduler_group_get_slot(group);
+
+	if (slot >= 0) {
+		WARN_ON(atomic_read(&csg_slot[slot].state) == CSG_SLOT_RUNNING);
+
+		if (atomic_read(&csg_slot[slot].state) == CSG_SLOT_READY2RUN) {
+			dev_dbg(kbdev->dev, "Stopping a queue on csi %d when Group-%d is in under transition to running state",
+				queue->csi_index, group->handle);
+			retry_needed = true;
+		}
+	}
+retry:
+	/* First wait for the group to reach a stable state. IDLE state is
+	 * an intermediate state that is only set by Scheduler at the start
+	 * of a tick (prior to scanout) for groups that received idle
+	 * notification, then later the idle group is moved to one of the
+	 * suspended states or the runnable state.
+	 */
+	while (group->run_state == KBASE_CSF_GROUP_IDLE) {
+		mutex_unlock(&scheduler->lock);
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+				group->run_state != KBASE_CSF_GROUP_IDLE,
+				CSF_STATE_WAIT_TIMEOUT_MS);
+		mutex_lock(&scheduler->lock);
+		if (!remaining) {
+			dev_warn(kbdev->dev,
+				 "Timed out waiting for state change of Group-%d when stopping a queue on csi %d",
+				 group->handle, queue->csi_index);
+		}
+	}
+
+	WARN_ON(group->run_state == KBASE_CSF_GROUP_IDLE);
+	/* Update the group state so that it can get scheduled soon */
+	update_idle_suspended_group_state(group);
+
+	mutex_unlock(&scheduler->lock);
+
+	/* This function is called when the queue group is either not on a CSG
+	 * slot or is on the slot but undergoing transition.
+	 *
+	 * To stop the queue, the function needs to wait either for the queue
+	 * group to be assigned a CSG slot (and that slot has to reach the
+	 * running state) or for the eviction of the queue group from the
+	 * scheduler's list.
+	 *
+	 * In order to evaluate the latter condition, the function doesn't
+	 * really need to lock the scheduler, as any update to the run_state
+	 * of the queue group by sched_evict_group() would be visible due
+	 * to implicit barriers provided by the kernel waitqueue macros.
+	 *
+	 * The group pointer cannot disappear meanwhile, as the high level
+	 * CSF context is locked. Therefore, the scheduler would be
+	 * the only one to update the run_state of the group.
+	 */
+	remaining = wait_event_timeout(kbdev->csf.event_wait,
+		can_halt_stream(kbdev, group),
+		kbase_csf_timeout_in_jiffies(20 * CSF_SCHEDULER_TIME_TICK_MS));
+
+	mutex_lock(&scheduler->lock);
+
+	if (remaining && queue_group_scheduled_locked(group)) {
+		slot = kbase_csf_scheduler_group_get_slot(group);
+
+		/* If the group is still on slot and slot is in running state
+		 * then explicitly stop the command stream interface of the
+		 * queue. Otherwise there are different cases to consider
+		 *
+		 * - If the queue group was already undergoing transition to
+		 *   resume/start state when this function was entered then it
+		 *   would not have disabled the command stream interface of the
+		 *   queue being stopped and the previous wait would have ended
+		 *   once the slot was in a running state with command stream
+		 *   interface still enabled.
+		 *   Now the group is going through another transition either
+		 *   to a suspend state or to a resume state (it could have
+		 *   been suspended before the scheduler lock was grabbed).
+		 *   In both scenarios need to wait again for the group to
+		 *   come on a slot and that slot to reach the running state,
+		 *   as that would guarantee that firmware will observe the
+		 *   command stream interface as disabled.
+		 *
+		 * - If the queue group was either off the slot or was
+		 *   undergoing transition to suspend state on entering this
+		 *   function, then the group would have been resumed with the
+		 *   queue's command stream interface in disabled state.
+		 *   So now if the group is undergoing another transition
+		 *   (after the resume) then just need to wait for the state
+		 *   bits in the ACK register of command stream interface to be
+		 *   set to STOP value. It is expected that firmware will
+		 *   process the stop/disable request of the command stream
+		 *   interface after resuming the group before it processes
+		 *   another state change request of the group.
+		 */
+		if ((slot >= 0) &&
+		    (atomic_read(&csg_slot[slot].state) == CSG_SLOT_RUNNING)) {
+			err = halt_stream_sync(queue);
+		} else if (retry_needed && !retried) {
+			retried = true;
+			goto retry;
+		} else if (slot >= 0) {
+			struct kbase_csf_global_iface *global_iface =
+					&kbdev->csf.global_iface;
+			struct kbase_csf_cmd_stream_group_info *ginfo =
+					&global_iface->groups[slot];
+			struct kbase_csf_cmd_stream_info *stream =
+					&ginfo->streams[queue->csi_index];
+			u32 cs_req =
+			    kbase_csf_firmware_cs_input_read(stream, CS_REQ);
+
+			if (!WARN_ON(CS_REQ_STATE_GET(cs_req) !=
+				     CS_REQ_STATE_STOP)) {
+				/* Timed wait */
+				remaining = wait_event_timeout(
+					kbdev->csf.event_wait,
+					(CS_ACK_STATE_GET(kbase_csf_firmware_cs_output(stream, CS_ACK))
+					== CS_ACK_STATE_STOP),
+					CSF_STATE_WAIT_TIMEOUT_MS);
+
+				if (!remaining) {
+					dev_warn(kbdev->dev,
+						 "Timed out waiting for queue stop ack on csi %d bound to group %d on slot %d",
+						 queue->csi_index,
+						 group->handle, group->csg_nr);
+					err = -ETIMEDOUT;
+				}
+			}
+		}
+	} else if (!remaining) {
+		dev_warn(kbdev->dev, "Group-%d failed to get a slot for stopping the queue on csi %d",
+			 group->handle, queue->csi_index);
+		err = -ETIMEDOUT;
+	}
+
+	return err;
+}
+
+static int wait_gpu_reset(struct kbase_device *kbdev)
+{
+	int ret = 0;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	while (kbase_reset_gpu_is_active(kbdev) && !ret) {
+		mutex_unlock(&kbdev->csf.scheduler.lock);
+		ret = kbase_reset_gpu_wait(kbdev);
+		mutex_lock(&kbdev->csf.scheduler.lock);
+	}
+
+	return ret;
+}
+
+int kbase_csf_scheduler_queue_stop(struct kbase_queue *queue)
+{
+	struct kbase_device *kbdev = queue->kctx->kbdev;
+	struct kbase_queue_group *group = queue->group;
+	bool const cs_enabled = queue->enabled;
+	int err = 0;
+
+	if (WARN_ON(!group))
+		return -EINVAL;
+
+	lockdep_assert_held(&queue->kctx->csf.lock);
+	mutex_lock(&kbdev->csf.scheduler.lock);
+
+	queue->enabled = false;
+	KBASE_KTRACE_ADD_CSF_GRP_Q(kbdev, CSI_STOP, group, queue, cs_enabled);
+
+	wait_gpu_reset(kbdev);
+
+	if (cs_enabled && queue_group_scheduled_locked(group)) {
+		struct kbase_csf_csg_slot *const csg_slot =
+			kbdev->csf.scheduler.csg_slots;
+		int slot = kbase_csf_scheduler_group_get_slot(group);
+
+		/* Since the group needs to be resumed in order to stop the queue,
+		 * check if GPU needs to be powered up.
+		 */
+		scheduler_wakeup(kbdev, true);
+
+		if ((slot >= 0) &&
+		    (atomic_read(&csg_slot[slot].state) == CSG_SLOT_RUNNING))
+			err = halt_stream_sync(queue);
+		else
+			err = sched_halt_stream(queue);
+
+		unassign_user_doorbell_from_queue(kbdev, queue);
+	}
+
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+	return err;
+}
+
+static void update_hw_active(struct kbase_queue *queue, bool active)
+{
+#ifdef CONFIG_MALI_BIFROST_NO_MALI
+	if (queue && queue->enabled) {
+		u32 *output_addr = (u32 *)(queue->user_io_addr + PAGE_SIZE);
+
+		output_addr[CS_ACTIVE / sizeof(u32)] = active;
+	}
+#else
+	CSTD_UNUSED(queue);
+	CSTD_UNUSED(active);
+#endif
+}
+
+static void program_cs_extract_init(struct kbase_queue *queue)
+{
+	u64 *input_addr = (u64 *)queue->user_io_addr;
+	u64 *output_addr = (u64 *)(queue->user_io_addr + PAGE_SIZE);
+
+	input_addr[CS_EXTRACT_INIT_LO / sizeof(u64)] =
+			output_addr[CS_EXTRACT_LO / sizeof(u64)];
+}
+
+static void program_cs(struct kbase_device *kbdev,
+		struct kbase_queue *queue)
+{
+	struct kbase_queue_group *group = queue->group;
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	struct kbase_csf_cmd_stream_info *stream;
+	u64 user_input;
+	u64 user_output;
+
+	if (WARN_ON(!group))
+		return;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (WARN_ON(!kbasep_csf_scheduler_group_is_on_slot_locked(group)))
+		return;
+
+	ginfo = &kbdev->csf.global_iface.groups[group->csg_nr];
+
+	if (WARN_ON(queue->csi_index < 0) ||
+	    WARN_ON(queue->csi_index >= ginfo->stream_num))
+		return;
+
+	assign_user_doorbell_to_queue(kbdev, queue);
+	if (queue->doorbell_nr == KBASEP_USER_DB_NR_INVALID)
+		return;
+
+	WARN_ON(queue->doorbell_nr != queue->group->doorbell_nr);
+
+	if (queue->enabled && queue_group_suspended_locked(group))
+		program_cs_extract_init(queue);
+
+	stream = &ginfo->streams[queue->csi_index];
+
+	kbase_csf_firmware_cs_input(stream, CS_BASE_LO,
+				    queue->base_addr & 0xFFFFFFFF);
+	kbase_csf_firmware_cs_input(stream, CS_BASE_HI,
+				    queue->base_addr >> 32);
+	kbase_csf_firmware_cs_input(stream, CS_SIZE,
+				    queue->size);
+
+	user_input = (queue->reg->start_pfn << PAGE_SHIFT);
+	kbase_csf_firmware_cs_input(stream, CS_USER_INPUT_LO,
+				    user_input & 0xFFFFFFFF);
+	kbase_csf_firmware_cs_input(stream, CS_USER_INPUT_HI,
+				    user_input >> 32);
+
+	user_output = ((queue->reg->start_pfn + 1) << PAGE_SHIFT);
+	kbase_csf_firmware_cs_input(stream, CS_USER_OUTPUT_LO,
+				    user_output & 0xFFFFFFFF);
+	kbase_csf_firmware_cs_input(stream, CS_USER_OUTPUT_HI,
+				    user_output >> 32);
+
+	kbase_csf_firmware_cs_input(stream, CS_CONFIG,
+		(queue->doorbell_nr << 8) | (queue->priority & 0xF));
+
+	/* Enable all interrupts for now */
+	kbase_csf_firmware_cs_input(stream, CS_ACK_IRQ_MASK, ~((u32)0));
+
+	/*
+	 * Enable the CSG idle notification once the stream's ringbuffer
+	 * becomes empty or the stream becomes sync_idle, waiting sync update
+	 * or protected mode switch.
+	 */
+	kbase_csf_firmware_cs_input_mask(stream, CS_REQ,
+			CS_REQ_IDLE_EMPTY_MASK | CS_REQ_IDLE_SYNC_WAIT_MASK,
+			CS_REQ_IDLE_EMPTY_MASK | CS_REQ_IDLE_SYNC_WAIT_MASK);
+
+	/* Set state to START/STOP */
+	kbase_csf_firmware_cs_input_mask(stream, CS_REQ,
+		queue->enabled ? CS_REQ_STATE_START : CS_REQ_STATE_STOP,
+		CS_REQ_STATE_MASK);
+
+	KBASE_KTRACE_ADD_CSF_GRP_Q(kbdev, CSI_START, group, queue, queue->enabled);
+
+	kbase_csf_ring_cs_kernel_doorbell(kbdev, queue);
+	update_hw_active(queue, true);
+}
+
+int kbase_csf_scheduler_queue_start(struct kbase_queue *queue)
+{
+	struct kbase_queue_group *group = queue->group;
+	struct kbase_device *kbdev = queue->kctx->kbdev;
+	bool const cs_enabled = queue->enabled;
+	int err = 0;
+	bool evicted = false;
+
+	lockdep_assert_held(&queue->kctx->csf.lock);
+
+	if (WARN_ON(!group || queue->bind_state != KBASE_CSF_QUEUE_BOUND))
+		return -EINVAL;
+
+	mutex_lock(&kbdev->csf.scheduler.lock);
+
+	KBASE_KTRACE_ADD_CSF_GRP_Q(kbdev, QUEUE_START, group, queue, group->run_state);
+	err = wait_gpu_reset(kbdev);
+
+	if (err) {
+		dev_warn(kbdev->dev, "Unsuccessful GPU reset detected when kicking queue (csi_index=%d) of group %d",
+			 queue->csi_index, group->handle);
+	} else if (group->run_state == KBASE_CSF_GROUP_FAULT_EVICTED) {
+		err = -EIO;
+		evicted = true;
+	} else if ((group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC)
+		   && CS_STATUS_WAIT_SYNC_WAIT_GET(queue->status_wait)) {
+		dev_dbg(kbdev->dev, "blocked queue(csi_index=%d) of group %d was kicked",
+			queue->csi_index, group->handle);
+	} else {
+		err = scheduler_group_schedule(group);
+
+		if (!err) {
+			queue->enabled = true;
+			if (kbasep_csf_scheduler_group_is_on_slot_locked(group)) {
+				if (cs_enabled) {
+					/* In normal situation, when a queue is
+					 * already running, the queue update
+					 * would be a doorbell kick on user
+					 * side. However, if such a kick is
+					 * shortly following a start or resume,
+					 * the queue may actually in transition
+					 * hence the said kick would enter the
+					 * kernel as the hw_active flag is yet
+					 * to be set. The sheduler needs to
+					 * give a kick to the corresponding
+					 * user door-bell on such a case.
+					 */
+					kbase_csf_ring_cs_user_doorbell(kbdev, queue);
+				} else
+					program_cs(kbdev, queue);
+			}
+			queue_delayed_work(system_long_wq,
+				&kbdev->csf.scheduler.ping_work,
+				msecs_to_jiffies(FIRMWARE_PING_INTERVAL_MS));
+		}
+	}
+
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+
+	if (evicted)
+		kbase_csf_term_descheduled_queue_group(group);
+
+	return err;
+}
+
+static enum kbase_csf_csg_slot_state update_csg_slot_status(
+				struct kbase_device *kbdev, s8 slot)
+{
+	struct kbase_csf_csg_slot *csg_slot =
+		&kbdev->csf.scheduler.csg_slots[slot];
+	struct kbase_csf_cmd_stream_group_info *ginfo =
+		&kbdev->csf.global_iface.groups[slot];
+	u32 state;
+	enum kbase_csf_csg_slot_state slot_state;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	state = CSG_ACK_STATE_GET(kbase_csf_firmware_csg_output(ginfo,
+			CSG_ACK));
+	slot_state = atomic_read(&csg_slot->state);
+
+	switch (slot_state) {
+	case CSG_SLOT_READY2RUN:
+		if ((state == CSG_ACK_STATE_START) ||
+		    (state == CSG_ACK_STATE_RESUME)) {
+			slot_state = CSG_SLOT_RUNNING;
+			atomic_set(&csg_slot->state, slot_state);
+			csg_slot->trigger_jiffies = jiffies;
+			KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_STARTED, csg_slot->resident_group, state);
+			dev_dbg(kbdev->dev, "Group %u running on slot %d\n",
+				csg_slot->resident_group->handle, slot);
+		}
+		break;
+	case CSG_SLOT_DOWN2STOP:
+		if ((state == CSG_ACK_STATE_SUSPEND) ||
+		    (state == CSG_ACK_STATE_TERMINATE)) {
+			slot_state = CSG_SLOT_STOPPED;
+			atomic_set(&csg_slot->state, slot_state);
+			csg_slot->trigger_jiffies = jiffies;
+			KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_STOPPED, csg_slot->resident_group, state);
+			dev_dbg(kbdev->dev, "Group %u stopped on slot %d\n",
+				csg_slot->resident_group->handle, slot);
+		}
+		break;
+	case CSG_SLOT_DOWN2STOP_TIMEDOUT:
+	case CSG_SLOT_READY2RUN_TIMEDOUT:
+	case CSG_SLOT_READY:
+	case CSG_SLOT_RUNNING:
+	case CSG_SLOT_STOPPED:
+		break;
+	default:
+		dev_warn(kbdev->dev, "Unknown CSG slot state %d", slot_state);
+		break;
+	}
+
+	return slot_state;
+}
+
+static bool csg_slot_running(struct kbase_device *kbdev, s8 slot)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	return (update_csg_slot_status(kbdev, slot) == CSG_SLOT_RUNNING);
+}
+
+static bool csg_slot_stopped_locked(struct kbase_device *kbdev, s8 slot)
+{
+	enum kbase_csf_csg_slot_state slot_state;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	slot_state = update_csg_slot_status(kbdev, slot);
+
+	return (slot_state == CSG_SLOT_STOPPED ||
+		slot_state == CSG_SLOT_READY);
+}
+
+static bool csg_slot_stopped_raw(struct kbase_device *kbdev, s8 slot)
+{
+	struct kbase_csf_cmd_stream_group_info *ginfo =
+		&kbdev->csf.global_iface.groups[slot];
+	u32 state;
+
+	state = CSG_ACK_STATE_GET(kbase_csf_firmware_csg_output(ginfo,
+			CSG_ACK));
+
+	if (state == CSG_ACK_STATE_SUSPEND || state == CSG_ACK_STATE_TERMINATE) {
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_STOPPED, kbdev->csf.scheduler.csg_slots[slot].resident_group, state);
+		dev_dbg(kbdev->dev, "(raw status) slot %d stopped\n", slot);
+		return true;
+	}
+
+	return false;
+}
+
+static void halt_csg_slot(struct kbase_queue_group *group, bool suspend)
+{
+	struct kbase_device *kbdev = group->kctx->kbdev;
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	struct kbase_csf_csg_slot *csg_slot =
+		kbdev->csf.scheduler.csg_slots;
+	s8 slot;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (WARN_ON(!kbasep_csf_scheduler_group_is_on_slot_locked(group)))
+		return;
+
+	slot = group->csg_nr;
+
+	/* When in transition, wait for it to complete */
+	if (atomic_read(&csg_slot[slot].state) == CSG_SLOT_READY2RUN) {
+		long remaining =
+		      kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+
+		dev_dbg(kbdev->dev, "slot %d wait for up-running\n", slot);
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+				csg_slot_running(kbdev, slot), remaining);
+		if (!remaining)
+			dev_warn(kbdev->dev,
+				 "slot %d timed out on up-running\n", slot);
+	}
+
+	if (csg_slot_running(kbdev, slot)) {
+		unsigned long flags;
+		struct kbase_csf_cmd_stream_group_info *ginfo =
+						&global_iface->groups[slot];
+		u32 halt_cmd = suspend ? CSG_REQ_STATE_SUSPEND :
+					 CSG_REQ_STATE_TERMINATE;
+
+		dev_dbg(kbdev->dev, "Halting(suspend=%d) group %d of context %d_%d on slot %d",
+			suspend, group->handle, group->kctx->tgid, group->kctx->id, slot);
+
+		spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, flags);
+		/* Set state to SUSPEND/TERMINATE */
+		kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ, halt_cmd,
+						  CSG_REQ_STATE_MASK);
+		spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock,
+					flags);
+		atomic_set(&csg_slot[slot].state, CSG_SLOT_DOWN2STOP);
+		csg_slot[slot].trigger_jiffies = jiffies;
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_STOP, group, halt_cmd);
+
+		kbase_csf_ring_csg_doorbell(kbdev, slot);
+	}
+}
+
+static void term_csg_slot(struct kbase_queue_group *group)
+{
+	halt_csg_slot(group, false);
+}
+
+static void suspend_csg_slot(struct kbase_queue_group *group)
+{
+	halt_csg_slot(group, true);
+}
+
+/**
+ * evaluate_sync_update() - Evaluate the sync wait condition the GPU command
+ *                          queue has been blocked on.
+ *
+ * @queue: Pointer to the GPU command queue
+ *
+ * Return: true if sync wait condition is satisfied.
+ */
+static bool evaluate_sync_update(struct kbase_queue *queue)
+{
+	enum kbase_csf_group_state run_state;
+	struct kbase_vmap_struct *mapping;
+	bool updated = false;
+	u32 *sync_ptr;
+	u32 sync_wait_cond;
+
+	if (WARN_ON(!queue))
+		return false;
+
+	run_state = queue->group->run_state;
+
+	if (WARN_ON((run_state != KBASE_CSF_GROUP_IDLE) &&
+		    (run_state != KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC)))
+		return false;
+
+	lockdep_assert_held(&queue->kctx->kbdev->csf.scheduler.lock);
+
+	sync_ptr = kbase_phy_alloc_mapping_get(queue->kctx, queue->sync_ptr,
+					&mapping);
+
+	if (!sync_ptr) {
+		dev_dbg(queue->kctx->kbdev->dev, "sync memory VA 0x%016llX already freed",
+			queue->sync_ptr);
+		return false;
+	}
+
+	sync_wait_cond =
+		CS_STATUS_WAIT_SYNC_WAIT_CONDITION_GET(queue->status_wait);
+
+	WARN_ON((sync_wait_cond != CS_STATUS_WAIT_SYNC_WAIT_CONDITION_GT) &&
+		(sync_wait_cond != CS_STATUS_WAIT_SYNC_WAIT_CONDITION_LE));
+
+	if (((sync_wait_cond == CS_STATUS_WAIT_SYNC_WAIT_CONDITION_GT) &&
+	     (*sync_ptr > queue->sync_value)) ||
+	    ((sync_wait_cond == CS_STATUS_WAIT_SYNC_WAIT_CONDITION_LE) &&
+	     (*sync_ptr <= queue->sync_value))) {
+		/* The sync wait condition is satisfied so the group to which
+		 * queue is bound can be re-scheduled.
+		 */
+		updated = true;
+	} else {
+		dev_dbg(queue->kctx->kbdev->dev, "sync memory not updated yet(%u)",
+			*sync_ptr);
+	}
+
+	kbase_phy_alloc_mapping_put(queue->kctx, mapping);
+
+	return updated;
+}
+
+/**
+ * save_slot_cs() -  Save the state for blocked GPU command queue.
+ *
+ * @ginfo: Pointer to the command stream group interface used by the group
+ *         the queue is bound to.
+ * @queue: Pointer to the GPU command queue.
+ *
+ * This function will check if GPU command queue is blocked on a sync wait and
+ * evaluate the wait condition. If the wait condition isn't satisfied it would
+ * save the state needed to reevaluate the condition in future.
+ * The group to which queue is bound shall be in idle state.
+ *
+ * Return: true if the queue is blocked on a sync wait operation.
+ */
+static
+bool save_slot_cs(struct kbase_csf_cmd_stream_group_info const *const ginfo,
+		struct kbase_queue *queue)
+{
+	struct kbase_csf_cmd_stream_info *const stream =
+		&ginfo->streams[queue->csi_index];
+	u32 status = kbase_csf_firmware_cs_output(stream, CS_STATUS_WAIT);
+	bool is_waiting = false;
+
+	WARN_ON(queue->group->run_state != KBASE_CSF_GROUP_IDLE);
+
+	if (CS_STATUS_WAIT_SYNC_WAIT_GET(status)) {
+		queue->status_wait = status;
+		queue->sync_ptr = kbase_csf_firmware_cs_output(stream,
+			CS_STATUS_WAIT_SYNC_POINTER_LO);
+		queue->sync_ptr |= (u64)kbase_csf_firmware_cs_output(stream,
+			CS_STATUS_WAIT_SYNC_POINTER_HI) << 32;
+		queue->sync_value = kbase_csf_firmware_cs_output(stream,
+			CS_STATUS_WAIT_SYNC_VALUE);
+
+		if (!evaluate_sync_update(queue)) {
+			is_waiting = true;
+		} else {
+			/* Sync object already got updated & met the condition
+			 * thus it doesn't need to be reevaluated and so can
+			 * clear the 'status_wait' here.
+			 */
+			queue->status_wait = 0;
+		}
+	} else {
+		/* Invalidate wait status info that would have been recorded if
+		 * this queue was blocked when the group (in idle state) was
+		 * suspended previously. After that the group could have been
+		 * unblocked due to the kicking of another queue bound to it &
+		 * so the wait status info would have stuck with this queue.
+		 */
+		queue->status_wait = 0;
+	}
+
+	return is_waiting;
+}
+
+/**
+ * Calculate how far in the future an event should be scheduled.
+ *
+ * The objective of this function is making sure that a minimum period of
+ * time is guaranteed between handling two consecutive events.
+ *
+ * This function guarantees a minimum period of time between two consecutive
+ * events: given the minimum period and the distance between the current time
+ * and the last event, the function returns the difference between the two.
+ * However, if more time than the minimum period has already elapsed
+ * since the last event, the function will return 0 to schedule work to handle
+ * the event with the lowest latency possible.
+ *
+ * @last_event: Timestamp of the last event, in jiffies.
+ * @time_now:   Timestamp of the new event to handle, in jiffies.
+ *              Must be successive to last_event.
+ * @period:     Minimum period between two events, in jiffies.
+ *
+ * Return:      Time to delay work to handle the current event, in jiffies
+ */
+static unsigned long get_schedule_delay(unsigned long last_event,
+					unsigned long time_now,
+					unsigned long period)
+{
+	const unsigned long t_distance = time_now - last_event;
+	const unsigned long delay_t = (t_distance < period) ?
+					(period - t_distance) : 0;
+
+	return delay_t;
+}
+
+static void schedule_in_cycle(struct kbase_queue_group *group, bool force)
+{
+	struct kbase_context *kctx = group->kctx;
+	struct kbase_device *kbdev = kctx->kbdev;
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	/* Only try to schedule work for this event if no requests are pending,
+	 * otherwise the function will end up canceling previous work requests,
+	 * and scheduler is configured to wake up periodically (or the schedule
+	 * of work needs to be enforced in situation such as entering into
+	 * protected mode).
+	 */
+	if ((likely(scheduler_timer_is_enabled_nolock(kbdev)) || force) &&
+			!scheduler->tock_pending_request) {
+		const unsigned long delay =
+			get_schedule_delay(scheduler->last_schedule, jiffies,
+					   CSF_SCHEDULER_TIME_TOCK_JIFFIES);
+		scheduler->tock_pending_request = true;
+		dev_dbg(kbdev->dev, "Kicking async for group %d\n",
+			group->handle);
+		mod_delayed_work(scheduler->wq, &scheduler->tock_work, delay);
+	}
+}
+
+static
+void insert_group_to_runnable(struct kbase_csf_scheduler *const scheduler,
+		struct kbase_queue_group *const group,
+		enum kbase_csf_group_state run_state)
+{
+	struct kbase_context *const kctx = group->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	WARN_ON(group->run_state != KBASE_CSF_GROUP_INACTIVE);
+
+	if (WARN_ON(group->priority >= BASE_QUEUE_GROUP_PRIORITY_COUNT))
+		return;
+
+	group->run_state = run_state;
+
+	if (run_state == KBASE_CSF_GROUP_RUNNABLE)
+		group->prepared_seq_num = KBASEP_GROUP_PREPARED_SEQ_NUM_INVALID;
+
+	list_add_tail(&group->link,
+			&kctx->csf.sched.runnable_groups[group->priority]);
+	kctx->csf.sched.num_runnable_grps++;
+	/* Add the kctx if not yet in runnable kctxs */
+	if (kctx->csf.sched.num_runnable_grps == 1) {
+		/* First runnable csg, adds to the runnable_kctxs */
+		INIT_LIST_HEAD(&kctx->csf.link);
+		list_add_tail(&kctx->csf.link, &scheduler->runnable_kctxs);
+	}
+
+	scheduler->total_runnable_grps++;
+
+	if (likely(scheduler_timer_is_enabled_nolock(kbdev)) &&
+	    (scheduler->total_runnable_grps == 1 ||
+	     scheduler->state == SCHED_SUSPENDED)) {
+		dev_dbg(kbdev->dev, "Kicking scheduler on first runnable group\n");
+		/* Fire a scheduling to start the time-slice */
+		mod_delayed_work(kbdev->csf.scheduler.wq,
+				 &kbdev->csf.scheduler.tick_work, 0);
+	} else
+		schedule_in_cycle(group, false);
+
+	/* Since a new group has become runnable, check if GPU needs to be
+	 * powered up.
+	 */
+	scheduler_wakeup(kbdev, false);
+}
+
+static
+void remove_group_from_runnable(struct kbase_csf_scheduler *const scheduler,
+		struct kbase_queue_group *group,
+		enum kbase_csf_group_state run_state)
+{
+	struct kbase_context *kctx = group->kctx;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	WARN_ON(!queue_group_scheduled_locked(group));
+
+	group->run_state = run_state;
+	list_del_init(&group->link);
+
+	if (scheduler->top_grp == group) {
+		/*
+		 * Note: this disables explicit rotation in the next scheduling
+		 * cycle. However, removing the top_grp is the same as an
+		 * implicit rotation (e.g. if we instead rotated the top_ctx
+		 * and then remove top_grp)
+		 *
+		 * This implicit rotation is assumed by the scheduler rotate
+		 * functions.
+		 */
+		scheduler->top_grp = NULL;
+
+		/*
+		 * Trigger a scheduling tock for a CSG containing protected
+		 * content in case there has been any in order to minimise
+		 * latency.
+		 */
+		group = scheduler_get_protm_enter_async_group(kctx->kbdev,
+							      NULL);
+		if (group)
+			schedule_in_cycle(group, true);
+	}
+
+	kctx->csf.sched.num_runnable_grps--;
+	if (kctx->csf.sched.num_runnable_grps == 0) {
+		/* drop the kctx */
+		list_del_init(&kctx->csf.link);
+		if (scheduler->top_ctx == kctx)
+			scheduler->top_ctx = NULL;
+	}
+
+	WARN_ON(scheduler->total_runnable_grps == 0);
+	scheduler->total_runnable_grps--;
+	if (!scheduler->total_runnable_grps &&
+	    scheduler->state != SCHED_SUSPENDED) {
+		dev_dbg(kctx->kbdev->dev, "Scheduler idle as no runnable groups");
+		mod_delayed_work(system_wq, &scheduler->gpu_idle_work,
+				 GPU_IDLE_POWEROFF_HYSTERESIS_DELAY);
+	}
+	KBASE_KTRACE_ADD_CSF_GRP(kctx->kbdev, SCHEDULER_TOP_GRP, scheduler->top_grp,
+			scheduler->num_active_address_spaces |
+			(((u64)scheduler->total_runnable_grps) << 32));
+}
+
+static void insert_group_to_idle_wait(struct kbase_queue_group *const group)
+{
+	struct kbase_context *kctx = group->kctx;
+
+	lockdep_assert_held(&kctx->kbdev->csf.scheduler.lock);
+
+	WARN_ON(group->run_state != KBASE_CSF_GROUP_IDLE);
+
+	list_add_tail(&group->link, &kctx->csf.sched.idle_wait_groups);
+	kctx->csf.sched.num_idle_wait_grps++;
+	group->run_state = KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC;
+	dev_dbg(kctx->kbdev->dev,
+		"Group-%d suspended on sync_wait, total wait_groups: %u\n",
+		group->handle, kctx->csf.sched.num_idle_wait_grps);
+}
+
+static void remove_group_from_idle_wait(struct kbase_queue_group *const group)
+{
+	struct kbase_context *kctx = group->kctx;
+
+	lockdep_assert_held(&kctx->kbdev->csf.scheduler.lock);
+
+	WARN_ON(group->run_state != KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC);
+
+	list_del_init(&group->link);
+	WARN_ON(kctx->csf.sched.num_idle_wait_grps == 0);
+	kctx->csf.sched.num_idle_wait_grps--;
+	group->run_state = KBASE_CSF_GROUP_INACTIVE;
+}
+
+static void deschedule_idle_wait_group(struct kbase_csf_scheduler *scheduler,
+		struct kbase_queue_group *group)
+{
+	lockdep_assert_held(&scheduler->lock);
+
+	if (WARN_ON(!group))
+		return;
+
+	remove_group_from_runnable(scheduler, group, KBASE_CSF_GROUP_IDLE);
+	insert_group_to_idle_wait(group);
+}
+
+static bool confirm_cs_idle(struct kbase_queue *queue)
+{
+	u64 *input_addr = (u64 *)queue->user_io_addr;
+	u64 *output_addr = (u64 *)(queue->user_io_addr + PAGE_SIZE);
+
+	return (input_addr[CS_INSERT_LO / sizeof(u64)] ==
+		output_addr[CS_EXTRACT_LO / sizeof(u64)]);
+}
+
+static void save_csg_slot(struct kbase_queue_group *group)
+{
+	struct kbase_device *kbdev = group->kctx->kbdev;
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	u32 state;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	if (WARN_ON(!kbasep_csf_scheduler_group_is_on_slot_locked(group)))
+		return;
+
+	ginfo = &kbdev->csf.global_iface.groups[group->csg_nr];
+
+	state =
+	    CSG_ACK_STATE_GET(kbase_csf_firmware_csg_output(ginfo, CSG_ACK));
+
+	if (!WARN_ON((state != CSG_ACK_STATE_SUSPEND) &&
+		     (state != CSG_ACK_STATE_TERMINATE))) {
+		int i;
+
+#ifdef CONFIG_MALI_BIFROST_NO_MALI
+		for (i = 0; i < MAX_SUPPORTED_STREAMS_PER_GROUP; i++)
+			update_hw_active(group->bound_queues[i], false);
+#endif
+		if (group->run_state == KBASE_CSF_GROUP_IDLE) {
+			bool sync_wait = false;
+			bool idle = true;
+
+			/* Loop through all bound CSs & save their context */
+			for (i = 0; i < MAX_SUPPORTED_STREAMS_PER_GROUP; i++) {
+				struct kbase_queue *const queue =
+					group->bound_queues[i];
+
+				if (queue && queue->enabled) {
+					if (save_slot_cs(ginfo, queue))
+						sync_wait = true;
+					else if (idle)
+						idle = confirm_cs_idle(queue);
+				}
+			}
+
+			/* Take the suspended group out of the runnable_groups
+			 * list of the context and move it to the
+			 * idle_wait_groups list.
+			 */
+			if (sync_wait && idle)
+				deschedule_idle_wait_group(scheduler, group);
+			else if (idle) {
+				group->run_state =
+					KBASE_CSF_GROUP_SUSPENDED_ON_IDLE;
+				dev_dbg(kbdev->dev, "Group-%d suspended: idle\n",
+					group->handle);
+			} else {
+				group->run_state = KBASE_CSF_GROUP_SUSPENDED;
+				atomic_inc(&scheduler->non_idle_suspended_grps);
+			}
+		} else {
+			group->run_state = KBASE_CSF_GROUP_SUSPENDED;
+			atomic_inc(&scheduler->non_idle_suspended_grps);
+		}
+	}
+}
+
+/* Cleanup_csg_slot after it has been vacated, ready for next csg run.
+ * Return whether there is a kctx address fault associated with the group
+ * for which the clean-up is done.
+ */
+static bool cleanup_csg_slot(struct kbase_queue_group *group)
+{
+	struct kbase_context *kctx = group->kctx;
+	struct kbase_device *kbdev = kctx->kbdev;
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	s8 slot;
+	struct kbase_csf_csg_slot *csg_slot;
+	unsigned long flags;
+	u32 i;
+	bool as_fault = false;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (WARN_ON(!kbasep_csf_scheduler_group_is_on_slot_locked(group)))
+		return as_fault;
+
+	slot = group->csg_nr;
+	csg_slot = &kbdev->csf.scheduler.csg_slots[slot];
+	ginfo = &global_iface->groups[slot];
+
+	/* Now loop through all the bound CSs, and clean them via a stop */
+	for (i = 0; i < ginfo->stream_num; i++) {
+		struct kbase_csf_cmd_stream_info *stream = &ginfo->streams[i];
+
+		if (group->bound_queues[i]) {
+			if (group->bound_queues[i]->enabled) {
+				kbase_csf_firmware_cs_input_mask(stream,
+					CS_REQ, CS_REQ_STATE_STOP,
+					CS_REQ_STATE_MASK);
+			}
+
+			unassign_user_doorbell_from_queue(kbdev,
+				group->bound_queues[i]);
+		}
+	}
+
+	unassign_user_doorbell_from_group(kbdev, group);
+
+	/* The csg does not need cleanup other than drop its AS */
+	spin_lock_irqsave(&kctx->kbdev->hwaccess_lock, flags);
+	as_fault = kbase_ctx_flag(kctx, KCTX_AS_DISABLED_ON_FAULT);
+	kbase_ctx_sched_release_ctx(kctx);
+	if (unlikely(group->faulted))
+		as_fault = true;
+	spin_unlock_irqrestore(&kctx->kbdev->hwaccess_lock, flags);
+
+	/* now marking the slot is vacant */
+	spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, flags);
+	kbdev->csf.scheduler.csg_slots[slot].resident_group = NULL;
+	group->csg_nr = KBASEP_CSG_NR_INVALID;
+	clear_bit(slot, kbdev->csf.scheduler.csg_slots_idle_mask);
+	set_bit(slot, kbdev->csf.scheduler.csgs_events_enable_mask);
+	clear_bit(slot, kbdev->csf.scheduler.csg_inuse_bitmap);
+	spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock, flags);
+
+	csg_slot->trigger_jiffies = jiffies;
+	atomic_set(&csg_slot->state, CSG_SLOT_READY);
+
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_CLEANED, group, slot);
+	dev_dbg(kbdev->dev, "Cleanup done for group %d on slot %d\n",
+		group->handle, slot);
+
+	KBASE_TLSTREAM_TL_KBASE_DEVICE_DEPROGRAM_CSG(kbdev,
+		kbdev->gpu_props.props.raw_props.gpu_id, slot);
+
+	return as_fault;
+}
+
+static void update_csg_slot_priority(struct kbase_queue_group *group, u8 prio)
+{
+	struct kbase_device *kbdev = group->kctx->kbdev;
+	struct kbase_csf_csg_slot *csg_slot;
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	s8 slot;
+	u8 prev_prio;
+	u32 ep_cfg;
+	u32 csg_req;
+	unsigned long flags;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (WARN_ON(!kbasep_csf_scheduler_group_is_on_slot_locked(group)))
+		return;
+
+	slot = group->csg_nr;
+	csg_slot = &kbdev->csf.scheduler.csg_slots[slot];
+	ginfo = &kbdev->csf.global_iface.groups[slot];
+
+	WARN_ON(!((group->run_state == KBASE_CSF_GROUP_RUNNABLE) ||
+		(group->run_state == KBASE_CSF_GROUP_IDLE)));
+
+	group->run_state = KBASE_CSF_GROUP_RUNNABLE;
+
+	if (csg_slot->priority == prio)
+		return;
+
+	/* Read the csg_ep_cfg back for updating the priority field */
+	ep_cfg = kbase_csf_firmware_csg_input_read(ginfo, CSG_EP_REQ);
+	prev_prio = CSG_EP_REQ_PRIORITY_GET(ep_cfg);
+	ep_cfg = CSG_EP_REQ_PRIORITY_SET(ep_cfg, prio);
+	kbase_csf_firmware_csg_input(ginfo, CSG_EP_REQ, ep_cfg);
+
+	spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, flags);
+	csg_req = kbase_csf_firmware_csg_output(ginfo, CSG_ACK);
+	csg_req ^= CSG_REQ_EP_CFG;
+	kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ, csg_req,
+					  CSG_REQ_EP_CFG);
+	spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock, flags);
+
+	csg_slot->priority = prio;
+
+	dev_dbg(kbdev->dev, "Priority for group %d of context %d_%d on slot %d to be updated from %u to %u\n",
+		group->handle, group->kctx->tgid, group->kctx->id, slot,
+		prev_prio, prio);
+
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_PRIO_UPDATE, group, prev_prio);
+
+	kbase_csf_ring_csg_doorbell(kbdev, slot);
+	set_bit(slot, kbdev->csf.scheduler.csg_slots_prio_update);
+}
+
+static void program_csg_slot(struct kbase_queue_group *group, s8 slot,
+		u8 prio)
+{
+	struct kbase_context *kctx = group->kctx;
+	struct kbase_device *kbdev = kctx->kbdev;
+	struct kbase_csf_global_iface *global_iface = &kbdev->csf.global_iface;
+	const u64 shader_core_mask =
+		kbase_pm_get_present_cores(kbdev, KBASE_PM_CORE_SHADER);
+	const u64 tiler_core_mask =
+		kbase_pm_get_present_cores(kbdev, KBASE_PM_CORE_TILER);
+	const u64 compute_mask = shader_core_mask & group->compute_mask;
+	const u64 fragment_mask = shader_core_mask & group->fragment_mask;
+	const u64 tiler_mask = tiler_core_mask & group->tiler_mask;
+	const u8 num_cores = kbdev->gpu_props.num_cores;
+	const u8 compute_max = min(num_cores, group->compute_max);
+	const u8 fragment_max = min(num_cores, group->fragment_max);
+	const u8 tiler_max = min(CSG_TILER_MAX, group->tiler_max);
+	struct kbase_csf_cmd_stream_group_info *ginfo;
+	u32 ep_cfg = 0;
+	u32 csg_req;
+	u32 state;
+	int i;
+	unsigned long flags;
+	const u64 normal_suspend_buf =
+		group->normal_suspend_buf.reg->start_pfn << PAGE_SHIFT;
+	struct kbase_csf_csg_slot *csg_slot =
+		&kbdev->csf.scheduler.csg_slots[slot];
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (WARN_ON(slot < 0) &&
+	    WARN_ON(slot >= global_iface->group_num))
+		return;
+
+	WARN_ON(atomic_read(&csg_slot->state) != CSG_SLOT_READY);
+
+	ginfo = &global_iface->groups[slot];
+
+	/* Pick an available address space for this context */
+	mutex_lock(&kbdev->mmu_hw_mutex);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_ctx_sched_retain_ctx(kctx);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
+
+	if (kctx->as_nr == KBASEP_AS_NR_INVALID) {
+		dev_dbg(kbdev->dev, "Could not get a valid AS for group %d of context %d_%d on slot %d\n",
+			group->handle, kctx->tgid, kctx->id, slot);
+		return;
+	}
+
+	spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, flags);
+	set_bit(slot, kbdev->csf.scheduler.csg_inuse_bitmap);
+	kbdev->csf.scheduler.csg_slots[slot].resident_group = group;
+	group->csg_nr = slot;
+	spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock, flags);
+
+	assign_user_doorbell_to_group(kbdev, group);
+
+	/* Now loop through all the bound & kicked CSs, and program them */
+	for (i = 0; i < MAX_SUPPORTED_STREAMS_PER_GROUP; i++) {
+		struct kbase_queue *queue = group->bound_queues[i];
+
+		if (queue)
+			program_cs(kbdev, queue);
+	}
+
+
+	/* Endpoint programming for CSG */
+	kbase_csf_firmware_csg_input(ginfo, CSG_ALLOW_COMPUTE_LO,
+				     compute_mask & U32_MAX);
+	kbase_csf_firmware_csg_input(ginfo, CSG_ALLOW_COMPUTE_HI,
+				     compute_mask >> 32);
+	kbase_csf_firmware_csg_input(ginfo, CSG_ALLOW_FRAGMENT_LO,
+				     fragment_mask & U32_MAX);
+	kbase_csf_firmware_csg_input(ginfo, CSG_ALLOW_FRAGMENT_HI,
+				     fragment_mask >> 32);
+	kbase_csf_firmware_csg_input(ginfo, CSG_ALLOW_OTHER,
+				     tiler_mask & U32_MAX);
+
+	ep_cfg = CSG_EP_REQ_COMPUTE_EP_SET(ep_cfg, compute_max);
+	ep_cfg = CSG_EP_REQ_FRAGMENT_EP_SET(ep_cfg, fragment_max);
+	ep_cfg = CSG_EP_REQ_TILER_EP_SET(ep_cfg, tiler_max);
+	ep_cfg = CSG_EP_REQ_PRIORITY_SET(ep_cfg, prio);
+	kbase_csf_firmware_csg_input(ginfo, CSG_EP_REQ, ep_cfg);
+
+	/* Program the address space number assigned to the context */
+	kbase_csf_firmware_csg_input(ginfo, CSG_CONFIG, kctx->as_nr);
+
+	kbase_csf_firmware_csg_input(ginfo, CSG_SUSPEND_BUF_LO,
+			normal_suspend_buf & U32_MAX);
+	kbase_csf_firmware_csg_input(ginfo, CSG_SUSPEND_BUF_HI,
+			normal_suspend_buf >> 32);
+
+	if (group->protected_suspend_buf.reg) {
+		const u64 protm_suspend_buf =
+			group->protected_suspend_buf.reg->start_pfn <<
+				PAGE_SHIFT;
+		kbase_csf_firmware_csg_input(ginfo, CSG_PROTM_SUSPEND_BUF_LO,
+			protm_suspend_buf & U32_MAX);
+		kbase_csf_firmware_csg_input(ginfo, CSG_PROTM_SUSPEND_BUF_HI,
+			protm_suspend_buf >> 32);
+	}
+
+	/* Enable all interrupts for now */
+	kbase_csf_firmware_csg_input(ginfo, CSG_ACK_IRQ_MASK, ~((u32)0));
+
+	spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, flags);
+	csg_req = kbase_csf_firmware_csg_output(ginfo, CSG_ACK);
+	csg_req ^= CSG_REQ_EP_CFG;
+	kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ, csg_req,
+					  CSG_REQ_EP_CFG);
+
+	/* Set state to START/RESUME */
+	if (queue_group_suspended_locked(group)) {
+		state = CSG_REQ_STATE_RESUME;
+		if (group->run_state == KBASE_CSF_GROUP_SUSPENDED)
+			atomic_dec(
+				&kbdev->csf.scheduler.non_idle_suspended_grps);
+	} else {
+		WARN_ON(group->run_state != KBASE_CSF_GROUP_RUNNABLE);
+		state = CSG_REQ_STATE_START;
+	}
+
+	kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ,
+			state, CSG_REQ_STATE_MASK);
+	spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock, flags);
+
+	/* Update status before rings the door-bell, marking ready => run */
+	atomic_set(&csg_slot->state, CSG_SLOT_READY2RUN);
+	csg_slot->trigger_jiffies = jiffies;
+	csg_slot->priority = prio;
+
+	/* Trace the programming of the CSG on the slot */
+	KBASE_TLSTREAM_TL_KBASE_DEVICE_PROGRAM_CSG(kbdev,
+		kbdev->gpu_props.props.raw_props.gpu_id, group->handle, slot);
+
+	dev_dbg(kbdev->dev, "Starting group %d of context %d_%d on slot %d with priority %u\n",
+		group->handle, kctx->tgid, kctx->id, slot, prio);
+
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, CSG_SLOT_START, group,
+				(((u64)ep_cfg) << 32) |
+				((((u32)kctx->as_nr) & 0xF) << 16) |
+				(state & (CSG_REQ_STATE_MASK >> CS_REQ_STATE_SHIFT)));
+
+	kbase_csf_ring_csg_doorbell(kbdev, slot);
+}
+
+static void remove_scheduled_group(struct kbase_device *kbdev,
+		struct kbase_queue_group *group)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	WARN_ON(group->prepared_seq_num ==
+		KBASEP_GROUP_PREPARED_SEQ_NUM_INVALID);
+	WARN_ON(list_empty(&group->link_to_schedule));
+
+	list_del_init(&group->link_to_schedule);
+	scheduler->ngrp_to_schedule--;
+	group->prepared_seq_num = KBASEP_GROUP_PREPARED_SEQ_NUM_INVALID;
+	group->kctx->csf.sched.ngrp_to_schedule--;
+}
+
+static void sched_evict_group(struct kbase_queue_group *group, bool fault)
+{
+	struct kbase_context *kctx = group->kctx;
+	struct kbase_device *kbdev = kctx->kbdev;
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (queue_group_scheduled_locked(group)) {
+		u32 i;
+
+		if (group->run_state == KBASE_CSF_GROUP_SUSPENDED)
+			atomic_dec(&scheduler->non_idle_suspended_grps);
+
+		for (i = 0; i < MAX_SUPPORTED_STREAMS_PER_GROUP; i++) {
+			if (group->bound_queues[i])
+				group->bound_queues[i]->enabled = false;
+		}
+
+		if (group->prepared_seq_num !=
+				KBASEP_GROUP_PREPARED_SEQ_NUM_INVALID)
+			remove_scheduled_group(kbdev, group);
+
+		if (group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC)
+			remove_group_from_idle_wait(group);
+		else {
+			remove_group_from_runnable(scheduler, group,
+						KBASE_CSF_GROUP_INACTIVE);
+		}
+
+		WARN_ON(group->run_state != KBASE_CSF_GROUP_INACTIVE);
+
+		if (fault)
+			group->run_state = KBASE_CSF_GROUP_FAULT_EVICTED;
+
+		KBASE_KTRACE_ADD_CSF_GRP(kbdev, GROUP_EVICT_SCHED, group,
+				(((u64)scheduler->total_runnable_grps) << 32) |
+				((u32)group->run_state));
+		dev_dbg(kbdev->dev, "group %d exited scheduler, num_runnable_grps %d\n",
+			group->handle, scheduler->total_runnable_grps);
+		/* Notify a group has been evicted */
+		wake_up_all(&kbdev->csf.event_wait);
+	}
+}
+
+static int term_group_sync(struct kbase_queue_group *group)
+{
+	struct kbase_device *kbdev = group->kctx->kbdev;
+	long remaining =
+		kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+	int err = 0;
+
+	term_csg_slot(group);
+
+	remaining = wait_event_timeout(kbdev->csf.event_wait,
+		csg_slot_stopped_locked(kbdev, group->csg_nr), remaining);
+
+	if (!remaining) {
+		dev_warn(kbdev->dev, "term request timed out for group %d on slot %d",
+			 group->handle, group->csg_nr);
+		if (kbase_prepare_to_reset_gpu(kbdev))
+			kbase_reset_gpu(kbdev);
+		err = -ETIMEDOUT;
+	}
+
+	return err;
+}
+
+void kbase_csf_scheduler_group_deschedule(struct kbase_queue_group *group)
+{
+	struct kbase_device *kbdev = group->kctx->kbdev;
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	long remaining =
+		kbase_csf_timeout_in_jiffies(CSG_SCHED_STOP_TIMEOUT_MS);
+	bool force = false;
+
+	lockdep_assert_held(&group->kctx->csf.lock);
+	mutex_lock(&scheduler->lock);
+
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, GROUP_DESCHEDULE, group, group->run_state);
+	while (queue_group_scheduled_locked(group)) {
+		u32 saved_state = scheduler->state;
+		bool reset = kbase_reset_gpu_is_active(kbdev);
+
+		if (!kbasep_csf_scheduler_group_is_on_slot_locked(group)) {
+			sched_evict_group(group, false);
+		} else if (reset || saved_state == SCHED_INACTIVE || force) {
+			bool as_faulty;
+
+			if (!reset)
+				term_group_sync(group);
+			/* Treat the csg been terminated */
+			as_faulty = cleanup_csg_slot(group);
+			/* remove from the scheduler list */
+			sched_evict_group(group, as_faulty);
+		}
+
+		/* waiting scheduler state to change */
+		if (queue_group_scheduled_locked(group)) {
+			mutex_unlock(&scheduler->lock);
+			remaining = wait_event_timeout(
+					kbdev->csf.event_wait,
+					saved_state != scheduler->state,
+					remaining);
+			if (!remaining) {
+				dev_warn(kbdev->dev, "Scheduler state change wait timed out for group %d on slot %d",
+					 group->handle, group->csg_nr);
+				force = true;
+			}
+			mutex_lock(&scheduler->lock);
+		}
+	}
+
+	mutex_unlock(&scheduler->lock);
+}
+
+/**
+ * scheduler_group_schedule() - Schedule a GPU command queue group on firmware
+ *
+ * @group: Pointer to the queue group to be scheduled.
+ *
+ * This function would enable the scheduling of GPU command queue group on
+ * firmware.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+static int scheduler_group_schedule(struct kbase_queue_group *group)
+{
+	struct kbase_context *kctx = group->kctx;
+	struct kbase_device *kbdev = kctx->kbdev;
+
+	lockdep_assert_held(&kctx->csf.lock);
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, GROUP_SCHEDULE, group, group->run_state);
+	if (group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC)
+		update_idle_suspended_group_state(group);
+	else if (queue_group_idle_locked(group)) {
+		WARN_ON(kctx->csf.sched.num_runnable_grps == 0);
+		WARN_ON(kbdev->csf.scheduler.total_runnable_grps == 0);
+
+		if (group->run_state == KBASE_CSF_GROUP_SUSPENDED_ON_IDLE)
+			update_idle_suspended_group_state(group);
+		else
+			group->run_state = KBASE_CSF_GROUP_RUNNABLE;
+	} else if (!queue_group_scheduled_locked(group)) {
+		insert_group_to_runnable(&kbdev->csf.scheduler, group,
+			KBASE_CSF_GROUP_RUNNABLE);
+	}
+
+	/* Since a group has become active now, check if GPU needs to be
+	 * powered up. Also rekick the Scheduler.
+	 */
+	scheduler_wakeup(kbdev, true);
+
+	return 0;
+}
+
+/**
+ * set_max_csg_slots() - Set the number of available command stream group slots
+ *
+ * @kbdev: Pointer of the GPU device.
+ *
+ * This function would set/limit the number of command stream group slots that
+ * can be used in the given tick/tock. It would be less than the total command
+ * stream group slots supported by firmware if the number of GPU address space
+ * slots required to utilize all the CSG slots is more than the available
+ * address space slots.
+ */
+static inline void set_max_csg_slots(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	unsigned int total_csg_slots = kbdev->csf.global_iface.group_num;
+	unsigned int max_address_space_slots = kbdev->nr_hw_address_spaces - 1;
+
+	WARN_ON(scheduler->num_active_address_spaces > total_csg_slots);
+
+	if (likely(scheduler->num_active_address_spaces <=
+		   max_address_space_slots))
+		scheduler->num_csg_slots_for_tick = total_csg_slots;
+}
+
+/**
+ * count_active_address_space() - Count the number of GPU address space slots
+ *
+ * @kbdev: Pointer of the GPU device.
+ * @kctx: Pointer of the Kbase context.
+ *
+ * This function would update the counter that is tracking the number of GPU
+ * address space slots that would be required to program the command stream
+ * group slots from the groups at the head of groups_to_schedule list.
+ */
+static inline void count_active_address_space(struct kbase_device *kbdev,
+		struct kbase_context *kctx)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	unsigned int total_csg_slots = kbdev->csf.global_iface.group_num;
+	unsigned int max_address_space_slots = kbdev->nr_hw_address_spaces - 1;
+
+	if (scheduler->ngrp_to_schedule <= total_csg_slots) {
+		if (kctx->csf.sched.ngrp_to_schedule == 1) {
+			scheduler->num_active_address_spaces++;
+
+			if (scheduler->num_active_address_spaces <=
+			    max_address_space_slots)
+				scheduler->num_csg_slots_for_tick++;
+		}
+	}
+}
+
+/**
+ * update_resident_groups_priority() - Update the priority of resident groups
+ *
+ * @kbdev:    The GPU device.
+ *
+ * This function will update the priority of all resident queue groups
+ * that are at the head of groups_to_schedule list, preceding the first
+ * non-resident group.
+ *
+ * This function will also adjust kbase_csf_scheduler.head_slot_priority on
+ * the priority update.
+ */
+static void update_resident_groups_priority(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	u32 num_groups = scheduler->num_csg_slots_for_tick;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+	while (!list_empty(&scheduler->groups_to_schedule)) {
+		struct kbase_queue_group *group =
+			list_first_entry(&scheduler->groups_to_schedule,
+					struct kbase_queue_group,
+					 link_to_schedule);
+		bool resident =
+			kbasep_csf_scheduler_group_is_on_slot_locked(group);
+
+		if ((group->prepared_seq_num >= num_groups) || !resident)
+			break;
+
+		update_csg_slot_priority(group,
+					 scheduler->head_slot_priority);
+
+		/* Drop the head group from the list */
+		remove_scheduled_group(kbdev, group);
+		scheduler->head_slot_priority--;
+	}
+}
+
+/**
+ * program_group_on_vacant_csg_slot() - Program a non-resident group on the
+ *                                      given vacant CSG slot.
+ * @kbdev:    Pointer to the GPU device.
+ * @slot:     Vacant command stream group slot number.
+ *
+ * This function will program a non-resident group at the head of
+ * kbase_csf_scheduler.groups_to_schedule list on the given vacant command
+ * stream group slot, provided the initial position of the non-resident
+ * group in the list is less than the number of CSG slots and there is
+ * an available GPU address space slot.
+ * kbase_csf_scheduler.head_slot_priority would also be adjusted after
+ * programming the slot.
+ */
+static void program_group_on_vacant_csg_slot(struct kbase_device *kbdev,
+		s8 slot)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct kbase_queue_group *const group =
+		list_empty(&scheduler->groups_to_schedule) ? NULL :
+			list_first_entry(&scheduler->groups_to_schedule,
+					struct kbase_queue_group,
+					link_to_schedule);
+	u32 num_groups = scheduler->num_csg_slots_for_tick;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+	if (group && (group->prepared_seq_num < num_groups)) {
+		bool ret = kbasep_csf_scheduler_group_is_on_slot_locked(group);
+
+		if (!WARN_ON(ret)) {
+			if (kctx_as_enabled(group->kctx) && !group->faulted) {
+				program_csg_slot(group,
+					 slot,
+					 scheduler->head_slot_priority);
+
+				if (likely(csg_slot_in_use(kbdev, slot))) {
+					/* Drop the head group from the list */
+					remove_scheduled_group(kbdev, group);
+					scheduler->head_slot_priority--;
+				}
+			} else
+				remove_scheduled_group(kbdev, group);
+		}
+	}
+}
+
+/**
+ * program_vacant_csg_slot() - Program the vacant CSG slot with a non-resident
+ *                             group and update the priority of resident groups.
+ *
+ * @kbdev:    Pointer to the GPU device.
+ * @slot:     Vacant command stream group slot number.
+ *
+ * This function will first update the priority of all resident queue groups
+ * that are at the head of groups_to_schedule list, preceding the first
+ * non-resident group, it will then try to program the given command stream
+ * group slot with the non-resident group. Finally update the priority of all
+ * resident queue groups following the non-resident group.
+ *
+ * kbase_csf_scheduler.head_slot_priority would also be adjusted.
+ */
+static void program_vacant_csg_slot(struct kbase_device *kbdev, s8 slot)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	struct kbase_csf_csg_slot *const csg_slot =
+				scheduler->csg_slots;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+	WARN_ON(atomic_read(&csg_slot[slot].state) != CSG_SLOT_READY);
+
+	/* First update priority for already resident groups (if any)
+	 * before the non-resident group
+	 */
+	update_resident_groups_priority(kbdev);
+
+	/* Now consume the vacant slot for the non-resident group */
+	program_group_on_vacant_csg_slot(kbdev, slot);
+
+	/* Now update priority for already resident groups (if any)
+	 * following the non-resident group
+	 */
+	update_resident_groups_priority(kbdev);
+}
+
+static bool slots_state_changed(struct kbase_device *kbdev,
+		unsigned long *slots_mask,
+		bool (*state_check_func)(struct kbase_device *, s8))
+{
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	DECLARE_BITMAP(changed_slots, MAX_SUPPORTED_CSGS) = {0};
+	bool changed = false;
+	u32 i;
+
+	for_each_set_bit(i, slots_mask, num_groups) {
+		if (state_check_func(kbdev, (s8)i)) {
+			set_bit(i, changed_slots);
+			changed = true;
+		}
+	}
+
+	if (changed)
+		bitmap_copy(slots_mask, changed_slots, MAX_SUPPORTED_CSGS);
+
+	return changed;
+}
+
+/**
+ * program_suspending_csg_slots() - Program the CSG slots vacated on suspension
+ *                                  of queue groups running on them.
+ *
+ * @kbdev:    Pointer to the GPU device.
+ *
+ * This function will first wait for the ongoing suspension to complete on a
+ * command stream group slot and will then program the vacant slot with the
+ * non-resident queue group inside the groups_to_schedule list.
+ * The programming of the non-resident queue group on the vacant slot could
+ * fail due to unavailability of free GPU address space slot and so the
+ * programming is re-attempted after the ongoing suspension has completed
+ * for all the command stream group slots.
+ * The priority of resident groups before and after the non-resident group
+ * in the groups_to_schedule list would also be updated.
+ * This would be repeated for all the slots undergoing suspension.
+ * GPU reset would be initiated if the wait for suspend times out.
+ */
+static void program_suspending_csg_slots(struct kbase_device *kbdev)
+{
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS);
+	DECLARE_BITMAP(evicted_mask, MAX_SUPPORTED_CSGS) = {0};
+	bool suspend_wait_failed = false;
+	long remaining =
+		kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	bitmap_complement(slot_mask, scheduler->csgs_events_enable_mask,
+		MAX_SUPPORTED_CSGS);
+
+	while (!bitmap_empty(slot_mask, MAX_SUPPORTED_CSGS)) {
+		DECLARE_BITMAP(changed, MAX_SUPPORTED_CSGS);
+
+		bitmap_copy(changed, slot_mask, MAX_SUPPORTED_CSGS);
+
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+			slots_state_changed(kbdev, changed,
+				csg_slot_stopped_raw),
+			remaining);
+
+		if (remaining) {
+			u32 i;
+
+			for_each_set_bit(i, changed, num_groups) {
+				struct kbase_queue_group *group =
+					scheduler->csg_slots[i].resident_group;
+
+				if (WARN_ON(!csg_slot_stopped_locked(kbdev, (s8)i))) {
+					continue;
+				}
+				/* The on slot csg is now stopped */
+				clear_bit(i, slot_mask);
+
+				if (likely(group)) {
+					bool as_fault;
+					/* Only do save/cleanup if the
+					 * group is not terminated during
+					 * the sleep.
+					 */
+					save_csg_slot(group);
+					as_fault = cleanup_csg_slot(group);
+					/* If AS fault detected, evict it */
+					if (as_fault) {
+						sched_evict_group(group, true);
+						set_bit(i, evicted_mask);
+					}
+				}
+
+				program_vacant_csg_slot(kbdev, (s8)i);
+			}
+		} else {
+			dev_warn(kbdev->dev, "Timed out waiting for CSG slots to suspend, slot_mask: 0x%*pb\n",
+				 num_groups, slot_mask);
+
+			if (kbase_prepare_to_reset_gpu(kbdev))
+				kbase_reset_gpu(kbdev);
+			suspend_wait_failed = true;
+			break;
+		}
+	}
+
+	if (!bitmap_empty(evicted_mask, MAX_SUPPORTED_CSGS))
+		dev_info(kbdev->dev, "Scheduler evicting slots: 0x%*pb\n",
+			 num_groups, evicted_mask);
+
+	if (unlikely(!suspend_wait_failed)) {
+		u32 i;
+
+		while (scheduler->ngrp_to_schedule &&
+			(scheduler->head_slot_priority > (MAX_CSG_SLOT_PRIORITY
+				- scheduler->num_csg_slots_for_tick))) {
+			i = find_first_zero_bit(scheduler->csg_inuse_bitmap,
+					num_groups);
+			if (WARN_ON(i == num_groups))
+				break;
+			program_vacant_csg_slot(kbdev, (s8)i);
+			if (WARN_ON(!csg_slot_in_use(kbdev, (int)i)))
+				break;
+		}
+	}
+}
+
+static void suspend_queue_group(struct kbase_queue_group *group)
+{
+	unsigned long flags;
+	struct kbase_csf_scheduler *const scheduler =
+		&group->kctx->kbdev->csf.scheduler;
+
+	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+	clear_bit(group->csg_nr, scheduler->csgs_events_enable_mask);
+	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+
+	/* If AS fault detected, terminate the group */
+	if (!kctx_as_enabled(group->kctx) || group->faulted)
+		term_csg_slot(group);
+	else
+		suspend_csg_slot(group);
+}
+
+static void wait_csg_slots_start(struct kbase_device *kbdev)
+{
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	long remaining =
+		kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+	DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS) = {0};
+	u32 i;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	/* extract start slot flags for check */
+	for (i = 0; i < num_groups; i++) {
+		if (atomic_read(&scheduler->csg_slots[i].state) ==
+		    CSG_SLOT_READY2RUN)
+			set_bit(i, slot_mask);
+	}
+
+	while (!bitmap_empty(slot_mask, MAX_SUPPORTED_CSGS)) {
+		DECLARE_BITMAP(changed, MAX_SUPPORTED_CSGS);
+
+		bitmap_copy(changed, slot_mask, MAX_SUPPORTED_CSGS);
+
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+			slots_state_changed(kbdev, changed, csg_slot_running),
+			remaining);
+
+		if (remaining) {
+			for_each_set_bit(i, changed, num_groups) {
+				struct kbase_queue_group *group =
+					scheduler->csg_slots[i].resident_group;
+
+				/* The on slot csg is now running */
+				clear_bit(i, slot_mask);
+				group->run_state = KBASE_CSF_GROUP_RUNNABLE;
+			}
+		} else {
+			dev_warn(kbdev->dev, "Timed out waiting for CSG slots to start, slots: 0x%*pb\n",
+				 num_groups, slot_mask);
+
+			if (kbase_prepare_to_reset_gpu(kbdev))
+				kbase_reset_gpu(kbdev);
+			break;
+		}
+	}
+}
+
+/**
+ * group_on_slot_is_idle() - Check if the queue group resident on a command
+ *                           stream group slot is idle.
+ *
+ * This function is called at the start of scheduling tick to check the
+ * idle status of a queue group resident on a command sream group slot.
+ * The group's idleness is determined by looping over all the bound command
+ * queues and checking their respective CS_STATUS_WAIT register as well as
+ * the insert and extract offsets.
+
+ * This function would be simplified in future after the changes under
+ * consideration with MIDHARC-3065 are introduced.
+ *
+ * @kbdev:  Pointer to the GPU device.
+ * @group:  Pointer to the resident group on the given slot.
+ * @slot:   The slot that the given group is resident on.
+ *
+ * Return: true if the group resident on slot is idle, otherwise false.
+ */
+static bool group_on_slot_is_idle(struct kbase_device *kbdev,
+			struct kbase_queue_group *group, unsigned long slot)
+{
+	struct kbase_csf_cmd_stream_group_info *ginfo =
+					&kbdev->csf.global_iface.groups[slot];
+	u32 i;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+	for (i = 0; i < MAX_SUPPORTED_STREAMS_PER_GROUP; i++) {
+		struct kbase_queue *queue = group->bound_queues[i];
+
+		if (queue && queue->enabled) {
+			struct kbase_csf_cmd_stream_info *stream =
+					&ginfo->streams[queue->csi_index];
+			u32 status = kbase_csf_firmware_cs_output(stream,
+							CS_STATUS_WAIT);
+
+			if (!CS_STATUS_WAIT_SYNC_WAIT_GET(status) &&
+			    !confirm_cs_idle(group->bound_queues[i]))
+				return false;
+		}
+	}
+
+	return true;
+}
+
+/**
+ * slots_update_state_changed() -  Check the handshake state of a subset of
+ *                                 command group slots.
+ *
+ * Checks the state of a subset of slots selected through the slots_mask
+ * bit_map. Records which slots' handshake completed and send it back in the
+ * slots_done bit_map.
+ *
+ * @kbdev:          The GPU device.
+ * @field_mask:     The field mask for checking the state in the csg_req/ack.
+ * @slots_mask:     A bit_map specifying the slots to check.
+ * @slots_done:     A cleared bit_map for returning the slots that
+ *                  have finished update.
+ *
+ * Return: true if the slots_done is set for at least one slot.
+ *         Otherwise false.
+ */
+static
+bool slots_update_state_changed(struct kbase_device *kbdev, u32 field_mask,
+		const unsigned long *slots_mask, unsigned long *slots_done)
+{
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	bool changed = false;
+	u32 i;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	for_each_set_bit(i, slots_mask, num_groups) {
+		struct kbase_csf_cmd_stream_group_info const *const ginfo =
+					    &kbdev->csf.global_iface.groups[i];
+		u32 state = kbase_csf_firmware_csg_input_read(ginfo, CSG_REQ);
+
+		state ^= kbase_csf_firmware_csg_output(ginfo, CSG_ACK);
+
+		if (!(state & field_mask)) {
+			set_bit(i, slots_done);
+			changed = true;
+		}
+	}
+
+	return changed;
+}
+
+/**
+ * wait_csg_slots_handshake_ack - Wait the req/ack handshakes to complete on
+ *                                the specified groups.
+ *
+ * This function waits for the acknowledgement of the request that have
+ * already been placed for the CSG slots by the caller. Currently used for
+ * the CSG priority update and status update requests.
+ *
+ * @kbdev:           Pointer to the GPU device.
+ * @field_mask:      The field mask for checking the state in the csg_req/ack.
+ * @slot_mask:       Bitmap reflecting the slots, the function will modify
+ *                   the acknowledged slots by clearing their corresponding
+ *                   bits.
+ * @wait_in_jiffies: Wait duration in jiffies, controlling the time-out.
+ *
+ * Return: 0 on all specified slots acknowledged; otherwise -ETIMEDOUT. For
+ *         timed out condition with unacknowledged slots, their bits remain
+ *         set in the slot_mask.
+ */
+static int wait_csg_slots_handshake_ack(struct kbase_device *kbdev,
+		u32 field_mask, unsigned long *slot_mask, long wait_in_jiffies)
+{
+	const u32 num_groups = kbdev->csf.global_iface.group_num;
+	long remaining = wait_in_jiffies;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	while (!bitmap_empty(slot_mask, num_groups) &&
+	       !kbase_reset_gpu_is_active(kbdev)) {
+		DECLARE_BITMAP(dones, MAX_SUPPORTED_CSGS) = { 0 };
+
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+				slots_update_state_changed(kbdev, field_mask,
+						   slot_mask, dones),
+				remaining);
+
+		if (remaining)
+			bitmap_andnot(slot_mask, slot_mask, dones, num_groups);
+		else
+			/* Timed-out on the wait */
+			return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static void wait_csg_slots_finish_prio_update(struct kbase_device *kbdev)
+{
+	unsigned long *slot_mask =
+			kbdev->csf.scheduler.csg_slots_prio_update;
+	long wait_time =
+		kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+	int ret = wait_csg_slots_handshake_ack(kbdev, CSG_REQ_EP_CFG_MASK,
+					       slot_mask, wait_time);
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (ret != 0) {
+		/* The update timeout is not regarded as a serious
+		 * issue, no major consequences are expected as a
+		 * result, so just warn the case.
+		 */
+		dev_warn(kbdev->dev, "Timeout, skipping the update wait: slot mask=0x%lx",
+			 slot_mask[0]);
+	}
+}
+
+void kbase_csf_scheduler_evict_ctx_slots(struct kbase_device *kbdev,
+		struct kbase_context *kctx, struct list_head *evicted_groups)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct kbase_queue_group *group;
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	u32 slot;
+	DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS) = {0};
+	DECLARE_BITMAP(terminated_slot_mask, MAX_SUPPORTED_CSGS);
+	long remaining =
+		kbase_csf_timeout_in_jiffies(DEFAULT_RESET_TIMEOUT_MS);
+
+	lockdep_assert_held(&kctx->csf.lock);
+	mutex_lock(&scheduler->lock);
+
+	KBASE_KTRACE_ADD(kbdev, EVICT_CTX_SLOTS, kctx, 0u);
+	for (slot = 0; slot < num_groups; slot++) {
+		group = kbdev->csf.scheduler.csg_slots[slot].resident_group;
+		if (group && group->kctx == kctx) {
+			term_csg_slot(group);
+			set_bit(slot, slot_mask);
+		}
+	}
+
+	dev_info(kbdev->dev, "Evicting context %d_%d slots: 0x%*pb\n",
+			kctx->tgid, kctx->id, num_groups, slot_mask);
+
+	bitmap_copy(terminated_slot_mask, slot_mask, MAX_SUPPORTED_CSGS);
+	/* Only check for GPU reset once - this thread has the scheduler lock,
+	 * so even if the return value of kbase_reset_gpu_is_active changes,
+	 * no reset work would be done anyway until the scheduler lock was
+	 * released.
+	 */
+	if (!kbase_reset_gpu_is_active(kbdev)) {
+		while (remaining
+			&& !bitmap_empty(slot_mask, MAX_SUPPORTED_CSGS)) {
+			DECLARE_BITMAP(changed, MAX_SUPPORTED_CSGS);
+
+			bitmap_copy(changed, slot_mask, MAX_SUPPORTED_CSGS);
+
+			remaining = wait_event_timeout(kbdev->csf.event_wait,
+				slots_state_changed(kbdev, changed,
+					csg_slot_stopped_raw),
+				remaining);
+
+			if (remaining)
+				bitmap_andnot(slot_mask, slot_mask, changed,
+					MAX_SUPPORTED_CSGS);
+		}
+	}
+
+	for_each_set_bit(slot, terminated_slot_mask, num_groups) {
+		bool as_fault;
+
+		group = scheduler->csg_slots[slot].resident_group;
+		as_fault = cleanup_csg_slot(group);
+		/* remove the group from the scheduler list */
+		sched_evict_group(group, as_fault);
+		/* return the evicted group to the caller */
+		list_add_tail(&group->link, evicted_groups);
+	}
+
+	if (!remaining) {
+		dev_warn(kbdev->dev, "Timeout on evicting ctx slots: 0x%*pb\n",
+				num_groups, slot_mask);
+		if (kbase_prepare_to_reset_gpu(kbdev))
+			kbase_reset_gpu(kbdev);
+	}
+
+	mutex_unlock(&scheduler->lock);
+}
+
+/**
+ * scheduler_slot_protm_ack - Acknowledging the protected region requests
+ * from the resident group on a given slot.
+ *
+ * The function assumes that the given slot is in stable running state and
+ * has already been judged by the caller on that any pending protected region
+ * requests of the resident group should be acknowledged.
+ *
+ * @kbdev:  Pointer to the GPU device.
+ * @group:  Pointer to the resident group on the given slot.
+ * @slot:   The slot that the given group is actively operating on.
+ *
+ * Return: true if the group has pending protm request(s) and is acknowledged.
+ *         The caller should arrange to enter the protected mode for servicing
+ *         it. Otherwise return false, indicating the group has no pending protm
+ *         request.
+ */
+static bool scheduler_slot_protm_ack(struct kbase_device *const kbdev,
+		struct kbase_queue_group *const group,
+		const int slot)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	bool protm_ack = false;
+	struct kbase_csf_cmd_stream_group_info *ginfo =
+		&kbdev->csf.global_iface.groups[slot];
+	u32 max_csi;
+	int i;
+
+	if (WARN_ON(scheduler->csg_slots[slot].resident_group != group))
+		return protm_ack;
+
+	lockdep_assert_held(&scheduler->lock);
+	lockdep_assert_held(&group->kctx->kbdev->csf.scheduler.interrupt_lock);
+
+	max_csi = ginfo->stream_num;
+	for (i = find_first_bit(group->protm_pending_bitmap, max_csi);
+	     i < max_csi;
+	     i = find_next_bit(group->protm_pending_bitmap, max_csi, i + 1)) {
+		struct kbase_queue *queue = group->bound_queues[i];
+
+		clear_bit(i, group->protm_pending_bitmap);
+
+		if (!WARN_ON(!queue) && queue->enabled) {
+			struct kbase_csf_cmd_stream_info *stream =
+						&ginfo->streams[i];
+			u32 cs_protm_ack = kbase_csf_firmware_cs_output(
+						stream, CS_ACK) &
+						CS_ACK_PROTM_PEND_MASK;
+			u32 cs_protm_req = kbase_csf_firmware_cs_input_read(
+						stream, CS_REQ) &
+						CS_REQ_PROTM_PEND_MASK;
+
+			if (cs_protm_ack == cs_protm_req) {
+				dev_dbg(kbdev->dev,
+					"PROTM-ack already done for queue-%d group-%d slot-%d",
+					queue->csi_index, group->handle, slot);
+				continue;
+			}
+
+			kbase_csf_firmware_cs_input_mask(stream, CS_REQ,
+						cs_protm_ack,
+						CS_ACK_PROTM_PEND_MASK);
+			protm_ack = true;
+			dev_dbg(kbdev->dev,
+				"PROTM-ack for queue-%d, group-%d slot-%d",
+				queue->csi_index, group->handle, slot);
+		}
+	}
+
+	return protm_ack;
+}
+
+/**
+ * scheduler_group_check_protm_enter - Request the given group to be evaluated
+ * for triggering the protected mode.
+ *
+ * The function assumes the given group is either an active running group or
+ * the scheduler internally maintained field scheduler->top_grp.
+ *
+ * If the GPU is not already running in protected mode and the input group
+ * has protected region requests from its bound queues, the requests are
+ * acknowledged and the GPU is instructed to enter the protected mode.
+ *
+ * @kbdev:     Pointer to the GPU device.
+ * @input_grp: Pointer to the GPU queue group.
+ */
+static void scheduler_group_check_protm_enter(struct kbase_device *const kbdev,
+				struct kbase_queue_group *const input_grp)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	unsigned long flags;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+
+	/* Firmware samples the PROTM_PEND ACK bit for command streams when
+	 * Host sends PROTM_ENTER global request. So if PROTM_PEND ACK bit
+	 * is set for a command stream after Host has sent the PROTM_ENTER
+	 * Global request, then there is no guarantee that firmware will
+	 * notice that prior to switching to protected mode. And firmware
+	 * may not again raise the PROTM_PEND interrupt for that command
+	 * stream later on. To avoid that uncertainty PROTM_PEND ACK bit
+	 * is not set for a command stream if the request to enter protected
+	 * mode has already been sent. It will be set later (after the exit
+	 * from protected mode has taken place) when the group to which
+	 * command stream is bound becomes the top group.
+	 *
+	 * The actual decision of entering protected mode is hinging on the
+	 * input group is the top priority group, or, in case the previous
+	 * top-group is evicted from the scheduler during the tick, its would
+	 * be replacement, and that it is currently in a stable state (i.e. the
+	 * slot state is running).
+	 */
+	if (!kbase_csf_scheduler_protected_mode_in_use(kbdev)) {
+		if (!WARN_ON(!input_grp)) {
+			const int slot =
+				kbase_csf_scheduler_group_get_slot_locked(
+					input_grp);
+
+			/* check the input_grp is running and requesting
+			 * protected mode
+			 */
+			if (slot >= 0 &&
+				atomic_read(
+					&scheduler->csg_slots[slot].state) ==
+					CSG_SLOT_RUNNING) {
+				if (kctx_as_enabled(input_grp->kctx) &&
+					scheduler_slot_protm_ack(kbdev,
+							input_grp, slot)) {
+					/* Option of acknowledging to multiple
+					 * CSGs from the same kctx is dropped,
+					 * after consulting with the
+					 * architecture team. See the comment in
+					 * GPUCORE-21394.
+					 */
+
+					/* Switch to protected mode */
+					scheduler->active_protm_grp = input_grp;
+					KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_ENTER_PROTM, input_grp, 0u);
+					spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+					kbase_csf_enter_protected_mode(kbdev);
+					return;
+				}
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+}
+
+static void scheduler_apply(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	const u32 total_csg_slots = kbdev->csf.global_iface.group_num;
+	const u32 available_csg_slots = scheduler->num_csg_slots_for_tick;
+	u32 suspend_cnt = 0;
+	u32 remain_cnt = 0;
+	u32 resident_cnt = 0;
+	struct kbase_queue_group *group;
+	u32 i;
+	u32 spare;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	/* Suspend those resident groups not in the run list */
+	for (i = 0; i < total_csg_slots; i++) {
+		group = scheduler->csg_slots[i].resident_group;
+		if (group) {
+			resident_cnt++;
+			if (group->prepared_seq_num >= available_csg_slots) {
+				suspend_queue_group(group);
+				suspend_cnt++;
+			} else
+				remain_cnt++;
+		}
+	}
+
+	/* If there are spare slots, apply heads in the list */
+	spare = (available_csg_slots > resident_cnt) ?
+		(available_csg_slots - resident_cnt) : 0;
+	while (!list_empty(&scheduler->groups_to_schedule)) {
+		group = list_first_entry(&scheduler->groups_to_schedule,
+				struct kbase_queue_group,
+				link_to_schedule);
+
+		if (kbasep_csf_scheduler_group_is_on_slot_locked(group) &&
+		    group->prepared_seq_num < available_csg_slots) {
+			/* One of the resident remainders */
+			update_csg_slot_priority(group,
+						scheduler->head_slot_priority);
+		} else if (spare != 0) {
+			s8 slot = (s8)find_first_zero_bit(
+				     kbdev->csf.scheduler.csg_inuse_bitmap,
+				     total_csg_slots);
+
+			if (WARN_ON(slot >= (s8)total_csg_slots))
+				break;
+
+			if (!kctx_as_enabled(group->kctx) || group->faulted) {
+				/* Drop the head group and continue */
+				remove_scheduled_group(kbdev, group);
+				continue;
+			}
+			program_csg_slot(group, slot,
+					 scheduler->head_slot_priority);
+			if (unlikely(!csg_slot_in_use(kbdev, slot)))
+				break;
+
+			spare--;
+		} else
+			break;
+
+		/* Drop the head csg from the list */
+		remove_scheduled_group(kbdev, group);
+		if (scheduler->head_slot_priority)
+			scheduler->head_slot_priority--;
+	}
+
+	/* Dealing with groups currently going through suspend */
+	program_suspending_csg_slots(kbdev);
+}
+
+static void scheduler_ctx_scan_groups(struct kbase_device *kbdev,
+		struct kbase_context *kctx, int priority)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct kbase_queue_group *group;
+
+	lockdep_assert_held(&scheduler->lock);
+	if (WARN_ON(priority < 0) ||
+	    WARN_ON(priority >= BASE_QUEUE_GROUP_PRIORITY_COUNT))
+		return;
+
+	if (!kctx_as_enabled(kctx))
+		return;
+
+	list_for_each_entry(group, &kctx->csf.sched.runnable_groups[priority],
+			    link) {
+		if (WARN_ON(!list_empty(&group->link_to_schedule)))
+			/* This would be a bug */
+			list_del_init(&group->link_to_schedule);
+
+		if (unlikely(group->faulted))
+			continue;
+
+		if (queue_group_idle_locked(group)) {
+			list_add_tail(&group->link_to_schedule,
+				      &scheduler->idle_groups_to_schedule);
+			continue;
+		}
+
+		if (!scheduler->ngrp_to_schedule) {
+			/* keep the top csg's origin */
+			scheduler->top_ctx = kctx;
+			scheduler->top_grp = group;
+		}
+
+		list_add_tail(&group->link_to_schedule,
+			      &scheduler->groups_to_schedule);
+		group->prepared_seq_num = scheduler->ngrp_to_schedule++;
+
+		kctx->csf.sched.ngrp_to_schedule++;
+		count_active_address_space(kbdev, kctx);
+	}
+}
+
+/**
+ * scheduler_rotate_groups() - Rotate the runnable queue groups to provide
+ *                             fairness of scheduling within a single
+ *                             kbase_context.
+ *
+ * Since only kbase_csf_scheduler's top_grp (i.e. the queue group assigned
+ * the highest slot priority) is guaranteed to get the resources that it
+ * needs we only rotate the kbase_context corresponding to it -
+ * kbase_csf_scheduler's top_ctx.
+ *
+ * The priority level chosen for rotation is the one containing the previous
+ * scheduling cycle's kbase_csf_scheduler's top_grp.
+ *
+ * In a 'fresh-slice-cycle' this always corresponds to the highest group
+ * priority in use by kbase_csf_scheduler's top_ctx. That is, it's the priority
+ * level of the previous scheduling cycle's first runnable kbase_context.
+ *
+ * We choose this priority level because when higher priority work is
+ * scheduled, we should always cause the scheduler to run and do a scan. The
+ * scan always enumerates the highest priority work first (whether that be
+ * based on process priority or group priority), and thus
+ * kbase_csf_scheduler's top_grp will point to the first of those high priority
+ * groups, which necessarily must be the highest priority group in
+ * kbase_csf_scheduler's top_ctx. The fresh-slice-cycle will run later and pick
+ * up that group appropriately.
+ *
+ * If kbase_csf_scheduler's top_grp was instead evicted (and thus is NULL),
+ * then no explicit rotation occurs on the next fresh-slice-cycle schedule, but
+ * will set up kbase_csf_scheduler's top_ctx again for the next scheduling
+ * cycle. Implicitly, a rotation had already occurred by removing
+ * the kbase_csf_scheduler's top_grp
+ *
+ * If kbase_csf_scheduler's top_grp became idle and all other groups belonging
+ * to kbase_csf_scheduler's top_grp's priority level in kbase_csf_scheduler's
+ * top_ctx are also idle, then the effect of this will be to rotate idle
+ * groups, which might not actually become resident in the next
+ * scheduling slice. However this is acceptable since a queue group becoming
+ * idle is implicitly a rotation (as above with evicted queue groups), as it
+ * automatically allows a new queue group to take the maximum slot priority
+ * whilst the idle kbase_csf_scheduler's top_grp ends up near the back of
+ * the kbase_csf_scheduler's groups_to_schedule list. In this example, it will
+ * be for a group in the next lowest priority level or in absence of those the
+ * next kbase_context's queue groups.
+ *
+ * @kbdev:    Pointer to the GPU device.
+ */
+static void scheduler_rotate_groups(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct kbase_context *const top_ctx = scheduler->top_ctx;
+	struct kbase_queue_group *const top_grp = scheduler->top_grp;
+
+	lockdep_assert_held(&scheduler->lock);
+	if (top_ctx && top_grp) {
+		struct list_head *list =
+			&top_ctx->csf.sched.runnable_groups[top_grp->priority];
+
+		WARN_ON(top_grp->kctx != top_ctx);
+		if (!WARN_ON(list_empty(list))) {
+			list_move_tail(&top_grp->link, list);
+			dev_dbg(kbdev->dev,
+			    "groups rotated for a context, num_runnable_groups: %u\n",
+			    scheduler->top_ctx->csf.sched.num_runnable_grps);
+		}
+	}
+}
+
+static void scheduler_rotate_ctxs(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct list_head *list = &scheduler->runnable_kctxs;
+
+	lockdep_assert_held(&scheduler->lock);
+	if (scheduler->top_ctx) {
+		if (!WARN_ON(list_empty(list))) {
+			struct kbase_context *pos;
+			bool found = false;
+
+			/* Locate the ctx on the list */
+			list_for_each_entry(pos, list, csf.link) {
+				if (scheduler->top_ctx == pos) {
+					found = true;
+					break;
+				}
+			}
+
+			if (!WARN_ON(!found)) {
+				list_move_tail(&pos->csf.link, list);
+				dev_dbg(kbdev->dev, "contexts rotated\n");
+			}
+		}
+	}
+}
+
+/**
+ * scheduler_update_idle_slots_status() - Get the status update for the command
+ *                       stream group slots for which the IDLE notification was
+ *                       received previously.
+ *
+ * This function sends a CSG status update request for all the command stream
+ * group slots present in the bitmap scheduler->csg_slots_idle_mask and wait
+ * for the request to complete.
+ * The bits set in the scheduler->csg_slots_idle_mask bitmap are cleared by
+ * this function.
+ *
+ * @kbdev:             Pointer to the GPU device.
+ * @csg_bitmap:        Bitmap of the command stream group slots for which
+ *                     the status update request completed successfully.
+ * @failed_csg_bitmap: Bitmap of the command stream group slots for which
+ *                     the status update request timedout.
+ */
+static void scheduler_update_idle_slots_status(struct kbase_device *kbdev,
+		unsigned long *csg_bitmap, unsigned long *failed_csg_bitmap)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	const u32 num_groups = kbdev->csf.global_iface.group_num;
+	struct kbase_csf_global_iface *const global_iface =
+						&kbdev->csf.global_iface;
+	unsigned long flags, i;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+	for_each_set_bit(i, scheduler->csg_slots_idle_mask, num_groups) {
+		struct kbase_csf_csg_slot *csg_slot = &scheduler->csg_slots[i];
+		struct kbase_queue_group *group = csg_slot->resident_group;
+		struct kbase_csf_cmd_stream_group_info *const ginfo =
+						&global_iface->groups[i];
+		u32 csg_req;
+
+		clear_bit(i, scheduler->csg_slots_idle_mask);
+
+		if (WARN_ON(!group))
+			continue;
+
+		csg_req = kbase_csf_firmware_csg_output(ginfo, CSG_ACK);
+		csg_req ^= CSG_REQ_STATUS_UPDATE_MASK;
+		kbase_csf_firmware_csg_input_mask(ginfo, CSG_REQ, csg_req,
+						  CSG_REQ_STATUS_UPDATE_MASK);
+
+		set_bit(i, csg_bitmap);
+	}
+	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+
+	/* The groups are aggregated into a single kernel doorbell request */
+	if (!bitmap_empty(csg_bitmap, num_groups)) {
+		long wt =
+		       kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+		u32 db_slots = (u32)csg_bitmap[0];
+
+		kbase_csf_ring_csg_slots_doorbell(kbdev, db_slots);
+
+		if (wait_csg_slots_handshake_ack(kbdev,
+				CSG_REQ_STATUS_UPDATE_MASK, csg_bitmap, wt)) {
+			dev_warn(kbdev->dev, "Timeout, treat groups as not idle: slot mask=0x%lx",
+				 csg_bitmap[0]);
+
+			/* Store the bitmap of timed out slots */
+			bitmap_copy(failed_csg_bitmap, csg_bitmap, num_groups);
+			csg_bitmap[0] = ~csg_bitmap[0] & db_slots;
+		} else {
+                       csg_bitmap[0] = db_slots;
+		}
+	}
+}
+
+/**
+ * scheduler_handle_idle_slots() - Update the idle status of queue groups
+ *                    resident on command stream group slots for which the
+ *                    IDLE notification was received previously.
+ *
+ * This function is called at the start of scheduling tick/tock to reconfirm
+ * the idle status of queue groups resident on command sream group slots for
+ * which idle notification was received previously, i.e. all the CSG slots
+ * present in the bitmap scheduler->csg_slots_idle_mask.
+ * The confirmation is done by sending the CSG status update request to the
+ * firmware. The idleness of a CSG is determined by looping over all the
+ * bound command streams and checking their respective CS_STATUS_WAIT register
+ * as well as the insert and extract offset.
+ * The run state of the groups resident on still idle CSG slots is changed to
+ * KBASE_CSF_GROUP_IDLE and the bitmap scheduler->csg_slots_idle_mask is
+ * updated accordingly.
+ * The bits corresponding to slots for which the status update request timedout
+ * remain set in scheduler->csg_slots_idle_mask.
+ *
+ * @kbdev:  Pointer to the GPU device.
+ */
+static void scheduler_handle_idle_slots(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	unsigned long flags, i;
+	DECLARE_BITMAP(csg_bitmap, MAX_SUPPORTED_CSGS) = { 0 };
+	DECLARE_BITMAP(failed_csg_bitmap, MAX_SUPPORTED_CSGS) = { 0 };
+
+	lockdep_assert_held(&scheduler->lock);
+
+	scheduler_update_idle_slots_status(kbdev, csg_bitmap,
+					   failed_csg_bitmap);
+
+	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+	for_each_set_bit(i, csg_bitmap, num_groups) {
+		struct kbase_csf_csg_slot *csg_slot = &scheduler->csg_slots[i];
+		struct kbase_queue_group *group = csg_slot->resident_group;
+
+		if (WARN_ON(atomic_read(&csg_slot->state) != CSG_SLOT_RUNNING))
+			continue;
+		if (WARN_ON(!group))
+			continue;
+		if (WARN_ON(group->run_state != KBASE_CSF_GROUP_RUNNABLE))
+			continue;
+		if (WARN_ON(group->priority >= BASE_QUEUE_GROUP_PRIORITY_COUNT))
+			continue;
+
+		if (group_on_slot_is_idle(kbdev, group, i)) {
+			group->run_state = KBASE_CSF_GROUP_IDLE;
+			set_bit(i, scheduler->csg_slots_idle_mask);
+		}
+	}
+
+	bitmap_or(scheduler->csg_slots_idle_mask,
+		  scheduler->csg_slots_idle_mask,
+		  failed_csg_bitmap, num_groups);
+	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+}
+
+static void scheduler_scan_idle_groups(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct kbase_queue_group *group, *n;
+
+	list_for_each_entry_safe(group, n, &scheduler->idle_groups_to_schedule,
+				 link_to_schedule) {
+
+		WARN_ON(!queue_group_idle_locked(group));
+
+		if (!scheduler->ngrp_to_schedule) {
+			/* keep the top csg's origin */
+			scheduler->top_ctx = group->kctx;
+			scheduler->top_grp = group;
+		}
+
+		group->prepared_seq_num = scheduler->ngrp_to_schedule++;
+		list_move_tail(&group->link_to_schedule,
+			       &scheduler->groups_to_schedule);
+
+		group->kctx->csf.sched.ngrp_to_schedule++;
+		count_active_address_space(kbdev, group->kctx);
+	}
+}
+
+static void scheduler_rotate(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	/* Dealing with rotation */
+	scheduler_rotate_groups(kbdev);
+	scheduler_rotate_ctxs(kbdev);
+}
+
+static struct kbase_queue_group *get_tock_top_group(
+	struct kbase_csf_scheduler *const scheduler)
+{
+	struct kbase_context *kctx;
+	int i;
+
+	lockdep_assert_held(&scheduler->lock);
+	for (i = 0; i < BASE_QUEUE_GROUP_PRIORITY_COUNT; ++i) {
+		list_for_each_entry(kctx,
+			&scheduler->runnable_kctxs, csf.link) {
+			struct kbase_queue_group *group;
+
+			list_for_each_entry(group,
+					&kctx->csf.sched.runnable_groups[i],
+					link) {
+				if (queue_group_idle_locked(group))
+					continue;
+
+				return group;
+			}
+		}
+	}
+
+	return NULL;
+}
+
+static int suspend_active_groups_on_powerdown(struct kbase_device *kbdev,
+					      bool is_suspend)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS) = { 0 };
+
+	int ret = suspend_active_queue_groups(kbdev, slot_mask);
+
+	if (ret) {
+		/* The suspend of CSGs failed, trigger the GPU reset and wait
+		 * for it to complete to be in a deterministic state.
+		 */
+		dev_warn(kbdev->dev, "Timed out waiting for CSG slots to suspend on power down, slot_mask: 0x%*pb\n",
+			 kbdev->csf.global_iface.group_num, slot_mask);
+
+		if (kbase_prepare_to_reset_gpu(kbdev))
+			kbase_reset_gpu(kbdev);
+
+		if (is_suspend) {
+			mutex_unlock(&scheduler->lock);
+			kbase_reset_gpu_wait(kbdev);
+			mutex_lock(&scheduler->lock);
+		}
+		return -1;
+	}
+
+	/* Check if the groups became active whilst the suspend was ongoing,
+	 * but only for the case where the system suspend is not in progress
+	 */
+	if (!is_suspend && atomic_read(&scheduler->non_idle_suspended_grps))
+		return -1;
+
+	return 0;
+}
+
+static void gpu_idle_worker(struct work_struct *work)
+{
+	struct kbase_device *kbdev = container_of(
+		work, struct kbase_device, csf.scheduler.gpu_idle_work.work);
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+
+	mutex_lock(&scheduler->lock);
+
+	if (!scheduler->total_runnable_grps) {
+		if (scheduler->state != SCHED_SUSPENDED) {
+			scheduler_suspend(kbdev);
+			dev_info(kbdev->dev, "Scheduler now suspended");
+		}
+	} else {
+		dev_dbg(kbdev->dev, "Scheduler couldn't be suspended");
+	}
+
+	mutex_unlock(&scheduler->lock);
+}
+
+static int scheduler_prepare(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	int i;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	/* Empty the groups_to_schedule */
+	while (!list_empty(&scheduler->groups_to_schedule)) {
+		struct kbase_queue_group *grp =
+			list_first_entry(&scheduler->groups_to_schedule,
+					 struct kbase_queue_group,
+					 link_to_schedule);
+
+		remove_scheduled_group(kbdev, grp);
+	}
+
+	/* Pre-scan init scheduler fields */
+	if (WARN_ON(scheduler->ngrp_to_schedule != 0))
+		scheduler->ngrp_to_schedule = 0;
+	scheduler->top_ctx = NULL;
+	scheduler->top_grp = NULL;
+	scheduler->head_slot_priority = MAX_CSG_SLOT_PRIORITY;
+	WARN_ON(!list_empty(&scheduler->idle_groups_to_schedule));
+	scheduler->num_active_address_spaces = 0;
+	scheduler->num_csg_slots_for_tick = 0;
+	bitmap_zero(scheduler->csg_slots_prio_update, MAX_SUPPORTED_CSGS);
+
+	/* Scan out to run groups */
+	for (i = 0; i < BASE_QUEUE_GROUP_PRIORITY_COUNT; ++i) {
+		struct kbase_context *kctx;
+
+		list_for_each_entry(kctx, &scheduler->runnable_kctxs, csf.link)
+			scheduler_ctx_scan_groups(kbdev, kctx, i);
+	}
+
+	scheduler_scan_idle_groups(kbdev);
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_TOP_GRP, scheduler->top_grp,
+			scheduler->num_active_address_spaces |
+			(((u64)scheduler->ngrp_to_schedule) << 32));
+	set_max_csg_slots(kbdev);
+	dev_dbg(kbdev->dev, "prepared groups length: %u, num_active_address_spaces: %u\n",
+		scheduler->ngrp_to_schedule, scheduler->num_active_address_spaces);
+	return 0;
+}
+
+static void scheduler_wait_protm_quit(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	long wt = kbase_csf_timeout_in_jiffies(CSF_STATE_WAIT_TIMEOUT_MS);
+	long remaining;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	remaining = wait_event_timeout(kbdev->csf.event_wait,
+			!kbase_csf_scheduler_protected_mode_in_use(kbdev), wt);
+
+	if (!remaining)
+		dev_warn(kbdev->dev, "Timeout, protm_quit wait skipped");
+}
+
+static void schedule_actions(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	unsigned long flags;
+	struct kbase_queue_group *protm_grp;
+	int ret;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	ret = kbase_pm_wait_for_desired_state(kbdev);
+	if (ret) {
+		dev_err(kbdev->dev, "Wait for MCU power on failed");
+		return;
+	}
+
+	scheduler_handle_idle_slots(kbdev);
+	scheduler_prepare(kbdev);
+	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+	protm_grp = scheduler->active_protm_grp;
+
+	/* Avoid update if the top-group remains unchanged and in protected
+	 * mode. For the said case, all the slots update is effectively
+	 * competing against the active protected mode group (typically the
+	 * top-group). If we update other slots, even on leaving the
+	 * top-group slot untouched, the firmware would exit the protected mode
+	 * for interacting with the host-driver. After it, as the top-group
+	 * would again raise the request for entering protected mode, we would
+	 * be actively doing the switching over twice without progressing the
+	 * queue jobs.
+	 */
+	if (protm_grp && scheduler->top_grp == protm_grp) {
+		dev_dbg(kbdev->dev, "Scheduler keep protm exec: group-%d",
+			protm_grp->handle);
+	} else if (scheduler->top_grp) {
+		if (protm_grp)
+			dev_dbg(kbdev->dev, "Scheduler drop protm exec: group-%d",
+				protm_grp->handle);
+
+		if (!bitmap_empty(scheduler->top_grp->protm_pending_bitmap,
+			     kbdev->csf.global_iface.groups[0].stream_num)) {
+			dev_dbg(kbdev->dev, "Scheduler prepare protm exec: group-%d of context %d_%d",
+				scheduler->top_grp->handle,
+				scheduler->top_grp->kctx->tgid,
+				scheduler->top_grp->kctx->id);
+
+			/* Due to GPUCORE-24491 only the top-group is allowed
+			 * to be on slot and all other on slot groups have to
+			 * be suspended before entering protected mode.
+			 * This would change in GPUCORE-24492.
+			 */
+			scheduler->num_csg_slots_for_tick = 1;
+		}
+
+		spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+
+		scheduler_apply(kbdev);
+		/* Scheduler is dropping the exec of the previous protm_grp,
+		 * Until the protm quit completes, the GPU is effectively
+		 * locked in the secure mode.
+		 */
+		if (protm_grp)
+			scheduler_wait_protm_quit(kbdev);
+
+		wait_csg_slots_start(kbdev);
+		wait_csg_slots_finish_prio_update(kbdev);
+
+		if (scheduler->num_csg_slots_for_tick == 1) {
+			scheduler_group_check_protm_enter(kbdev,
+						scheduler->top_grp);
+		}
+
+		return;
+	}
+
+	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+	return;
+}
+
+static void schedule_on_tock(struct work_struct *work)
+{
+	struct kbase_device *kbdev = container_of(work, struct kbase_device,
+					csf.scheduler.tock_work.work);
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+
+	mutex_lock(&scheduler->lock);
+
+	if (kbase_reset_gpu_is_active(kbdev) ||
+	    (scheduler->state == SCHED_SUSPENDED)) {
+		mutex_unlock(&scheduler->lock);
+		return;
+	}
+
+	WARN_ON(!(scheduler->state == SCHED_INACTIVE));
+	scheduler->state = SCHED_BUSY;
+
+	/* Undertaking schedule action steps */
+	KBASE_KTRACE_ADD(kbdev, SCHEDULER_TOCK, NULL, 0u);
+	schedule_actions(kbdev);
+
+	/* Record time information */
+	scheduler->last_schedule = jiffies;
+
+	/* Tock is serviced */
+	scheduler->tock_pending_request = false;
+
+	scheduler->state = SCHED_INACTIVE;
+	mutex_unlock(&scheduler->lock);
+
+	dev_dbg(kbdev->dev,
+		"Waking up for event after schedule-on-tock completes.");
+	wake_up_all(&kbdev->csf.event_wait);
+}
+
+static void schedule_on_tick(struct work_struct *work)
+{
+	struct kbase_device *kbdev = container_of(work, struct kbase_device,
+					csf.scheduler.tick_work.work);
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+
+	mutex_lock(&scheduler->lock);
+
+	if (kbase_reset_gpu_is_active(kbdev) ||
+	    (scheduler->state == SCHED_SUSPENDED)) {
+		mutex_unlock(&scheduler->lock);
+		return;
+	}
+
+	scheduler->state = SCHED_BUSY;
+
+	/* Do scheduling stuff */
+	scheduler_rotate(kbdev);
+
+	/* Undertaking schedule action steps */
+	KBASE_KTRACE_ADD(kbdev, SCHEDULER_TICK, NULL, 0u);
+	schedule_actions(kbdev);
+
+	/* Record time information */
+	scheduler->last_schedule = jiffies;
+
+	/* Kicking next scheduling if needed */
+	if (likely(scheduler_timer_is_enabled_nolock(kbdev)) &&
+			(scheduler->total_runnable_grps > 0)) {
+		mod_delayed_work(scheduler->wq, &scheduler->tick_work,
+				  CSF_SCHEDULER_TIME_TICK_JIFFIES);
+		dev_dbg(kbdev->dev, "scheduling for next tick, num_runnable_groups:%u\n",
+			scheduler->total_runnable_grps);
+	}
+
+	scheduler->state = SCHED_INACTIVE;
+	mutex_unlock(&scheduler->lock);
+
+	dev_dbg(kbdev->dev, "Waking up for event after schedule-on-tick completes.");
+	wake_up_all(&kbdev->csf.event_wait);
+}
+
+int wait_csg_slots_suspend(struct kbase_device *kbdev,
+			   const unsigned long *slot_mask,
+			   unsigned int timeout_ms)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	long remaining = kbase_csf_timeout_in_jiffies(timeout_ms);
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	int err = 0;
+	DECLARE_BITMAP(slot_mask_local, MAX_SUPPORTED_CSGS);
+
+	lockdep_assert_held(&scheduler->lock);
+
+	bitmap_copy(slot_mask_local, slot_mask, MAX_SUPPORTED_CSGS);
+
+	while (!bitmap_empty(slot_mask_local, MAX_SUPPORTED_CSGS)
+		&& remaining) {
+		DECLARE_BITMAP(changed, MAX_SUPPORTED_CSGS);
+
+		bitmap_copy(changed, slot_mask_local, MAX_SUPPORTED_CSGS);
+
+		remaining = wait_event_timeout(kbdev->csf.event_wait,
+			slots_state_changed(kbdev, changed,
+				csg_slot_stopped_locked),
+			remaining);
+
+		if (remaining) {
+			u32 i;
+
+			for_each_set_bit(i, changed, num_groups) {
+				struct kbase_queue_group *group;
+
+				if (WARN_ON(!csg_slot_stopped_locked(kbdev, (s8)i)))
+					continue;
+
+				/* The on slot csg is now stopped */
+				clear_bit(i, slot_mask_local);
+
+				group = scheduler->csg_slots[i].resident_group;
+				if (likely(group)) {
+					/* Only do save/cleanup if the
+					 * group is not terminated during
+					 * the sleep.
+					 */
+					save_csg_slot(group);
+					if (cleanup_csg_slot(group))
+						sched_evict_group(group, true);
+				}
+			}
+		} else {
+			dev_warn(kbdev->dev, "Timed out waiting for CSG slots to suspend, slot_mask: 0x%*pb\n",
+				 num_groups, slot_mask_local);
+			err = -ETIMEDOUT;
+		}
+	}
+
+	return err;
+}
+
+static int suspend_active_queue_groups(struct kbase_device *kbdev,
+				       unsigned long *slot_mask)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+	u32 slot_num;
+	int ret;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	for (slot_num = 0; slot_num < num_groups; slot_num++) {
+		struct kbase_queue_group *group =
+			scheduler->csg_slots[slot_num].resident_group;
+
+		if (group) {
+			suspend_queue_group(group);
+			set_bit(slot_num, slot_mask);
+		}
+	}
+
+	ret = wait_csg_slots_suspend(kbdev, slot_mask,
+			CSG_SUSPEND_ON_RESET_WAIT_TIMEOUT_MS);
+	return ret;
+}
+
+static int suspend_active_queue_groups_on_reset(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS) = { 0 };
+	int ret;
+
+	mutex_lock(&scheduler->lock);
+
+	ret = suspend_active_queue_groups(kbdev, slot_mask);
+	if (ret) {
+		dev_warn(kbdev->dev, "Timed out waiting for CSG slots to suspend before reset, slot_mask: 0x%*pb\n",
+			 kbdev->csf.global_iface.group_num, slot_mask);
+	}
+
+	if (!bitmap_empty(slot_mask, MAX_SUPPORTED_CSGS)) {
+		int ret2;
+
+		/* Need to flush the GPU cache to ensure suspend buffer
+		 * contents are not lost on reset of GPU.
+		 * Do this even if suspend operation had timedout for some of
+		 * the CSG slots.
+		 */
+		kbase_gpu_start_cache_clean(kbdev);
+		ret2 = kbase_gpu_wait_cache_clean_timeout(kbdev,
+				DEFAULT_RESET_TIMEOUT_MS);
+		if (ret2) {
+			dev_warn(kbdev->dev, "Timed out waiting for cache clean to complete before reset");
+			if (!ret)
+				ret = ret2;
+		}
+	}
+
+	mutex_unlock(&scheduler->lock);
+
+	return ret;
+}
+
+static void scheduler_inner_reset(struct kbase_device *kbdev)
+{
+	u32 const num_groups = kbdev->csf.global_iface.group_num;
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	unsigned long flags;
+
+	WARN_ON(csgs_active(kbdev));
+
+	/* Cancel any potential queued delayed work(s) */
+	cancel_delayed_work_sync(&scheduler->tick_work);
+	cancel_delayed_work_sync(&scheduler->tock_work);
+	cancel_delayed_work_sync(&scheduler->ping_work);
+
+	mutex_lock(&scheduler->lock);
+
+	spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+	bitmap_fill(scheduler->csgs_events_enable_mask, MAX_SUPPORTED_CSGS);
+	scheduler->active_protm_grp = NULL;
+	memset(kbdev->csf.scheduler.csg_slots, 0,
+	       num_groups * sizeof(struct kbase_csf_csg_slot));
+	bitmap_zero(kbdev->csf.scheduler.csg_inuse_bitmap, num_groups);
+	spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+
+	scheduler->top_ctx = NULL;
+	scheduler->top_grp = NULL;
+
+	KBASE_KTRACE_ADD_CSF_GRP(kbdev, SCHEDULER_TOP_GRP, scheduler->top_grp,
+			scheduler->num_active_address_spaces |
+			(((u64)scheduler->total_runnable_grps) << 32));
+
+	mutex_unlock(&scheduler->lock);
+}
+
+void kbase_csf_scheduler_reset(struct kbase_device *kbdev)
+{
+	struct kbase_context *kctx;
+
+	WARN_ON(!kbase_reset_gpu_is_active(kbdev));
+
+	KBASE_KTRACE_ADD(kbdev, SCHEDULER_RESET, NULL, 0u);
+	if (!kbase_csf_scheduler_protected_mode_in_use(kbdev) &&
+	    !suspend_active_queue_groups_on_reset(kbdev)) {
+		/* As all groups have been successfully evicted from the CSG
+		 * slots, clear out thee scheduler data fields and return
+		 */
+		scheduler_inner_reset(kbdev);
+		return;
+	}
+
+	mutex_lock(&kbdev->kctx_list_lock);
+
+	/* The loop to iterate over the kbase contexts is present due to lock
+	 * ordering issue between kctx->csf.lock & kbdev->csf.scheduler.lock.
+	 * CSF ioctls first take kctx->csf.lock which is context-specific and
+	 * then take kbdev->csf.scheduler.lock for global actions like assigning
+	 * a CSG slot.
+	 * If the lock ordering constraint was not there then could have
+	 * directly looped over the active queue groups.
+	 */
+	list_for_each_entry(kctx, &kbdev->kctx_list, kctx_list_link) {
+		/* Firmware reload would reinitialize the CSG & CS interface IO
+		 * pages, so just need to internally mark the currently active
+		 * queue groups as terminated (similar to the unexpected OoM
+		 * event case).
+		 * No further work can now get executed for the active groups
+		 * (new groups would have to be created to execute work) and
+		 * in near future Clients would be duly informed of this
+		 * reset. The resources (like User IO pages, GPU queue memory)
+		 * allocated for the associated queues would be freed when the
+		 * Clients do the teardown when they become aware of the reset.
+		 */
+		kbase_csf_active_queue_groups_reset(kbdev, kctx);
+	}
+
+	mutex_unlock(&kbdev->kctx_list_lock);
+
+	/* After queue groups reset, the scheduler data fields clear out */
+	scheduler_inner_reset(kbdev);
+}
+
+static void firmware_aliveness_monitor(struct work_struct *work)
+{
+	struct kbase_device *kbdev = container_of(work, struct kbase_device,
+					csf.scheduler.ping_work.work);
+	int err;
+
+	/* Get the scheduler mutex to ensure that reset will not change while
+	 * this function is being executed as otherwise calling kbase_reset_gpu
+	 * when reset is already occurring is a programming error.
+	 */
+	mutex_lock(&kbdev->csf.scheduler.lock);
+
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+	if (fw_debug) {
+		/* ping requests cause distraction in firmware debugging */
+		goto exit;
+	}
+#endif
+
+	if (kbdev->csf.scheduler.state == SCHED_SUSPENDED)
+		goto exit;
+
+	if (kbase_reset_gpu_is_active(kbdev))
+		goto exit;
+
+	if (get_nr_active_csgs(kbdev) != 1)
+		goto exit;
+
+	if (kbase_csf_scheduler_protected_mode_in_use(kbdev))
+		goto exit;
+
+	if (kbase_pm_context_active_handle_suspend(kbdev,
+			KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE)) {
+		/* Suspend pending - no real need to ping */
+		goto exit;
+	}
+
+	kbase_pm_wait_for_desired_state(kbdev);
+
+	err = kbase_csf_firmware_ping(kbdev);
+
+	if (err) {
+		if (kbase_prepare_to_reset_gpu(kbdev))
+			kbase_reset_gpu(kbdev);
+	} else if (get_nr_active_csgs(kbdev) == 1) {
+		queue_delayed_work(system_long_wq,
+			&kbdev->csf.scheduler.ping_work,
+			msecs_to_jiffies(FIRMWARE_PING_INTERVAL_MS));
+	}
+
+	kbase_pm_context_idle(kbdev);
+exit:
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+}
+
+int kbase_csf_scheduler_group_copy_suspend_buf(struct kbase_queue_group *group,
+		struct kbase_suspend_copy_buffer *sus_buf)
+{
+	struct kbase_context *const kctx = group->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	int err;
+
+	lockdep_assert_held(&kctx->csf.lock);
+	mutex_lock(&scheduler->lock);
+
+	err = wait_gpu_reset(kbdev);
+	if (err) {
+		dev_warn(kbdev->dev, "Error while waiting for the GPU reset to complete when suspending group %d on slot %d",
+			 group->handle, group->csg_nr);
+		goto exit;
+	}
+
+	if (kbasep_csf_scheduler_group_is_on_slot_locked(group)) {
+		DECLARE_BITMAP(slot_mask, MAX_SUPPORTED_CSGS) = {0};
+
+		set_bit(kbase_csf_scheduler_group_get_slot(group), slot_mask);
+
+		if (!WARN_ON(scheduler->state == SCHED_SUSPENDED))
+			suspend_queue_group(group);
+		err = wait_csg_slots_suspend(kbdev, slot_mask,
+				CSF_STATE_WAIT_TIMEOUT_MS);
+		if (err) {
+			dev_warn(kbdev->dev, "Timed out waiting for the group %d to suspend on slot %d",
+					group->handle, group->csg_nr);
+			goto exit;
+		}
+	}
+
+	if (queue_group_suspended_locked(group)) {
+		unsigned int target_page_nr = 0, i = 0;
+		u64 offset = sus_buf->offset;
+		size_t to_copy = sus_buf->size;
+
+		if (scheduler->state != SCHED_SUSPENDED) {
+			/* Similar to the case of HW counters, need to flush
+			 * the GPU cache before reading from the suspend buffer
+			 * pages as they are mapped and cached on GPU side.
+			 */
+			kbase_gpu_start_cache_clean(kbdev);
+			kbase_gpu_wait_cache_clean(kbdev);
+		} else {
+			/* Make sure power down transitions have completed,
+			 * i.e. L2 has been powered off as that would ensure
+			 * its contents are flushed to memory.
+			 * This is needed as Scheduler doesn't wait for the
+			 * power down to finish.
+			 */
+			kbase_pm_wait_for_desired_state(kbdev);
+		}
+
+		for (i = 0; i < PFN_UP(sus_buf->size) &&
+				target_page_nr < sus_buf->nr_pages; i++) {
+			struct page *pg =
+				as_page(group->normal_suspend_buf.phy[i]);
+			void *sus_page = kmap(pg);
+
+			if (sus_page) {
+				kbase_sync_single_for_cpu(kbdev,
+					kbase_dma_addr(pg),
+					PAGE_SIZE, DMA_BIDIRECTIONAL);
+
+				err = kbase_mem_copy_to_pinned_user_pages(
+						sus_buf->pages, sus_page,
+						&to_copy, sus_buf->nr_pages,
+						&target_page_nr, offset);
+				kunmap(pg);
+				if (err)
+					break;
+			} else {
+				err = -ENOMEM;
+				break;
+			}
+		}
+		schedule_in_cycle(group, false);
+	} else {
+		/* If addr-space fault, the group may have been evicted */
+		err = -EIO;
+	}
+
+exit:
+	mutex_unlock(&scheduler->lock);
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_csf_scheduler_group_copy_suspend_buf);
+
+/**
+ * group_sync_updated() - Evaluate sync wait condition of all blocked command
+ *                        queues of the group.
+ *
+ * @group: Pointer to the command queue group that has blocked command queue(s)
+ *         bound to it.
+ *
+ * Return: true if sync wait condition is satisfied for at least one blocked
+ *         queue of the group.
+ */
+static bool group_sync_updated(struct kbase_queue_group *group)
+{
+	bool updated = false;
+	int stream;
+
+	WARN_ON(group->run_state != KBASE_CSF_GROUP_SUSPENDED_ON_WAIT_SYNC);
+
+	for (stream = 0; stream < MAX_SUPPORTED_STREAMS_PER_GROUP; ++stream) {
+		struct kbase_queue *const queue = group->bound_queues[stream];
+
+		/* To check the necessity of sync-wait evaluation,
+		 * we rely on the cached 'status_wait' instead of reading it
+		 * directly from shared memory as the CSG has been already
+		 * evicted from the CSG slot, thus this CSG doesn't have
+		 * valid information in the shared memory.
+		 */
+		if (queue && queue->enabled &&
+		    CS_STATUS_WAIT_SYNC_WAIT_GET(queue->status_wait))
+			if (evaluate_sync_update(queue)) {
+				updated = true;
+				queue->status_wait = 0;
+			}
+	}
+
+	return updated;
+}
+
+/**
+ * scheduler_get_protm_enter_async_group() -  Check if the GPU queue group
+ *                          can be now allowed to execute in protected mode.
+ *
+ * @kbdev:    Pointer to the GPU device.
+ * @group:    Pointer to the GPU queue group.
+ *
+ * This function is called outside the scheduling tick/tock to determine
+ * if the given GPU queue group can now execute in protected mode or not.
+ * If the group pointer passed is NULL then the evaluation is done for the
+ * scheduler->top_grp (or the second top-group).
+ *
+ * It returns the same group pointer, that was passed as an argument, if that
+ * group matches the scheduler->top_grp and has pending protected region
+ * requests otherwise NULL is returned.
+ *
+ * If the group pointer passed is NULL then the pointer to scheduler->top_grp
+ * is returned if that has pending protected region requests otherwise NULL is
+ * returned.
+ *
+ * If the scheduler->top_grp is NULL, which may happen when the top-group is
+ * evicted during the tick, the second top-group (as a replacement of the
+ * top-group) is used for the match check and also for the evaluation of
+ * pending protected region requests if the group pointer passed is NULL.
+ *
+ * Return: the pointer to queue group that can currently execute in protected
+ *         mode or NULL.
+ */
+static struct kbase_queue_group *scheduler_get_protm_enter_async_group(
+		struct kbase_device *const kbdev,
+		struct kbase_queue_group *const group)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	struct kbase_queue_group *match_grp, *input_grp;
+
+	lockdep_assert_held(&scheduler->lock);
+
+	if (scheduler->state != SCHED_INACTIVE)
+		return NULL;
+
+	match_grp = scheduler->top_grp ? scheduler->top_grp :
+					 get_tock_top_group(scheduler);
+	input_grp = group ? group : match_grp;
+
+	if (input_grp && (input_grp == match_grp)) {
+		struct kbase_csf_cmd_stream_group_info *ginfo =
+				&kbdev->csf.global_iface.groups[0];
+		unsigned long *pending =
+				input_grp->protm_pending_bitmap;
+		unsigned long flags;
+
+		spin_lock_irqsave(&scheduler->interrupt_lock, flags);
+
+		if (kbase_csf_scheduler_protected_mode_in_use(kbdev) ||
+		    bitmap_empty(pending, ginfo->stream_num))
+			input_grp = NULL;
+
+		spin_unlock_irqrestore(&scheduler->interrupt_lock, flags);
+	} else {
+		input_grp = NULL;
+	}
+
+	return input_grp;
+}
+
+void kbase_csf_scheduler_group_protm_enter(struct kbase_queue_group *group)
+{
+	struct kbase_device *const kbdev = group->kctx->kbdev;
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+
+	mutex_lock(&scheduler->lock);
+
+	/* Check if the group is now eligible for execution in protected mode
+	 * and accordingly undertake full scheduling actions as due to
+	 * GPUCORE-24491 the on slot groups other than the top group have to
+	 * be suspended first before entering protected mode.
+	 */
+	if (scheduler_get_protm_enter_async_group(kbdev, group))
+		schedule_actions(kbdev);
+
+	mutex_unlock(&scheduler->lock);
+}
+
+/**
+ * check_group_sync_update_worker() - Check the sync wait condition for all the
+ *                                    blocked queue groups
+ *
+ * @work:    Pointer to the context-specific work item for evaluating the wait
+ *           condition for all the queue groups in idle_wait_groups list.
+ *
+ * This function checks the gpu queues of all the groups present in
+ * idle_wait_groups list of a context. If the sync wait condition
+ * for at least one queue bound to the group has been satisfied then
+ * the group is moved to the per context list of runnable groups so
+ * that Scheduler can consider scheduling the group in next tick.
+ */
+static void check_group_sync_update_worker(struct work_struct *work)
+{
+	struct kbase_context *const kctx = container_of(work,
+		struct kbase_context, csf.sched.sync_update_work);
+	struct kbase_csf_scheduler *const scheduler =
+		&kctx->kbdev->csf.scheduler;
+
+	mutex_lock(&scheduler->lock);
+
+	if (kctx->csf.sched.num_idle_wait_grps != 0) {
+		struct kbase_queue_group *group, *temp;
+
+		list_for_each_entry_safe(group, temp,
+				&kctx->csf.sched.idle_wait_groups, link) {
+			if (group_sync_updated(group)) {
+				/* Move this group back in to the runnable
+				 * groups list of the context.
+				 */
+				update_idle_suspended_group_state(group);
+				KBASE_KTRACE_ADD_CSF_GRP(kctx->kbdev, GROUP_SYNC_UPDATE_DONE, group, 0u);
+			}
+		}
+	} else {
+		WARN_ON(!list_empty(&kctx->csf.sched.idle_wait_groups));
+	}
+
+	mutex_unlock(&scheduler->lock);
+}
+
+static
+enum kbase_csf_event_callback_action check_group_sync_update_cb(void *param)
+{
+	struct kbase_context *const kctx = param;
+
+	KBASE_KTRACE_ADD(kctx->kbdev, SYNC_UPDATE_EVENT, kctx, 0u);
+	queue_work(kctx->csf.sched.sync_update_wq,
+		&kctx->csf.sched.sync_update_work);
+
+	return KBASE_CSF_EVENT_CALLBACK_KEEP;
+}
+
+int kbase_csf_scheduler_context_init(struct kbase_context *kctx)
+{
+	int priority;
+	int err;
+
+	for (priority = 0; priority < BASE_QUEUE_GROUP_PRIORITY_COUNT;
+	     ++priority) {
+		INIT_LIST_HEAD(&kctx->csf.sched.runnable_groups[priority]);
+	}
+
+	kctx->csf.sched.num_runnable_grps = 0;
+	INIT_LIST_HEAD(&kctx->csf.sched.idle_wait_groups);
+	kctx->csf.sched.num_idle_wait_grps = 0;
+	kctx->csf.sched.ngrp_to_schedule = 0;
+
+	kctx->csf.sched.sync_update_wq =
+		alloc_ordered_workqueue("mali_kbase_csf_sync_update_wq",
+			WQ_HIGHPRI);
+	if (!kctx->csf.sched.sync_update_wq) {
+		dev_err(kctx->kbdev->dev,
+			"Failed to initialize scheduler context workqueue");
+		return -ENOMEM;
+	}
+
+	INIT_WORK(&kctx->csf.sched.sync_update_work,
+		check_group_sync_update_worker);
+
+	err = kbase_csf_event_wait_add(kctx, check_group_sync_update_cb, kctx);
+
+	if (err) {
+		dev_err(kctx->kbdev->dev,
+			"Failed to register a sync update callback");
+		destroy_workqueue(kctx->csf.sched.sync_update_wq);
+	}
+
+	return err;
+}
+
+void kbase_csf_scheduler_context_term(struct kbase_context *kctx)
+{
+	kbase_csf_event_wait_remove(kctx, check_group_sync_update_cb, kctx);
+	cancel_work_sync(&kctx->csf.sched.sync_update_work);
+	destroy_workqueue(kctx->csf.sched.sync_update_wq);
+}
+
+int kbase_csf_scheduler_init(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	u32 num_groups = kbdev->csf.global_iface.group_num;
+
+	bitmap_zero(scheduler->csg_inuse_bitmap, num_groups);
+	bitmap_zero(scheduler->csg_slots_idle_mask, num_groups);
+
+	scheduler->csg_slots = kcalloc(num_groups,
+				sizeof(*scheduler->csg_slots), GFP_KERNEL);
+	if (!scheduler->csg_slots) {
+		dev_err(kbdev->dev,
+			"Failed to allocate memory for csg slot status array\n");
+		return -ENOMEM;
+	}
+
+	scheduler->timer_enabled = true;
+
+	scheduler->wq = alloc_ordered_workqueue("csf_scheduler_wq", WQ_HIGHPRI);
+	if (!scheduler->wq) {
+		dev_err(kbdev->dev, "Failed to allocate scheduler workqueue\n");
+
+		kfree(scheduler->csg_slots);
+		scheduler->csg_slots = NULL;
+
+		return -ENOMEM;
+	}
+
+	INIT_DEFERRABLE_WORK(&scheduler->tick_work, schedule_on_tick);
+	INIT_DEFERRABLE_WORK(&scheduler->tock_work, schedule_on_tock);
+
+	INIT_DEFERRABLE_WORK(&scheduler->ping_work, firmware_aliveness_monitor);
+	BUILD_BUG_ON(GLB_REQ_WAIT_TIMEOUT_MS >= FIRMWARE_PING_INTERVAL_MS);
+
+	mutex_init(&scheduler->lock);
+	spin_lock_init(&scheduler->interrupt_lock);
+
+	/* Internal lists */
+	INIT_LIST_HEAD(&scheduler->runnable_kctxs);
+	INIT_LIST_HEAD(&scheduler->groups_to_schedule);
+	INIT_LIST_HEAD(&scheduler->idle_groups_to_schedule);
+
+	BUILD_BUG_ON(MAX_SUPPORTED_CSGS >
+		(sizeof(scheduler->csgs_events_enable_mask) * BITS_PER_BYTE));
+	bitmap_fill(scheduler->csgs_events_enable_mask, MAX_SUPPORTED_CSGS);
+	scheduler->state = SCHED_SUSPENDED;
+	scheduler->pm_active_count = 0;
+	scheduler->ngrp_to_schedule = 0;
+	scheduler->total_runnable_grps = 0;
+	scheduler->top_ctx = NULL;
+	scheduler->top_grp = NULL;
+	scheduler->last_schedule = 0;
+	scheduler->tock_pending_request = false;
+	scheduler->active_protm_grp = NULL;
+	scheduler_doorbell_init(kbdev);
+
+	INIT_DEFERRABLE_WORK(&scheduler->gpu_idle_work, gpu_idle_worker);
+	atomic_set(&scheduler->non_idle_suspended_grps, 0);
+
+	return 0;
+}
+
+void kbase_csf_scheduler_term(struct kbase_device *kbdev)
+{
+	if (kbdev->csf.scheduler.csg_slots) {
+		WARN_ON(csgs_active(kbdev));
+		cancel_delayed_work_sync(&kbdev->csf.scheduler.gpu_idle_work);
+		cancel_delayed_work_sync(&kbdev->csf.scheduler.ping_work);
+		destroy_workqueue(kbdev->csf.scheduler.wq);
+		mutex_destroy(&kbdev->csf.scheduler.lock);
+		kfree(kbdev->csf.scheduler.csg_slots);
+		kbdev->csf.scheduler.csg_slots = NULL;
+	}
+}
+
+/**
+ * scheduler_enable_tick_timer_nolock - Enable the scheduler tick timer.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * This function will restart the scheduler tick so that regular scheduling can
+ * be resumed without any explicit trigger (like kicking of GPU queues). This
+ * is a variant of kbase_csf_scheduler_enable_tick_timer() that assumes the
+ * CSF scheduler lock to already have been held.
+ */
+static void scheduler_enable_tick_timer_nolock(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+
+	lockdep_assert_held(&kbdev->csf.scheduler.lock);
+
+	if (unlikely(!scheduler_timer_is_enabled_nolock(kbdev)))
+		return;
+
+	WARN_ON((scheduler->state != SCHED_INACTIVE) &&
+		(scheduler->state != SCHED_SUSPENDED));
+	WARN_ON(delayed_work_pending(&scheduler->tick_work));
+
+	if (scheduler->total_runnable_grps > 0) {
+		mod_delayed_work(scheduler->wq, &scheduler->tick_work, 0);
+		dev_dbg(kbdev->dev, "Re-enabling the scheduler timer\n");
+	}
+}
+
+void kbase_csf_scheduler_enable_tick_timer(struct kbase_device *kbdev)
+{
+	mutex_lock(&kbdev->csf.scheduler.lock);
+	scheduler_enable_tick_timer_nolock(kbdev);
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+}
+
+bool kbase_csf_scheduler_timer_is_enabled(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+	bool enabled;
+
+	mutex_lock(&scheduler->lock);
+	enabled = scheduler_timer_is_enabled_nolock(kbdev);
+	mutex_unlock(&scheduler->lock);
+
+	return enabled;
+}
+
+void kbase_csf_scheduler_timer_set_enabled(struct kbase_device *kbdev,
+		bool enable)
+{
+	struct kbase_csf_scheduler *const scheduler = &kbdev->csf.scheduler;
+	bool currently_enabled;
+
+	mutex_lock(&scheduler->lock);
+
+	currently_enabled = scheduler_timer_is_enabled_nolock(kbdev);
+	if (currently_enabled && !enable) {
+		scheduler->timer_enabled = false;
+
+		cancel_delayed_work(&scheduler->tick_work);
+		cancel_delayed_work(&scheduler->tock_work);
+	} else if (!currently_enabled && enable) {
+		scheduler->timer_enabled = true;
+
+		scheduler_enable_tick_timer_nolock(kbdev);
+	}
+
+	mutex_unlock(&scheduler->lock);
+}
+
+void kbase_csf_scheduler_kick(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+
+	mutex_lock(&scheduler->lock);
+
+	if (unlikely(scheduler_timer_is_enabled_nolock(kbdev)))
+		goto out;
+
+	if (scheduler->total_runnable_grps > 0) {
+		mod_delayed_work(scheduler->wq, &scheduler->tick_work, 0);
+		dev_dbg(kbdev->dev, "Kicking the scheduler manually\n");
+	}
+
+out:
+	mutex_unlock(&scheduler->lock);
+}
+
+void kbase_csf_scheduler_pm_suspend(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+
+	/* Cancel any potential queued delayed work(s) */
+	cancel_delayed_work_sync(&scheduler->tick_work);
+	cancel_delayed_work_sync(&scheduler->tock_work);
+
+	mutex_lock(&scheduler->lock);
+
+	WARN_ON(!kbase_pm_is_suspending(kbdev));
+
+	if (scheduler->state != SCHED_SUSPENDED) {
+		suspend_active_groups_on_powerdown(kbdev, true);
+		dev_info(kbdev->dev, "Scheduler PM suspend");
+		scheduler_suspend(kbdev);
+	}
+	mutex_unlock(&scheduler->lock);
+}
+
+void kbase_csf_scheduler_pm_resume(struct kbase_device *kbdev)
+{
+	struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+
+	mutex_lock(&scheduler->lock);
+
+	WARN_ON(kbase_pm_is_suspending(kbdev));
+
+	if (scheduler->total_runnable_grps > 0) {
+		WARN_ON(scheduler->state != SCHED_SUSPENDED);
+		dev_info(kbdev->dev, "Scheduler PM resume");
+		scheduler_wakeup(kbdev, true);
+	}
+	mutex_unlock(&scheduler->lock);
+}
+
+void kbase_csf_scheduler_pm_active(struct kbase_device *kbdev)
+{
+	unsigned long flags;
+	u32 prev_count;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	prev_count = kbdev->csf.scheduler.pm_active_count++;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	/* On 0 => 1, make a pm_ctx_active request */
+	if (!prev_count)
+		kbase_pm_context_active(kbdev);
+	else
+		WARN_ON(prev_count == U32_MAX);
+}
+
+void kbase_csf_scheduler_pm_idle(struct kbase_device *kbdev)
+{
+	unsigned long flags;
+	u32 prev_count;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	prev_count = kbdev->csf.scheduler.pm_active_count--;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	if (prev_count == 1)
+		kbase_pm_context_idle(kbdev);
+	else
+		WARN_ON(prev_count == 0);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h
new file mode 100644
index 000000000000..1b1c0681f64d
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_scheduler.h
@@ -0,0 +1,408 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_SCHEDULER_H_
+#define _KBASE_CSF_SCHEDULER_H_
+
+#include "mali_kbase_csf.h"
+
+/**
+ * kbase_csf_scheduler_queue_start() - Enable the running of GPU command queue
+ *                                     on firmware.
+ *
+ * @queue: Pointer to the GPU command queue to be started.
+ *
+ * This function would enable the start of a command stream interface, within a
+ * command stream group, to which the @queue was bound.
+ * If the command stream group is already scheduled and resident, the command
+ * stream interface will be started right away, otherwise once the group is
+ * made resident.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_scheduler_queue_start(struct kbase_queue *queue);
+
+/**
+ * kbase_csf_scheduler_queue_stop() - Disable the running of GPU command queue
+ *                                    on firmware.
+ *
+ * @queue: Pointer to the GPU command queue to be stopped.
+ *
+ * This function would stop the command stream interface, within a command
+ * stream group, to which the @queue was bound.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_scheduler_queue_stop(struct kbase_queue *queue);
+
+/**
+ * kbase_csf_scheduler_group_protm_enter - Handle the protm enter event for the
+ *                                         GPU command queue group.
+ *
+ * @group: The command queue group.
+ *
+ * This function could request the firmware to enter the protected mode
+ * and allow the execution of protected region instructions for all the
+ * bound queues of the group that have protm pending bit set in their
+ * respective CS_ACK register.
+ */
+void kbase_csf_scheduler_group_protm_enter(struct kbase_queue_group *group);
+
+/**
+ * kbase_csf_scheduler_group_get_slot() - Checks if a queue group is
+ *                           programmed on a firmware Command Stream Group slot
+ *                           and returns the slot number.
+ *
+ * @group: The command queue group.
+ *
+ * Return: The slot number, if the group is programmed on a slot.
+ *         Otherwise returns a negative number.
+ *
+ * Note: This function should not be used if the interrupt_lock is held. Use
+ * kbase_csf_scheduler_group_get_slot_locked() instead.
+ */
+int kbase_csf_scheduler_group_get_slot(struct kbase_queue_group *group);
+
+/**
+ * kbase_csf_scheduler_group_get_slot_locked() - Checks if a queue group is
+ *                           programmed on a firmware Command Stream Group slot
+ *                           and returns the slot number.
+ *
+ * @group: The command queue group.
+ *
+ * Return: The slot number, if the group is programmed on a slot.
+ *         Otherwise returns a negative number.
+ *
+ * Note: Caller must hold the interrupt_lock.
+ */
+int kbase_csf_scheduler_group_get_slot_locked(struct kbase_queue_group *group);
+
+/**
+ * kbase_csf_scheduler_group_events_enabled() - Checks if interrupt events
+ *                                     should be handled for a queue group.
+ *
+ * @kbdev: The device of the group.
+ * @group: The queue group.
+ *
+ * Return: true if interrupt events should be handled.
+ *
+ * Note: Caller must hold the interrupt_lock.
+ */
+bool kbase_csf_scheduler_group_events_enabled(struct kbase_device *kbdev,
+		struct kbase_queue_group *group);
+
+/**
+ * kbase_csf_scheduler_get_group_on_slot()- Gets the queue group that has been
+ *                          programmed to a firmware Command Stream Group slot.
+ *
+ * @kbdev: The GPU device.
+ * @slot:  The slot for which to get the queue group.
+ *
+ * Return: Pointer to the programmed queue group.
+ *
+ * Note: Caller must hold the interrupt_lock.
+ */
+struct kbase_queue_group *kbase_csf_scheduler_get_group_on_slot(
+		struct kbase_device *kbdev, int slot);
+
+/**
+ * kbase_csf_scheduler_group_deschedule() - Deschedule a GPU command queue
+ *                                          group from the firmware.
+ *
+ * @group: Pointer to the queue group to be scheduled.
+ *
+ * This function would disable the scheduling of GPU command queue group on
+ * firmware.
+ */
+void kbase_csf_scheduler_group_deschedule(struct kbase_queue_group *group);
+
+/**
+ * kbase_csf_scheduler_evict_ctx_slots() - Evict all GPU command queue groups
+ *                                         of a given context that are active
+ *                                         running from the firmware.
+ *
+ * @kbdev:          The GPU device.
+ * @kctx:           Kbase context for the evict operation.
+ * @evicted_groups: List_head for returning evicted active queue groups.
+ *
+ * This function would disable the scheduling of GPU command queue groups active
+ * on firmware slots from the given Kbase context. The affected groups are
+ * added to the supplied list_head argument.
+ */
+void kbase_csf_scheduler_evict_ctx_slots(struct kbase_device *kbdev,
+		struct kbase_context *kctx, struct list_head *evicted_groups);
+
+/**
+ * kbase_csf_scheduler_context_init() - Initialize the context-specific part
+ *                                      for CSF scheduler.
+ *
+ * @kctx: Pointer to kbase context that is being created.
+ *
+ * This function must be called during Kbase context creation.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_scheduler_context_init(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_scheduler_init - Initialize the CSF scheduler
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * The scheduler does the arbitration for the command stream group slots
+ * provided by the firmware between the GPU command queue groups created
+ * by the Clients.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_scheduler_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_context_init() - Terminate the context-specific part
+ *                                      for CSF scheduler.
+ *
+ * @kctx: Pointer to kbase context that is being terminated.
+ *
+ * This function must be called during Kbase context termination.
+ */
+void kbase_csf_scheduler_context_term(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_scheduler_term - Terminate the CSF scheduler.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * This should be called when unload of firmware is done on device
+ * termination.
+ */
+void kbase_csf_scheduler_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_reset - Reset the state of all active GPU command
+ *                             queue groups.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * This function will first iterate through all the active/scheduled GPU
+ * command queue groups and suspend them (to avoid losing work for groups
+ * that are not stuck). The groups that could not get suspended would be
+ * descheduled and marked as terminated (which will then lead to unbinding
+ * of all the queues bound to them) and also no more work would be allowed
+ * to execute for them.
+ *
+ * This is similar to the action taken in response to an unexpected OoM event.
+ * No explicit re-initialization is done for CSG & CS interface I/O pages;
+ * instead, that happens implicitly on firmware reload.
+ *
+ * Should be called only after initiating the GPU reset.
+ */
+void kbase_csf_scheduler_reset(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_enable_tick_timer - Enable the scheduler tick timer.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * This function will restart the scheduler tick so that regular scheduling can
+ * be resumed without any explicit trigger (like kicking of GPU queues).
+ */
+void kbase_csf_scheduler_enable_tick_timer(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_group_copy_suspend_buf - Suspend a queue
+ *		group and copy suspend buffer.
+ *
+ * This function is called to suspend a queue group and copy the suspend_buffer
+ * contents to the input buffer provided.
+ *
+ * @group:	Pointer to the queue group to be suspended.
+ * @sus_buf:	Pointer to the structure which contains details of the
+ *		user buffer and its kernel pinned pages to which we need to copy
+ *		the group suspend buffer.
+ *
+ * Return:	0 on success, or negative on failure.
+ */
+int kbase_csf_scheduler_group_copy_suspend_buf(struct kbase_queue_group *group,
+		struct kbase_suspend_copy_buffer *sus_buf);
+
+/**
+ * kbase_csf_scheduler_lock - Acquire the global Scheduler lock.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * This function will take the global scheduler lock, in order to serialize
+ * against the Scheduler actions, for access to CS IO pages.
+ */
+static inline void kbase_csf_scheduler_lock(struct kbase_device *kbdev)
+{
+	mutex_lock(&kbdev->csf.scheduler.lock);
+}
+
+/**
+ * kbase_csf_scheduler_unlock - Release the global Scheduler lock.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+static inline void kbase_csf_scheduler_unlock(struct kbase_device *kbdev)
+{
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+}
+
+/**
+ * kbase_csf_scheduler_spin_lock - Acquire Scheduler interrupt spinlock.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ * @flags: Pointer to the memory location that would store the previous
+ *         interrupt state.
+ *
+ * This function will take the global scheduler lock, in order to serialize
+ * against the Scheduler actions, for access to CS IO pages.
+ */
+static inline void kbase_csf_scheduler_spin_lock(struct kbase_device *kbdev,
+						 unsigned long *flags)
+{
+	spin_lock_irqsave(&kbdev->csf.scheduler.interrupt_lock, *flags);
+}
+
+/**
+ * kbase_csf_scheduler_spin_unlock - Release Scheduler interrupt spinlock.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ * @flags: Previously stored interrupt state when Scheduler interrupt
+ *         spinlock was acquired.
+ */
+static inline void kbase_csf_scheduler_spin_unlock(struct kbase_device *kbdev,
+						   unsigned long flags)
+{
+	spin_unlock_irqrestore(&kbdev->csf.scheduler.interrupt_lock, flags);
+}
+
+/**
+ * kbase_csf_scheduler_spin_lock_assert_held - Assert if the Scheduler
+ *                                          interrupt spinlock is held.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+static inline void
+kbase_csf_scheduler_spin_lock_assert_held(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->csf.scheduler.interrupt_lock);
+}
+
+/**
+ * kbase_csf_scheduler_timer_is_enabled() - Check if the scheduler wakes up
+ * automatically for periodic tasks.
+ *
+ * @kbdev: Pointer to the device
+ *
+ * Return: true if the scheduler is configured to wake up periodically
+ */
+bool kbase_csf_scheduler_timer_is_enabled(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_timer_set_enabled() - Enable/disable periodic
+ * scheduler tasks.
+ *
+ * @kbdev:  Pointer to the device
+ * @enable: Whether to enable periodic scheduler tasks
+ */
+void kbase_csf_scheduler_timer_set_enabled(struct kbase_device *kbdev,
+		bool enable);
+
+/**
+ * kbase_csf_scheduler_kick - Perform pending scheduling tasks once.
+ *
+ * Note: This function is only effective if the scheduling timer is disabled.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_scheduler_kick(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_protected_mode_in_use() - Check if the scheduler is
+ * running with protected mode tasks.
+ *
+ * @kbdev: Pointer to the device
+ *
+ * Return: true if the scheduler is running with protected mode tasks
+ */
+static inline bool kbase_csf_scheduler_protected_mode_in_use(
+					struct kbase_device *kbdev)
+{
+	return (kbdev->csf.scheduler.active_protm_grp != NULL);
+}
+
+/**
+ * kbase_csf_scheduler_pm_active - Perform scheduler power active operation
+ *
+ * Note: This function will increase the scheduler's internal pm_active_count
+ * value, ensuring that both GPU and MCU are powered for access.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_scheduler_pm_active(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_pm_idle - Perform the scheduler power idle operation
+ *
+ * Note: This function will decrease the scheduler's internal pm_active_count
+ * value. On reaching 0, the MCU and GPU could be powered off.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ */
+void kbase_csf_scheduler_pm_idle(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_pm_resume - Reactivate the scheduler on system resume
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * This function will make the scheduler resume the scheduling of queue groups
+ * and take the power managemenet reference, if there are any runnable groups.
+ */
+void kbase_csf_scheduler_pm_resume(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_scheduler_pm_suspend - Idle the scheduler on system suspend
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * This function will make the scheduler suspend all the running queue groups
+ * and drop its power managemenet reference.
+ */
+void kbase_csf_scheduler_pm_suspend(struct kbase_device *kbdev);
+
+#endif /* _KBASE_CSF_SCHEDULER_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c
new file mode 100644
index 000000000000..60cae15bc8ef
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.c
@@ -0,0 +1,584 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_csf_tiler_heap.h"
+#include "mali_kbase_csf_tiler_heap_def.h"
+#include "mali_kbase_csf_heap_context_alloc.h"
+
+/**
+ * encode_chunk_ptr - Encode the address and size of a chunk as an integer.
+ *
+ * The size and address of the next chunk in a list are packed into a single
+ * 64-bit value for storage in a chunk's header. This function returns that
+ * value.
+ *
+ * @chunk_size: Size of a tiler heap chunk, in bytes.
+ * @chunk_addr: GPU virtual address of the same tiler heap chunk.
+ *
+ * Return: Next chunk pointer suitable for writing into a chunk header.
+ */
+static u64 encode_chunk_ptr(u32 const chunk_size, u64 const chunk_addr)
+{
+	u64 encoded_size, encoded_addr;
+
+	WARN_ON(chunk_size & ~CHUNK_SIZE_MASK);
+	WARN_ON(chunk_addr & ~CHUNK_ADDR_MASK);
+
+	encoded_size =
+		(u64)(chunk_size >> CHUNK_HDR_NEXT_SIZE_ENCODE_SHIFT) <<
+		CHUNK_HDR_NEXT_SIZE_POS;
+
+	encoded_addr =
+		(chunk_addr >> CHUNK_HDR_NEXT_ADDR_ENCODE_SHIFT) <<
+		CHUNK_HDR_NEXT_ADDR_POS;
+
+	return (encoded_size & CHUNK_HDR_NEXT_SIZE_MASK) |
+		(encoded_addr & CHUNK_HDR_NEXT_ADDR_MASK);
+}
+
+/**
+ * get_last_chunk - Get the last chunk of a tiler heap
+ *
+ * @heap:  Pointer to the tiler heap.
+ *
+ * Return: The address of the most recently-linked chunk, or NULL if none.
+ */
+static struct kbase_csf_tiler_heap_chunk *get_last_chunk(
+	struct kbase_csf_tiler_heap *const heap)
+{
+	lockdep_assert_held(&heap->kctx->csf.tiler_heaps.lock);
+
+	if (list_empty(&heap->chunks_list))
+		return NULL;
+
+	return list_last_entry(&heap->chunks_list,
+		struct kbase_csf_tiler_heap_chunk, link);
+}
+
+/**
+ * link_chunk - Link a chunk into a tiler heap
+ *
+ * Unless the @chunk is the first in the kernel's list of chunks belonging to
+ * a given tiler heap, this function stores the size and address of the @chunk
+ * in the header of the preceding chunk. This requires the GPU memory region
+ * containing the header to be be mapped temporarily, which can fail.
+ *
+ * @heap:  Pointer to the tiler heap.
+ * @chunk: Pointer to the heap chunk to be linked.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+static int link_chunk(struct kbase_csf_tiler_heap *const heap,
+	struct kbase_csf_tiler_heap_chunk *const chunk)
+{
+	struct kbase_csf_tiler_heap_chunk *const prev = get_last_chunk(heap);
+
+	if (prev) {
+		struct kbase_context *const kctx = heap->kctx;
+		struct kbase_vmap_struct map;
+		u64 *const prev_hdr = kbase_vmap_prot(kctx, prev->gpu_va,
+			sizeof(*prev_hdr), KBASE_REG_CPU_WR, &map);
+
+		if (unlikely(!prev_hdr)) {
+			dev_err(kctx->kbdev->dev,
+				"Failed to map tiler heap chunk 0x%llX\n",
+				prev->gpu_va);
+			return -ENOMEM;
+		}
+
+		*prev_hdr = encode_chunk_ptr(heap->chunk_size, chunk->gpu_va);
+		kbase_vunmap(kctx, &map);
+
+		dev_dbg(kctx->kbdev->dev,
+			"Linked tiler heap chunks, 0x%llX -> 0x%llX\n",
+			prev->gpu_va, chunk->gpu_va);
+	}
+
+	return 0;
+}
+
+/**
+ * init_chunk - Initialize and link a tiler heap chunk
+ *
+ * Zero-initialize a new chunk's header (including its pointer to the next
+ * chunk, which doesn't exist yet) and then update the previous chunk's
+ * header to link the new chunk into the chunk list.
+ *
+ * @heap:  Pointer to the tiler heap.
+ * @chunk: Pointer to the heap chunk to be initialized and linked.
+ * @link_with_prev: Flag to indicate if the new chunk needs to be linked with
+ *                  the previously allocated chunk.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+static int init_chunk(struct kbase_csf_tiler_heap *const heap,
+	struct kbase_csf_tiler_heap_chunk *const chunk, bool link_with_prev)
+{
+	struct kbase_vmap_struct map;
+	struct u64 *chunk_hdr = NULL;
+	struct kbase_context *const kctx = heap->kctx;
+
+	if (unlikely(chunk->gpu_va & ~CHUNK_ADDR_MASK)) {
+		dev_err(kctx->kbdev->dev,
+			"Tiler heap chunk address is unusable\n");
+		return -EINVAL;
+	}
+
+	chunk_hdr = kbase_vmap_prot(kctx,
+		chunk->gpu_va, CHUNK_HDR_SIZE, KBASE_REG_CPU_WR, &map);
+
+	if (unlikely(!chunk_hdr)) {
+		dev_err(kctx->kbdev->dev,
+			"Failed to map a tiler heap chunk header\n");
+		return -ENOMEM;
+	}
+
+	memset(chunk_hdr, 0, CHUNK_HDR_SIZE);
+	kbase_vunmap(kctx, &map);
+
+	if (link_with_prev)
+		return link_chunk(heap, chunk);
+	else
+		return 0;
+}
+
+/**
+ * create_chunk - Create a tiler heap chunk
+ *
+ * This function allocates a chunk of memory for a tiler heap and adds it to
+ * the end of the list of chunks associated with that heap. The size of the
+ * chunk is not a parameter because it is configured per-heap not per-chunk.
+ *
+ * @heap: Pointer to the tiler heap for which to allocate memory.
+ * @link_with_prev: Flag to indicate if the chunk to be allocated needs to be
+ *                  linked with the previously allocated chunk.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+static int create_chunk(struct kbase_csf_tiler_heap *const heap,
+		bool link_with_prev)
+{
+	int err = 0;
+	struct kbase_context *const kctx = heap->kctx;
+	u64 nr_pages = PFN_UP(heap->chunk_size);
+	u64 flags = BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR |
+		BASE_MEM_PROT_CPU_WR | BASEP_MEM_NO_USER_FREE |
+		BASE_MEM_COHERENT_LOCAL;
+	struct kbase_csf_tiler_heap_chunk *chunk = NULL;
+
+	flags |= base_mem_group_id_set(kctx->jit_group_id);
+
+#if defined(CONFIG_MALI_BIFROST_DEBUG) || defined(CONFIG_MALI_VECTOR_DUMP)
+	flags |= BASE_MEM_PROT_CPU_RD;
+#endif
+
+	lockdep_assert_held(&kctx->csf.tiler_heaps.lock);
+
+	chunk = kzalloc(sizeof(*chunk), GFP_KERNEL);
+	if (unlikely(!chunk)) {
+		dev_err(kctx->kbdev->dev,
+			"No kernel memory for a new tiler heap chunk\n");
+		return -ENOMEM;
+	}
+
+	/* Allocate GPU memory for the new chunk. */
+	INIT_LIST_HEAD(&chunk->link);
+	chunk->region = kbase_mem_alloc(kctx, nr_pages, nr_pages, 0,
+		&flags, &chunk->gpu_va);
+
+	if (unlikely(!chunk->region)) {
+		dev_err(kctx->kbdev->dev,
+			"Failed to allocate a tiler heap chunk\n");
+		err = -ENOMEM;
+	} else {
+		err = init_chunk(heap, chunk, link_with_prev);
+		if (unlikely(err)) {
+			kbase_gpu_vm_lock(kctx);
+			chunk->region->flags &= ~KBASE_REG_NO_USER_FREE;
+			kbase_mem_free_region(kctx, chunk->region);
+			kbase_gpu_vm_unlock(kctx);
+		}
+	}
+
+	if (unlikely(err)) {
+		kfree(chunk);
+	} else {
+		list_add_tail(&chunk->link, &heap->chunks_list);
+		heap->chunk_count++;
+
+		dev_dbg(kctx->kbdev->dev, "Created tiler heap chunk 0x%llX\n",
+			chunk->gpu_va);
+	}
+
+	return err;
+}
+
+/**
+ * delete_chunk - Delete a tiler heap chunk
+ *
+ * This function frees a tiler heap chunk previously allocated by @create_chunk
+ * and removes it from the list of chunks associated with the heap.
+ *
+ * WARNING: The deleted chunk is not unlinked from the list of chunks used by
+ *          the GPU, therefore it is only safe to use this function when
+ *          deleting a heap.
+ *
+ * @heap:  Pointer to the tiler heap for which @chunk was allocated.
+ * @chunk: Pointer to a chunk to be deleted.
+ */
+static void delete_chunk(struct kbase_csf_tiler_heap *const heap,
+	struct kbase_csf_tiler_heap_chunk *const chunk)
+{
+	struct kbase_context *const kctx = heap->kctx;
+
+	lockdep_assert_held(&kctx->csf.tiler_heaps.lock);
+
+	kbase_gpu_vm_lock(kctx);
+	chunk->region->flags &= ~KBASE_REG_NO_USER_FREE;
+	kbase_mem_free_region(kctx, chunk->region);
+	kbase_gpu_vm_unlock(kctx);
+	list_del(&chunk->link);
+	heap->chunk_count--;
+	kfree(chunk);
+}
+
+/**
+ * delete_all_chunks - Delete all chunks belonging to a tiler heap
+ *
+ * This function empties the list of chunks associated with a tiler heap by
+ * freeing all chunks previously allocated by @create_chunk.
+ *
+ * @heap: Pointer to a tiler heap.
+ */
+static void delete_all_chunks(struct kbase_csf_tiler_heap *heap)
+{
+	struct list_head *entry = NULL, *tmp = NULL;
+	struct kbase_context *const kctx = heap->kctx;
+
+	lockdep_assert_held(&kctx->csf.tiler_heaps.lock);
+
+	list_for_each_safe(entry, tmp, &heap->chunks_list) {
+		struct kbase_csf_tiler_heap_chunk *chunk = list_entry(
+			entry, struct kbase_csf_tiler_heap_chunk, link);
+
+		delete_chunk(heap, chunk);
+	}
+}
+
+/**
+ * create_initial_chunks - Create the initial list of chunks for a tiler heap
+ *
+ * This function allocates a given number of chunks for a tiler heap and
+ * adds them to the list of chunks associated with that heap.
+ *
+ * @heap:    Pointer to the tiler heap for which to allocate memory.
+ * @nchunks: Number of chunks to create.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+static int create_initial_chunks(struct kbase_csf_tiler_heap *const heap,
+	u32 const nchunks)
+{
+	int err = 0;
+	u32 i;
+
+	for (i = 0; (i < nchunks) && likely(!err); i++)
+		err = create_chunk(heap, true);
+
+	if (unlikely(err))
+		delete_all_chunks(heap);
+
+	return err;
+}
+
+/**
+ * delete_heap - Delete a tiler heap
+ *
+ * This function frees any chunks allocated for a tiler heap previously
+ * initialized by @kbase_csf_tiler_heap_init and removes it from the list of
+ * heaps associated with the kbase context. The heap context structure used by
+ * the firmware is also freed.
+ *
+ * @heap: Pointer to a tiler heap to be deleted.
+ */
+static void delete_heap(struct kbase_csf_tiler_heap *heap)
+{
+	struct kbase_context *const kctx = heap->kctx;
+
+	dev_dbg(kctx->kbdev->dev, "Deleting tiler heap 0x%llX\n", heap->gpu_va);
+
+	lockdep_assert_held(&kctx->csf.tiler_heaps.lock);
+
+	delete_all_chunks(heap);
+
+	/* We could optimize context destruction by not freeing leaked heap
+	 * contexts but it doesn't seem worth the extra complexity.
+	 */
+	kbase_csf_heap_context_allocator_free(&kctx->csf.tiler_heaps.ctx_alloc,
+		heap->gpu_va);
+
+	list_del(&heap->link);
+	kfree(heap);
+}
+
+/**
+ * find_tiler_heap - Find a tiler heap from the address of its heap context
+ *
+ * Each tiler heap managed by the kernel has an associated heap context
+ * structure used by the firmware. This function finds a tiler heap object from
+ * the GPU virtual address of its associated heap context. The heap context
+ * should have been allocated by @kbase_csf_heap_context_allocator_alloc in the
+ * same @kctx.
+ *
+ * @kctx:        Pointer to the kbase context to search for a tiler heap.
+ * @heap_gpu_va: GPU virtual address of a heap context structure.
+ *
+ * Return: pointer to the tiler heap object, or NULL if not found.
+ */
+static struct kbase_csf_tiler_heap *find_tiler_heap(
+	struct kbase_context *const kctx, u64 const heap_gpu_va)
+{
+	struct kbase_csf_tiler_heap *heap = NULL;
+
+	lockdep_assert_held(&kctx->csf.tiler_heaps.lock);
+
+	list_for_each_entry(heap, &kctx->csf.tiler_heaps.list, link) {
+		if (heap_gpu_va == heap->gpu_va)
+			return heap;
+	}
+
+	dev_dbg(kctx->kbdev->dev, "Tiler heap 0x%llX was not found\n",
+		heap_gpu_va);
+
+	return NULL;
+}
+
+int kbase_csf_tiler_heap_context_init(struct kbase_context *const kctx)
+{
+	int err = kbase_csf_heap_context_allocator_init(
+		&kctx->csf.tiler_heaps.ctx_alloc, kctx);
+
+	if (unlikely(err))
+		return err;
+
+	INIT_LIST_HEAD(&kctx->csf.tiler_heaps.list);
+	mutex_init(&kctx->csf.tiler_heaps.lock);
+
+	dev_dbg(kctx->kbdev->dev, "Initialized a context for tiler heaps\n");
+
+	return 0;
+}
+
+void kbase_csf_tiler_heap_context_term(struct kbase_context *const kctx)
+{
+	struct list_head *entry = NULL, *tmp = NULL;
+
+	dev_dbg(kctx->kbdev->dev, "Terminating a context for tiler heaps\n");
+
+	mutex_lock(&kctx->csf.tiler_heaps.lock);
+
+	list_for_each_safe(entry, tmp, &kctx->csf.tiler_heaps.list) {
+		struct kbase_csf_tiler_heap *heap = list_entry(
+			entry, struct kbase_csf_tiler_heap, link);
+		delete_heap(heap);
+	}
+
+	mutex_unlock(&kctx->csf.tiler_heaps.lock);
+	mutex_destroy(&kctx->csf.tiler_heaps.lock);
+
+	kbase_csf_heap_context_allocator_term(&kctx->csf.tiler_heaps.ctx_alloc);
+}
+
+int kbase_csf_tiler_heap_init(struct kbase_context *const kctx,
+	u32 const chunk_size, u32 const initial_chunks, u32 const max_chunks,
+	u16 const target_in_flight, u64 *const heap_gpu_va,
+	u64 *const first_chunk_va)
+{
+	int err = 0;
+	struct kbase_csf_tiler_heap *heap = NULL;
+	struct kbase_csf_heap_context_allocator *const ctx_alloc =
+		&kctx->csf.tiler_heaps.ctx_alloc;
+
+	dev_dbg(kctx->kbdev->dev,
+		"Creating a tiler heap with %u chunks (limit: %u) of size %u\n",
+		initial_chunks, max_chunks, chunk_size);
+
+	if (chunk_size == 0)
+		return -EINVAL;
+
+	if (chunk_size & ~CHUNK_SIZE_MASK)
+		return -EINVAL;
+
+	if (initial_chunks == 0)
+		return -EINVAL;
+
+	if (initial_chunks > max_chunks)
+		return -EINVAL;
+
+	if (target_in_flight == 0)
+		return -EINVAL;
+
+	heap = kzalloc(sizeof(*heap), GFP_KERNEL);
+	if (unlikely(!heap)) {
+		dev_err(kctx->kbdev->dev,
+			"No kernel memory for a new tiler heap\n");
+		return -ENOMEM;
+	}
+
+	heap->kctx = kctx;
+	heap->chunk_size = chunk_size;
+	heap->max_chunks = max_chunks;
+	heap->target_in_flight = target_in_flight;
+	INIT_LIST_HEAD(&heap->chunks_list);
+
+	heap->gpu_va = kbase_csf_heap_context_allocator_alloc(ctx_alloc);
+
+	mutex_lock(&kctx->csf.tiler_heaps.lock);
+
+	if (unlikely(!heap->gpu_va)) {
+		dev_err(kctx->kbdev->dev,
+			"Failed to allocate a tiler heap context\n");
+		err = -ENOMEM;
+	} else {
+		err = create_initial_chunks(heap, initial_chunks);
+		if (unlikely(err)) {
+			kbase_csf_heap_context_allocator_free(ctx_alloc,
+				heap->gpu_va);
+		}
+	}
+
+	if (unlikely(err)) {
+		kfree(heap);
+	} else {
+		struct kbase_csf_tiler_heap_chunk const *first_chunk =
+			list_first_entry(&heap->chunks_list,
+				struct kbase_csf_tiler_heap_chunk, link);
+
+		list_add(&heap->link, &kctx->csf.tiler_heaps.list);
+
+		*heap_gpu_va = heap->gpu_va;
+		*first_chunk_va = first_chunk->gpu_va;
+
+		dev_dbg(kctx->kbdev->dev, "Created tiler heap 0x%llX\n",
+			heap->gpu_va);
+	}
+
+	mutex_unlock(&kctx->csf.tiler_heaps.lock);
+
+	return err;
+}
+
+int kbase_csf_tiler_heap_term(struct kbase_context *const kctx,
+	u64 const heap_gpu_va)
+{
+	int err = 0;
+	struct kbase_csf_tiler_heap *heap = NULL;
+
+	mutex_lock(&kctx->csf.tiler_heaps.lock);
+
+	heap = find_tiler_heap(kctx, heap_gpu_va);
+	if (likely(heap))
+		delete_heap(heap);
+	else
+		err = -EINVAL;
+
+	mutex_unlock(&kctx->csf.tiler_heaps.lock);
+
+	return err;
+}
+
+/**
+ * alloc_new_chunk - Allocate a new chunk for the tiler heap.
+ *
+ * This function will allocate a new chunk for the chunked tiler heap depending
+ * on the settings provided by userspace when the heap was created and the
+ * heap's statistics (like number of render passes in-flight).
+ *
+ * @heap:         Pointer to the tiler heap.
+ * @nr_in_flight: Number of render passes that are in-flight, must not be zero.
+ * @new_chunk_ptr: Where to store the GPU virtual address & size of the new
+ *                 chunk allocated for the heap.
+ *
+ * Return: 0 if a new chunk was allocated otherwise an appropriate negative
+ *         error code.
+ */
+static int alloc_new_chunk(struct kbase_csf_tiler_heap *heap,
+		u32 nr_in_flight, u64 *new_chunk_ptr)
+{
+	int err = -ENOMEM;
+
+	lockdep_assert_held(&heap->kctx->csf.tiler_heaps.lock);
+
+	if (!nr_in_flight)
+		return -EINVAL;
+
+	if ((nr_in_flight <= heap->target_in_flight) &&
+	    (heap->chunk_count < heap->max_chunks)) {
+		/* Not exceeded the target number of render passes yet so be
+		 * generous with memory.
+		 */
+		err = create_chunk(heap, false);
+
+		if (likely(!err)) {
+			struct kbase_csf_tiler_heap_chunk *new_chunk =
+							get_last_chunk(heap);
+			if (!WARN_ON(!new_chunk)) {
+				*new_chunk_ptr =
+					encode_chunk_ptr(heap->chunk_size,
+							 new_chunk->gpu_va);
+				return 0;
+			}
+		}
+	}
+
+	/* A new chunk wasn't allocated this time, check if the allocation can
+	 * be retried later.
+	 */
+	if (nr_in_flight > 1) {
+		/* Can retry as there are some ongoing fragment
+		 * jobs which are expected to free up chunks.
+		 */
+		err = -EBUSY;
+	}
+
+	return err;
+}
+
+int kbase_csf_tiler_heap_alloc_new_chunk(struct kbase_context *kctx,
+	u64 gpu_heap_va, u32 nr_in_flight, u64 *new_chunk_ptr)
+{
+	struct kbase_csf_tiler_heap *heap;
+	int err = -EINVAL;
+
+	mutex_lock(&kctx->csf.tiler_heaps.lock);
+
+	heap = find_tiler_heap(kctx, gpu_heap_va);
+
+	if (likely(heap)) {
+		err = alloc_new_chunk(heap, nr_in_flight,
+			new_chunk_ptr);
+	}
+
+	mutex_unlock(&kctx->csf.tiler_heaps.lock);
+
+	return err;
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.h
new file mode 100644
index 000000000000..1a4729df6ca3
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap.h
@@ -0,0 +1,113 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_TILER_HEAP_H_
+#define _KBASE_CSF_TILER_HEAP_H_
+
+#include <mali_kbase.h>
+
+/**
+ * kbase_csf_tiler_heap_context_init - Initialize the tiler heaps context for a
+ *                                     GPU address space
+ *
+ * @kctx: Pointer to the kbase context being initialized.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+int kbase_csf_tiler_heap_context_init(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_tiler_heap_context_term - Terminate the tiler heaps context for a
+ *                                     GPU address space
+ *
+ * This function deletes any chunked tiler heaps that weren't deleted before
+ * context termination.
+ *
+ * @kctx: Pointer to the kbase context being terminated.
+ */
+void kbase_csf_tiler_heap_context_term(struct kbase_context *kctx);
+
+/**
+ * kbase_csf_tiler_heap_init - Initialize a chunked tiler memory heap.
+ *
+ * @kctx: Pointer to the kbase context in which to allocate resources for the
+ *        tiler heap.
+ * @chunk_size: Size of each chunk, in bytes. Must be page-aligned.
+ * @initial_chunks: The initial number of chunks to allocate. Must not be
+ *                  zero or greater than @max_chunks.
+ * @max_chunks: The maximum number of chunks that the heap should be allowed
+ *              to use. Must not be less than @initial_chunks.
+ * @target_in_flight: Number of render-passes that the driver should attempt to
+ *                    keep in flight for which allocation of new chunks is
+ *                    allowed. Must not be zero.
+ * @gpu_heap_va: Where to store the GPU virtual address of the context that was
+ *               set up for the tiler heap.
+ * @first_chunk_va: Where to store the GPU virtual address of the first chunk
+ *                  allocated for the heap. This points to the header of the
+ *                  heap chunk and not to the low address of free memory in it.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+int kbase_csf_tiler_heap_init(struct kbase_context *kctx,
+	u32 chunk_size, u32 initial_chunks, u32 max_chunks,
+	u16 target_in_flight, u64 *gpu_heap_va,
+	u64 *first_chunk_va);
+
+/**
+ * kbasep_cs_tiler_heap_term - Terminate a chunked tiler memory heap.
+ *
+ * This function will terminate a chunked tiler heap and cause all the chunks
+ * (initial and those added during out-of-memory processing) to be freed.
+ * It is the caller's responsibility to ensure no further operations on this
+ * heap will happen before calling this function.
+ *
+ * @kctx: Pointer to the kbase context in which the tiler heap was initialized.
+ * @gpu_heap_va: The GPU virtual address of the context that was set up for the
+ *               tiler heap.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+int kbase_csf_tiler_heap_term(struct kbase_context *kctx, u64 gpu_heap_va);
+
+/**
+ * kbase_csf_tiler_heap_alloc_new_chunk - Allocate a new chunk for tiler heap.
+ *
+ * This function will allocate a new chunk for the chunked tiler heap depending
+ * on the settings provided by userspace when the heap was created and the
+ * heap's statistics (like number of render passes in-flight).
+ * It would return an appropriate error code if a new chunk couldn't be
+ * allocated.
+ *
+ * @kctx: Pointer to the kbase context in which the tiler heap was initialized.
+ * @gpu_heap_va:  GPU virtual address of the heap context.
+ * @nr_in_flight: Number of render passes that are in-flight, must not be zero.
+ * @new_chunk_ptr: Where to store the GPU virtual address & size of the new
+ *                 chunk allocated for the heap.
+ *
+ * Return: 0 if a new chunk was allocated otherwise an appropriate negative
+ *         error code (like -EBUSY when a free chunk is expected to be
+ *         available upon completion of a render pass and -EINVAL when
+ *         invalid value was passed for one of the argument).
+ */
+int kbase_csf_tiler_heap_alloc_new_chunk(struct kbase_context *kctx,
+	u64 gpu_heap_va, u32 nr_in_flight, u64 *new_chunk_ptr);
+#endif
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.c
new file mode 100644
index 000000000000..5d744b81fe4a
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.c
@@ -0,0 +1,107 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_csf_tiler_heap_debugfs.h"
+#include "mali_kbase_csf_tiler_heap_def.h"
+#include <mali_kbase.h>
+#include <linux/seq_file.h>
+
+#ifdef CONFIG_DEBUG_FS
+
+/**
+ * kbasep_csf_tiler_heap_debugfs_show() - Print tiler heap information for per context
+ *
+ * @file: The seq_file for printing to
+ * @data: The debugfs dentry private data, a pointer to kbase_context
+ *
+ * Return: Negative error code or 0 on success.
+ */
+static int kbasep_csf_tiler_heap_debugfs_show(struct seq_file *file, void *data)
+{
+	struct kbase_context *kctx = file->private;
+	struct kbase_csf_tiler_heap_context *tiler_heaps_p = &kctx->csf.tiler_heaps;
+	struct kbase_csf_tiler_heap *heap;
+	struct kbase_csf_tiler_heap_chunk *chunk;
+
+	seq_printf(file, "MALI_CSF_TILER_HEAP_DEBUGFS_VERSION: v%u\n", MALI_CSF_TILER_HEAP_DEBUGFS_VERSION);
+
+	mutex_lock(&tiler_heaps_p->lock);
+
+	list_for_each_entry(heap, &tiler_heaps_p->list, link) {
+		if (heap->kctx != kctx)
+			continue;
+
+		seq_printf(file, "HEAP(gpu_va = 0x%llx):\n", heap->gpu_va);
+		seq_printf(file, "\tchunk_size = %u\n", heap->chunk_size);
+		seq_printf(file, "\tchunk_count = %u\n", heap->chunk_count);
+		seq_printf(file, "\tmax_chunks = %u\n", heap->max_chunks);
+		seq_printf(file, "\ttarget_in_flight = %u\n", heap->target_in_flight);
+
+		list_for_each_entry(chunk, &heap->chunks_list, link)
+			seq_printf(file, "\t\tchunk gpu_va = 0x%llx\n",
+				   chunk->gpu_va);
+	}
+
+	mutex_unlock(&tiler_heaps_p->lock);
+
+	return 0;
+}
+
+static int kbasep_csf_tiler_heap_debugfs_open(struct inode *in, struct file *file)
+{
+	return single_open(file, kbasep_csf_tiler_heap_debugfs_show, in->i_private);
+}
+
+static const struct file_operations kbasep_csf_tiler_heap_debugfs_fops = {
+	.open = kbasep_csf_tiler_heap_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+void kbase_csf_tiler_heap_debugfs_init(struct kbase_context *kctx)
+{
+	struct dentry *file;
+
+	if (WARN_ON(!kctx || IS_ERR_OR_NULL(kctx->kctx_dentry)))
+		return;
+
+	file = debugfs_create_file("tiler_heaps", 0444, kctx->kctx_dentry,
+			kctx, &kbasep_csf_tiler_heap_debugfs_fops);
+
+	if (IS_ERR_OR_NULL(file)) {
+		dev_warn(kctx->kbdev->dev,
+				"Unable to create tiler heap debugfs entry");
+	}
+}
+
+
+#else
+/*
+ * Stub functions for when debugfs is disabled
+ */
+void kbase_csf_tiler_heap_debugfs_init(struct kbase_context *kctx)
+{
+}
+
+#endif /* CONFIG_DEBUG_FS */
+
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.h
new file mode 100644
index 000000000000..44c580d82068
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_debugfs.h
@@ -0,0 +1,38 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_TILER_HEAP_DEBUGFS_H_
+#define _KBASE_CSF_TILER_HEAP_DEBUGFS_H_
+
+/* Forward declaration */
+struct kbase_context;
+
+#define MALI_CSF_TILER_HEAP_DEBUGFS_VERSION 0
+
+/**
+ * kbase_csf_tiler_heap_debugfs_init() - Create a debugfs entry for per context tiler heap
+ *
+ * @kctx: The kbase_context for which to create the debugfs entry
+ */
+void kbase_csf_tiler_heap_debugfs_init(struct kbase_context *kctx);
+
+#endif /* _KBASE_CSF_TILER_HEAP_DEBUGFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_def.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_def.h
new file mode 100644
index 000000000000..1f9e208904a9
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tiler_heap_def.h
@@ -0,0 +1,112 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_TILER_HEAP_DEF_H_
+#define _KBASE_CSF_TILER_HEAP_DEF_H_
+
+#include <mali_kbase.h>
+
+/* Size of a tiler heap chunk header, in bytes. */
+#define CHUNK_HDR_SIZE ((size_t)64)
+
+/* Bit-position of the next chunk's size when stored in a chunk header. */
+#define CHUNK_HDR_NEXT_SIZE_POS (0)
+
+/* Bit-position of the next chunk's address when stored in a chunk header. */
+#define CHUNK_HDR_NEXT_ADDR_POS (12)
+
+/* Bitmask of the next chunk's size when stored in a chunk header. */
+#define CHUNK_HDR_NEXT_SIZE_MASK (((u64)1 << CHUNK_HDR_NEXT_ADDR_POS) - 1u)
+
+/* Bitmask of the address of the next chunk when stored in a chunk header. */
+#define CHUNK_HDR_NEXT_ADDR_MASK (~CHUNK_HDR_NEXT_SIZE_MASK)
+
+/* Right-shift before storing the next chunk's size in a chunk header. */
+#define CHUNK_HDR_NEXT_SIZE_ENCODE_SHIFT (12)
+
+/* Right-shift before storing the next chunk's address in a chunk header. */
+#define CHUNK_HDR_NEXT_ADDR_ENCODE_SHIFT (12)
+
+/* Bitmask of valid chunk sizes. This is also the maximum chunk size, in bytes.
+ */
+#define CHUNK_SIZE_MASK \
+	((CHUNK_HDR_NEXT_SIZE_MASK >> CHUNK_HDR_NEXT_SIZE_POS) << \
+	 CHUNK_HDR_NEXT_SIZE_ENCODE_SHIFT)
+
+/* Bitmask of valid chunk addresses. This is also the highest address. */
+#define CHUNK_ADDR_MASK \
+	((CHUNK_HDR_NEXT_ADDR_MASK >> CHUNK_HDR_NEXT_ADDR_POS) << \
+	 CHUNK_HDR_NEXT_ADDR_ENCODE_SHIFT)
+
+/**
+ * struct kbase_csf_tiler_heap_chunk - A tiler heap chunk managed by the kernel
+ *
+ * Chunks are allocated upon initialization of a tiler heap or in response to
+ * out-of-memory events from the firmware. Chunks are always fully backed by
+ * physical memory to avoid the overhead of processing GPU page faults. The
+ * allocated GPU memory regions are linked together independent of the list of
+ * kernel objects of this type.
+ *
+ * @link:   Link to this chunk in a list of chunks belonging to a
+ *          @kbase_csf_tiler_heap.
+ * @region: Pointer to the GPU memory region allocated for the chunk.
+ * @gpu_va: GPU virtual address of the start of the memory region.
+ *          This points to the header of the chunk and not to the low address
+ *          of free memory within it.
+ */
+struct kbase_csf_tiler_heap_chunk {
+	struct list_head link;
+	struct kbase_va_region *region;
+	u64 gpu_va;
+};
+
+/**
+ * struct kbase_csf_tiler_heap - A tiler heap managed by the kernel
+ *
+ * @kctx:            Pointer to the kbase context with which this heap is
+ *                   associated.
+ * @link:            Link to this heap in a list of tiler heaps belonging to
+ *                   the @kbase_csf_tiler_heap_context.
+ * @chunk_size:      Size of each chunk, in bytes. Must be page-aligned.
+ * @chunk_count:     The number of chunks currently allocated. Must not be
+ *                   zero or greater than @max_chunks.
+ * @max_chunks:      The maximum number of chunks that the heap should be
+ *                   allowed to use. Must not be less than @chunk_count.
+ * @target_in_flight: Number of render-passes that the driver should attempt
+ *                    to keep in flight for which allocation of new chunks is
+ *                    allowed. Must not be zero.
+ * @gpu_va:          The GPU virtual address of the heap context structure that
+ *                   was allocated for the firmware. This is also used to
+ *                   uniquely identify the heap.
+ * @chunks_list:     Linked list of allocated chunks.
+ */
+struct kbase_csf_tiler_heap {
+	struct kbase_context *kctx;
+	struct list_head link;
+	u32 chunk_size;
+	u32 chunk_count;
+	u32 max_chunks;
+	u16 target_in_flight;
+	u64 gpu_va;
+	struct list_head chunks_list;
+};
+#endif /* !_KBASE_CSF_TILER_HEAP_DEF_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c
new file mode 100644
index 000000000000..495ff2850500
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.c
@@ -0,0 +1,169 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/mutex.h>
+#include <linux/sysfs.h>
+#include <linux/of.h>
+
+#include "mali_kbase.h"
+#include "mali_kbase_config_defaults.h"
+#include "mali_kbase_csf_firmware.h"
+#include "mali_kbase_csf_timeout.h"
+#include "backend/gpu/mali_kbase_pm_internal.h"
+
+/**
+ * set_timeout - set a new global progress timeout.
+ *
+ * @kbdev:   Instance of a GPU platform device that implements a command
+ *           stream front-end interface.
+ * @timeout: the maximum number of GPU cycles without forward progress to allow
+ *           to elapse before terminating a GPU command queue group.
+ *
+ * Return:   0 on success, or negative on failure
+ *           (e.g. -ERANGE if the requested timeout is too large).
+ */
+static int set_timeout(struct kbase_device *const kbdev, u64 const timeout)
+{
+	if (timeout > GLB_PROGRESS_TIMER_TIMEOUT_MAX) {
+		dev_err(kbdev->dev, "Timeout %llu is too large.\n", timeout);
+		return -ERANGE;
+	}
+
+	dev_dbg(kbdev->dev, "New progress timeout: %llu cycles\n", timeout);
+
+	atomic64_set(&kbdev->csf.progress_timeout, timeout);
+
+	return 0;
+}
+
+/**
+ * progress_timeout_store - Store the progress_timeout device attribute.
+ * @dev:   The device that has the attribute.
+ * @attr:  The attributes of the sysfs file.
+ * @buf:   The value written to the sysfs file.
+ * @count: The number of bytes written to the sysfs file.
+ *
+ * This function is called when the progress_timeout sysfs file is written to.
+ * It checks the data written, and if valid updates the progress timeout value.
+ *
+ * Return: @count if the function succeeded. An error code on failure.
+ */
+static ssize_t progress_timeout_store(struct device * const dev,
+		struct device_attribute * const attr, const char * const buf,
+		size_t const count)
+{
+	struct kbase_device *const kbdev = dev_get_drvdata(dev);
+	int err;
+	u64 timeout;
+
+	if (!kbdev)
+		return -ENODEV;
+
+	err = kstrtou64(buf, 0, &timeout);
+	if (err) {
+		dev_err(kbdev->dev,
+			"Couldn't process progress_timeout write operation.\n"
+			"Use format <progress_timeout>\n");
+		return err;
+	}
+
+	err = set_timeout(kbdev, timeout);
+	if (!err) {
+		kbase_csf_scheduler_pm_active(kbdev);
+
+		err = kbase_pm_wait_for_desired_state(kbdev);
+		if (!err)
+			err = kbase_csf_firmware_set_timeout(kbdev, timeout);
+
+		kbase_csf_scheduler_pm_idle(kbdev);
+	}
+
+	if (err)
+		return err;
+
+	return count;
+}
+
+/**
+ * progress_timeout_show - Show the progress_timeout device attribute.
+ * @dev: The device that has the attribute.
+ * @attr: The attributes of the sysfs file.
+ * @buf:  The output buffer to receive the global timeout.
+ *
+ * This function is called to get the progress timeout value.
+ *
+ * Return: The number of bytes output to @buf.
+ */
+static ssize_t progress_timeout_show(struct device * const dev,
+		struct device_attribute * const attr, char * const buf)
+{
+	struct kbase_device *const kbdev = dev_get_drvdata(dev);
+	int err;
+
+	if (!kbdev)
+		return -ENODEV;
+
+	err = scnprintf(buf, PAGE_SIZE, "%llu\n", kbase_csf_timeout_get(kbdev));
+
+	return err;
+
+}
+
+static DEVICE_ATTR(progress_timeout, 0644, progress_timeout_show,
+	progress_timeout_store);
+
+int kbase_csf_timeout_init(struct kbase_device *const kbdev)
+{
+	u64 timeout = DEFAULT_PROGRESS_TIMEOUT;
+	int err;
+
+#ifdef CONFIG_OF
+	err = of_property_read_u64(kbdev->dev->of_node,
+		"progress_timeout", &timeout);
+	if (!err)
+		dev_info(kbdev->dev, "Found progress_timeout = %llu in Devicetree\n",
+			timeout);
+#endif
+
+	err = set_timeout(kbdev, timeout);
+	if (err)
+		return err;
+
+	err = sysfs_create_file(&kbdev->dev->kobj,
+		&dev_attr_progress_timeout.attr);
+	if (err)
+		dev_err(kbdev->dev, "SysFS file creation failed\n");
+
+	return err;
+}
+
+void kbase_csf_timeout_term(struct kbase_device * const kbdev)
+{
+	sysfs_remove_file(&kbdev->dev->kobj, &dev_attr_progress_timeout.attr);
+}
+
+u64 kbase_csf_timeout_get(struct kbase_device *const kbdev)
+{
+	return atomic64_read(&kbdev->csf.progress_timeout);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.h
new file mode 100644
index 000000000000..d0156c09a60f
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_timeout.h
@@ -0,0 +1,69 @@
+/*
+ *
+ * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_TIMEOUT_H_
+#define _KBASE_CSF_TIMEOUT_H_
+
+struct kbase_device;
+
+/**
+ * kbase_csf_timeout_init - Initialize the progress timeout.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface. Must be zero-initialized.
+ *
+ * The progress timeout is the number of GPU clock cycles allowed to elapse
+ * before the driver terminates a GPU command queue group in which a task is
+ * making no forward progress on an endpoint (e.g. a shader core). This function
+ * determines the initial value and also creates a sysfs file to allow the
+ * timeout to be reconfigured later.
+ *
+ * Reconfigures the global firmware interface to enable the current timeout.
+ *
+ * Return: 0 on success, or negative on failure.
+ */
+int kbase_csf_timeout_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_timeout_term - Terminate the progress timeout.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * Removes the sysfs file which allowed the timeout to be reconfigured.
+ * Does nothing if called on a zero-initialized object.
+ */
+void kbase_csf_timeout_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_timeout_get - get the current global progress timeout.
+ *
+ * @kbdev: Instance of a GPU platform device that implements a command
+ *         stream front-end interface.
+ *
+ * Return: the maximum number of GPU cycles that is allowed to elapse without
+ *         forward progress before the driver terminates a GPU command queue
+ *         group.
+ */
+u64 kbase_csf_timeout_get(struct kbase_device *const kbdev);
+
+#endif /* _KBASE_CSF_TIMEOUT_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c
new file mode 100644
index 000000000000..5079a8e5af8c
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.c
@@ -0,0 +1,555 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_csf_tl_reader.h"
+
+#include "mali_kbase_csf_trace_buffer.h"
+#include "mali_kbase_reset_gpu.h"
+
+#include "tl/mali_kbase_tlstream.h"
+#include "tl/mali_kbase_tl_serialize.h"
+#include "tl/mali_kbase_tracepoints.h"
+
+#include "mali_kbase_pm.h"
+#include "mali_kbase_hwaccess_time.h"
+
+#include <linux/gcd.h>
+#include <linux/math64.h>
+#include <asm/arch_timer.h>
+
+#ifdef CONFIG_DEBUG_FS
+#include "tl/mali_kbase_timeline_priv.h"
+#include <linux/debugfs.h>
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 7, 0))
+#define DEFINE_DEBUGFS_ATTRIBUTE DEFINE_SIMPLE_ATTRIBUTE
+#endif
+#endif
+
+/** Name of the CSFFW timeline tracebuffer. */
+#define KBASE_CSFFW_TRACEBUFFER_NAME "timeline"
+/** Name of the timeline header metatadata */
+#define KBASE_CSFFW_TIMELINE_HEADER_NAME "timeline_header"
+
+/**
+ * CSFFW timeline message.
+ *
+ * @msg_id: Message ID.
+ * @timestamp: Timestamp of the event.
+ * @cycle_counter: Cycle number of the event.
+ *
+ * Contain fields that are common for all CSFFW timeline messages.
+ */
+struct kbase_csffw_tl_message {
+	u32 msg_id;
+	u64 timestamp;
+	u64 cycle_counter;
+} __packed __aligned(4);
+
+#ifdef CONFIG_DEBUG_FS
+static int kbase_csf_tl_debugfs_poll_interval_read(void *data, u64 *val)
+{
+	struct kbase_device *kbdev = (struct kbase_device *)data;
+	struct kbase_csf_tl_reader *self = &kbdev->timeline->csf_tl_reader;
+
+	*val = self->timer_interval;
+
+	return 0;
+}
+
+static int kbase_csf_tl_debugfs_poll_interval_write(void *data, u64 val)
+{
+	struct kbase_device *kbdev = (struct kbase_device *)data;
+	struct kbase_csf_tl_reader *self = &kbdev->timeline->csf_tl_reader;
+
+	if (val > KBASE_CSF_TL_READ_INTERVAL_MAX || val < KBASE_CSF_TL_READ_INTERVAL_MIN) {
+		return -EINVAL;
+	}
+
+	self->timer_interval = (u32)val;
+
+	return 0;
+}
+
+DEFINE_DEBUGFS_ATTRIBUTE(kbase_csf_tl_poll_interval_fops,
+		kbase_csf_tl_debugfs_poll_interval_read,
+		kbase_csf_tl_debugfs_poll_interval_write, "%llu\n");
+
+void kbase_csf_tl_reader_debugfs_init(struct kbase_device *kbdev)
+{
+	debugfs_create_file("csf_tl_poll_interval_in_ms", S_IRUGO | S_IWUSR,
+		kbdev->debugfs_instr_directory, kbdev,
+		&kbase_csf_tl_poll_interval_fops);
+}
+#endif
+
+/**
+ * get_cpu_gpu_time() - Get current CPU and GPU timestamps.
+ *
+ * @kbdev:	Kbase device.
+ * @cpu_ts:	Output CPU timestamp.
+ * @gpu_ts:	Output GPU timestamp.
+ * @gpu_cycle:  Output GPU cycle counts.
+ */
+static void get_cpu_gpu_time(
+	struct kbase_device *kbdev,
+	u64 *cpu_ts,
+	u64 *gpu_ts,
+	u64 *gpu_cycle)
+{
+	struct timespec64 ts;
+
+	kbase_pm_context_active(kbdev);
+	kbase_backend_get_gpu_time(kbdev, gpu_cycle, gpu_ts, &ts);
+	kbase_pm_context_idle(kbdev);
+
+	if (cpu_ts)
+		*cpu_ts = ts.tv_sec * NSEC_PER_SEC + ts.tv_nsec;
+}
+
+/**
+ * kbase_ts_converter_init() - Initialize system timestamp converter.
+ *
+ * @self:	System Timestamp Converter instance.
+ *
+ * Return: Zero on success, -1 otherwise.
+ */
+static int kbase_ts_converter_init(
+	struct kbase_ts_converter *self,
+	struct kbase_device *kbdev)
+{
+	u64 cpu_ts = 0;
+	u64 gpu_ts = 0;
+	u64 freq;
+	u64 common_factor;
+
+	get_cpu_gpu_time(kbdev, &cpu_ts, &gpu_ts, NULL);
+	freq = arch_timer_get_cntfrq();
+
+	if (!freq) {
+		dev_warn(kbdev->dev, "arch_timer_get_rate() is zero!");
+		return -1;
+	}
+
+	common_factor = gcd(NSEC_PER_SEC, freq);
+
+	self->multiplier = div64_u64(NSEC_PER_SEC, common_factor);
+	self->divisor = div64_u64(freq, common_factor);
+	self->offset =
+		cpu_ts - div64_u64(gpu_ts * self->multiplier, self->divisor);
+
+	return 0;
+}
+
+/**
+ * kbase_ts_converter_convert() - Convert GPU timestamp to CPU timestamp.
+ *
+ * @self:	System Timestamp Converter instance.
+ * @gpu_ts:	System timestamp value to converter.
+ *
+ * Return: The CPU timestamp.
+ */
+void kbase_ts_converter_convert(
+	const struct kbase_ts_converter *self,
+	u64 *gpu_ts)
+{
+	u64 old_gpu_ts = *gpu_ts;
+	*gpu_ts = div64_u64(old_gpu_ts * self->multiplier,
+		self->divisor) + self->offset;
+}
+
+/**
+ * tl_reader_overflow_notify() - Emit stream overflow tracepoint.
+ *
+ * @self:		CSFFW TL Reader instance.
+ * @msg_buf_start:	Start of the message.
+ * @msg_buf_end:	End of the message buffer.
+ */
+static void tl_reader_overflow_notify(
+	const struct kbase_csf_tl_reader *self,
+	u8 *const msg_buf_start,
+	u8 *const msg_buf_end)
+{
+	struct kbase_device *kbdev = self->kbdev;
+	struct kbase_csffw_tl_message message = {0};
+
+	/* Reuse the timestamp and cycle count from current event if possible */
+	if (msg_buf_start + sizeof(message) <= msg_buf_end)
+		memcpy(&message, msg_buf_start, sizeof(message));
+
+	KBASE_TLSTREAM_TL_KBASE_CSFFW_TLSTREAM_OVERFLOW(
+		kbdev, message.timestamp, message.cycle_counter);
+}
+
+/**
+ * tl_reader_overflow_check() - Check if an overflow has happened
+ *
+ * @self:	CSFFW TL Reader instance.
+ * @event_id:	Incoming event id.
+ *
+ * Return: True, if an overflow has happened, False otherwise.
+ */
+static bool tl_reader_overflow_check(
+	struct kbase_csf_tl_reader *self,
+	u16 event_id)
+{
+	struct kbase_device *kbdev = self->kbdev;
+	bool has_overflow = false;
+
+	/* 0 is a special event_id and reserved for the very first tracepoint
+	 * after reset, we should skip overflow check when reset happened.
+	 */
+	if (event_id != 0) {
+		has_overflow = self->got_first_event
+			&& self->expected_event_id != event_id;
+
+		if (has_overflow)
+			dev_warn(kbdev->dev,
+				"CSFFW overflow, event_id: %u, expected: %u.",
+				event_id, self->expected_event_id);
+	}
+
+	self->got_first_event = true;
+	self->expected_event_id = event_id + 1;
+	/* When event_id reaches its max value, it skips 0 and wraps to 1. */
+	if (self->expected_event_id == 0)
+		self->expected_event_id++;
+
+	return has_overflow;
+}
+
+/**
+ * tl_reader_reset() - Reset timeline tracebuffer reader state machine.
+ *
+ * @self:	CSFFW TL Reader instance.
+ *
+ * Reset the reader to the default state, i.e. set all the
+ * mutable fields to zero.
+ */
+static void tl_reader_reset(struct kbase_csf_tl_reader *self)
+{
+	self->got_first_event = false;
+	self->is_active = false;
+	self->expected_event_id = 0;
+	self->tl_header.btc = 0;
+}
+
+void kbase_csf_tl_reader_flush_buffer(struct kbase_csf_tl_reader *self)
+{
+	struct kbase_device *kbdev = self->kbdev;
+	struct kbase_tlstream *stream = self->stream;
+
+	u8  *read_buffer = self->read_buffer;
+	const size_t read_buffer_size = sizeof(self->read_buffer);
+
+	u32 bytes_read;
+	u8 *csffw_data_begin;
+	u8 *csffw_data_end;
+	u8 *csffw_data_it;
+
+	unsigned long flags;
+
+	spin_lock_irqsave(&self->read_lock, flags);
+
+	/* If not running, early exit. */
+	if (!self->is_active) {
+		spin_unlock_irqrestore(&self->read_lock, flags);
+		return;
+	}
+
+	/* Copying the whole buffer in a single shot. We assume
+	 * that the buffer will not contain partially written messages.
+	 */
+	bytes_read = kbase_csf_firmware_trace_buffer_read_data(
+		self->trace_buffer, read_buffer, read_buffer_size);
+	csffw_data_begin = read_buffer;
+	csffw_data_end   = read_buffer + bytes_read;
+
+	for (csffw_data_it = csffw_data_begin;
+	     csffw_data_it < csffw_data_end;) {
+		u32 event_header;
+		u16 event_id;
+		u16 event_size;
+		unsigned long acq_flags;
+		char *buffer;
+
+		/* Can we safely read event_id? */
+		if (csffw_data_it + sizeof(event_header) > csffw_data_end) {
+			dev_warn(
+				kbdev->dev,
+				"Unable to parse CSFFW tracebuffer event header.");
+			break;
+		}
+
+		/* Read and parse the event header. */
+		memcpy(&event_header, csffw_data_it, sizeof(event_header));
+		event_id   = (event_header >> 0)  & 0xFFFF;
+		event_size = (event_header >> 16) & 0xFFFF;
+		csffw_data_it += sizeof(event_header);
+
+		/* Detect if an overflow has happened. */
+		if (tl_reader_overflow_check(self, event_id))
+			tl_reader_overflow_notify(self,
+				csffw_data_it,
+				csffw_data_end);
+
+		/* Can we safely read the message body? */
+		if (csffw_data_it + event_size > csffw_data_end) {
+			dev_warn(kbdev->dev,
+				"event_id: %u, can't read with event_size: %u.",
+				event_id, event_size);
+			break;
+		}
+
+		/* Convert GPU timestamp to CPU timestamp. */
+		{
+			struct kbase_csffw_tl_message *msg =
+				(struct kbase_csffw_tl_message *) csffw_data_it;
+			kbase_ts_converter_convert(
+				&self->ts_converter,
+				&msg->timestamp);
+		}
+
+		/* Copy the message out to the tl_stream. */
+		buffer = kbase_tlstream_msgbuf_acquire(
+			stream, event_size, &acq_flags);
+		kbasep_serialize_bytes(buffer, 0, csffw_data_it, event_size);
+		kbase_tlstream_msgbuf_release(stream, acq_flags);
+		csffw_data_it += event_size;
+	}
+
+	spin_unlock_irqrestore(&self->read_lock, flags);
+}
+
+static void kbasep_csf_tl_reader_read_callback(struct timer_list *timer)
+{
+	struct kbase_csf_tl_reader *self =
+		container_of(timer, struct kbase_csf_tl_reader, read_timer);
+
+	int rcode;
+
+	kbase_csf_tl_reader_flush_buffer(self);
+
+	rcode = mod_timer(&self->read_timer,
+		jiffies + msecs_to_jiffies(self->timer_interval));
+
+	CSTD_UNUSED(rcode);
+}
+
+/**
+ * tl_reader_init_late() - Late CSFFW TL Reader initialization.
+ *
+ * @self:	CSFFW TL Reader instance.
+ * @kbdev:	Kbase device.
+ *
+ * Late initialization is done once at kbase_csf_tl_reader_start() time.
+ * This is because the firmware image is not parsed
+ * by the kbase_csf_tl_reader_init() time.
+ *
+ * Return: Zero on success, -1 otherwise.
+ */
+static int tl_reader_init_late(
+	struct kbase_csf_tl_reader *self,
+	struct kbase_device *kbdev)
+{
+	struct firmware_trace_buffer *tb;
+	size_t hdr_size = 0;
+	const char *hdr = NULL;
+
+	if (self->kbdev)
+		return 0;
+
+	tb = kbase_csf_firmware_get_trace_buffer(
+		kbdev, KBASE_CSFFW_TRACEBUFFER_NAME);
+	hdr = kbase_csf_firmware_get_timeline_metadata(
+		kbdev, KBASE_CSFFW_TIMELINE_HEADER_NAME, &hdr_size);
+
+	if (!tb) {
+		dev_warn(
+			kbdev->dev,
+			"'%s' tracebuffer is not present in the firmware image.",
+			KBASE_CSFFW_TRACEBUFFER_NAME);
+		return -1;
+	}
+
+	if (!hdr) {
+		dev_warn(
+			kbdev->dev,
+			"'%s' timeline metadata is not present in the firmware image.",
+			KBASE_CSFFW_TIMELINE_HEADER_NAME);
+		return -1;
+	}
+
+	if (kbase_ts_converter_init(&self->ts_converter, kbdev)) {
+		return -1;
+	}
+
+	self->kbdev = kbdev;
+	self->trace_buffer = tb;
+	self->tl_header.data = hdr;
+	self->tl_header.size = hdr_size;
+
+	return 0;
+}
+
+/**
+ * tl_reader_update_enable_bit() - Update the first bit of a CSFFW tracebuffer.
+ *
+ * @self:	CSFFW TL Reader instance.
+ * @value:	The value to set.
+ *
+ * Update the first bit of a CSFFW tracebufer and then reset the GPU.
+ * This is to make these changes visible to the MCU.
+ *
+ * Return: 0 on success, -EAGAIN if a GPU reset was in progress.
+ */
+static int tl_reader_update_enable_bit(
+	struct kbase_csf_tl_reader *self,
+	bool value)
+{
+	struct kbase_device *kbdev = self->kbdev;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+	/* If there is already a GPU reset pending then inform
+	 * the User to retry the update.
+	 */
+	if (kbase_reset_gpu_silent(kbdev)) {
+		spin_unlock_irqrestore(
+			&kbdev->hwaccess_lock, flags);
+		dev_warn(
+			kbdev->dev,
+			"GPU reset already in progress when enabling firmware timeline.");
+		return -EAGAIN;
+	}
+
+	/* GPU reset request has been placed, now update the
+	 * firmware image. GPU reset will take place only after
+	 * hwaccess_lock is released.
+	 */
+	kbase_csf_firmware_trace_buffer_update_trace_enable_bit(
+		self->trace_buffer, 0, value);
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return 0;
+}
+
+void kbase_csf_tl_reader_init(struct kbase_csf_tl_reader *self,
+	struct kbase_tlstream *stream)
+{
+	self->timer_interval = KBASE_CSF_TL_READ_INTERVAL_DEFAULT;
+
+	kbase_timer_setup(&self->read_timer,
+		kbasep_csf_tl_reader_read_callback);
+
+	self->stream = stream;
+
+	/* This will be initialized by tl_reader_init_late() */
+	self->kbdev = NULL;
+	self->trace_buffer = NULL;
+	self->tl_header.data = NULL;
+	self->tl_header.size = 0;
+
+	spin_lock_init(&self->read_lock);
+
+	tl_reader_reset(self);
+}
+
+void kbase_csf_tl_reader_term(struct kbase_csf_tl_reader *self)
+{
+	del_timer_sync(&self->read_timer);
+}
+
+int kbase_csf_tl_reader_start(struct kbase_csf_tl_reader *self,
+	struct kbase_device *kbdev)
+{
+	int rcode;
+
+	/* If already running, early exit. */
+	if (self->is_active)
+		return 0;
+
+	if (tl_reader_init_late(self, kbdev)) {
+#if defined(CONFIG_MALI_BIFROST_NO_MALI)
+		dev_warn(
+			kbdev->dev,
+			"CSFFW timeline is not available for MALI_BIFROST_NO_MALI builds!");
+		return 0;
+#else
+		return -EINVAL;
+#endif
+	}
+
+	tl_reader_reset(self);
+
+	self->is_active = true;
+	/* Set bytes to copy to the header size. This is to trigger copying
+	 * of the header to the user space.
+	 */
+	self->tl_header.btc = self->tl_header.size;
+
+	/* Enable the tracebuffer on the CSFFW side. */
+	rcode = tl_reader_update_enable_bit(self, true);
+	if (rcode != 0)
+		return rcode;
+
+	rcode = mod_timer(&self->read_timer,
+		jiffies + msecs_to_jiffies(self->timer_interval));
+
+	return 0;
+}
+
+void kbase_csf_tl_reader_stop(struct kbase_csf_tl_reader *self)
+{
+	unsigned long flags;
+
+	/* If is not running, early exit. */
+	if (!self->is_active)
+		return;
+
+	/* Disable the tracebuffer on the CSFFW side. */
+	tl_reader_update_enable_bit(self, false);
+
+	del_timer_sync(&self->read_timer);
+
+	spin_lock_irqsave(&self->read_lock, flags);
+
+	tl_reader_reset(self);
+
+	spin_unlock_irqrestore(&self->read_lock, flags);
+}
+
+void kbase_csf_tl_reader_reset(struct kbase_csf_tl_reader *self)
+{
+	u64 gpu_cycle = 0;
+	struct kbase_device *kbdev = self->kbdev;
+
+	if (!kbdev)
+		return;
+
+	kbase_csf_tl_reader_flush_buffer(self);
+
+	get_cpu_gpu_time(kbdev, NULL, NULL, &gpu_cycle);
+	KBASE_TLSTREAM_TL_KBASE_CSFFW_RESET(kbdev, gpu_cycle);
+}
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.h
new file mode 100644
index 000000000000..f5ce9d629f55
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_tl_reader.h
@@ -0,0 +1,181 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSFFW_TL_READER_H_
+#define _KBASE_CSFFW_TL_READER_H_
+
+#include <linux/spinlock.h>
+#include <linux/timer.h>
+#include <asm/page.h>
+
+/** The number of pages used for CSFFW trace buffer. Can be tweaked. */
+#define KBASE_CSF_TL_BUFFER_NR_PAGES 4
+/** CSFFW Timeline read polling minimum period in milliseconds. */
+#define KBASE_CSF_TL_READ_INTERVAL_MIN 20
+/** CSFFW Timeline read polling default period in milliseconds. */
+#define KBASE_CSF_TL_READ_INTERVAL_DEFAULT 200
+/** CSFFW Timeline read polling maximum period in milliseconds. */
+#define KBASE_CSF_TL_READ_INTERVAL_MAX (60*1000)
+
+struct firmware_trace_buffer;
+struct kbase_tlstream;
+struct kbase_device;
+
+/**
+ * System timestamp to CPU timestamp converter state.
+ *
+ * @multiplier:	Numerator of the converter's fraction.
+ * @divisor:	Denominator of the converter's fraction.
+ * @offset:	Converter's offset term.
+ *
+ * According to Generic timer spec, system timer:
+ * - Increments at a fixed frequency
+ * - Starts operating from zero
+ *
+ * Hence CPU time is a linear function of System Time.
+ *
+ * CPU_ts = alpha * SYS_ts + beta
+ *
+ * Where
+ * - alpha = 10^9/SYS_ts_freq
+ * - beta is calculated by two timer samples taken at the same time:
+ *   beta = CPU_ts_s - SYS_ts_s * alpha
+ *
+ * Since alpha is a rational number, we minimizing possible
+ * rounding error by simplifying the ratio. Thus alpha is stored
+ * as a simple `multiplier / divisor` ratio.
+ *
+ */
+struct kbase_ts_converter {
+	u64 multiplier;
+	u64 divisor;
+	s64 offset;
+};
+
+/**
+ * struct kbase_csf_tl_reader - CSFFW timeline reader state.
+ *
+ * @read_timer:        Timer used for periodical tracebufer reading.
+ * @timer_interval:    Timer polling period in milliseconds.
+ * @stream:            Timeline stream where to the tracebuffer content
+ *                     is copied.
+ * @kbdev:             KBase device.
+ * @trace_buffer:      CSF Firmware timeline tracebuffer.
+ * @tl_header.data:    CSFFW Timeline header content.
+ * @tl_header.size:    CSFFW Timeline header size.
+ * @tl_header.btc:     CSFFW Timeline header remaining bytes to copy to
+ *                     the user space.
+ * @ts_converter:      Timestamp converter state.
+ * @got_first_event:   True, if a CSFFW timelime session has been enabled
+ *                     and the first event was received.
+ * @is_active:         True, if a CSFFW timelime session has been enabled.
+ * @expected_event_id: The last 16 bit event ID received from CSFFW. It
+ *                     is only valid when got_first_event is true.
+ * @read_buffer:       Temporary buffer used for CSFFW timeline data
+ *                     reading from the tracebufer.
+ */
+struct kbase_csf_tl_reader {
+	struct timer_list read_timer;
+	u32 timer_interval;
+	struct kbase_tlstream *stream;
+
+	struct kbase_device *kbdev;
+	struct firmware_trace_buffer *trace_buffer;
+	struct {
+		const char *data;
+		size_t size;
+		size_t btc;
+	} tl_header;
+	struct kbase_ts_converter ts_converter;
+
+	bool got_first_event;
+	bool is_active;
+	u16 expected_event_id;
+
+	u8 read_buffer[PAGE_SIZE * KBASE_CSF_TL_BUFFER_NR_PAGES];
+	spinlock_t read_lock;
+};
+
+/**
+ * kbase_csf_tl_reader_init() - Initialize CSFFW Timelime Stream Reader.
+ *
+ * @self:	CSFFW TL Reader instance.
+ * @stream:	Destination timeline stream.
+ */
+void kbase_csf_tl_reader_init(struct kbase_csf_tl_reader *self,
+	struct kbase_tlstream *stream);
+
+/**
+ * kbase_csf_tl_reader_term() - Terminate CSFFW Timelime Stream Reader.
+ *
+ * @self:	CSFFW TL Reader instance.
+ */
+void kbase_csf_tl_reader_term(struct kbase_csf_tl_reader *self);
+
+/**
+ *  kbase_csf_tl_reader_flush_buffer() -
+ *   Flush trace from buffer into CSFFW timeline stream.
+ *
+ * @self:    CSFFW TL Reader instance.
+ */
+
+void kbase_csf_tl_reader_flush_buffer(struct kbase_csf_tl_reader *self);
+
+/**
+ * kbase_csf_tl_reader_start() -
+ *	Start asynchronous copying of CSFFW timeline stream.
+ *
+ * @self:	CSFFW TL Reader instance.
+ * @kbdev:	Kbase device.
+ *
+ * Return: zero on success, a negative error code otherwise.
+ */
+int kbase_csf_tl_reader_start(struct kbase_csf_tl_reader *self,
+	struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_tl_reader_stop() -
+ *	Stop asynchronous copying of CSFFW timeline stream.
+ *
+ * @self:	CSFFW TL Reader instance.
+ */
+void kbase_csf_tl_reader_stop(struct kbase_csf_tl_reader *self);
+
+#ifdef CONFIG_DEBUG_FS
+/**
+ * kbase_csf_tl_reader_debugfs_init() -
+ *	Initialize debugfs for CSFFW Timelime Stream Reader.
+ *
+ * @kbdev:	Kbase device.
+ */
+void kbase_csf_tl_reader_debugfs_init(struct kbase_device *kbdev);
+#endif
+
+/**
+ * kbase_csf_tl_reader_reset() -
+ *	Reset CSFFW timeline reader, it should be called before reset CSFFW.
+ *
+ * @self:	CSFFW TL Reader instance.
+ */
+void kbase_csf_tl_reader_reset(struct kbase_csf_tl_reader *self);
+
+#endif /* _KBASE_CSFFW_TL_READER_H_ */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c
new file mode 100644
index 000000000000..4d68766b8b9a
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.c
@@ -0,0 +1,623 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase.h"
+#include "mali_kbase_defs.h"
+#include "mali_kbase_csf_firmware.h"
+#include "mali_kbase_csf_trace_buffer.h"
+#include "mali_kbase_reset_gpu.h"
+#include "mali_kbase_csf_tl_reader.h"
+
+#include <linux/list.h>
+#include <linux/mman.h>
+
+/**
+ * struct firmware_trace_buffer - Trace Buffer within the MCU firmware
+ *
+ * The firmware relays information to the host by writing on memory buffers
+ * which are allocated and partially configured by the host. These buffers
+ * are called Trace Buffers: each of them has a specific purpose and is
+ * identified by a name and a set of memory addresses where the host can
+ * set pointers to host-allocated structures.
+ *
+ * @kbdev:        Pointer to the Kbase device.
+ * @node:         List head linking all trace buffers to
+ *                kbase_device:csf.firmware_trace_buffers
+ * @data_mapping: MCU shared memory mapping used for the data buffer.
+ * @type:         The type of the trace buffer.
+ * @trace_enable_entry_count: Number of Trace Enable bits.
+ * @gpu_va:       Structure containing all the Firmware addresses
+ *                that are accessed by the MCU.
+ * @size_address:    The address where the MCU shall read the size of
+ *                   the data buffer.
+ * @insert_address:  The address that shall be dereferenced by the MCU
+ *                   to write the Insert offset.
+ * @extract_address: The address that shall be dereferenced by the MCU
+ *                   to read the Extract offset.
+ * @data_address:    The address that shall be dereferenced by the MCU
+ *                   to write the Trace Buffer.
+ * @trace_enable:    The address where the MCU shall read the array of
+ *                   Trace Enable bits describing which trace points
+ *                   and features shall be enabled.
+ * @cpu_va:          Structure containing CPU addresses of variables which
+ *                   are permanently mapped on the CPU address space.
+ * @insert_cpu_va:   CPU virtual address of the Insert variable.
+ * @extract_cpu_va:  CPU virtual address of the Extract variable.
+ * @num_pages: Size of the data buffer, in pages.
+ * @trace_enable_init_mask: Initial value for the trace enable bit mask.
+ * @name:  NULL terminated string which contains the name of the trace buffer.
+ */
+struct firmware_trace_buffer {
+	struct kbase_device *kbdev;
+	struct list_head node;
+	struct kbase_csf_mapping data_mapping;
+	u32 type;
+	u32 trace_enable_entry_count;
+	struct gpu_va {
+		u32 size_address;
+		u32 insert_address;
+		u32 extract_address;
+		u32 data_address;
+		u32 trace_enable;
+	} gpu_va;
+	struct cpu_va {
+		u32 *insert_cpu_va;
+		u32 *extract_cpu_va;
+	} cpu_va;
+	u32 num_pages;
+	u32 trace_enable_init_mask[CSF_FIRMWARE_TRACE_ENABLE_INIT_MASK_MAX];
+	char name[1]; /* this field must be last */
+};
+
+/**
+ * struct firmware_trace_buffer_data - Configuration data for trace buffers
+ *
+ * Describe how to set up a trace buffer interface.
+ * Trace buffers are identified by name and they require a data buffer and
+ * an initial mask of values for the trace enable bits.
+ *
+ * @name: Name identifier of the trace buffer
+ * @trace_enable_init_mask: Initial value to assign to the trace enable bits
+ * @size: Size of the data buffer to allocate for the trace buffer, in pages.
+ *        The size of a data buffer must always be a power of 2.
+ */
+struct firmware_trace_buffer_data {
+	char name[64];
+	u32 trace_enable_init_mask[CSF_FIRMWARE_TRACE_ENABLE_INIT_MASK_MAX];
+	size_t size;
+};
+
+/**
+ * Table of configuration data for trace buffers.
+ *
+ * This table contains the configuration data for the trace buffers that are
+ * expected to be parsed from the firmware.
+ */
+static const struct firmware_trace_buffer_data
+trace_buffer_data[] = {
+#ifndef MALI_KBASE_BUILD
+	{ "fwutf", {0}, 1 },
+#endif
+	{ FW_TRACE_BUF_NAME, {0}, 4 },
+	{ "benchmark", {0}, 2 },
+	{ "timeline",  {0}, KBASE_CSF_TL_BUFFER_NR_PAGES },
+};
+
+int kbase_csf_firmware_trace_buffers_init(struct kbase_device *kbdev)
+{
+	struct firmware_trace_buffer *trace_buffer;
+	int ret = 0;
+	u32 mcu_rw_offset = 0, mcu_write_offset = 0;
+	const u32 cache_line_alignment = kbase_get_cache_line_alignment(kbdev);
+
+	if (list_empty(&kbdev->csf.firmware_trace_buffers.list)) {
+		dev_dbg(kbdev->dev, "No trace buffers to initialise\n");
+		return 0;
+	}
+
+	/* GPU-readable,writable memory used for Extract variables */
+	ret = kbase_csf_firmware_mcu_shared_mapping_init(
+			kbdev, 1, PROT_WRITE,
+			KBASE_REG_GPU_RD | KBASE_REG_GPU_WR,
+			&kbdev->csf.firmware_trace_buffers.mcu_rw);
+	if (ret != 0) {
+		dev_err(kbdev->dev, "Failed to map GPU-rw MCU shared memory\n");
+		goto out;
+	}
+
+	/* GPU-writable memory used for Insert variables */
+	ret = kbase_csf_firmware_mcu_shared_mapping_init(
+			kbdev, 1, PROT_READ, KBASE_REG_GPU_WR,
+			&kbdev->csf.firmware_trace_buffers.mcu_write);
+	if (ret != 0) {
+		dev_err(kbdev->dev, "Failed to map GPU-writable MCU shared memory\n");
+		goto out;
+	}
+
+	list_for_each_entry(trace_buffer, &kbdev->csf.firmware_trace_buffers.list, node) {
+		u32 extract_gpu_va, insert_gpu_va, data_buffer_gpu_va,
+			trace_enable_size_dwords;
+		u32 *extract_cpu_va, *insert_cpu_va;
+		unsigned int i;
+
+		/* GPU-writable data buffer for the individual trace buffer */
+		ret = kbase_csf_firmware_mcu_shared_mapping_init(
+				kbdev, trace_buffer->num_pages, PROT_READ, KBASE_REG_GPU_WR,
+				&trace_buffer->data_mapping);
+		if (ret) {
+			dev_err(kbdev->dev, "Failed to map GPU-writable MCU shared memory for a trace buffer\n");
+			goto out;
+		}
+
+		extract_gpu_va =
+			(kbdev->csf.firmware_trace_buffers.mcu_rw.va_reg->start_pfn << PAGE_SHIFT) +
+			mcu_rw_offset;
+		extract_cpu_va = (u32*)(
+			kbdev->csf.firmware_trace_buffers.mcu_rw.cpu_addr +
+			mcu_rw_offset);
+		insert_gpu_va =
+			(kbdev->csf.firmware_trace_buffers.mcu_write.va_reg->start_pfn << PAGE_SHIFT) +
+			mcu_write_offset;
+		insert_cpu_va = (u32*)(
+			kbdev->csf.firmware_trace_buffers.mcu_write.cpu_addr +
+			mcu_write_offset);
+		data_buffer_gpu_va =
+			(trace_buffer->data_mapping.va_reg->start_pfn << PAGE_SHIFT);
+
+		/* Initialize the Extract variable */
+		*extract_cpu_va = 0;
+
+		/* Each FW address shall be mapped and set individually, as we can't
+		 * assume anything about their location in the memory address space.
+		 */
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.data_address, data_buffer_gpu_va);
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.insert_address, insert_gpu_va);
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.extract_address, extract_gpu_va);
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.size_address,
+				trace_buffer->num_pages << PAGE_SHIFT);
+
+		trace_enable_size_dwords =
+				(trace_buffer->trace_enable_entry_count + 31) >> 5;
+
+		for (i = 0; i < trace_enable_size_dwords; i++) {
+			kbase_csf_update_firmware_memory(
+					kbdev, trace_buffer->gpu_va.trace_enable + i*4,
+					trace_buffer->trace_enable_init_mask[i]);
+		}
+
+		/* Store CPU virtual addresses for permanently mapped variables */
+		trace_buffer->cpu_va.insert_cpu_va = insert_cpu_va;
+		trace_buffer->cpu_va.extract_cpu_va = extract_cpu_va;
+
+		/* Update offsets */
+		mcu_write_offset += cache_line_alignment;
+		mcu_rw_offset += cache_line_alignment;
+	}
+
+out:
+	return ret;
+}
+
+void kbase_csf_firmware_trace_buffers_term(struct kbase_device *kbdev)
+{
+	if (list_empty(&kbdev->csf.firmware_trace_buffers.list))
+		return;
+
+	while (!list_empty(&kbdev->csf.firmware_trace_buffers.list)) {
+		struct firmware_trace_buffer *trace_buffer;
+
+		trace_buffer = list_first_entry(&kbdev->csf.firmware_trace_buffers.list,
+				struct firmware_trace_buffer, node);
+		kbase_csf_firmware_mcu_shared_mapping_term(kbdev, &trace_buffer->data_mapping);
+		list_del(&trace_buffer->node);
+
+		kfree(trace_buffer);
+	}
+
+	kbase_csf_firmware_mcu_shared_mapping_term(
+			kbdev, &kbdev->csf.firmware_trace_buffers.mcu_rw);
+	kbase_csf_firmware_mcu_shared_mapping_term(
+			kbdev, &kbdev->csf.firmware_trace_buffers.mcu_write);
+}
+
+int kbase_csf_firmware_parse_trace_buffer_entry(struct kbase_device *kbdev,
+		const u32 *entry, unsigned int size)
+{
+	const char *name = (char *)&entry[7];
+	const unsigned int name_len = size - TRACE_BUFFER_ENTRY_NAME_OFFSET;
+	struct firmware_trace_buffer *trace_buffer;
+	unsigned int i;
+
+	/* Allocate enough space for struct firmware_trace_buffer and the
+	 * trace buffer name (with NULL termination).
+	 */
+	trace_buffer =
+		kmalloc(sizeof(*trace_buffer) + name_len + 1, GFP_KERNEL);
+
+	if (!trace_buffer)
+		return -ENOMEM;
+
+	memcpy(&trace_buffer->name, name, name_len);
+	trace_buffer->name[name_len] = '\0';
+
+	for (i = 0; i < ARRAY_SIZE(trace_buffer_data); i++) {
+		if (!strcmp(trace_buffer_data[i].name, trace_buffer->name)) {
+			unsigned int j;
+
+			trace_buffer->kbdev = kbdev;
+			trace_buffer->type = entry[0];
+			trace_buffer->gpu_va.size_address = entry[1];
+			trace_buffer->gpu_va.insert_address = entry[2];
+			trace_buffer->gpu_va.extract_address = entry[3];
+			trace_buffer->gpu_va.data_address = entry[4];
+			trace_buffer->gpu_va.trace_enable = entry[5];
+			trace_buffer->trace_enable_entry_count = entry[6];
+			trace_buffer->num_pages = trace_buffer_data[i].size;
+
+			for (j = 0; j < CSF_FIRMWARE_TRACE_ENABLE_INIT_MASK_MAX; j++) {
+				trace_buffer->trace_enable_init_mask[j] =
+					trace_buffer_data[i].trace_enable_init_mask[j];
+			}
+			break;
+		}
+	}
+
+	if (i < ARRAY_SIZE(trace_buffer_data)) {
+		list_add(&trace_buffer->node, &kbdev->csf.firmware_trace_buffers.list);
+		dev_dbg(kbdev->dev, "Trace buffer '%s'", trace_buffer->name);
+	} else {
+		dev_dbg(kbdev->dev, "Unknown trace buffer '%s'", trace_buffer->name);
+		kfree(trace_buffer);
+	}
+
+	return 0;
+}
+
+void kbase_csf_firmware_reload_trace_buffers_data(struct kbase_device *kbdev)
+{
+	struct firmware_trace_buffer *trace_buffer;
+	u32 mcu_rw_offset = 0, mcu_write_offset = 0;
+	const u32 cache_line_alignment = kbase_get_cache_line_alignment(kbdev);
+
+	list_for_each_entry(trace_buffer, &kbdev->csf.firmware_trace_buffers.list, node) {
+		u32 extract_gpu_va, insert_gpu_va, data_buffer_gpu_va,
+			trace_enable_size_dwords;
+		u32 *extract_cpu_va, *insert_cpu_va;
+		unsigned int i;
+
+		/* Rely on the fact that all required mappings already exist */
+		extract_gpu_va =
+			(kbdev->csf.firmware_trace_buffers.mcu_rw.va_reg->start_pfn << PAGE_SHIFT) +
+			mcu_rw_offset;
+		extract_cpu_va = (u32*)(
+			kbdev->csf.firmware_trace_buffers.mcu_rw.cpu_addr +
+			mcu_rw_offset);
+		insert_gpu_va =
+			(kbdev->csf.firmware_trace_buffers.mcu_write.va_reg->start_pfn << PAGE_SHIFT) +
+			mcu_write_offset;
+		insert_cpu_va = (u32*)(
+			kbdev->csf.firmware_trace_buffers.mcu_write.cpu_addr +
+			mcu_write_offset);
+		data_buffer_gpu_va =
+			(trace_buffer->data_mapping.va_reg->start_pfn << PAGE_SHIFT);
+
+		/* Notice that the function only re-updates firmware memory locations
+		 * with information that allows access to the trace buffers without
+		 * really resetting their state. For instance, the Insert offset will
+		 * not change and, as a consequence, the Extract offset is not going
+		 * to be reset to keep consistency.
+		 */
+
+		/* Each FW address shall be mapped and set individually, as we can't
+		 * assume anything about their location in the memory address space.
+		 */
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.data_address, data_buffer_gpu_va);
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.insert_address, insert_gpu_va);
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.extract_address, extract_gpu_va);
+		kbase_csf_update_firmware_memory(
+				kbdev, trace_buffer->gpu_va.size_address,
+				trace_buffer->num_pages << PAGE_SHIFT);
+
+		trace_enable_size_dwords =
+				(trace_buffer->trace_enable_entry_count + 31) >> 5;
+
+		for (i = 0; i < trace_enable_size_dwords; i++) {
+			kbase_csf_update_firmware_memory(
+					kbdev, trace_buffer->gpu_va.trace_enable + i*4,
+					trace_buffer->trace_enable_init_mask[i]);
+		}
+
+		/* Store CPU virtual addresses for permanently mapped variables,
+		 * as they might have slightly changed.
+		 */
+		trace_buffer->cpu_va.insert_cpu_va = insert_cpu_va;
+		trace_buffer->cpu_va.extract_cpu_va = extract_cpu_va;
+
+		/* Update offsets */
+		mcu_write_offset += cache_line_alignment;
+		mcu_rw_offset += cache_line_alignment;
+	}
+}
+
+struct firmware_trace_buffer *kbase_csf_firmware_get_trace_buffer(
+	struct kbase_device *kbdev, const char *name)
+{
+	struct firmware_trace_buffer *trace_buffer;
+
+	list_for_each_entry(trace_buffer, &kbdev->csf.firmware_trace_buffers.list, node) {
+		if (!strcmp(trace_buffer->name, name))
+			return trace_buffer;
+	}
+
+	return NULL;
+}
+EXPORT_SYMBOL(kbase_csf_firmware_get_trace_buffer);
+
+unsigned int kbase_csf_firmware_trace_buffer_get_trace_enable_bits_count(
+	const struct firmware_trace_buffer *trace_buffer)
+{
+	return trace_buffer->trace_enable_entry_count;
+}
+EXPORT_SYMBOL(kbase_csf_firmware_trace_buffer_get_trace_enable_bits_count);
+
+void kbase_csf_firmware_trace_buffer_update_trace_enable_bit(
+	struct firmware_trace_buffer *tb, unsigned int bit, bool value)
+{
+	if (bit < tb->trace_enable_entry_count) {
+		unsigned int trace_enable_reg_offset = bit >> 5;
+		u32 trace_enable_bit_mask = 1u << (bit & 0x1F);
+
+		if (value) {
+			tb->trace_enable_init_mask[trace_enable_reg_offset] |=
+				trace_enable_bit_mask;
+		} else {
+			tb->trace_enable_init_mask[trace_enable_reg_offset] &=
+				~trace_enable_bit_mask;
+		}
+
+		/* This is not strictly needed as the caller is supposed to
+		 * reload the firmware image (through GPU reset) after updating
+		 * the bitmask. Otherwise there is no guarantee that firmware
+		 * will take into account the updated bitmask for all types of
+		 * trace buffers, since firmware could continue to use the
+		 * value of bitmask it cached after the boot.
+		 */
+		kbase_csf_update_firmware_memory(tb->kbdev,
+			tb->gpu_va.trace_enable + trace_enable_reg_offset*4,
+			tb->trace_enable_init_mask[trace_enable_reg_offset]);
+	}
+}
+EXPORT_SYMBOL(kbase_csf_firmware_trace_buffer_update_trace_enable_bit);
+
+bool kbase_csf_firmware_trace_buffer_is_empty(
+	const struct firmware_trace_buffer *trace_buffer)
+{
+	return *(trace_buffer->cpu_va.insert_cpu_va) ==
+			*(trace_buffer->cpu_va.extract_cpu_va);
+}
+EXPORT_SYMBOL(kbase_csf_firmware_trace_buffer_is_empty);
+
+unsigned int kbase_csf_firmware_trace_buffer_read_data(
+	struct firmware_trace_buffer *trace_buffer, u8 *data, unsigned int num_bytes)
+{
+	unsigned int bytes_copied;
+	u8 *data_cpu_va = trace_buffer->data_mapping.cpu_addr;
+	u32 extract_offset = *(trace_buffer->cpu_va.extract_cpu_va);
+	u32 insert_offset = *(trace_buffer->cpu_va.insert_cpu_va);
+	u32 buffer_size = trace_buffer->num_pages << PAGE_SHIFT;
+
+	if (insert_offset >= extract_offset) {
+		bytes_copied = min_t(unsigned int, num_bytes,
+			(insert_offset - extract_offset));
+		memcpy(data, &data_cpu_va[extract_offset], bytes_copied);
+		extract_offset += bytes_copied;
+	} else {
+		unsigned int bytes_copied_head, bytes_copied_tail;
+
+		bytes_copied_tail = min_t(unsigned int, num_bytes,
+			(buffer_size - extract_offset));
+		memcpy(data, &data_cpu_va[extract_offset], bytes_copied_tail);
+
+		bytes_copied_head = min_t(unsigned int,
+			(num_bytes - bytes_copied_tail), insert_offset);
+		memcpy(&data[bytes_copied_tail], data_cpu_va, bytes_copied_head);
+
+		bytes_copied = bytes_copied_head + bytes_copied_tail;
+		extract_offset += bytes_copied;
+		if (extract_offset >= buffer_size)
+			extract_offset = bytes_copied_head;
+	}
+
+	*(trace_buffer->cpu_va.extract_cpu_va) = extract_offset;
+
+	return bytes_copied;
+}
+EXPORT_SYMBOL(kbase_csf_firmware_trace_buffer_read_data);
+
+#ifdef CONFIG_DEBUG_FS
+
+#define U32_BITS 32
+static u64 get_trace_buffer_active_mask64(struct firmware_trace_buffer *tb)
+{
+	u64 active_mask = tb->trace_enable_init_mask[0];
+
+	if (tb->trace_enable_entry_count > U32_BITS)
+		active_mask |= (u64)tb->trace_enable_init_mask[1] << U32_BITS;
+
+	return active_mask;
+}
+
+static void update_trace_buffer_active_mask64(struct firmware_trace_buffer *tb,
+		u64 mask)
+{
+	unsigned int i;
+
+	for (i = 0; i < tb->trace_enable_entry_count; i++)
+		kbase_csf_firmware_trace_buffer_update_trace_enable_bit(tb, i,
+							(mask >> i) & 1);
+}
+
+static int set_trace_buffer_active_mask64(struct firmware_trace_buffer *tb,
+		u64 mask)
+{
+	struct kbase_device *kbdev = tb->kbdev;
+	unsigned long flags;
+	int err = 0;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	/* If there is already a GPU reset pending, need a retry */
+	if (kbase_reset_gpu_silent(kbdev))
+		err = -EAGAIN;
+	else
+		update_trace_buffer_active_mask64(tb, mask);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return err;
+}
+
+static int kbase_csf_firmware_trace_enable_mask_read(void *data, u64 *val)
+{
+	struct kbase_device *kbdev = (struct kbase_device *)data;
+	struct firmware_trace_buffer *tb =
+		kbase_csf_firmware_get_trace_buffer(kbdev, FW_TRACE_BUF_NAME);
+
+	if (tb == NULL) {
+		dev_err(kbdev->dev, "Couldn't get the firmware trace buffer");
+		return -EIO;
+	}
+	/* The enabled traces limited to u64 here, regarded practical */
+	*val = get_trace_buffer_active_mask64(tb);
+	return 0;
+}
+
+static int kbase_csf_firmware_trace_enable_mask_write(void *data, u64 val)
+{
+	struct kbase_device *kbdev = (struct kbase_device *)data;
+	struct firmware_trace_buffer *tb =
+		kbase_csf_firmware_get_trace_buffer(kbdev, FW_TRACE_BUF_NAME);
+	u64 new_mask;
+	unsigned int enable_bits_count;
+
+	if (tb == NULL) {
+		dev_err(kbdev->dev, "Couldn't get the firmware trace buffer");
+		return -EIO;
+	}
+
+	/* Ignore unsupported types */
+	enable_bits_count =
+	    kbase_csf_firmware_trace_buffer_get_trace_enable_bits_count(tb);
+	if (enable_bits_count > 64) {
+		dev_dbg(kbdev->dev, "Limit enabled bits count from %u to 64",
+			enable_bits_count);
+		enable_bits_count = 64;
+	}
+	new_mask = val & ((1 << enable_bits_count) - 1);
+
+	if (new_mask != get_trace_buffer_active_mask64(tb))
+		return set_trace_buffer_active_mask64(tb, new_mask);
+	else
+		return 0;
+}
+
+static int kbasep_csf_firmware_trace_debugfs_open(struct inode *in,
+		struct file *file)
+{
+	struct kbase_device *kbdev = in->i_private;
+
+	file->private_data = kbdev;
+	dev_dbg(kbdev->dev, "Opened firmware trace buffer dump debugfs file");
+
+	return 0;
+}
+
+static ssize_t kbasep_csf_firmware_trace_debugfs_read(struct file *file,
+		char __user *buf, size_t size, loff_t *ppos)
+{
+	struct kbase_device *kbdev = file->private_data;
+	u8 *pbyte;
+	unsigned int n_read;
+	unsigned long not_copied;
+	/* Limit the kernel buffer to no more than two pages */
+	size_t mem = MIN(size, 2 * PAGE_SIZE);
+	unsigned long flags;
+
+	struct firmware_trace_buffer *tb =
+		kbase_csf_firmware_get_trace_buffer(kbdev, FW_TRACE_BUF_NAME);
+
+	if (tb == NULL) {
+		dev_err(kbdev->dev, "Couldn't get the firmware trace buffer");
+		return -EIO;
+	}
+
+	pbyte = kmalloc(mem, GFP_KERNEL);
+	if (pbyte == NULL) {
+		dev_err(kbdev->dev, "Couldn't allocate memory for trace buffer dump");
+		return -ENOMEM;
+	}
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	n_read = kbase_csf_firmware_trace_buffer_read_data(tb, pbyte, mem);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	/* Do the copy, if we have obtained some trace data */
+	not_copied = (n_read) ? copy_to_user(buf, pbyte, n_read) : 0;
+	kfree(pbyte);
+
+	if (!not_copied) {
+		*ppos += n_read;
+		return n_read;
+	}
+
+	dev_err(kbdev->dev, "Couldn't copy trace buffer data to user space buffer");
+	return -EFAULT;
+}
+
+
+DEFINE_SIMPLE_ATTRIBUTE(kbase_csf_firmware_trace_enable_mask_fops,
+		kbase_csf_firmware_trace_enable_mask_read,
+		kbase_csf_firmware_trace_enable_mask_write, "%llx\n");
+
+static const struct file_operations kbasep_csf_firmware_trace_debugfs_fops = {
+	.owner = THIS_MODULE,
+	.open = kbasep_csf_firmware_trace_debugfs_open,
+	.read = kbasep_csf_firmware_trace_debugfs_read,
+	.llseek = no_llseek,
+};
+
+void kbase_csf_firmware_trace_buffer_debugfs_init(struct kbase_device *kbdev)
+{
+	debugfs_create_file("fw_trace_enable_mask", 0644,
+			    kbdev->mali_debugfs_directory, kbdev,
+			    &kbase_csf_firmware_trace_enable_mask_fops);
+
+	debugfs_create_file("fw_traces", 0444,
+			    kbdev->mali_debugfs_directory, kbdev,
+			    &kbasep_csf_firmware_trace_debugfs_fops);
+}
+#endif /* CONFIG_DEBUG_FS */
diff --git a/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h
new file mode 100644
index 000000000000..2cac55e0664d
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/csf/mali_kbase_csf_trace_buffer.h
@@ -0,0 +1,177 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CSF_TRACE_BUFFER_H_
+#define _KBASE_CSF_TRACE_BUFFER_H_
+
+#include <linux/types.h>
+
+#define CSF_FIRMWARE_TRACE_ENABLE_INIT_MASK_MAX (4)
+#define FW_TRACE_BUF_NAME "fwlog"
+
+/* Forward declarations */
+struct firmware_trace_buffer;
+struct kbase_device;
+
+/**
+ * kbase_csf_firmware_trace_buffers_init - Initialize trace buffers
+ *
+ * Allocate resources for trace buffers. In particular:
+ * - One memory page of GPU-readable, CPU-writable memory is used for
+ *   the Extract variables of all trace buffers.
+ * - One memory page of GPU-writable, CPU-readable memory is used for
+ *   the Insert variables of all trace buffers.
+ * - A data buffer of GPU-writable, CPU-readable memory is allocated
+ *   for each trace buffer.
+ *
+ * After that, firmware addresses are written with pointers to the
+ * insert, extract and data buffer variables. The size and the trace
+ * enable bits are not dereferenced by the GPU and shall be written
+ * in the firmware addresses directly.
+ *
+ * This function relies on the assumption that the list of
+ * firmware_trace_buffer elements in the device has already been
+ * populated with data from the firmware image parsing.
+ *
+ * Return: 0 if success, or an error code on failure.
+ *
+ * @kbdev: Device pointer
+ */
+int kbase_csf_firmware_trace_buffers_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_trace_buffer_term - Terminate trace buffers
+ *
+ * @kbdev: Device pointer
+ */
+void kbase_csf_firmware_trace_buffers_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_parse_trace_buffer_entry - Process a "trace buffer" section
+ *
+ * Read a "trace buffer" section adding metadata for the related trace buffer
+ * to the kbase_device:csf.firmware_trace_buffers list.
+ *
+ * Unexpected trace buffers will not be parsed and, as a consequence,
+ * will not be initialized.
+ *
+ * Return: 0 if successful, negative error code on failure.
+ *
+ * @kbdev: Kbase device structure
+ * @entry: Pointer to the section
+ * @size:  Size (in bytes) of the section
+ */
+int kbase_csf_firmware_parse_trace_buffer_entry(struct kbase_device *kbdev,
+		const u32 *entry, unsigned int size);
+
+/**
+ * kbase_csf_firmware_reload_trace_buffers_data -
+ * Reload trace buffers data for firmware reboot
+ *
+ * Helper function used when rebooting the firmware to reload the initial setup
+ * for all the trace buffers which have been previously parsed and initialized.
+ *
+ * Almost all of the operations done in the initialization process are
+ * replicated, with the difference that they might be done in a different order
+ * and that the variables of a given trace buffer may be mapped to different
+ * offsets within the same existing mappings.
+ *
+ * In other words, the re-initialization done by this function will be
+ * equivalent but not necessarily identical to the original initialization.
+ *
+ * @kbdev: Device pointer
+ */
+void kbase_csf_firmware_reload_trace_buffers_data(struct kbase_device *kbdev);
+
+/**
+ * kbase_csf_firmware_get_trace_buffer - Get a trace buffer
+ *
+ * Return: handle to a trace buffer, given the name, or NULL if a trace buffer
+ *         with that name couldn't be found.
+ *
+ * @kbdev: Device pointer
+ * @name:  Name of the trace buffer to find
+ */
+struct firmware_trace_buffer *kbase_csf_firmware_get_trace_buffer(
+	struct kbase_device *kbdev, const char *name);
+
+/**
+ * kbase_csf_firmware_trace_buffer_get_trace_enable_bits_count -
+ * Get number of trace enable bits for a trace buffer
+ *
+ * Return: Number of trace enable bits in a trace buffer.
+ *
+ * @trace_buffer: Trace buffer handle
+ */
+unsigned int kbase_csf_firmware_trace_buffer_get_trace_enable_bits_count(
+	const struct firmware_trace_buffer *trace_buffer);
+
+/**
+ * kbase_csf_firmware_trace_buffer_update_trace_enable_bit -
+ * Update a trace enable bit
+ *
+ * Update the value of a given trace enable bit.
+ *
+ * @trace_buffer: Trace buffer handle
+ * @bit:          Bit to update
+ * @value:        New value for the given bit
+ */
+void kbase_csf_firmware_trace_buffer_update_trace_enable_bit(
+	struct firmware_trace_buffer *trace_buffer, unsigned int bit, bool value);
+
+/**
+ * kbase_csf_firmware_trace_buffer_is_empty - Empty trace buffer predicate
+ *
+ * Return: True if the trace buffer is empty, or false otherwise.
+ *
+ * @trace_buffer: Trace buffer handle
+ */
+bool kbase_csf_firmware_trace_buffer_is_empty(
+	const struct firmware_trace_buffer *trace_buffer);
+
+/**
+ * kbase_csf_firmware_trace_buffer_read_data - Read data from a trace buffer
+ *
+ * Read available data from a trace buffer. The client provides a data buffer
+ * of a given size and the maximum number of bytes to read.
+ *
+ * Return: Number of bytes read from the trace buffer.
+ *
+ * @trace_buffer: Trace buffer handle
+ * @data:         Pointer to a client-allocated where data shall be written.
+ * @num_bytes:    Maximum number of bytes to read from the trace buffer.
+ */
+unsigned int kbase_csf_firmware_trace_buffer_read_data(
+	struct firmware_trace_buffer *trace_buffer, u8 *data, unsigned int num_bytes);
+
+#ifdef CONFIG_DEBUG_FS
+/**
+ * kbase_csf_fw_trace_buffer_debugfs_init() - Add debugfs entries for setting
+ *                                         enable mask and dumping the binary
+ *                                         firmware trace buffer
+ *
+ * @kbdev: Pointer to the device
+ */
+void kbase_csf_firmware_trace_buffer_debugfs_init(struct kbase_device *kbdev);
+#endif /* CONFIG_DEBUG_FS */
+
+#endif /* _KBASE_CSF_TRACE_BUFFER_H_ */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h
new file mode 100644
index 000000000000..32181d711193
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_csf.h
@@ -0,0 +1,116 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * ***** IMPORTANT: THIS IS NOT A NORMAL HEADER FILE         *****
+ * *****            DO NOT INCLUDE DIRECTLY                  *****
+ * *****            THE LACK OF HEADER GUARDS IS INTENTIONAL *****
+ */
+
+/*
+ * The purpose of this header file is just to contain a list of trace code
+ * identifiers
+ *
+ * When updating this file, also remember to update
+ * mali_kbase_debug_linux_ktrace_csf.h
+ *
+ * IMPORTANT: THIS FILE MUST NOT BE USED FOR ANY OTHER PURPOSE OTHER THAN THAT
+ * DESCRIBED IN mali_kbase_debug_ktrace_codes.h
+ */
+
+#if 0 /* Dummy section to avoid breaking formatting */
+int dummy_array[] = {
+#endif
+	/*
+	 * Generic CSF events
+	 */
+	KBASE_KTRACE_CODE_MAKE_CODE(EVICT_CTX_SLOTS),
+	/* info_val[0:7]   == fw version_minor
+	 * info_val[15:8]  == fw version_major
+	 * info_val[63:32] == fw version_hash
+	 */
+	KBASE_KTRACE_CODE_MAKE_CODE(FIRMWARE_BOOT),
+	KBASE_KTRACE_CODE_MAKE_CODE(FIRMWARE_REBOOT),
+	KBASE_KTRACE_CODE_MAKE_CODE(SCHEDULER_TOCK),
+	KBASE_KTRACE_CODE_MAKE_CODE(SCHEDULER_TICK),
+	KBASE_KTRACE_CODE_MAKE_CODE(SCHEDULER_RESET),
+	KBASE_KTRACE_CODE_MAKE_CODE(SCHEDULER_EXIT_PROTM),
+	KBASE_KTRACE_CODE_MAKE_CODE(SYNC_UPDATE_EVENT),
+
+	/*
+	 * Group events
+	 */
+	/* info_val[2:0] == CSG_REQ state issued
+	 * info_val[19:16] == as_nr
+	 * info_val[63:32] == endpoint config (max number of endpoints allowed)
+	 */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_SLOT_START),
+	/* info_val == CSG_REQ state issued */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_SLOT_STOP),
+	/* info_val == CSG_ACK state */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_SLOT_STARTED),
+	/* info_val == CSG_ACK state */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_SLOT_STOPPED),
+	/* info_val == slot cleaned */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_SLOT_CLEANED),
+	/* info_val == previous priority */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_PRIO_UPDATE),
+	/* info_val == CSG_REQ ^ CSG_ACK */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_SYNC_UPDATE_INTERRUPT),
+	/* info_val == CSG_REQ ^ CSG_ACK */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSG_IDLE_INTERRUPT),
+	KBASE_KTRACE_CODE_MAKE_CODE(GROUP_SYNC_UPDATE_DONE),
+	/* info_val == run state of the group */
+	KBASE_KTRACE_CODE_MAKE_CODE(GROUP_DESCHEDULE),
+	/* info_val == run state of the group */
+	KBASE_KTRACE_CODE_MAKE_CODE(GROUP_SCHEDULE),
+	/* info_val[31:0] == new run state of the evicted group
+	 * info_val[63:32] == number of runnable groups
+	 */
+	KBASE_KTRACE_CODE_MAKE_CODE(GROUP_EVICT_SCHED),
+	KBASE_KTRACE_CODE_MAKE_CODE(SCHEDULER_ENTER_PROTM),
+	/* info_val[31:0] == number of GPU address space slots in use
+	 * info_val[63:32] == number of runnable groups
+	 */
+	KBASE_KTRACE_CODE_MAKE_CODE(SCHEDULER_TOP_GRP),
+
+	/*
+	 * Group + Queue events
+	 */
+	/* info_val == queue->enabled */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSI_START),
+	/* info_val == queue->enabled before stop */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSI_STOP),
+	KBASE_KTRACE_CODE_MAKE_CODE(CSI_STOP_REQUESTED),
+	/* info_val == CS_REQ ^ CS_ACK */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSI_FAULT_INTERRUPT),
+	/* info_val == CS_REQ ^ CS_ACK */
+	KBASE_KTRACE_CODE_MAKE_CODE(CSI_TILER_OOM_INTERRUPT),
+	/* info_val == group->run_State (for group the queue is bound to) */
+	KBASE_KTRACE_CODE_MAKE_CODE(QUEUE_START),
+	KBASE_KTRACE_CODE_MAKE_CODE(QUEUE_STOP),
+
+#if 0 /* Dummy section to avoid breaking formatting */
+};
+#endif
+
+/* ***** THE LACK OF HEADER GUARDS IS INTENTIONAL ***** */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_jm.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_jm.h
index d534f3006c9b..b201e49bd0f2 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_jm.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_codes_jm.h
@@ -30,6 +30,9 @@
  * The purpose of this header file is just to contain a list of trace code
  * identifiers
  *
+ * When updating this file, also remember to update
+ * mali_kbase_debug_linux_ktrace_jm.h
+ *
  * IMPORTANT: THIS FILE MUST NOT BE USED FOR ANY OTHER PURPOSE OTHER THAN THAT
  * DESCRIBED IN mali_kbase_debug_ktrace_codes.h
  */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c
new file mode 100644
index 000000000000..2ea901b666c2
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.c
@@ -0,0 +1,143 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+#include <mali_kbase.h>
+#include "debug/mali_kbase_debug_ktrace_internal.h"
+#include "debug/backend/mali_kbase_debug_ktrace_csf.h"
+
+#if KBASE_KTRACE_TARGET_RBUF
+
+void kbasep_ktrace_backend_format_header(char *buffer, int sz, s32 *written)
+{
+	*written += MAX(snprintf(buffer + *written, MAX(sz - *written, 0),
+			"group,slot,prio,csi"), 0);
+}
+
+void kbasep_ktrace_backend_format_msg(struct kbase_ktrace_msg *trace_msg,
+		char *buffer, int sz, s32 *written)
+{
+	const struct kbase_ktrace_backend * const be_msg = &trace_msg->backend;
+	/* At present, no need to check for KBASE_KTRACE_FLAG_BACKEND, as the
+	 * other backend-specific flags currently imply this anyway
+	 */
+
+	/* group parts */
+	if (be_msg->flags & KBASE_KTRACE_FLAG_CSF_GROUP) {
+		const s8 slot = be_msg->csg_nr;
+		/* group,slot, */
+		*written += MAX(snprintf(buffer + *written,
+				MAX(sz - *written, 0),
+				"%u,%d,", be_msg->group_handle, slot), 0);
+
+		/* prio */
+		if (slot >= 0)
+			*written += MAX(snprintf(buffer + *written,
+					MAX(sz - *written, 0),
+					"%u", be_msg->slot_prio), 0);
+
+		/* , */
+		*written += MAX(snprintf(buffer + *written,
+				MAX(sz - *written, 0),
+				","), 0);
+	} else {
+		/* No group,slot,prio fields, but ensure ending with "," */
+		*written += MAX(snprintf(buffer + *written,
+				MAX(sz - *written, 0),
+				",,,"), 0);
+	}
+
+	/* queue parts: csi */
+	if (trace_msg->backend.flags & KBASE_KTRACE_FLAG_CSF_QUEUE)
+		*written += MAX(snprintf(buffer + *written,
+				MAX(sz - *written, 0),
+				"%d", be_msg->csi_index), 0);
+
+	/* Don't end with a trailing "," - this is a 'standalone' formatted
+	 * msg, caller will handle the delimiters
+	 */
+}
+
+void kbasep_ktrace_add_csf(struct kbase_device *kbdev,
+		enum kbase_ktrace_code code, struct kbase_queue_group *group,
+		struct kbase_queue *queue, kbase_ktrace_flag_t flags,
+		u64 info_val)
+{
+	unsigned long irqflags;
+	struct kbase_ktrace_msg *trace_msg;
+	struct kbase_context *kctx = NULL;
+
+	spin_lock_irqsave(&kbdev->ktrace.lock, irqflags);
+
+	/* Reserve and update indices */
+	trace_msg = kbasep_ktrace_reserve(&kbdev->ktrace);
+
+	/* Determine the kctx */
+	if (group)
+		kctx = group->kctx;
+	else if (queue)
+		kctx = queue->kctx;
+
+	/* Fill the common part of the message (including backend.flags) */
+	kbasep_ktrace_msg_init(&kbdev->ktrace, trace_msg, code, kctx, flags,
+			info_val);
+
+	/* Indicate to the common code that backend-specific parts will be
+	 * valid
+	 */
+	trace_msg->backend.flags |= KBASE_KTRACE_FLAG_BACKEND;
+
+	/* Fill the CSF-specific parts of the message
+	 *
+	 * Generally, no need to use default initializers when queue/group not
+	 * present - can usually check the flags instead.
+	 */
+
+	if (queue) {
+		trace_msg->backend.flags |= KBASE_KTRACE_FLAG_CSF_QUEUE;
+		trace_msg->backend.csi_index = queue->csi_index;
+	}
+
+	if (group) {
+		const s8 slot = group->csg_nr;
+
+		trace_msg->backend.flags |= KBASE_KTRACE_FLAG_CSF_GROUP;
+
+		trace_msg->backend.csg_nr = slot;
+
+		if (slot >= 0) {
+			struct kbase_csf_csg_slot *csg_slot = &kbdev->csf.scheduler.csg_slots[slot];
+
+			trace_msg->backend.slot_prio = csg_slot->priority;
+		}
+		/* slot >=0 indicates whether slot_prio valid, so no need to
+		 * initialize in the case where it's invalid
+		 */
+
+		trace_msg->backend.group_handle = group->handle;
+	}
+
+	WARN_ON((trace_msg->backend.flags & ~KBASE_KTRACE_FLAG_ALL));
+
+	/* Done */
+	spin_unlock_irqrestore(&kbdev->ktrace.lock, irqflags);
+}
+
+#endif /* KBASE_KTRACE_TARGET_RBUF */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.h
new file mode 100644
index 000000000000..b055ff82a116
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_csf.h
@@ -0,0 +1,148 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_DEBUG_KTRACE_CSF_H_
+#define _KBASE_DEBUG_KTRACE_CSF_H_
+
+/*
+ * KTrace target for internal ringbuffer
+ */
+#if KBASE_KTRACE_TARGET_RBUF
+/**
+ * kbasep_ktrace_add_csf - internal function to add trace about Command Stream
+ *                        Frontend
+ * @kbdev:    kbase device
+ * @code:     trace code
+ * @group:    queue group, or NULL if no queue group
+ * @queue:    queue, or NULL if no queue
+ * @flags:    flags about the message
+ * @info_val: generic information about @code to add to the trace
+ *
+ * PRIVATE: do not use directly. Use KBASE_KTRACE_ADD_CSF() instead.
+ */
+
+void kbasep_ktrace_add_csf(struct kbase_device *kbdev,
+		enum kbase_ktrace_code code, struct kbase_queue_group *group,
+		struct kbase_queue *queue, kbase_ktrace_flag_t flags,
+		u64 info_val);
+
+#define KBASE_KTRACE_RBUF_ADD_CSF(kbdev, code, group, queue, flags, info_val) \
+	kbasep_ktrace_add_csf(kbdev, KBASE_KTRACE_CODE(code), group, queue, \
+			flags, info_val)
+
+#else /* KBASE_KTRACE_TARGET_RBUF */
+
+#define KBASE_KTRACE_RBUF_ADD_CSF(kbdev, code, group, queue, flags, info_val) \
+	do {\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(code);\
+		CSTD_UNUSED(group);\
+		CSTD_UNUSED(queue);		\
+		CSTD_UNUSED(flags);\
+		CSTD_UNUSED(info_val);\
+		CSTD_NOP(0);\
+	} while (0)
+
+#endif /* KBASE_KTRACE_TARGET_RBUF */
+
+/*
+ * KTrace target for Linux's ftrace
+ *
+ * Note: the header file(s) that define the trace_mali_<...> tracepoints are
+ * included by the parent header file
+ */
+#if KBASE_KTRACE_TARGET_FTRACE
+
+#define KBASE_KTRACE_FTRACE_ADD_CSF(kbdev, code, group, queue, info_val) \
+	trace_mali_##code(kbdev, group, queue, info_val)
+
+#else /* KBASE_KTRACE_TARGET_FTRACE */
+
+#define KBASE_KTRACE_FTRACE_ADD_CSF(kbdev, code, group, queue, info_val) \
+	do {\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(code);\
+		CSTD_UNUSED(group);\
+		CSTD_UNUSED(queue);\
+		CSTD_UNUSED(info_val);\
+		CSTD_NOP(0);\
+	} while (0)
+
+#endif /* KBASE_KTRACE_TARGET_FTRACE */
+
+/*
+ * Master set of macros to route KTrace to any of the targets
+ */
+
+/**
+ * KBASE_KTRACE_ADD_CSF_GRP - Add trace values about a group, with info
+ * @kbdev:    kbase device
+ * @code:     trace code
+ * @group:    queue group, or NULL if no queue group
+ * @info_val: generic information about @code to add to the trace
+ *
+ * Note: Any functions called through this macro will still be evaluated in
+ * Release builds (CONFIG_MALI_BIFROST_DEBUG not defined). Therefore, when
+ * KBASE_KTRACE_ENABLE == 0 any functions called to get the parameters supplied
+ * to this macro must:
+ * a) be static or static inline, and
+ * b) just return 0 and have no other statements present in the body.
+ */
+#define KBASE_KTRACE_ADD_CSF_GRP(kbdev, code, group, info_val) \
+	do { \
+		/* capture values that could come from non-pure function calls */ \
+		struct kbase_queue_group *__group = group; \
+		u64 __info_val = info_val; \
+		KBASE_KTRACE_RBUF_ADD_CSF(kbdev, code, __group, NULL, 0u, \
+				__info_val); \
+		KBASE_KTRACE_FTRACE_ADD_CSF(kbdev, code, __group, NULL, \
+				__info_val); \
+	} while (0)
+
+/**
+ * KBASE_KTRACE_ADD_CSF_GRP_Q - Add trace values about a group, queue, with info
+ * @kbdev:    kbase device
+ * @code:     trace code
+ * @group:    queue group, or NULL if no queue group
+ * @queue:    queue, or NULL if no queue
+ * @info_val: generic information about @code to add to the trace
+ *
+ * Note: Any functions called through this macro will still be evaluated in
+ * Release builds (CONFIG_MALI_BIFROST_DEBUG not defined). Therefore, when
+ * KBASE_KTRACE_ENABLE == 0 any functions called to get the parameters supplied
+ * to this macro must:
+ * a) be static or static inline, and
+ * b) just return 0 and have no other statements present in the body.
+ */
+#define KBASE_KTRACE_ADD_CSF_GRP_Q(kbdev, code, group, queue, info_val) \
+	do { \
+		/* capture values that could come from non-pure function calls */ \
+		struct kbase_queue_group *__group = group; \
+		struct kbase_queue *__queue = queue; \
+		u64 __info_val = info_val; \
+		KBASE_KTRACE_RBUF_ADD_CSF(kbdev, code, __group, __queue, 0u, \
+				__info_val); \
+		KBASE_KTRACE_FTRACE_ADD_CSF(kbdev, code, __group, \
+				__queue, __info_val); \
+	} while (0)
+
+#endif /* _KBASE_DEBUG_KTRACE_CSF_H_ */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_csf.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_csf.h
new file mode 100644
index 000000000000..f265fe9a9753
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_csf.h
@@ -0,0 +1,85 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_DEBUG_KTRACE_DEFS_CSF_H_
+#define _KBASE_DEBUG_KTRACE_DEFS_CSF_H_
+
+#if KBASE_KTRACE_TARGET_RBUF
+/**
+ * DOC: KTrace version history, CSF variant
+ *
+ * 1.0:
+ * First version, with version information in the header.
+ *
+ * 1.1:
+ * kctx field is no longer a pointer, and is now an ID of the format %d_%u as
+ * used by kctx directories in mali debugfs entries: (tgid creating the kctx),
+ * (unique kctx id)
+ *
+ * ftrace backend now outputs kctx field (as %d_%u format).
+ *
+ * Add fields group, slot, prio, csi into backend-specific part.
+ */
+#define KBASE_KTRACE_VERSION_MAJOR 1
+#define KBASE_KTRACE_VERSION_MINOR 1
+
+/* indicates if the trace message has valid queue-group related info. */
+#define KBASE_KTRACE_FLAG_CSF_GROUP     (((kbase_ktrace_flag_t)1) << 0)
+
+/* indicates if the trace message has valid queue related info. */
+#define KBASE_KTRACE_FLAG_CSF_QUEUE     (((kbase_ktrace_flag_t)1) << 1)
+
+/* Collect all the flags together for debug checking */
+#define KBASE_KTRACE_FLAG_BACKEND_ALL \
+		(KBASE_KTRACE_FLAG_CSF_GROUP | KBASE_KTRACE_FLAG_CSF_QUEUE)
+
+
+/**
+ * struct kbase_ktrace_backend - backend specific part of a trace message
+ *
+ * @code:         Identifies the event, refer to enum kbase_ktrace_code.
+ * @flags:        indicates information about the trace message itself. Used
+ *                during dumping of the message.
+ * @group_handle: Handle identifying the associated queue group. Only valid
+ *                when @flags contains KBASE_KTRACE_FLAG_CSF_GROUP.
+ * @csg_nr:       Number/index of the associated queue group's command stream
+ *                group to which it is mapped, or negative if none associated.
+ *                Only valid when @flags contains KBASE_KTRACE_FLAG_CSF_GROUP.
+ * @slot_prio:    The priority of the slot for the associated group, if it was
+ *                scheduled. Hence, only valid when @csg_nr >=0 and @flags
+ *                contains KBASE_KTRACE_FLAG_CSF_GROUP.
+ * @csi_index:    ID of the associated queue's Command Stream HW interface.
+ *                Only valid when @flags contains KBASE_KTRACE_FLAG_CSF_QUEUE.
+ */
+struct kbase_ktrace_backend {
+	/* Place 64 and 32-bit members together */
+	/* Pack smaller members together */
+	kbase_ktrace_code_t code;
+	kbase_ktrace_flag_t flags;
+	u8 group_handle;
+	s8 csg_nr;
+	u8 slot_prio;
+	s8 csi_index;
+};
+
+#endif /* KBASE_KTRACE_TARGET_RBUF */
+#endif /* _KBASE_DEBUG_KTRACE_DEFS_CSF_H_ */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h
index 55b66adff7c7..ea8e01a87f3f 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_defs_jm.h
@@ -23,18 +23,39 @@
 #ifndef _KBASE_DEBUG_KTRACE_DEFS_JM_H_
 #define _KBASE_DEBUG_KTRACE_DEFS_JM_H_
 
+#if KBASE_KTRACE_TARGET_RBUF
 /**
  * DOC: KTrace version history, JM variant
+ *
  * 1.0:
- * - Original version (implicit, header did not carry version information)
+ * Original version (implicit, header did not carry version information).
+ *
  * 2.0:
- * - Introduced version information into the header
- * - some changes of parameter names in header
- * - trace now uses all 64-bits of info_val
- * - Non-JM specific parts moved to using info_val instead of refcount/gpu_addr
+ * Introduced version information into the header.
+ *
+ * Some changes of parameter names in header.
+ *
+ * Trace now uses all 64-bits of info_val.
+ *
+ * Non-JM specific parts moved to using info_val instead of refcount/gpu_addr.
+ *
+ * 2.1:
+ * kctx field is no longer a pointer, and is now an ID of the format %d_%u as
+ * used by kctx directories in mali debugfs entries: (tgid creating the kctx),
+ * (unique kctx id).
+ *
+ * ftrace backend now outputs kctx field (as %d_%u format).
+ *
  */
 #define KBASE_KTRACE_VERSION_MAJOR 2
-#define KBASE_KTRACE_VERSION_MINOR 0
+#define KBASE_KTRACE_VERSION_MINOR 1
+#endif /* KBASE_KTRACE_TARGET_RBUF */
+
+/*
+ * Note: mali_kbase_debug_ktrace_jm.h needs these value even if the RBUF target
+ * is disabled (they get discarded with CSTD_UNUSED(), but they're still
+ * referenced)
+ */
 
 /* indicates if the trace message has a valid refcount member */
 #define KBASE_KTRACE_FLAG_JM_REFCOUNT (((kbase_ktrace_flag_t)1) << 0)
@@ -43,6 +64,11 @@
 /* indicates if the trace message has valid atom related info. */
 #define KBASE_KTRACE_FLAG_JM_ATOM     (((kbase_ktrace_flag_t)1) << 2)
 
+#if KBASE_KTRACE_TARGET_RBUF
+/* Collect all the flags together for debug checking */
+#define KBASE_KTRACE_FLAG_BACKEND_ALL \
+		(KBASE_KTRACE_FLAG_JM_REFCOUNT | KBASE_KTRACE_FLAG_JM_JOBSLOT \
+		| KBASE_KTRACE_FLAG_JM_ATOM)
 
 /**
  * struct kbase_ktrace_backend - backend specific part of a trace message
@@ -71,5 +97,6 @@ struct kbase_ktrace_backend {
 	u8 jobslot;
 	u8 refcount;
 };
+#endif /* KBASE_KTRACE_TARGET_RBUF */
 
 #endif /* _KBASE_DEBUG_KTRACE_DEFS_JM_H_ */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c
index e651a09fba4d..1b821281f09f 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.c
@@ -106,6 +106,8 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 	/* Clamp refcount */
 	trace_msg->backend.refcount = MIN((unsigned int)refcount, 0xFF);
 
+	WARN_ON((trace_msg->backend.flags & ~KBASE_KTRACE_FLAG_ALL));
+
 	/* Done */
 	spin_unlock_irqrestore(&kbdev->ktrace.lock, irqflags);
 }
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h
index cf3dc1e274e9..adfcb1aa556e 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_ktrace_jm.h
@@ -47,89 +47,24 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 		kbase_ktrace_flag_t flags, int refcount, int jobslot,
 		u64 info_val);
 
-#define KBASE_KTRACE_RBUF_ADD_JM_SLOT(kbdev, code, kctx, katom, gpu_addr, \
-		jobslot) \
-	kbasep_ktrace_add_jm(kbdev, KBASE_KTRACE_CODE(code), kctx, katom, \
-			gpu_addr, KBASE_KTRACE_FLAG_JM_JOBSLOT, 0, jobslot, 0)
-
-#define KBASE_KTRACE_RBUF_ADD_JM_SLOT_INFO(kbdev, code, kctx, katom, gpu_addr, \
-		jobslot, info_val) \
+#define KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, gpu_addr, flags, \
+		refcount, jobslot, info_val) \
 	kbasep_ktrace_add_jm(kbdev, KBASE_KTRACE_CODE(code), kctx, katom, \
-			gpu_addr, KBASE_KTRACE_FLAG_JM_JOBSLOT, 0, jobslot, \
-			info_val)
-
-#define KBASE_KTRACE_RBUF_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, gpu_addr, \
-		refcount) \
-	kbasep_ktrace_add_jm(kbdev, KBASE_KTRACE_CODE(code), kctx, katom, \
-			gpu_addr, KBASE_KTRACE_FLAG_JM_REFCOUNT, refcount, 0, 0)
-#define KBASE_KTRACE_RBUF_ADD_JM_REFCOUNT_INFO(kbdev, code, kctx, katom, \
-		gpu_addr, refcount, info_val) \
-	kbasep_ktrace_add_jm(kbdev, KBASE_KTRACE_CODE(code), kctx, katom, \
-			gpu_addr, KBASE_KTRACE_FLAG_JM_REFCOUNT, refcount, 0, \
-			info_val)
-
-#define KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, gpu_addr, info_val) \
-	kbasep_ktrace_add_jm(kbdev, KBASE_KTRACE_CODE(code), kctx, katom, \
-			gpu_addr, 0, 0, 0, info_val)
+			gpu_addr, flags, refcount, jobslot, info_val)
 
 #else /* KBASE_KTRACE_TARGET_RBUF */
-#define KBASE_KTRACE_RBUF_ADD_JM_SLOT(kbdev, code, kctx, katom, gpu_addr, \
-		jobslot) \
-	do {\
-		CSTD_UNUSED(kbdev);\
-		CSTD_NOP(code);\
-		CSTD_UNUSED(kctx);\
-		CSTD_UNUSED(katom);\
-		CSTD_UNUSED(gpu_addr);\
-		CSTD_UNUSED(jobslot);\
-		CSTD_NOP(0);\
-	} while (0)
 
-#define KBASE_KTRACE_RBUF_ADD_JM_SLOT_INFO(kbdev, code, kctx, katom, gpu_addr, \
-		jobslot, info_val) \
-	do {\
-		CSTD_UNUSED(kbdev);\
-		CSTD_NOP(code);\
-		CSTD_UNUSED(kctx);\
-		CSTD_UNUSED(katom);\
-		CSTD_UNUSED(gpu_addr);\
-		CSTD_UNUSED(jobslot);\
-		CSTD_UNUSED(info_val);\
-		CSTD_NOP(0);\
-	} while (0)
-
-#define KBASE_KTRACE_RBUF_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, gpu_addr, \
-		refcount) \
+#define KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, gpu_addr, flags, \
+		refcount, jobslot, info_val) \
 	do {\
 		CSTD_UNUSED(kbdev);\
 		CSTD_NOP(code);\
 		CSTD_UNUSED(kctx);\
 		CSTD_UNUSED(katom);\
 		CSTD_UNUSED(gpu_addr);\
+		CSTD_UNUSED(flags);\
 		CSTD_UNUSED(refcount);\
-		CSTD_NOP(0);\
-	} while (0)
-
-#define KBASE_KTRACE_RBUF_ADD_JM_REFCOUNT_INFO(kbdev, code, kctx, katom, \
-		gpu_addr, refcount, info_val) \
-	do {\
-		CSTD_UNUSED(kbdev);\
-		CSTD_NOP(code);\
-		CSTD_UNUSED(kctx);\
-		CSTD_UNUSED(katom);\
-		CSTD_UNUSED(gpu_addr);\
-		CSTD_UNUSED(info_val);\
-		CSTD_NOP(0);\
-	} while (0)
-
-#define KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, gpu_addr, \
-		info_val)\
-	do {\
-		CSTD_UNUSED(kbdev);\
-		CSTD_NOP(code);\
-		CSTD_UNUSED(kctx);\
-		CSTD_UNUSED(katom);\
-		CSTD_UNUSED(gpu_addr);\
+		CSTD_UNUSED(jobslot);\
 		CSTD_UNUSED(info_val);\
 		CSTD_NOP(0);\
 	} while (0)
@@ -137,27 +72,30 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 
 /*
  * KTrace target for Linux's ftrace
+ *
+ * Note: the header file(s) that define the trace_mali_<...> tracepoints are
+ * included by the parent header file
  */
 #if KBASE_KTRACE_TARGET_FTRACE
 #define KBASE_KTRACE_FTRACE_ADD_JM_SLOT(kbdev, code, kctx, katom, gpu_addr, \
 		jobslot) \
-	trace_mali_##code(jobslot, 0)
+	trace_mali_##code(kctx, jobslot, 0)
 
 #define KBASE_KTRACE_FTRACE_ADD_JM_SLOT_INFO(kbdev, code, kctx, katom, \
 		gpu_addr, jobslot, info_val) \
-	trace_mali_##code(jobslot, info_val)
+	trace_mali_##code(kctx, jobslot, info_val)
 
 #define KBASE_KTRACE_FTRACE_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, \
 		gpu_addr, refcount) \
-	trace_mali_##code(refcount, 0)
+	trace_mali_##code(kctx, refcount, 0)
 
 #define KBASE_KTRACE_FTRACE_ADD_JM_REFCOUNT_INFO(kbdev, code, kctx, katom, \
 		gpu_addr, refcount, info_val) \
-	trace_mali_##code(refcount, info_val)
+	trace_mali_##code(kctx, refcount, info_val)
 
 #define KBASE_KTRACE_FTRACE_ADD_JM(kbdev, code, kctx, katom, gpu_addr, \
 		info_val) \
-	trace_mali_##code(gpu_addr, info_val)
+	trace_mali_##code(kctx, gpu_addr, info_val)
 #else /* KBASE_KTRACE_TARGET_FTRACE */
 #define KBASE_KTRACE_FTRACE_ADD_JM_SLOT(kbdev, code, kctx, katom, gpu_addr, \
 		jobslot) \
@@ -247,7 +185,9 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 		/* capture values that could come from non-pure function calls */ \
 		u64 __gpu_addr = gpu_addr; \
 		int __jobslot = jobslot; \
-		KBASE_KTRACE_RBUF_ADD_JM_SLOT(kbdev, code, kctx, katom, __gpu_addr, __jobslot); \
+		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr, \
+				KBASE_KTRACE_FLAG_JM_JOBSLOT, 0, __jobslot, \
+				0); \
 		KBASE_KTRACE_FTRACE_ADD_JM_SLOT(kbdev, code, kctx, katom, __gpu_addr, __jobslot); \
 	} while (0)
 
@@ -275,7 +215,9 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 		u64 __gpu_addr = gpu_addr; \
 		int __jobslot = jobslot; \
 		u64 __info_val = info_val; \
-		KBASE_KTRACE_RBUF_ADD_JM_SLOT_INFO(kbdev, code, kctx, katom, __gpu_addr, __jobslot, __info_val); \
+		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr, \
+				KBASE_KTRACE_FLAG_JM_JOBSLOT, 0, __jobslot, \
+				__info_val); \
 		KBASE_KTRACE_FTRACE_ADD_JM_SLOT_INFO(kbdev, code, kctx, katom, __gpu_addr, __jobslot, __info_val); \
 	} while (0)
 
@@ -301,7 +243,9 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 		/* capture values that could come from non-pure function calls */ \
 		u64 __gpu_addr = gpu_addr; \
 		int __refcount = refcount; \
-		KBASE_KTRACE_RBUF_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, __gpu_addr, __refcount); \
+		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr, \
+				KBASE_KTRACE_FLAG_JM_REFCOUNT, __refcount, 0, \
+				0u); \
 		KBASE_KTRACE_FTRACE_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, __gpu_addr, __refcount); \
 	} while (0)
 
@@ -330,7 +274,9 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 		u64 __gpu_addr = gpu_addr; \
 		int __refcount = refcount; \
 		u64 __info_val = info_val; \
-		KBASE_KTRACE_RBUF_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, __gpu_addr, __refcount, __info_val); \
+		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr, \
+				KBASE_KTRACE_FLAG_JM_REFCOUNT, __refcount, 0, \
+				__info_val); \
 		KBASE_KTRACE_FTRACE_ADD_JM_REFCOUNT(kbdev, code, kctx, katom, __gpu_addr, __refcount, __info_val); \
 	} while (0)
 
@@ -355,7 +301,8 @@ void kbasep_ktrace_add_jm(struct kbase_device *kbdev,
 		/* capture values that could come from non-pure function calls */ \
 		u64 __gpu_addr = gpu_addr; \
 		u64 __info_val = info_val; \
-		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr, __info_val); \
+		KBASE_KTRACE_RBUF_ADD_JM(kbdev, code, kctx, katom, __gpu_addr, \
+				0u, 0, 0, __info_val); \
 		KBASE_KTRACE_FTRACE_ADD_JM(kbdev, code, kctx, katom, __gpu_addr, __info_val); \
 	} while (0)
 
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h
new file mode 100644
index 000000000000..d103e5766456
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_csf.h
@@ -0,0 +1,147 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * NOTE: This must **only** be included through mali_linux_trace.h,
+ * otherwise it will fail to setup tracepoints correctly
+ */
+
+#if !defined(_KBASE_DEBUG_LINUX_KTRACE_CSF_H_) || defined(TRACE_HEADER_MULTI_READ)
+#define _KBASE_DEBUG_LINUX_KTRACE_CSF_H_
+
+/*
+ * Generic CSF events - using the common DEFINE_MALI_ADD_EVENT
+ */
+DEFINE_MALI_ADD_EVENT(EVICT_CTX_SLOTS);
+DEFINE_MALI_ADD_EVENT(FIRMWARE_BOOT);
+DEFINE_MALI_ADD_EVENT(FIRMWARE_REBOOT);
+DEFINE_MALI_ADD_EVENT(SCHEDULER_TOCK);
+DEFINE_MALI_ADD_EVENT(SCHEDULER_TICK);
+DEFINE_MALI_ADD_EVENT(SCHEDULER_RESET);
+DEFINE_MALI_ADD_EVENT(SCHEDULER_EXIT_PROTM);
+DEFINE_MALI_ADD_EVENT(SYNC_UPDATE_EVENT);
+
+DECLARE_EVENT_CLASS(mali_csf_grp_q_template,
+	TP_PROTO(struct kbase_device *kbdev, struct kbase_queue_group *group,
+			struct kbase_queue *queue, u64 info_val),
+	TP_ARGS(kbdev, group, queue, info_val),
+	TP_STRUCT__entry(
+		__field(u64, info_val)
+		__field(pid_t, kctx_tgid)
+		__field(u32, kctx_id)
+		__field(u8, group_handle)
+		__field(s8, csg_nr)
+		__field(u8, slot_prio)
+		__field(s8, csi_index)
+	),
+	TP_fast_assign(
+		{
+			struct kbase_context *kctx = NULL;
+
+			__entry->info_val = info_val;
+			/* Note: if required in future, we could record some
+			 * flags in __entry about whether the group/queue parts
+			 * are valid, and add that to the trace message e.g.
+			 * by using __print_flags()/__print_symbolic()
+			 */
+			if (queue) {
+				/* Note: kctx overridden by group->kctx later if group is valid */
+				kctx = queue->kctx;
+				__entry->csi_index = queue->csi_index;
+			} else {
+				__entry->csi_index = -1;
+			}
+
+			if (group) {
+				kctx = group->kctx;
+				__entry->group_handle = group->handle;
+				__entry->csg_nr = group->csg_nr;
+				if (group->csg_nr >= 0)
+					__entry->slot_prio = kbdev->csf.scheduler.csg_slots[group->csg_nr].priority;
+				else
+					__entry->slot_prio = 0u;
+			} else {
+				__entry->group_handle = 0u;
+				__entry->csg_nr = -1;
+				__entry->slot_prio = 0u;
+			}
+			__entry->kctx_id = (kctx) ? kctx->id : 0u;
+			__entry->kctx_tgid = (kctx) ? kctx->tgid : 0;
+		}
+
+	),
+	TP_printk("kctx=%d_%u group=%u slot=%d prio=%u csi=%d info=0x%llx",
+			__entry->kctx_tgid, __entry->kctx_id,
+			__entry->group_handle, __entry->csg_nr,
+			__entry->slot_prio, __entry->csi_index,
+			__entry->info_val)
+);
+
+/*
+ * Group events
+ */
+#define DEFINE_MALI_CSF_GRP_EVENT(name) \
+	DEFINE_EVENT_PRINT(mali_csf_grp_q_template, mali_##name, \
+	TP_PROTO(struct kbase_device *kbdev, struct kbase_queue_group *group, \
+			struct kbase_queue *queue, u64 info_val), \
+	TP_ARGS(kbdev, group, queue, info_val), \
+	TP_printk("kctx=%d_%u group=%u slot=%d prio=%u info=0x%llx", \
+		__entry->kctx_tgid, __entry->kctx_id, __entry->group_handle, \
+		__entry->csg_nr, __entry->slot_prio, __entry->info_val))
+
+DEFINE_MALI_CSF_GRP_EVENT(CSG_SLOT_START);
+DEFINE_MALI_CSF_GRP_EVENT(CSG_SLOT_STOP);
+DEFINE_MALI_CSF_GRP_EVENT(CSG_SLOT_STARTED);
+DEFINE_MALI_CSF_GRP_EVENT(CSG_SLOT_STOPPED);
+DEFINE_MALI_CSF_GRP_EVENT(CSG_SLOT_CLEANED);
+DEFINE_MALI_CSF_GRP_EVENT(CSG_PRIO_UPDATE);
+DEFINE_MALI_CSF_GRP_EVENT(CSG_SYNC_UPDATE_INTERRUPT);
+DEFINE_MALI_CSF_GRP_EVENT(CSG_IDLE_INTERRUPT);
+DEFINE_MALI_CSF_GRP_EVENT(GROUP_SYNC_UPDATE_DONE);
+DEFINE_MALI_CSF_GRP_EVENT(GROUP_DESCHEDULE);
+DEFINE_MALI_CSF_GRP_EVENT(GROUP_SCHEDULE);
+DEFINE_MALI_CSF_GRP_EVENT(GROUP_EVICT_SCHED);
+DEFINE_MALI_CSF_GRP_EVENT(SCHEDULER_ENTER_PROTM);
+DEFINE_MALI_CSF_GRP_EVENT(SCHEDULER_TOP_GRP);
+
+#undef DEFINE_MALI_CSF_GRP_EVENT
+
+/*
+ * Group + Queue events
+ */
+#define DEFINE_MALI_CSF_GRP_Q_EVENT(name)  \
+	DEFINE_EVENT(mali_csf_grp_q_template, mali_##name, \
+	TP_PROTO(struct kbase_device *kbdev, struct kbase_queue_group *group, \
+			struct kbase_queue *queue, u64 info_val), \
+	TP_ARGS(kbdev, group, queue, info_val))
+
+DEFINE_MALI_CSF_GRP_Q_EVENT(CSI_START);
+DEFINE_MALI_CSF_GRP_Q_EVENT(CSI_STOP);
+DEFINE_MALI_CSF_GRP_Q_EVENT(CSI_STOP_REQUESTED);
+DEFINE_MALI_CSF_GRP_Q_EVENT(CSI_FAULT_INTERRUPT);
+DEFINE_MALI_CSF_GRP_Q_EVENT(CSI_TILER_OOM_INTERRUPT);
+DEFINE_MALI_CSF_GRP_Q_EVENT(QUEUE_START);
+DEFINE_MALI_CSF_GRP_Q_EVENT(QUEUE_STOP);
+
+#undef DEFINE_MALI_CSF_GRP_Q_EVENT
+
+#endif /* !defined(_KBASE_DEBUG_LINUX_KTRACE_CSF_H_) || defined(TRACE_HEADER_MULTI_READ) */
diff --git a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h
index d964e5ade3b8..037b1edecd8e 100644
--- a/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h
+++ b/drivers/gpu/arm/bifrost/debug/backend/mali_kbase_debug_linux_ktrace_jm.h
@@ -29,23 +29,28 @@
 #define _KBASE_DEBUG_LINUX_KTRACE_JM_H_
 
 DECLARE_EVENT_CLASS(mali_jm_slot_template,
-	TP_PROTO(int jobslot, u64 info_val),
-	TP_ARGS(jobslot, info_val),
+	TP_PROTO(struct kbase_context *kctx, int jobslot, u64 info_val),
+	TP_ARGS(kctx, jobslot, info_val),
 	TP_STRUCT__entry(
+		__field(pid_t, kctx_tgid)
+		__field(u32, kctx_id)
 		__field(unsigned int, jobslot)
 		__field(u64, info_val)
 	),
 	TP_fast_assign(
+		__entry->kctx_id = (kctx) ? kctx->id : 0u;
+		__entry->kctx_tgid = (kctx) ? kctx->tgid : 0;
 		__entry->jobslot = jobslot;
 		__entry->info_val = info_val;
 	),
-	TP_printk("jobslot=%u info=0x%llx", __entry->jobslot, __entry->info_val)
+	TP_printk("kctx=%d_%u jobslot=%u info=0x%llx", __entry->kctx_tgid,
+			__entry->kctx_id, __entry->jobslot, __entry->info_val)
 );
 
 #define DEFINE_MALI_JM_SLOT_EVENT(name) \
 DEFINE_EVENT(mali_jm_slot_template, mali_##name, \
-	TP_PROTO(int jobslot, u64 info_val), \
-	TP_ARGS(jobslot, info_val))
+	TP_PROTO(struct kbase_context *kctx, int jobslot, u64 info_val), \
+	TP_ARGS(kctx, jobslot, info_val))
 DEFINE_MALI_JM_SLOT_EVENT(JM_SUBMIT);
 DEFINE_MALI_JM_SLOT_EVENT(JM_JOB_DONE);
 DEFINE_MALI_JM_SLOT_EVENT(JM_UPDATE_HEAD);
@@ -75,23 +80,28 @@ DEFINE_MALI_JM_SLOT_EVENT(JS_POLICY_DEQUEUE_JOB_IRQ);
 #undef DEFINE_MALI_JM_SLOT_EVENT
 
 DECLARE_EVENT_CLASS(mali_jm_refcount_template,
-	TP_PROTO(int refcount, u64 info_val),
-	TP_ARGS(refcount, info_val),
+	TP_PROTO(struct kbase_context *kctx, int refcount, u64 info_val),
+	TP_ARGS(kctx, refcount, info_val),
 	TP_STRUCT__entry(
+		__field(pid_t, kctx_tgid)
+		__field(u32, kctx_id)
 		__field(unsigned int, refcount)
 		__field(u64, info_val)
 	),
 	TP_fast_assign(
+		__entry->kctx_id = (kctx) ? kctx->id : 0u;
+		__entry->kctx_tgid = (kctx) ? kctx->tgid : 0;
 		__entry->refcount = refcount;
 		__entry->info_val = info_val;
 	),
-	TP_printk("refcount=%u info=0x%llx", __entry->refcount, __entry->info_val)
+	TP_printk("kctx=%d_%u refcount=%u info=0x%llx", __entry->kctx_tgid,
+			__entry->kctx_id, __entry->refcount, __entry->info_val)
 );
 
 #define DEFINE_MALI_JM_REFCOUNT_EVENT(name) \
 DEFINE_EVENT(mali_jm_refcount_template, mali_##name, \
-	TP_PROTO(int refcount, u64 info_val), \
-	TP_ARGS(refcount, info_val))
+	TP_PROTO(struct kbase_context *kctx, int refcount, u64 info_val), \
+	TP_ARGS(kctx, refcount, info_val))
 DEFINE_MALI_JM_REFCOUNT_EVENT(JS_ADD_JOB);
 DEFINE_MALI_JM_REFCOUNT_EVENT(JS_REMOVE_JOB);
 DEFINE_MALI_JM_REFCOUNT_EVENT(JS_TRY_SCHEDULE_HEAD_CTX);
@@ -106,23 +116,28 @@ DEFINE_MALI_JM_REFCOUNT_EVENT(JS_POLICY_FOREACH_CTX_JOBS);
 #undef DEFINE_MALI_JM_REFCOUNT_EVENT
 
 DECLARE_EVENT_CLASS(mali_jm_add_template,
-	TP_PROTO(u64 gpu_addr, u64 info_val),
-	TP_ARGS(gpu_addr, info_val),
+	TP_PROTO(struct kbase_context *kctx, u64 gpu_addr, u64 info_val),
+	TP_ARGS(kctx, gpu_addr, info_val),
 	TP_STRUCT__entry(
+		__field(pid_t, kctx_tgid)
+		__field(u32, kctx_id)
 		__field(u64, gpu_addr)
 		__field(u64, info_val)
 	),
 	TP_fast_assign(
+		__entry->kctx_id = (kctx) ? kctx->id : 0u;
+		__entry->kctx_tgid = (kctx) ? kctx->tgid : 0;
 		__entry->gpu_addr = gpu_addr;
 		__entry->info_val = info_val;
 	),
-	TP_printk("gpu_addr=0x%llx info=0x%llx", __entry->gpu_addr, __entry->info_val)
+	TP_printk("kctx=%d_%u gpu_addr=0x%llx info=0x%llx", __entry->kctx_tgid,
+			__entry->kctx_id, __entry->gpu_addr, __entry->info_val)
 );
 
 #define DEFINE_MALI_JM_ADD_EVENT(name) \
 DEFINE_EVENT(mali_jm_add_template, mali_##name, \
-	TP_PROTO(u64 gpu_addr, u64 info_val), \
-	TP_ARGS(gpu_addr, info_val))
+	TP_PROTO(struct kbase_context *kctx, u64 gpu_addr, u64 info_val), \
+	TP_ARGS(kctx, gpu_addr, info_val))
 DEFINE_MALI_JM_ADD_EVENT(JD_DONE_WORKER);
 DEFINE_MALI_JM_ADD_EVENT(JD_DONE_WORKER_END);
 DEFINE_MALI_JM_ADD_EVENT(JD_CANCEL_WORKER);
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c
index 6322abb11ac8..a13c0ba20c94 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.c
@@ -27,11 +27,6 @@ int kbase_ktrace_init(struct kbase_device *kbdev)
 #if KBASE_KTRACE_TARGET_RBUF
 	struct kbase_ktrace_msg *rbuf;
 
-	/* See also documentation of enum kbase_ktrace_code */
-	compiletime_assert(sizeof(kbase_ktrace_code_t) == sizeof(unsigned long long) ||
-			KBASE_KTRACE_CODE_COUNT <= (1ull << (sizeof(kbase_ktrace_code_t) * BITS_PER_BYTE)),
-			"kbase_ktrace_code_t not wide enough for KBASE_KTRACE_CODE_COUNT");
-
 	rbuf = kmalloc_array(KBASE_KTRACE_SIZE, sizeof(*rbuf), GFP_KERNEL);
 
 	if (!rbuf)
@@ -91,15 +86,24 @@ static void kbasep_ktrace_format_msg(struct kbase_ktrace_msg *trace_msg,
 
 	/* Initial part of message:
 	 *
-	 * secs,thread_id,cpu,code,kctx,
+	 * secs,thread_id,cpu,code,
 	 */
 	written += MAX(snprintf(buffer + written, MAX(sz - written, 0),
-			"%d.%.6d,%d,%d,%s,%p,",
+			"%d.%.6d,%d,%d,%s,",
 			(int)trace_msg->timestamp.tv_sec,
 			(int)(trace_msg->timestamp.tv_nsec / 1000),
 			trace_msg->thread_id, trace_msg->cpu,
-			kbasep_ktrace_code_string[trace_msg->backend.code],
-			trace_msg->kctx), 0);
+			kbasep_ktrace_code_string[trace_msg->backend.code]), 0);
+
+	/* kctx part: */
+	if (trace_msg->kctx_tgid) {
+		written += MAX(snprintf(buffer + written, MAX(sz - written, 0),
+				"%d_%u",
+				trace_msg->kctx_tgid, trace_msg->kctx_id), 0);
+	}
+	/* Trailing comma */
+	written += MAX(snprintf(buffer + written, MAX(sz - written, 0),
+			","), 0);
 
 	/* Backend parts */
 	kbasep_ktrace_backend_format_msg(trace_msg, buffer, sz,
@@ -156,8 +160,16 @@ void kbasep_ktrace_msg_init(struct kbase_ktrace *ktrace,
 
 	ktime_get_real_ts64(&trace_msg->timestamp);
 
-	trace_msg->kctx = kctx;
-
+	/* No need to store a flag about whether there was a kctx, tgid==0 is
+	 * sufficient
+	 */
+	if (kctx) {
+		trace_msg->kctx_tgid = kctx->tgid;
+		trace_msg->kctx_id = kctx->id;
+	} else {
+		trace_msg->kctx_tgid = 0;
+		trace_msg->kctx_id = 0;
+	}
 	trace_msg->info_val = info_val;
 	trace_msg->backend.code = code;
 	trace_msg->backend.flags = flags;
@@ -170,6 +182,8 @@ void kbasep_ktrace_add(struct kbase_device *kbdev, enum kbase_ktrace_code code,
 	unsigned long irqflags;
 	struct kbase_ktrace_msg *trace_msg;
 
+	WARN_ON((flags & ~KBASE_KTRACE_FLAG_COMMON_ALL));
+
 	spin_lock_irqsave(&kbdev->ktrace.lock, irqflags);
 
 	/* Reserve and update indices */
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.h b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.h
index 9b1905cd32b7..e4e2e8c35001 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.h
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace.h
@@ -37,7 +37,15 @@
 #ifndef _KBASE_DEBUG_KTRACE_H_
 #define _KBASE_DEBUG_KTRACE_H_
 
+#if KBASE_KTRACE_TARGET_FTRACE
+#include "mali_linux_trace.h"
+#endif
+
+#if MALI_USE_CSF
+#include "debug/backend/mali_kbase_debug_ktrace_csf.h"
+#else
 #include "debug/backend/mali_kbase_debug_ktrace_jm.h"
+#endif
 
 /**
  * kbase_ktrace_init - initialize kbase ktrace.
@@ -140,10 +148,9 @@ void kbasep_ktrace_dump(struct kbase_device *kbdev);
  * KTrace target for Linux's ftrace
  */
 #if KBASE_KTRACE_TARGET_FTRACE
-#include "mali_linux_trace.h"
 
 #define KBASE_KTRACE_FTRACE_ADD(kbdev, code, kctx, info_val) \
-	trace_mali_##code(info_val)
+	trace_mali_##code(kctx, info_val)
 
 #else /* KBASE_KTRACE_TARGET_FTRACE */
 #define KBASE_KTRACE_FTRACE_ADD(kbdev, code, kctx, info_val) \
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h
index 364ed6091e6e..b50bceee4244 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_codes.h
@@ -30,6 +30,9 @@
  * The purpose of this header file is just to contain a list of trace code
  * identifiers
  *
+ * When updating this file, also remember to update
+ * mali_kbase_debug_linux_ktrace.h
+ *
  * Each identifier is wrapped in a macro, so that its string form and enum form
  * can be created
  *
@@ -144,7 +147,11 @@ int dummy_array[] = {
 	KBASE_KTRACE_CODE_MAKE_CODE(SCHED_RELEASE_CTX),
 
 
+#if MALI_USE_CSF
+#include "debug/backend/mali_kbase_debug_ktrace_codes_csf.h"
+#else
 #include "debug/backend/mali_kbase_debug_ktrace_codes_jm.h"
+#endif
 	/*
 	 * Unused code just to make it easier to not have a comma at the end.
 	 * All other codes MUST come before this
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_defs.h b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_defs.h
index eda31c7afbda..c680feb86387 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_defs.h
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_ktrace_defs.h
@@ -58,6 +58,16 @@
 #define KBASE_KTRACE_TARGET_RBUF 0
 #endif /* KBASE_KTRACE_ENABLE */
 
+/*
+ * Note: Some backends define flags in this type even if the RBUF target is
+ * disabled (they get discarded with CSTD_UNUSED(), but they're still
+ * referenced)
+ */
+typedef u8 kbase_ktrace_flag_t;
+
+#if KBASE_KTRACE_TARGET_RBUF
+typedef u8 kbase_ktrace_code_t;
+
 /*
  * NOTE: KBASE_KTRACE_VERSION_MAJOR, KBASE_KTRACE_VERSION_MINOR are kept in
  * the backend, since updates can be made to one backend in a way that doesn't
@@ -67,10 +77,6 @@
  * updated.
  */
 
-#if KBASE_KTRACE_TARGET_RBUF
-typedef u8 kbase_ktrace_flag_t;
-typedef u8 kbase_ktrace_code_t;
-
 /*
  * struct kbase_ktrace_backend - backend specific part of a trace message
  *
@@ -78,9 +84,15 @@ typedef u8 kbase_ktrace_code_t;
  * a kbase_ktrace_flag_t 'flags' member
  */
 struct kbase_ktrace_backend;
+#endif /* KBASE_KTRACE_TARGET_RBUF */
 
+#if MALI_USE_CSF
+#include "debug/backend/mali_kbase_debug_ktrace_defs_csf.h"
+#else
 #include "debug/backend/mali_kbase_debug_ktrace_defs_jm.h"
+#endif
 
+#if KBASE_KTRACE_TARGET_RBUF
 /* Indicates if the trace message has backend related info.
  *
  * If not set, consider the &kbase_ktrace_backend part of a &kbase_ktrace_msg
@@ -90,6 +102,13 @@ struct kbase_ktrace_backend;
  */
 #define KBASE_KTRACE_FLAG_BACKEND     (((kbase_ktrace_flag_t)1) << 7)
 
+/* Collect all the common flags together for debug checking */
+#define KBASE_KTRACE_FLAG_COMMON_ALL \
+		(KBASE_KTRACE_FLAG_BACKEND)
+
+#define KBASE_KTRACE_FLAG_ALL \
+		(KBASE_KTRACE_FLAG_COMMON_ALL | KBASE_KTRACE_FLAG_BACKEND_ALL)
+
 #define KBASE_KTRACE_SHIFT 8 /* 256 entries */
 #define KBASE_KTRACE_SIZE (1 << KBASE_KTRACE_SHIFT)
 #define KBASE_KTRACE_MASK ((1 << KBASE_KTRACE_SHIFT)-1)
@@ -121,11 +140,10 @@ enum kbase_ktrace_code {
  *             added.
  * @cpu:       indicates which CPU the @thread_id was scheduled on when the
  *             trace message was added.
- * @kctx:      Pointer to the kbase context for which the trace message was
- *             added. Will be NULL for certain trace messages associated with
- *             the &kbase_device itself, such as power management events.
- *             Will point to the appropriate context corresponding to
- *             backend-specific events.
+ * @kctx_tgid: Thread group ID of the &kbase_context associated with the
+ *             message, or 0 if none associated.
+ * @kctx_id:   Unique identifier of the &kbase_context associated with the
+ *             message. Only valid if @kctx_tgid != 0.
  * @info_val:  value specific to the type of event being traced. Refer to the
  *             specific code in enum kbase_ktrace_code
  * @backend:   backend-specific trace information. All backends must implement
@@ -135,7 +153,8 @@ struct kbase_ktrace_msg {
 	struct timespec64 timestamp;
 	u32 thread_id;
 	u32 cpu;
-	void *kctx;
+	pid_t kctx_tgid;
+	u32 kctx_id;
 	u64 info_val;
 
 	struct kbase_ktrace_backend backend;
@@ -148,5 +167,17 @@ struct kbase_ktrace {
 	struct kbase_ktrace_msg *rbuf;
 };
 
+
+static inline void kbase_ktrace_compiletime_asserts(void)
+{
+	/* See also documentation of enum kbase_ktrace_code */
+	compiletime_assert(sizeof(kbase_ktrace_code_t) == sizeof(unsigned long long) ||
+			KBASE_KTRACE_CODE_COUNT <= (1ull << (sizeof(kbase_ktrace_code_t) * BITS_PER_BYTE)),
+			"kbase_ktrace_code_t not wide enough for KBASE_KTRACE_CODE_COUNT");
+	compiletime_assert((KBASE_KTRACE_FLAG_BACKEND_ALL & KBASE_KTRACE_FLAG_COMMON_ALL) == 0,
+			"KTrace backend flags intersect with KTrace common flags");
+
+}
+
 #endif /* KBASE_KTRACE_TARGET_RBUF */
 #endif /* _KBASE_DEBUG_KTRACE_DEFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h
index 18e4f7c4f069..27f687faf072 100644
--- a/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h
+++ b/drivers/gpu/arm/bifrost/debug/mali_kbase_debug_linux_ktrace.h
@@ -31,21 +31,29 @@
 #if KBASE_KTRACE_TARGET_FTRACE
 
 DECLARE_EVENT_CLASS(mali_add_template,
-	TP_PROTO(u64 info_val),
-	TP_ARGS(info_val),
+	TP_PROTO(struct kbase_context *kctx, u64 info_val),
+	TP_ARGS(kctx, info_val),
 	TP_STRUCT__entry(
+		__field(pid_t, kctx_tgid)
+		__field(u32, kctx_id)
 		__field(u64, info_val)
 	),
 	TP_fast_assign(
+		__entry->kctx_id = (kctx) ? kctx->id : 0u;
+		__entry->kctx_tgid = (kctx) ? kctx->tgid : 0;
 		__entry->info_val = info_val;
 	),
-	TP_printk("info=0x%llx", __entry->info_val)
+	TP_printk("kctx=%d_%u info=0x%llx", __entry->kctx_tgid,
+			__entry->kctx_id, __entry->info_val)
 );
 
+/* DEFINE_MALI_ADD_EVENT is available also to backends for backend-specific
+ * simple trace codes
+ */
 #define DEFINE_MALI_ADD_EVENT(name) \
 DEFINE_EVENT(mali_add_template, mali_##name, \
-	TP_PROTO(u64 info_val), \
-	TP_ARGS(info_val))
+	TP_PROTO(struct kbase_context *kctx, u64 info_val), \
+	TP_ARGS(kctx, info_val))
 DEFINE_MALI_ADD_EVENT(CORE_CTX_DESTROY);
 DEFINE_MALI_ADD_EVENT(CORE_CTX_HWINSTR_TERM);
 DEFINE_MALI_ADD_EVENT(CORE_GPU_IRQ);
@@ -90,9 +98,13 @@ DEFINE_MALI_ADD_EVENT(PM_WAKE_WAITERS);
 DEFINE_MALI_ADD_EVENT(SCHED_RETAIN_CTX_NOLOCK);
 DEFINE_MALI_ADD_EVENT(SCHED_RELEASE_CTX);
 
-#undef DEFINE_MALI_ADD_EVENT
-
+#if MALI_USE_CSF
+#include "mali_kbase_debug_linux_ktrace_csf.h"
+#else
 #include "mali_kbase_debug_linux_ktrace_jm.h"
+#endif
+
+#undef DEFINE_MALI_ADD_EVENT
 
 #endif /* KBASE_KTRACE_TARGET_FTRACE */
 
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c
new file mode 100644
index 000000000000..d8b3fff6a214
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_csf.c
@@ -0,0 +1,274 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "../mali_kbase_device_internal.h"
+#include "../mali_kbase_device.h"
+
+#include <mali_kbase_config_defaults.h>
+#include <mali_kbase_hwaccess_backend.h>
+#include <mali_kbase_ctx_sched.h>
+#include <mali_kbase_reset_gpu.h>
+#include <csf/mali_kbase_csf.h>
+
+#ifdef CONFIG_MALI_BIFROST_NO_MALI
+#include <mali_kbase_model_linux.h>
+#endif
+
+#include <mali_kbase.h>
+#include <backend/gpu/mali_kbase_irq_internal.h>
+#include <backend/gpu/mali_kbase_js_internal.h>
+#include <backend/gpu/mali_kbase_pm_internal.h>
+#include <backend/gpu/mali_kbase_clk_rate_trace_mgr.h>
+
+static void kbase_device_csf_firmware_term(struct kbase_device *kbdev)
+{
+	kbase_clk_rate_trace_manager_term(kbdev);
+	kbase_csf_firmware_term(kbdev);
+}
+
+static int kbase_device_csf_firmware_init(struct kbase_device *kbdev)
+{
+	int err = kbase_csf_firmware_init(kbdev);
+
+	if (!err) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbdev->pm.backend.mcu_state = KBASE_MCU_ON;
+		kbdev->csf.firmware_inited = true;
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	}
+
+	/* Post firmware init, idle condition is restored. Note this is
+	 * a deferral action step from the late init stage for CSF.
+	 */
+	kbase_pm_context_idle(kbdev);
+
+	if (!err)
+		kbase_clk_rate_trace_manager_init(kbdev);
+
+	return err;
+}
+
+/**
+ * kbase_backend_late_init - Perform any backend-specific initialization.
+ * @kbdev:	Device pointer
+ *
+ * Return: 0 on success, or an error code on failure.
+ */
+static int kbase_backend_late_init(struct kbase_device *kbdev)
+{
+	int err;
+
+	err = kbase_hwaccess_pm_init(kbdev);
+	if (err)
+		return err;
+
+	err = kbase_reset_gpu_init(kbdev);
+	if (err)
+		goto fail_reset_gpu_init;
+
+	err = kbase_hwaccess_pm_powerup(kbdev, PM_HW_ISSUES_DETECT);
+	if (err)
+		goto fail_pm_powerup;
+
+	err = kbase_backend_timer_init(kbdev);
+	if (err)
+		goto fail_timer;
+
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+#ifndef CONFIG_MALI_BIFROST_NO_MALI
+	if (kbasep_common_test_interrupt_handlers(kbdev) != 0) {
+		dev_err(kbdev->dev, "Interrupt assignment check failed.\n");
+		err = -EINVAL;
+		goto fail_interrupt_test;
+	}
+#endif /* !CONFIG_MALI_BIFROST_NO_MALI */
+#endif /* CONFIG_MALI_BIFROST_DEBUG */
+
+	/* Do the initialisation of devfreq.
+	 * Devfreq needs backend_timer_init() for completion of its
+	 * initialisation and it also needs to catch the first callback
+	 * occurrence of the runtime_suspend event for maintaining state
+	 * coherence with the backend power management, hence needs to be
+	 * placed before the kbase_pm_context_idle().
+	 */
+	err = kbase_backend_devfreq_init(kbdev);
+	if (err)
+		goto fail_devfreq_init;
+
+	/* Update gpuprops with L2_FEATURES if applicable */
+	err = kbase_gpuprops_update_l2_features(kbdev);
+	if (err)
+		goto fail_update_l2_features;
+
+	init_waitqueue_head(&kbdev->hwaccess.backend.reset_wait);
+
+	return 0;
+
+fail_update_l2_features:
+fail_devfreq_init:
+
+#ifdef CONFIG_MALI_BIFROST_DEBUG
+#ifndef CONFIG_MALI_BIFROST_NO_MALI
+fail_interrupt_test:
+#endif /* !CONFIG_MALI_BIFROST_NO_MALI */
+#endif /* CONFIG_MALI_BIFROST_DEBUG */
+
+	kbase_backend_timer_term(kbdev);
+fail_timer:
+	kbase_hwaccess_pm_halt(kbdev);
+fail_pm_powerup:
+	kbase_reset_gpu_term(kbdev);
+fail_reset_gpu_init:
+	kbase_hwaccess_pm_term(kbdev);
+
+	return err;
+}
+
+/**
+ * kbase_backend_late_term - Perform any backend-specific termination.
+ * @kbdev:	Device pointer
+ */
+static void kbase_backend_late_term(struct kbase_device *kbdev)
+{
+	kbase_backend_devfreq_term(kbdev);
+	kbase_hwaccess_pm_halt(kbdev);
+	kbase_reset_gpu_term(kbdev);
+	kbase_hwaccess_pm_term(kbdev);
+}
+
+static const struct kbase_device_init dev_init[] = {
+#ifdef CONFIG_MALI_BIFROST_NO_MALI
+	{kbase_gpu_device_create, kbase_gpu_device_destroy,
+			"Dummy model initialization failed"},
+#else
+	{assign_irqs, NULL,
+			"IRQ search failed"},
+	{registers_map, registers_unmap,
+			"Register map failed"},
+#endif
+	{power_control_init, power_control_term,
+			"Power control initialization failed"},
+	{kbase_device_io_history_init, kbase_device_io_history_term,
+			"Register access history initialization failed"},
+	{kbase_device_early_init, kbase_device_early_term,
+			"Early device initialization failed"},
+	{kbase_device_populate_max_freq, NULL,
+			"Populating max frequency failed"},
+	{kbase_device_misc_init, kbase_device_misc_term,
+			"Miscellaneous device initialization failed"},
+	{kbase_ctx_sched_init, kbase_ctx_sched_term,
+			"Context scheduler initialization failed"},
+	{kbase_mem_init, kbase_mem_term,
+			"Memory subsystem initialization failed"},
+	{kbase_csf_protected_memory_init, kbase_csf_protected_memory_term,
+			"Protected memory allocator initialization failed"},
+	{kbase_device_coherency_init, NULL,
+			"Device coherency init failed"},
+	{kbase_protected_mode_init, kbase_protected_mode_term,
+			"Protected mode subsystem initialization failed"},
+	{kbase_device_list_init, kbase_device_list_term,
+			"Device list setup failed"},
+	{kbase_device_timeline_init, kbase_device_timeline_term,
+			"Timeline stream initialization failed"},
+	{kbase_clk_rate_trace_manager_init,
+			kbase_clk_rate_trace_manager_term,
+			"Clock rate trace manager initialization failed"},
+	{kbase_device_hwcnt_backend_jm_init,
+			kbase_device_hwcnt_backend_jm_term,
+			"GPU hwcnt backend creation failed"},
+	{kbase_device_hwcnt_context_init, kbase_device_hwcnt_context_term,
+			"GPU hwcnt context initialization failed"},
+	{kbase_device_hwcnt_virtualizer_init,
+			kbase_device_hwcnt_virtualizer_term,
+			"GPU hwcnt virtualizer initialization failed"},
+	{kbase_device_vinstr_init, kbase_device_vinstr_term,
+			"Virtual instrumentation initialization failed"},
+	{kbase_backend_late_init, kbase_backend_late_term,
+			"Late backend initialization failed"},
+	{kbase_device_csf_firmware_init, kbase_device_csf_firmware_term,
+			"Firmware initialization failed"},
+#ifdef MALI_KBASE_BUILD
+	{kbase_device_debugfs_init, kbase_device_debugfs_term,
+			"DebugFS initialization failed"},
+	/* Sysfs init needs to happen before registering the device with
+	 * misc_register(), otherwise it causes a race condition between
+	 * registering the device and a uevent event being generated for
+	 * userspace, causing udev rules to run which might expect certain
+	 * sysfs attributes present. As a result of the race condition
+	 * we avoid, some Mali sysfs entries may have appeared to udev
+	 * to not exist.
+	 * For more information, see
+	 * https://www.kernel.org/doc/Documentation/driver-model/device.txt, the
+	 * paragraph that starts with "Word of warning", currently the
+	 * second-last paragraph.
+	 */
+	{kbase_sysfs_init, kbase_sysfs_term, "SysFS group creation failed"},
+	{kbase_device_misc_register, kbase_device_misc_deregister,
+			"Misc device registration failed"},
+#ifdef CONFIG_MALI_BUSLOG
+	{buslog_init, buslog_term, "Bus log client registration failed"},
+#endif
+	{kbase_gpuprops_populate_user_buffer, kbase_gpuprops_free_user_buffer,
+			"GPU property population failed"},
+#endif
+};
+
+static void kbase_device_term_partial(struct kbase_device *kbdev,
+		unsigned int i)
+{
+	while (i-- > 0) {
+		if (dev_init[i].term)
+			dev_init[i].term(kbdev);
+	}
+}
+
+void kbase_device_term(struct kbase_device *kbdev)
+{
+	kbase_device_term_partial(kbdev, ARRAY_SIZE(dev_init));
+	kbase_mem_halt(kbdev);
+}
+
+int kbase_device_init(struct kbase_device *kbdev)
+{
+	int err = 0;
+	unsigned int i = 0;
+
+	dev_info(kbdev->dev, "Kernel DDK version %s", MALI_RELEASE_NAME);
+
+	kbase_device_id_init(kbdev);
+	kbase_disjoint_init(kbdev);
+
+	for (i = 0; i < ARRAY_SIZE(dev_init); i++) {
+		err = dev_init[i].init(kbdev);
+		if (err) {
+			dev_err(kbdev->dev, "%s error = %d\n",
+						dev_init[i].err_mes, err);
+			kbase_device_term_partial(kbdev, i);
+			break;
+		}
+	}
+
+	return err;
+}
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c
new file mode 100644
index 000000000000..97bcc1d23aa3
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_csf.c
@@ -0,0 +1,161 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <gpu/mali_kbase_gpu_fault.h>
+#include <backend/gpu/mali_kbase_instr_internal.h>
+#include <backend/gpu/mali_kbase_pm_internal.h>
+#include <device/mali_kbase_device.h>
+#include <mali_kbase_reset_gpu.h>
+#include <mmu/mali_kbase_mmu.h>
+#include <mali_kbase_ctx_sched.h>
+
+/**
+ * kbase_report_gpu_fault - Report a GPU fault of the device.
+ *
+ * @kbdev:    Kbase device pointer
+ * @status:   Fault status
+ * @as_nr:    Faulty address space
+ * @as_valid: true if address space is valid
+ *
+ * This function is called from the interrupt handler when a GPU fault occurs.
+ */
+static void kbase_report_gpu_fault(struct kbase_device *kbdev, u32 status,
+		u32 as_nr, bool as_valid)
+{
+	u64 address = (u64) kbase_reg_read(kbdev,
+			GPU_CONTROL_REG(GPU_FAULTADDRESS_HI)) << 32;
+
+	address |= kbase_reg_read(kbdev,
+			GPU_CONTROL_REG(GPU_FAULTADDRESS_LO));
+
+	/* Report GPU fault for all contexts in case either
+	 * the address space is invalid or it's MCU address space.
+	 */
+	kbase_mmu_gpu_fault_interrupt(kbdev, status, as_nr, address, as_valid);
+}
+
+static bool kbase_gpu_fault_interrupt(struct kbase_device *kbdev)
+{
+	const u32 status = kbase_reg_read(kbdev,
+			GPU_CONTROL_REG(GPU_FAULTSTATUS));
+	const bool as_valid = status & GPU_FAULTSTATUS_JASID_VALID_FLAG;
+	const u32 as_nr = (status & GPU_FAULTSTATUS_JASID_MASK) >>
+			GPU_FAULTSTATUS_JASID_SHIFT;
+	bool bus_fault = (status & GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK) ==
+			GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_BUS_FAULT;
+	bool clear_gpu_fault = true;
+
+	if (bus_fault) {
+		/* If as_valid, reset gpu when ASID is for MCU. */
+		if (!as_valid || (as_nr == MCU_AS_NR)) {
+			kbase_report_gpu_fault(kbdev, status, as_nr, as_valid);
+
+			dev_err(kbdev->dev, "GPU bus fault triggering gpu-reset ...\n");
+			if (kbase_prepare_to_reset_gpu(kbdev))
+				kbase_reset_gpu(kbdev);
+		} else {
+			/* Handle Bus fault */
+			if (kbase_mmu_bus_fault_interrupt(kbdev, status, as_nr))
+				clear_gpu_fault = false;
+		}
+	} else
+		kbase_report_gpu_fault(kbdev, status, as_nr, as_valid);
+
+	return clear_gpu_fault;
+}
+
+void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
+{
+	bool clear_gpu_fault = false;
+
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ, NULL, val);
+	if (val & GPU_FAULT)
+		clear_gpu_fault = kbase_gpu_fault_interrupt(kbdev);
+
+	if (val & GPU_PROTECTED_FAULT) {
+		struct kbase_csf_scheduler *scheduler = &kbdev->csf.scheduler;
+		unsigned long flags;
+
+		dev_err_ratelimited(kbdev->dev, "GPU fault in protected mode");
+
+		/* Mask the protected fault interrupt to avoid the potential
+		 * deluge of such interrupts. It will be unmasked on GPU reset.
+		 */
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK),
+				GPU_IRQ_REG_ALL & ~GPU_PROTECTED_FAULT);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+		kbase_csf_scheduler_spin_lock(kbdev, &flags);
+		if (!WARN_ON(!kbase_csf_scheduler_protected_mode_in_use(kbdev)))
+			scheduler->active_protm_grp->faulted = true;
+		kbase_csf_scheduler_spin_unlock(kbdev, flags);
+
+		if (kbase_prepare_to_reset_gpu(kbdev))
+			kbase_reset_gpu(kbdev);
+	}
+
+	if (val & RESET_COMPLETED)
+		kbase_pm_reset_done(kbdev);
+
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_CLEAR, NULL, val);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), val);
+
+	/* kbase_pm_check_transitions (called by kbase_pm_power_changed) must
+	 * be called after the IRQ has been cleared. This is because it might
+	 * trigger further power transitions and we don't want to miss the
+	 * interrupt raised to notify us that these further transitions have
+	 * finished. The same applies to kbase_clean_caches_done() - if another
+	 * clean was queued, it might trigger another clean, which might
+	 * generate another interrupt which shouldn't be missed.
+	 */
+
+	if (val & CLEAN_CACHES_COMPLETED)
+		kbase_clean_caches_done(kbdev);
+
+	if (val & (POWER_CHANGED_ALL | MCU_STATUS_GPU_IRQ)) {
+		kbase_pm_power_changed(kbdev);
+	} else if (val & CLEAN_CACHES_COMPLETED) {
+		/* If cache line evict messages can be lost when shader cores
+		 * power down then we need to flush the L2 cache before powering
+		 * down cores. When the flush completes, the shaders' state
+		 * machine needs to be re-invoked to proceed with powering down
+		 * cores.
+		 */
+		if (kbdev->pm.backend.l2_always_on ||
+			kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_921))
+			kbase_pm_power_changed(kbdev);
+	}
+
+	if (clear_gpu_fault) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
+				GPU_COMMAND_CLEAR_FAULT);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	}
+
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_DONE, NULL, val);
+}
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c
new file mode 100644
index 000000000000..a11d778071b5
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_hw_jm.c
@@ -0,0 +1,100 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <gpu/mali_kbase_gpu_fault.h>
+#include <backend/gpu/mali_kbase_instr_internal.h>
+#include <backend/gpu/mali_kbase_pm_internal.h>
+#include <device/mali_kbase_device.h>
+#include <mali_kbase_reset_gpu.h>
+#include <mmu/mali_kbase_mmu.h>
+
+/**
+ * kbase_report_gpu_fault - Report a GPU fault.
+ * @kbdev:    Kbase device pointer
+ * @multiple: Zero if only GPU_FAULT was raised, non-zero if MULTIPLE_GPU_FAULTS
+ *            was also set
+ *
+ * This function is called from the interrupt handler when a GPU fault occurs.
+ * It reports the details of the fault using dev_warn().
+ */
+static void kbase_report_gpu_fault(struct kbase_device *kbdev, int multiple)
+{
+	u32 status = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_FAULTSTATUS));
+	u64 address = (u64) kbase_reg_read(kbdev,
+			GPU_CONTROL_REG(GPU_FAULTADDRESS_HI)) << 32;
+
+	address |= kbase_reg_read(kbdev,
+			GPU_CONTROL_REG(GPU_FAULTADDRESS_LO));
+
+	dev_warn(kbdev->dev, "GPU Fault 0x%08x (%s) at 0x%016llx",
+		status,
+		kbase_gpu_exception_name(status & 0xFF),
+		address);
+	if (multiple)
+		dev_warn(kbdev->dev, "There were multiple GPU faults - some have not been reported\n");
+}
+
+void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val)
+{
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ, NULL, val);
+	if (val & GPU_FAULT)
+		kbase_report_gpu_fault(kbdev, val & MULTIPLE_GPU_FAULTS);
+
+	if (val & RESET_COMPLETED)
+		kbase_pm_reset_done(kbdev);
+
+	if (val & PRFCNT_SAMPLE_COMPLETED)
+		kbase_instr_hwcnt_sample_done(kbdev);
+
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_CLEAR, NULL, val);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), val);
+
+	/* kbase_pm_check_transitions (called by kbase_pm_power_changed) must
+	 * be called after the IRQ has been cleared. This is because it might
+	 * trigger further power transitions and we don't want to miss the
+	 * interrupt raised to notify us that these further transitions have
+	 * finished. The same applies to kbase_clean_caches_done() - if another
+	 * clean was queued, it might trigger another clean, which might
+	 * generate another interrupt which shouldn't be missed.
+	 */
+
+	if (val & CLEAN_CACHES_COMPLETED)
+		kbase_clean_caches_done(kbdev);
+
+	if (val & POWER_CHANGED_ALL) {
+		kbase_pm_power_changed(kbdev);
+	} else if (val & CLEAN_CACHES_COMPLETED) {
+		/* If cache line evict messages can be lost when shader cores
+		 * power down then we need to flush the L2 cache before powering
+		 * down cores. When the flush completes, the shaders' state
+		 * machine needs to be re-invoked to proceed with powering down
+		 * cores.
+		 */
+		if (kbdev->pm.backend.l2_always_on ||
+			kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_TTRX_921))
+			kbase_pm_power_changed(kbdev);
+	}
+
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_IRQ_DONE, NULL, val);
+}
diff --git a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c
index 45cbab74b00f..8e853eb82fa1 100644
--- a/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c
+++ b/drivers/gpu/arm/bifrost/device/backend/mali_kbase_device_jm.c
@@ -43,6 +43,7 @@
 #include <backend/gpu/mali_kbase_js_internal.h>
 #include <backend/gpu/mali_kbase_pm_internal.h>
 #include <mali_kbase_dummy_job_wa.h>
+#include <backend/gpu/mali_kbase_clk_rate_trace_mgr.h>
 
 /**
  * kbase_backend_late_init - Perform any backend-specific initialization.
@@ -178,8 +179,11 @@ static const struct kbase_device_init dev_init[] = {
 			"Job JS devdata initialization failed"},
 	{kbase_device_timeline_init, kbase_device_timeline_term,
 			"Timeline stream initialization failed"},
-	{kbase_device_hwcnt_backend_gpu_init,
-			kbase_device_hwcnt_backend_gpu_term,
+	{kbase_clk_rate_trace_manager_init,
+			kbase_clk_rate_trace_manager_term,
+			"Clock rate trace manager initialization failed"},
+	{kbase_device_hwcnt_backend_jm_init,
+			kbase_device_hwcnt_backend_jm_term,
 			"GPU hwcnt backend creation failed"},
 	{kbase_device_hwcnt_context_init, kbase_device_hwcnt_context_term,
 			"GPU hwcnt context initialization failed"},
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device.c b/drivers/gpu/arm/bifrost/device/mali_kbase_device.c
index 76f14e5aa2da..76fb33a5e881 100644
--- a/drivers/gpu/arm/bifrost/device/mali_kbase_device.c
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device.c
@@ -21,8 +21,6 @@
  *
  */
 
-
-
 /*
  * Base kernel device APIs
  */
@@ -47,9 +45,9 @@
 #include "mali_kbase_hwcnt_virtualizer.h"
 
 #include "mali_kbase_device.h"
-#include "mali_kbase_device_internal.h"
 #include "backend/gpu/mali_kbase_pm_internal.h"
 #include "backend/gpu/mali_kbase_irq_internal.h"
+#include "mali_kbase_regs_history_debugfs.h"
 
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
 #include "arbiter/mali_kbase_arbiter_pm.h"
@@ -75,42 +73,27 @@ struct kbase_device *kbase_device_alloc(void)
 	return kzalloc(sizeof(struct kbase_device), GFP_KERNEL);
 }
 
-static int kbase_device_as_init(struct kbase_device *kbdev, int i)
-{
-	kbdev->as[i].number = i;
-	kbdev->as[i].bf_data.addr = 0ULL;
-	kbdev->as[i].pf_data.addr = 0ULL;
-
-	kbdev->as[i].pf_wq = alloc_workqueue("mali_mmu%d", 0, 1, i);
-	if (!kbdev->as[i].pf_wq)
-		return -EINVAL;
-
-	INIT_WORK(&kbdev->as[i].work_pagefault, page_fault_worker);
-	INIT_WORK(&kbdev->as[i].work_busfault, bus_fault_worker);
-
-	return 0;
-}
-
-static void kbase_device_as_term(struct kbase_device *kbdev, int i)
-{
-	destroy_workqueue(kbdev->as[i].pf_wq);
-}
-
+/**
+ * kbase_device_all_as_init() - Initialise address space objects of the device.
+ *
+ * @kbdev: Pointer to kbase device.
+ *
+ * Return: 0 on success otherwise non-zero.
+ */
 static int kbase_device_all_as_init(struct kbase_device *kbdev)
 {
-	int i, err;
+	int i, err = 0;
 
 	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
-		err = kbase_device_as_init(kbdev, i);
+		err = kbase_mmu_as_init(kbdev, i);
 		if (err)
-			goto free_workqs;
+			break;
 	}
 
-	return 0;
-
-free_workqs:
-	for (; i > 0; i--)
-		kbase_device_as_term(kbdev, i);
+	if (err) {
+		while (i-- > 0)
+			kbase_mmu_as_term(kbdev, i);
+	}
 
 	return err;
 }
@@ -120,7 +103,7 @@ static void kbase_device_all_as_term(struct kbase_device *kbdev)
 	int i;
 
 	for (i = 0; i < kbdev->nr_hw_address_spaces; i++)
-		kbase_device_as_term(kbdev, i);
+		kbase_mmu_as_term(kbdev, i);
 }
 
 int kbase_device_misc_init(struct kbase_device * const kbdev)
@@ -194,7 +177,7 @@ int kbase_device_misc_init(struct kbase_device * const kbdev)
 
 	err = kbase_device_all_as_init(kbdev);
 	if (err)
-		goto as_init_failed;
+		goto dma_set_mask_failed;
 
 	spin_lock_init(&kbdev->hwcnt.lock);
 
@@ -231,7 +214,6 @@ int kbase_device_misc_init(struct kbase_device * const kbdev)
 	kbase_ktrace_term(kbdev);
 term_as:
 	kbase_device_all_as_term(kbdev);
-as_init_failed:
 dma_set_mask_failed:
 fail:
 	return err;
@@ -271,14 +253,14 @@ void kbase_increment_device_id(void)
 	kbase_dev_nr++;
 }
 
-int kbase_device_hwcnt_backend_gpu_init(struct kbase_device *kbdev)
+int kbase_device_hwcnt_backend_jm_init(struct kbase_device *kbdev)
 {
-	return kbase_hwcnt_backend_gpu_create(kbdev, &kbdev->hwcnt_gpu_iface);
+	return kbase_hwcnt_backend_jm_create(kbdev, &kbdev->hwcnt_gpu_iface);
 }
 
-void kbase_device_hwcnt_backend_gpu_term(struct kbase_device *kbdev)
+void kbase_device_hwcnt_backend_jm_term(struct kbase_device *kbdev)
 {
-	kbase_hwcnt_backend_gpu_destroy(&kbdev->hwcnt_gpu_iface);
+	kbase_hwcnt_backend_jm_destroy(&kbdev->hwcnt_gpu_iface);
 }
 
 int kbase_device_hwcnt_context_init(struct kbase_device *kbdev)
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device.h b/drivers/gpu/arm/bifrost/device/mali_kbase_device.h
index 16f1d7098688..33264bcc0464 100644
--- a/drivers/gpu/arm/bifrost/device/mali_kbase_device.h
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device.h
@@ -69,3 +69,109 @@ int kbase_device_init(struct kbase_device *kbdev);
  *
  */
 void kbase_device_term(struct kbase_device *kbdev);
+
+/**
+ * kbase_reg_write - write to GPU register
+ * @kbdev:  Kbase device pointer
+ * @offset: Offset of register
+ * @value:  Value to write
+ *
+ * Caller must ensure the GPU is powered (@kbdev->pm.gpu_powered != false).
+ */
+void kbase_reg_write(struct kbase_device *kbdev, u32 offset, u32 value);
+
+/**
+ * kbase_reg_read - read from GPU register
+ * @kbdev:  Kbase device pointer
+ * @offset: Offset of register
+ *
+ * Caller must ensure the GPU is powered (@kbdev->pm.gpu_powered != false).
+ *
+ * Return: Value in desired register
+ */
+u32 kbase_reg_read(struct kbase_device *kbdev, u32 offset);
+
+/**
+ * kbase_is_gpu_removed() - Has the GPU been removed.
+ * @kbdev:    Kbase device pointer
+ *
+ * When Kbase takes too long to give up the GPU, the Arbiter
+ * can remove it.  This will then be followed by a GPU lost event.
+ * This function will return true if the GPU has been removed.
+ * When this happens register reads will be zero. A zero GPU_ID is
+ * invalid so this is used to detect when GPU is removed.
+ *
+ * Return: True if GPU removed
+ */
+bool kbase_is_gpu_removed(struct kbase_device *kbdev);
+
+/**
+ * kbase_gpu_start_cache_clean - Start a cache clean
+ * @kbdev: Kbase device
+ *
+ * Issue a cache clean and invalidate command to hardware. This function will
+ * take hwaccess_lock.
+ */
+void kbase_gpu_start_cache_clean(struct kbase_device *kbdev);
+
+/**
+ * kbase_gpu_start_cache_clean_nolock - Start a cache clean
+ * @kbdev: Kbase device
+ *
+ * Issue a cache clean and invalidate command to hardware. hwaccess_lock
+ * must be held by the caller.
+ */
+void kbase_gpu_start_cache_clean_nolock(struct kbase_device *kbdev);
+
+/**
+ * kbase_gpu_wait_cache_clean - Wait for cache cleaning to finish
+ * @kbdev: Kbase device
+ *
+ * This function will take hwaccess_lock, and may sleep.
+ */
+void kbase_gpu_wait_cache_clean(struct kbase_device *kbdev);
+
+/**
+ * kbase_gpu_wait_cache_clean_timeout - Wait for certain time for cache
+ *                                      cleaning to finish
+ * @kbdev: Kbase device
+ * @wait_timeout_ms: Time in milliseconds, to wait for cache clean to complete.
+ *
+ * This function will take hwaccess_lock, and may sleep. This is supposed to be
+ * called from paths (like GPU reset) where an indefinite wait for the
+ * completion of cache clean operation can cause deadlock, as the operation may
+ * never complete.
+ *
+ * Return: 0 if successful or a negative error code on failure.
+ */
+int kbase_gpu_wait_cache_clean_timeout(struct kbase_device *kbdev,
+		unsigned int wait_timeout_ms);
+
+/**
+ * kbase_gpu_cache_clean_wait_complete - Called after the cache cleaning is
+ *                                       finished. Would also be called after
+ *                                       the GPU reset.
+ * @kbdev: Kbase device
+ *
+ * Caller must hold the hwaccess_lock.
+ */
+void kbase_gpu_cache_clean_wait_complete(struct kbase_device *kbdev);
+
+/**
+ * kbase_clean_caches_done - Issue preiously queued cache clean request or
+ *                           wake up the requester that issued cache clean.
+ * @kbdev: Kbase device
+ *
+ * Caller must hold the hwaccess_lock.
+ */
+void kbase_clean_caches_done(struct kbase_device *kbdev);
+
+/**
+ * kbase_gpu_interrupt - GPU interrupt handler
+ * @kbdev: Kbase device pointer
+ * @val:   The value of the GPU IRQ status register which triggered the call
+ *
+ * This function is called from the interrupt handler when a GPU irq is to be
+ * handled.
+ */
+void kbase_gpu_interrupt(struct kbase_device *kbdev, u32 val);
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c b/drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c
new file mode 100644
index 000000000000..3a75c6c05cfa
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device_hw.c
@@ -0,0 +1,184 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *
+ * (C) COPYRIGHT 2014-2016, 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <gpu/mali_kbase_gpu_fault.h>
+#include <backend/gpu/mali_kbase_instr_internal.h>
+#include <backend/gpu/mali_kbase_pm_internal.h>
+#include <device/mali_kbase_device.h>
+#include <mali_kbase_reset_gpu.h>
+#include <mmu/mali_kbase_mmu.h>
+
+#if !defined(CONFIG_MALI_BIFROST_NO_MALI)
+void kbase_reg_write(struct kbase_device *kbdev, u32 offset, u32 value)
+{
+	KBASE_DEBUG_ASSERT(kbdev->pm.backend.gpu_powered);
+	KBASE_DEBUG_ASSERT(kbdev->dev != NULL);
+
+	writel(value, kbdev->reg + offset);
+
+#ifdef CONFIG_DEBUG_FS
+	if (unlikely(kbdev->io_history.enabled))
+		kbase_io_history_add(&kbdev->io_history, kbdev->reg + offset,
+				value, 1);
+#endif /* CONFIG_DEBUG_FS */
+	dev_dbg(kbdev->dev, "w: reg %08x val %08x", offset, value);
+}
+
+KBASE_EXPORT_TEST_API(kbase_reg_write);
+
+u32 kbase_reg_read(struct kbase_device *kbdev, u32 offset)
+{
+	u32 val;
+
+	KBASE_DEBUG_ASSERT(kbdev->pm.backend.gpu_powered);
+	KBASE_DEBUG_ASSERT(kbdev->dev != NULL);
+
+	val = readl(kbdev->reg + offset);
+
+#ifdef CONFIG_DEBUG_FS
+	if (unlikely(kbdev->io_history.enabled))
+		kbase_io_history_add(&kbdev->io_history, kbdev->reg + offset,
+				val, 0);
+#endif /* CONFIG_DEBUG_FS */
+	dev_dbg(kbdev->dev, "r: reg %08x val %08x", offset, val);
+
+	return val;
+}
+
+KBASE_EXPORT_TEST_API(kbase_reg_read);
+
+bool kbase_is_gpu_removed(struct kbase_device *kbdev)
+{
+	u32 val;
+
+	val = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_ID));
+
+	return val == 0;
+}
+#endif /* !defined(CONFIG_MALI_BIFROST_NO_MALI) */
+
+void kbase_gpu_start_cache_clean_nolock(struct kbase_device *kbdev)
+{
+	u32 irq_mask;
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (kbdev->cache_clean_in_progress) {
+		/* If this is called while another clean is in progress, we
+		 * can't rely on the current one to flush any new changes in
+		 * the cache. Instead, trigger another cache clean immediately
+		 * after this one finishes.
+		 */
+		kbdev->cache_clean_queued = true;
+		return;
+	}
+
+	/* Enable interrupt */
+	irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK));
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK),
+				irq_mask | CLEAN_CACHES_COMPLETED);
+
+	KBASE_KTRACE_ADD(kbdev, CORE_GPU_CLEAN_INV_CACHES, NULL, 0);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
+					GPU_COMMAND_CLEAN_INV_CACHES);
+
+	kbdev->cache_clean_in_progress = true;
+}
+
+void kbase_gpu_start_cache_clean(struct kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_gpu_start_cache_clean_nolock(kbdev);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+}
+
+void kbase_gpu_cache_clean_wait_complete(struct kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbdev->cache_clean_queued = false;
+	kbdev->cache_clean_in_progress = false;
+	wake_up(&kbdev->cache_clean_wait);
+}
+
+void kbase_clean_caches_done(struct kbase_device *kbdev)
+{
+	u32 irq_mask;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+	if (kbdev->cache_clean_queued) {
+		kbdev->cache_clean_queued = false;
+
+		KBASE_KTRACE_ADD(kbdev, CORE_GPU_CLEAN_INV_CACHES, NULL, 0);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
+				GPU_COMMAND_CLEAN_INV_CACHES);
+	} else {
+		/* Disable interrupt */
+		irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK));
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK),
+				irq_mask & ~CLEAN_CACHES_COMPLETED);
+
+		kbase_gpu_cache_clean_wait_complete(kbdev);
+	}
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+}
+
+static inline bool get_cache_clean_flag(struct kbase_device *kbdev)
+{
+	bool cache_clean_in_progress;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	cache_clean_in_progress = kbdev->cache_clean_in_progress;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return cache_clean_in_progress;
+}
+
+void kbase_gpu_wait_cache_clean(struct kbase_device *kbdev)
+{
+	while (get_cache_clean_flag(kbdev)) {
+		wait_event_interruptible(kbdev->cache_clean_wait,
+				!kbdev->cache_clean_in_progress);
+	}
+}
+
+int kbase_gpu_wait_cache_clean_timeout(struct kbase_device *kbdev,
+				unsigned int wait_timeout_ms)
+{
+	long remaining = msecs_to_jiffies(wait_timeout_ms);
+
+	while (remaining && get_cache_clean_flag(kbdev)) {
+		remaining = wait_event_timeout(kbdev->cache_clean_wait,
+					!kbdev->cache_clean_in_progress,
+					remaining);
+	}
+
+	return (remaining ? 0 : -ETIMEDOUT);
+}
diff --git a/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h b/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h
index 9f96db050bfe..54644582eac5 100644
--- a/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h
+++ b/drivers/gpu/arm/bifrost/device/mali_kbase_device_internal.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -43,8 +43,8 @@ void kbase_device_vinstr_term(struct kbase_device *kbdev);
 int kbase_device_timeline_init(struct kbase_device *kbdev);
 void kbase_device_timeline_term(struct kbase_device *kbdev);
 
-int kbase_device_hwcnt_backend_gpu_init(struct kbase_device *kbdev);
-void kbase_device_hwcnt_backend_gpu_term(struct kbase_device *kbdev);
+int kbase_device_hwcnt_backend_jm_init(struct kbase_device *kbdev);
+void kbase_device_hwcnt_backend_jm_term(struct kbase_device *kbdev);
 
 int kbase_device_hwcnt_context_init(struct kbase_device *kbdev);
 void kbase_device_hwcnt_context_term(struct kbase_device *kbdev);
diff --git a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c
new file mode 100644
index 000000000000..f7e9b125ba8b
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_csf.c
@@ -0,0 +1,105 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include "csf/mali_gpu_csf_registers.h"
+#include "../mali_kbase_gpu_fault.h"
+
+const char *kbase_gpu_exception_name(u32 const exception_code)
+{
+	const char *e;
+
+	switch (exception_code) {
+	/* Command Stream exceptions */
+	case CS_FAULT_EXCEPTION_TYPE_CS_RESOURCE_TERMINATED:
+		e = "CS_RESOURCE_TERMINATED";
+		break;
+	case CS_FAULT_EXCEPTION_TYPE_CS_INHERIT_FAULT:
+		e = "CS_INHERIT_FAULT";
+		break;
+	/* Command Stream fatal exceptions */
+	case CS_FATAL_EXCEPTION_TYPE_CS_CONFIG_FAULT:
+		e = "CS_CONFIG_FAULT";
+		break;
+	case CS_FATAL_EXCEPTION_TYPE_CS_ENDPOINT_FAULT:
+		e = "FATAL_CS_ENDPOINT_FAULT";
+		break;
+	case CS_FATAL_EXCEPTION_TYPE_CS_BUS_FAULT:
+		e = "FATAL_CS_BUS_FAULT";
+		break;
+	case CS_FATAL_EXCEPTION_TYPE_CS_INVALID_INSTRUCTION:
+		e = "FATAL_CS_INVALID_INSTRUCTION";
+		break;
+	case CS_FATAL_EXCEPTION_TYPE_CS_CALL_STACK_OVERFLOW:
+		e = "FATAL_CS_CALL_STACK_OVERFLOW";
+		break;
+	/* Shader exceptions */
+	case CS_FAULT_EXCEPTION_TYPE_INSTR_INVALID_PC:
+		e = "INSTR_INVALID_PC";
+		break;
+	case CS_FAULT_EXCEPTION_TYPE_INSTR_INVALID_ENC:
+		e = "INSTR_INVALID_ENC";
+		break;
+	case CS_FAULT_EXCEPTION_TYPE_INSTR_BARRIER_FAULT:
+		e = "INSTR_BARRIER_FAULT";
+		break;
+	/* Misc exceptions */
+	case CS_FAULT_EXCEPTION_TYPE_DATA_INVALID_FAULT:
+		e = "DATA_INVALID_FAULT";
+		break;
+	case CS_FAULT_EXCEPTION_TYPE_TILE_RANGE_FAULT:
+		e = "TILE_RANGE_FAULT";
+		break;
+	case CS_FAULT_EXCEPTION_TYPE_ADDR_RANGE_FAULT:
+		e = "ADDR_RANGE_FAULT";
+		break;
+	case CS_FAULT_EXCEPTION_TYPE_IMPRECISE_FAULT:
+		e = "IMPRECISE_FAULT";
+		break;
+	/* FW exceptions */
+	case CS_FATAL_EXCEPTION_TYPE_FIRMWARE_INTERNAL_ERROR:
+		e = "FIRMWARE_INTERNAL_ERROR";
+		break;
+	case CS_FAULT_EXCEPTION_TYPE_RESOURCE_EVICTION_TIMEOUT:
+		e = "RESOURCE_EVICTION_TIMEOUT";
+		break;
+	/* GPU Fault */
+	case GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_BUS_FAULT:
+		e = "GPU_BUS_FAULT";
+		break;
+	case GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_SHAREABILITY_FAULT:
+		e = "GPU_SHAREABILITY_FAULT";
+		break;
+	case GPU_FAULTSTATUS_EXCEPTION_TYPE_SYSTEM_SHAREABILITY_FAULT:
+		e = "SYSTEM_SHAREABILITY_FAULT";
+		break;
+	case GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_CACHEABILITY_FAULT:
+		e = "GPU_CACHEABILITY_FAULT";
+		break;
+	/* Any other exception code is unknown */
+	default:
+		e = "UNKNOWN";
+		break;
+	}
+
+	return e;
+}
diff --git a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_jm.c b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_jm.c
index 63132dc80fa5..56f541516489 100644
--- a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_jm.c
+++ b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_fault_jm.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -119,8 +119,6 @@ const char *kbase_gpu_exception_name(u32 const exception_code)
 		e = "TRANSLATION_FAULT";
 		break;
 	case 0xC8:
-		e = "PERMISSION_FAULT";
-		break;
 	case 0xC9:
 	case 0xCA:
 	case 0xCB:
@@ -141,8 +139,6 @@ const char *kbase_gpu_exception_name(u32 const exception_code)
 		e = "TRANSTAB_BUS_FAULT";
 		break;
 	case 0xD8:
-		e = "ACCESS_FLAG";
-		break;
 	case 0xD9:
 	case 0xDA:
 	case 0xDB:
diff --git a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_csf.h b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_csf.h
new file mode 100644
index 000000000000..ff6e4ae47184
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_csf.h
@@ -0,0 +1,297 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_GPU_REGMAP_CSF_H_
+#define _KBASE_GPU_REGMAP_CSF_H_
+
+#if !MALI_USE_CSF
+#error "Cannot be compiled with JM"
+#endif
+
+#include "csf/mali_gpu_csf_control_registers.h"
+#define GPU_CONTROL_MCU_REG(r)  (GPU_CONTROL_MCU + (r))
+
+
+/* Set to implementation defined, outer caching */
+#define AS_MEMATTR_AARCH64_OUTER_IMPL_DEF 0x88ull
+/* Set to write back memory, outer caching */
+#define AS_MEMATTR_AARCH64_OUTER_WA       0x8Dull
+/* Set to inner non-cacheable, outer-non-cacheable
+ * Setting defined by the alloc bits is ignored, but set to a valid encoding:
+ * - no-alloc on read
+ * - no alloc on write
+ */
+#define AS_MEMATTR_AARCH64_NON_CACHEABLE  0x4Cull
+/* Set to shared memory, that is inner cacheable on ACE and inner or outer
+ * shared, otherwise inner non-cacheable.
+ * Outer cacheable if inner or outer shared, otherwise outer non-cacheable.
+ */
+#define AS_MEMATTR_AARCH64_SHARED         0x8ull
+
+/* Symbols for default MEMATTR to use
+ * Default is - HW implementation defined caching
+ */
+#define AS_MEMATTR_INDEX_DEFAULT               0
+#define AS_MEMATTR_INDEX_DEFAULT_ACE           3
+
+/* HW implementation defined caching */
+#define AS_MEMATTR_INDEX_IMPL_DEF_CACHE_POLICY 0
+/* Force cache on */
+#define AS_MEMATTR_INDEX_FORCE_TO_CACHE_ALL    1
+/* Write-alloc */
+#define AS_MEMATTR_INDEX_WRITE_ALLOC           2
+/* Outer coherent, inner implementation defined policy */
+#define AS_MEMATTR_INDEX_OUTER_IMPL_DEF        3
+/* Outer coherent, write alloc inner */
+#define AS_MEMATTR_INDEX_OUTER_WA              4
+/* Normal memory, inner non-cacheable, outer non-cacheable (ARMv8 mode only) */
+#define AS_MEMATTR_INDEX_NON_CACHEABLE         5
+/* Normal memory, shared between MCU and Host */
+#define AS_MEMATTR_INDEX_SHARED                6
+
+/* Configuration bits for the Command Stream Frontend. */
+#define CSF_CONFIG 0xF00
+
+/* CSF_CONFIG register */
+#define CSF_CONFIG_FORCE_COHERENCY_FEATURES_SHIFT 2
+
+/* GPU control registers */
+#define MCU_CONTROL             0x700
+#define MCU_STATUS              0x704
+
+#define MCU_CNTRL_ENABLE        (1 << 0)
+#define MCU_CNTRL_AUTO          (1 << 1)
+#define MCU_CNTRL_DISABLE       (0)
+
+#define MCU_STATUS_HALTED        (1 << 1)
+
+#define PRFCNT_BASE_LO   0x060  /* (RW) Performance counter memory
+				 * region base address, low word
+				 */
+#define PRFCNT_BASE_HI   0x064  /* (RW) Performance counter memory
+				 * region base address, high word
+				 */
+#define PRFCNT_CONFIG    0x068  /* (RW) Performance counter
+				 * configuration
+				 */
+
+#define PRFCNT_CSHW_EN   0x06C  /* (RW) Performance counter
+				 * enable for Command Stream Hardware
+				 */
+
+#define PRFCNT_SHADER_EN 0x070  /* (RW) Performance counter enable
+				 * flags for shader cores
+				 */
+#define PRFCNT_TILER_EN  0x074  /* (RW) Performance counter enable
+				 * flags for tiler
+				 */
+#define PRFCNT_MMU_L2_EN 0x07C  /* (RW) Performance counter enable
+				 * flags for MMU/L2 cache
+				 */
+
+/* JOB IRQ flags */
+#define JOB_IRQ_GLOBAL_IF       (1 << 31)   /* Global interface interrupt received */
+
+/* GPU_COMMAND codes */
+#define GPU_COMMAND_CODE_NOP                0x00 /* No operation, nothing happens */
+#define GPU_COMMAND_CODE_RESET              0x01 /* Reset the GPU */
+#define GPU_COMMAND_CODE_PRFCNT             0x02 /* Clear or sample performance counters */
+#define GPU_COMMAND_CODE_TIME               0x03 /* Configure time sources */
+#define GPU_COMMAND_CODE_FLUSH_CACHES       0x04 /* Flush caches */
+#define GPU_COMMAND_CODE_SET_PROTECTED_MODE 0x05 /* Places the GPU in protected mode */
+#define GPU_COMMAND_CODE_FINISH_HALT        0x06 /* Halt CSF */
+#define GPU_COMMAND_CODE_CLEAR_FAULT        0x07 /* Clear GPU_FAULTSTATUS and GPU_FAULTADDRESS, TODX */
+
+/* GPU_COMMAND_RESET payloads */
+
+/* This will leave the state of active jobs UNDEFINED, but will leave the external bus in a defined and idle state.
+ * Power domains will remain powered on.
+ */
+#define GPU_COMMAND_RESET_PAYLOAD_FAST_RESET 0x00
+
+/* This will leave the state of active command streams UNDEFINED, but will leave the external bus in a defined and
+ * idle state.
+ */
+#define GPU_COMMAND_RESET_PAYLOAD_SOFT_RESET 0x01
+
+/* This reset will leave the state of currently active streams UNDEFINED, will likely lose data, and may leave
+ * the system bus in an inconsistent state. Use only as a last resort when nothing else works.
+ */
+#define GPU_COMMAND_RESET_PAYLOAD_HARD_RESET 0x02
+
+/* GPU_COMMAND_PRFCNT payloads */
+#define GPU_COMMAND_PRFCNT_PAYLOAD_SAMPLE 0x01 /* Sample performance counters */
+#define GPU_COMMAND_PRFCNT_PAYLOAD_CLEAR  0x02 /* Clear performance counters */
+
+/* GPU_COMMAND_TIME payloads */
+#define GPU_COMMAND_TIME_DISABLE 0x00 /* Disable cycle counter */
+#define GPU_COMMAND_TIME_ENABLE  0x01 /* Enable cycle counter */
+
+/* GPU_COMMAND_FLUSH_CACHES payloads */
+#define GPU_COMMAND_FLUSH_PAYLOAD_NONE             0x00 /* No flush */
+#define GPU_COMMAND_FLUSH_PAYLOAD_CLEAN            0x01 /* Clean the caches */
+#define GPU_COMMAND_FLUSH_PAYLOAD_INVALIDATE       0x02 /* Invalidate the caches */
+#define GPU_COMMAND_FLUSH_PAYLOAD_CLEAN_INVALIDATE 0x03 /* Clean and invalidate the caches */
+
+/* GPU_COMMAND command + payload */
+#define GPU_COMMAND_CODE_PAYLOAD(opcode, payload) \
+	((u32)opcode | ((u32)payload << 8))
+
+/* Final GPU_COMMAND form */
+/* No operation, nothing happens */
+#define GPU_COMMAND_NOP \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_NOP, 0)
+
+/* Stop all external bus interfaces, and then reset the entire GPU. */
+#define GPU_COMMAND_SOFT_RESET \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_RESET, GPU_COMMAND_RESET_PAYLOAD_SOFT_RESET)
+
+/* Immediately reset the entire GPU. */
+#define GPU_COMMAND_HARD_RESET \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_RESET, GPU_COMMAND_RESET_PAYLOAD_HARD_RESET)
+
+/* Clear all performance counters, setting them all to zero. */
+#define GPU_COMMAND_PRFCNT_CLEAR \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_PRFCNT, GPU_COMMAND_PRFCNT_PAYLOAD_CLEAR)
+
+/* Sample all performance counters, writing them out to memory */
+#define GPU_COMMAND_PRFCNT_SAMPLE \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_PRFCNT, GPU_COMMAND_PRFCNT_PAYLOAD_SAMPLE)
+
+/* Starts the cycle counter, and system timestamp propagation */
+#define GPU_COMMAND_CYCLE_COUNT_START \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_TIME, GPU_COMMAND_TIME_ENABLE)
+
+/* Stops the cycle counter, and system timestamp propagation */
+#define GPU_COMMAND_CYCLE_COUNT_STOP \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_TIME, GPU_COMMAND_TIME_DISABLE)
+
+/* Clean all caches */
+#define GPU_COMMAND_CLEAN_CACHES \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_CACHES, GPU_COMMAND_FLUSH_PAYLOAD_CLEAN)
+
+/* Clean and invalidate all caches */
+#define GPU_COMMAND_CLEAN_INV_CACHES \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FLUSH_CACHES, GPU_COMMAND_FLUSH_PAYLOAD_CLEAN_INVALIDATE)
+
+/* Places the GPU in protected mode */
+#define GPU_COMMAND_SET_PROTECTED_MODE \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_SET_PROTECTED_MODE, 0)
+
+/* Halt CSF */
+#define GPU_COMMAND_FINISH_HALT \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_FINISH_HALT, 0)
+
+/* Clear GPU faults */
+#define GPU_COMMAND_CLEAR_FAULT \
+	GPU_COMMAND_CODE_PAYLOAD(GPU_COMMAND_CODE_CLEAR_FAULT, 0)
+
+/* End Command Values */
+
+/* GPU_FAULTSTATUS register */
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_SHIFT 0
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK (0xFFul)
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GET(reg_val) \
+	(((reg_val)&GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK) \
+	 >> GPU_FAULTSTATUS_EXCEPTION_TYPE_SHIFT)
+#define GPU_FAULTSTATUS_ACCESS_TYPE_SHIFT 8
+#define GPU_FAULTSTATUS_ACCESS_TYPE_MASK \
+	(0x3ul << GPU_FAULTSTATUS_ACCESS_TYPE_SHIFT)
+
+#define GPU_FAULTSTATUS_ADDR_VALID_SHIFT 10
+#define GPU_FAULTSTATUS_ADDR_VALID_FLAG \
+	(1ul << GPU_FAULTSTATUS_ADDR_VALID_SHIFT)
+
+#define GPU_FAULTSTATUS_JASID_VALID_SHIFT 11
+#define GPU_FAULTSTATUS_JASID_VALID_FLAG \
+	(1ul << GPU_FAULTSTATUS_JASID_VALID_SHIFT)
+
+#define GPU_FAULTSTATUS_JASID_SHIFT 12
+#define GPU_FAULTSTATUS_JASID_MASK (0xF << GPU_FAULTSTATUS_JASID_SHIFT)
+#define GPU_FAULTSTATUS_JASID_GET(reg_val) \
+	(((reg_val)&GPU_FAULTSTATUS_JASID_MASK) >> GPU_FAULTSTATUS_JASID_SHIFT)
+#define GPU_FAULTSTATUS_JASID_SET(reg_val, value) \
+	(((reg_val) & ~GPU_FAULTSTATUS_JASID_MASK) |  \
+	(((value) << GPU_FAULTSTATUS_JASID_SHIFT) & GPU_FAULTSTATUS_JASID_MASK))
+
+#define GPU_FAULTSTATUS_SOURCE_ID_SHIFT 16
+#define GPU_FAULTSTATUS_SOURCE_ID_MASK \
+	(0xFFFFul << GPU_FAULTSTATUS_SOURCE_ID_SHIFT)
+/* End GPU_FAULTSTATUS register */
+
+/* GPU_FAULTSTATUS_ACCESS_TYPE values */
+#define GPU_FAULTSTATUS_ACCESS_TYPE_ATOMIC 0x0
+#define GPU_FAULTSTATUS_ACCESS_TYPE_EXECUTE 0x1
+#define GPU_FAULTSTATUS_ACCESS_TYPE_READ 0x2
+#define GPU_FAULTSTATUS_ACCESS_TYPE_WRITE 0x3
+/* End of GPU_FAULTSTATUS_ACCESS_TYPE values */
+
+/* TODO: Remove once 10.x.6 headers became available */
+#define GPU_EXCEPTION_TYPE_SW_FAULT_0 ((u8)0x70)
+#define GPU_EXCEPTION_TYPE_SW_FAULT_1 ((u8)0x71)
+
+/* GPU_FAULTSTATUS_EXCEPTION_TYPE values */
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_OK 0x00
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_BUS_FAULT 0x80
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_SHAREABILITY_FAULT 0x88
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_SYSTEM_SHAREABILITY_FAULT 0x89
+#define GPU_FAULTSTATUS_EXCEPTION_TYPE_GPU_CACHEABILITY_FAULT 0x8A
+/* End of GPU_FAULTSTATUS_EXCEPTION_TYPE values */
+
+#define GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT GPU_U(10)
+#define GPU_FAULTSTATUS_ADDRESS_VALID_MASK (GPU_U(0x1) << GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT)
+#define GPU_FAULTSTATUS_ADDRESS_VALID_GET(reg_val) \
+	(((reg_val)&GPU_FAULTSTATUS_ADDRESS_VALID_MASK) >> GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT)
+#define GPU_FAULTSTATUS_ADDRESS_VALID_SET(reg_val, value) \
+	(((reg_val) & ~GPU_FAULTSTATUS_ADDRESS_VALID_MASK) |  \
+	(((value) << GPU_FAULTSTATUS_ADDRESS_VALID_SHIFT) & GPU_FAULTSTATUS_ADDRESS_VALID_MASK))
+
+/* IRQ flags */
+#define GPU_FAULT               (1 << 0)    /* A GPU Fault has occurred */
+#define GPU_PROTECTED_FAULT     (1 << 1)    /* A GPU fault has occurred in protected mode */
+#define RESET_COMPLETED         (1 << 8)    /* Set when a reset has completed.  */
+#define POWER_CHANGED_SINGLE    (1 << 9)    /* Set when a single core has finished powering up or down. */
+#define POWER_CHANGED_ALL       (1 << 10)   /* Set when all cores have finished powering up or down. */
+#define CLEAN_CACHES_COMPLETED  (1 << 17)   /* Set when a cache clean operation has completed. */
+#define DOORBELL_MIRROR         (1 << 18)   /* Mirrors the doorbell interrupt line to the CPU */
+#define MCU_STATUS_GPU_IRQ      (1 << 19)   /* MCU requires attention */
+
+/*
+ * In Debug build,
+ * GPU_IRQ_REG_COMMON | POWER_CHANGED_SINGLE is used to clear and unmask interupts sources of GPU_IRQ
+ * by writing it onto GPU_IRQ_CLEAR/MASK registers.
+ *
+ * In Release build,
+ * GPU_IRQ_REG_COMMON is used.
+ *
+ * Note:
+ * CLEAN_CACHES_COMPLETED - Used separately for cache operation.
+ * DOORBELL_MIRROR - Do not have it included for GPU_IRQ_REG_COMMON
+ *                   as it can't be cleared by GPU_IRQ_CLEAR, thus interrupt storm might happen
+ */
+#define GPU_IRQ_REG_COMMON (GPU_FAULT | GPU_PROTECTED_FAULT | RESET_COMPLETED \
+			| POWER_CHANGED_ALL | MCU_STATUS_GPU_IRQ)
+
+/* GPU_CONTROL_MCU.GPU_IRQ_RAWSTAT */
+#define PRFCNT_SAMPLE_COMPLETED (1 << 16)   /* Set when performance count sample has completed */
+
+#endif /* _KBASE_GPU_REGMAP_CSF_H_ */
diff --git a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_jm.h b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_jm.h
index 258ff33348fe..c9c2fbd49058 100644
--- a/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_jm.h
+++ b/drivers/gpu/arm/bifrost/gpu/backend/mali_kbase_gpu_regmap_jm.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,6 +23,9 @@
 #ifndef _KBASE_GPU_REGMAP_JM_H_
 #define _KBASE_GPU_REGMAP_JM_H_
 
+#if MALI_USE_CSF
+#error "Cannot be compiled with CSF"
+#endif
 
 /* Set to implementation defined, outer caching */
 #define AS_MEMATTR_AARCH64_OUTER_IMPL_DEF 0x88ull
@@ -259,4 +262,27 @@
 #define GPU_COMMAND_CLEAN_INV_CACHES   0x08 /* Clean and invalidate all caches */
 #define GPU_COMMAND_SET_PROTECTED_MODE 0x09 /* Places the GPU in protected mode */
 
+/* IRQ flags */
+#define GPU_FAULT               (1 << 0)    /* A GPU Fault has occurred */
+#define MULTIPLE_GPU_FAULTS     (1 << 7)    /* More than one GPU Fault occurred.  */
+#define RESET_COMPLETED         (1 << 8)    /* Set when a reset has completed.  */
+#define POWER_CHANGED_SINGLE    (1 << 9)    /* Set when a single core has finished powering up or down. */
+#define POWER_CHANGED_ALL       (1 << 10)   /* Set when all cores have finished powering up or down. */
+#define PRFCNT_SAMPLE_COMPLETED (1 << 16)   /* Set when a performance count sample has completed. */
+#define CLEAN_CACHES_COMPLETED  (1 << 17)   /* Set when a cache clean operation has completed. */
+
+/*
+ * In Debug build,
+ * GPU_IRQ_REG_COMMON | POWER_CHANGED_SINGLE is used to clear and enable interupts sources of GPU_IRQ
+ * by writing it onto GPU_IRQ_CLEAR/MASK registers.
+ *
+ * In Release build,
+ * GPU_IRQ_REG_COMMON is used.
+ *
+ * Note:
+ * CLEAN_CACHES_COMPLETED - Used separately for cache operation.
+ */
+#define GPU_IRQ_REG_COMMON (GPU_FAULT | MULTIPLE_GPU_FAULTS | RESET_COMPLETED \
+		| POWER_CHANGED_ALL | PRFCNT_SAMPLE_COMPLETED)
+
 #endif /* _KBASE_GPU_REGMAP_JM_H_ */
diff --git a/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_fault.h b/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_fault.h
index b59b9d15f945..e63c3881a3ca 100644
--- a/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_fault.h
+++ b/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_fault.h
@@ -33,17 +33,6 @@
  */
 const char *kbase_gpu_exception_name(u32 exception_code);
 
-/** Returns the name associated with a Mali fatal exception code
- *
- * @fatal_exception_code: fatal exception code
- *
- * This function is called from the interrupt handler when a GPU fatal
- * exception occurs.
- *
- * Return: name associated with the fatal exception code
- */
-const char *kbase_gpu_fatal_exception_name(u32 const fatal_exception_code);
-
 /**
  * kbase_gpu_access_type_name - Convert MMU_AS_CONTROL.FAULTSTATUS.ACCESS_TYPE
  * into string.
diff --git a/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h b/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h
index 9f3d6b1d5b51..31d55264c67f 100644
--- a/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h
+++ b/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_id.h
@@ -98,6 +98,7 @@
 #define GPU_ID2_PRODUCT_TNAX              GPU_ID2_MODEL_MAKE(9, 1)
 #define GPU_ID2_PRODUCT_TBEX              GPU_ID2_MODEL_MAKE(9, 2)
 #define GPU_ID2_PRODUCT_LBEX              GPU_ID2_MODEL_MAKE(9, 4)
+#define GPU_ID2_PRODUCT_TBAX              GPU_ID2_MODEL_MAKE(9, 5)
 #define GPU_ID2_PRODUCT_TDUX              GPU_ID2_MODEL_MAKE(10, 1)
 #define GPU_ID2_PRODUCT_TODX              GPU_ID2_MODEL_MAKE(10, 2)
 #define GPU_ID2_PRODUCT_TGRX              GPU_ID2_MODEL_MAKE(10, 3)
diff --git a/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_regmap.h b/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_regmap.h
index 51024308fafe..d8066f43768b 100644
--- a/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_regmap.h
+++ b/drivers/gpu/arm/bifrost/gpu/mali_kbase_gpu_regmap.h
@@ -25,7 +25,11 @@
 
 #include "mali_kbase_gpu_coherency.h"
 #include "mali_kbase_gpu_id.h"
+#if MALI_USE_CSF
+#include "backend/mali_kbase_gpu_regmap_csf.h"
+#else
 #include "backend/mali_kbase_gpu_regmap_jm.h"
+#endif
 
 /* Begin Register Offsets */
 /* GPU control registers */
@@ -222,21 +226,7 @@
 
 /* End Register Offsets */
 
-/* IRQ flags */
-#define GPU_FAULT               (1 << 0)    /* A GPU Fault has occurred */
-#define MULTIPLE_GPU_FAULTS     (1 << 7)    /* More than one GPU Fault occurred. */
-#define RESET_COMPLETED         (1 << 8)    /* Set when a reset has completed. */
-#define POWER_CHANGED_SINGLE    (1 << 9)    /* Set when a single core has finished powering up or down. */
-#define POWER_CHANGED_ALL       (1 << 10)   /* Set when all cores have finished powering up or down. */
-
-#define PRFCNT_SAMPLE_COMPLETED (1 << 16)   /* Set when a performance count sample has completed. */
-#define CLEAN_CACHES_COMPLETED  (1 << 17)   /* Set when a cache clean operation has completed. */
-
-/* Include POWER_CHANGED_SINGLE in debug builds for use in irq latency test.
- */
-#define GPU_IRQ_REG_COMMON (GPU_FAULT | MULTIPLE_GPU_FAULTS | RESET_COMPLETED \
-		| POWER_CHANGED_ALL | PRFCNT_SAMPLE_COMPLETED)
-
+/* Include POWER_CHANGED_SINGLE in debug builds for use in irq latency test. */
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 #define GPU_IRQ_REG_ALL (GPU_IRQ_REG_COMMON | POWER_CHANGED_SINGLE)
 #else /* CONFIG_MALI_BIFROST_DEBUG */
@@ -290,6 +280,7 @@
 #define AS_FAULTSTATUS_EXCEPTION_TYPE_MASK (0xFF << AS_FAULTSTATUS_EXCEPTION_TYPE_SHIFT)
 #define AS_FAULTSTATUS_EXCEPTION_TYPE_GET(reg_val) \
 	(((reg_val)&AS_FAULTSTATUS_EXCEPTION_TYPE_MASK) >> AS_FAULTSTATUS_EXCEPTION_TYPE_SHIFT)
+#define AS_FAULTSTATUS_EXCEPTION_TYPE_TRANSLATION_FAULT_0 0xC0
 
 #define AS_FAULTSTATUS_ACCESS_TYPE_SHIFT 8
 #define AS_FAULTSTATUS_ACCESS_TYPE_MASK (0x3 << AS_FAULTSTATUS_ACCESS_TYPE_SHIFT)
diff --git a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c
index 0386b066da03..67adb65306dd 100644
--- a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c
+++ b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2016-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2016-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -49,7 +49,8 @@ static const struct kbase_ipa_model_ops *kbase_ipa_all_model_ops[] = {
 	&kbase_g51_ipa_model_ops,
 	&kbase_g77_ipa_model_ops,
 	&kbase_tnax_ipa_model_ops,
-	&kbase_tbex_ipa_model_ops
+	&kbase_tbex_ipa_model_ops,
+	&kbase_tbax_ipa_model_ops
 };
 
 int kbase_ipa_model_recalculate(struct kbase_ipa_model *model)
@@ -115,6 +116,8 @@ const char *kbase_ipa_model_name_from_id(u32 gpu_id)
 		return "mali-g77-power-model";
 	case GPU_ID2_PRODUCT_TBEX:
 		return "mali-tbex-power-model";
+	case GPU_ID2_PRODUCT_TBAX:
+		return "mali-tbax-power-model";
 	default:
 		return KBASE_IPA_FALLBACK_MODEL_NAME;
 	}
diff --git a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.h b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.h
index 30129e72a3a1..f43f3d9416b4 100644
--- a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.h
+++ b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2016-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2016-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -211,6 +211,7 @@ extern const struct kbase_ipa_model_ops kbase_g51_ipa_model_ops;
 extern const struct kbase_ipa_model_ops kbase_g77_ipa_model_ops;
 extern const struct kbase_ipa_model_ops kbase_tnax_ipa_model_ops;
 extern const struct kbase_ipa_model_ops kbase_tbex_ipa_model_ops;
+extern const struct kbase_ipa_model_ops kbase_tbax_ipa_model_ops;
 
 /**
  * kbase_get_real_power() - get the real power consumption of the GPU
diff --git a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_common.c b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_common.c
index 9fae8f1d0522..702db1623101 100644
--- a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_common.c
+++ b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_common.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2017-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2017-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -145,6 +145,9 @@ int kbase_ipa_attach_vinstr(struct kbase_ipa_model_vinstr_data *model_data)
 
 	kbase_hwcnt_enable_map_enable_all(&enable_map);
 
+	/* Disable cycle counter only. */
+	enable_map.clk_enable_map = 0;
+
 	errcode = kbase_hwcnt_virtualizer_client_create(
 		hvirt, &enable_map, &model_data->hvirt_cli);
 	kbase_hwcnt_enable_map_free(&enable_map);
diff --git a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_g7x.c b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_g7x.c
index 270b75e07b0d..83174eb66ded 100644
--- a/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_g7x.c
+++ b/drivers/gpu/arm/bifrost/ipa/mali_kbase_ipa_vinstr_g7x.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2016-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2016-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -415,6 +415,39 @@ static const struct kbase_ipa_group ipa_groups_def_tbex[] = {
 	},
 };
 
+static const struct kbase_ipa_group ipa_groups_def_tbax[] = {
+	{
+		.name = "l2_access",
+		.default_value = 599800,
+		.op = kbase_g7x_sum_all_memsys_blocks,
+		.counter_block_offset = MEMSYS_L2_ANY_LOOKUP,
+	},
+	{
+		.name = "exec_instr_msg",
+		.default_value = 1830200,
+		.op = kbase_g7x_sum_all_shader_cores,
+		.counter_block_offset = SC_EXEC_INSTR_MSG,
+	},
+	{
+		.name = "exec_instr_fma",
+		.default_value = 407300,
+		.op = kbase_g7x_sum_all_shader_cores,
+		.counter_block_offset = SC_EXEC_INSTR_FMA,
+	},
+	{
+		.name = "tex_filt_num_operations",
+		.default_value = 224500,
+		.op = kbase_g7x_sum_all_shader_cores,
+		.counter_block_offset = SC_TEX_FILT_NUM_OPERATIONS,
+	},
+	{
+		.name = "gpu_active",
+		.default_value = 153800,
+		.op = kbase_g7x_jm_single_counter,
+		.counter_block_offset = JM_GPU_ACTIVE,
+	},
+};
+
 
 #define IPA_POWER_MODEL_OPS(gpu, init_token) \
 	const struct kbase_ipa_model_ops kbase_ ## gpu ## _ipa_model_ops = { \
@@ -449,6 +482,7 @@ STANDARD_POWER_MODEL(g52_r1, 1000);
 STANDARD_POWER_MODEL(g51, 1000);
 STANDARD_POWER_MODEL(g77, 1000);
 STANDARD_POWER_MODEL(tbex, 1000);
+STANDARD_POWER_MODEL(tbax, 1000);
 
 /* g52 is an alias of g76 (TNOX) for IPA */
 ALIAS_POWER_MODEL(g52, g76);
diff --git a/drivers/gpu/arm/bifrost/jm/mali_base_jm_kernel.h b/drivers/gpu/arm/bifrost/jm/mali_base_jm_kernel.h
index 879a436152d9..9367cc5431cf 100644
--- a/drivers/gpu/arm/bifrost/jm/mali_base_jm_kernel.h
+++ b/drivers/gpu/arm/bifrost/jm/mali_base_jm_kernel.h
@@ -80,7 +80,8 @@
  */
 #define BASE_MEM_COHERENT_LOCAL ((base_mem_alloc_flags)1 << 11)
 
-/* Should be cached on the CPU
+/* IN/OUT */
+/* Should be cached on the CPU, returned if actually cached
  */
 #define BASE_MEM_CACHED_CPU ((base_mem_alloc_flags)1 << 12)
 
@@ -155,18 +156,25 @@
 /* Use the GPU VA chosen by the kernel client */
 #define BASE_MEM_FLAG_MAP_FIXED ((base_mem_alloc_flags)1 << 27)
 
+/* OUT */
+/* Kernel side cache sync ops required */
+#define BASE_MEM_KERNEL_SYNC ((base_mem_alloc_flags)1 << 28)
+
+/* Force trimming of JIT allocations when creating a new allocation */
+#define BASEP_MEM_PERFORM_JIT_TRIM ((base_mem_alloc_flags)1 << 29)
+
 /* Number of bits used as flags for base memory management
  *
  * Must be kept in sync with the base_mem_alloc_flags flags
  */
-#define BASE_MEM_FLAGS_NR_BITS 28
+#define BASE_MEM_FLAGS_NR_BITS 30
 
 /* A mask of all the flags which are only valid for allocations within kbase,
  * and may not be passed from user space.
  */
 #define BASEP_MEM_FLAGS_KERNEL_ONLY \
 	(BASEP_MEM_PERMANENT_KERNEL_MAPPING | BASEP_MEM_NO_USER_FREE | \
-	 BASE_MEM_FLAG_MAP_FIXED)
+	 BASE_MEM_FLAG_MAP_FIXED | BASEP_MEM_PERFORM_JIT_TRIM)
 
 /* A mask for all output bits, excluding IN/OUT bits.
  */
@@ -192,6 +200,28 @@
 #define BASE_MEM_FIRST_FREE_ADDRESS            ((BITS_PER_LONG << 12) + \
 						BASE_MEM_COOKIE_BASE)
 
+/* Similar to BASE_MEM_TILER_ALIGN_TOP, memory starting from the end of the
+ * initial commit is aligned to 'extent' pages, where 'extent' must be a power
+ * of 2 and no more than BASE_MEM_TILER_ALIGN_TOP_EXTENT_MAX_PAGES
+ */
+#define BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP  (1 << 0)
+
+/**
+ * If set, the heap info address points to a u32 holding the used size in bytes;
+ * otherwise it points to a u64 holding the lowest address of unused memory.
+ */
+#define BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE  (1 << 1)
+
+/**
+ * Valid set of just-in-time memory allocation flags
+ *
+ * Note: BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE cannot be set if heap_info_gpu_addr
+ * in %base_jit_alloc_info is 0 (atom with BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE set
+ * and heap_info_gpu_addr being 0 will be rejected).
+ */
+#define BASE_JIT_ALLOC_VALID_FLAGS \
+	(BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP | BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE)
+
 /**
  * typedef base_context_create_flags - Flags to pass to ::base_context_init.
  *
@@ -787,6 +817,54 @@ struct base_jd_atom_v2 {
 	u8 padding[7];
 };
 
+/**
+ * struct base_jd_atom - Same as base_jd_atom_v2, but has an extra seq_nr
+ *                          at the beginning.
+ *
+ * @seq_nr:        Sequence number of logical grouping of atoms.
+ * @jc:            GPU address of a job chain or (if BASE_JD_REQ_END_RENDERPASS
+ *                 is set in the base_jd_core_req) the CPU address of a
+ *                 base_jd_fragment object.
+ * @udata:         User data.
+ * @extres_list:   List of external resources.
+ * @nr_extres:     Number of external resources or JIT allocations.
+ * @jit_id:        Zero-terminated array of IDs of just-in-time memory
+ *                 allocations written to by the atom. When the atom
+ *                 completes, the value stored at the
+ *                 &struct_base_jit_alloc_info.heap_info_gpu_addr of
+ *                 each allocation is read in order to enforce an
+ *                 overall physical memory usage limit.
+ * @pre_dep:       Pre-dependencies. One need to use SETTER function to assign
+ *                 this field; this is done in order to reduce possibility of
+ *                 improper assignment of a dependency field.
+ * @atom_number:   Unique number to identify the atom.
+ * @prio:          Atom priority. Refer to base_jd_prio for more details.
+ * @device_nr:     Core group when BASE_JD_REQ_SPECIFIC_COHERENT_GROUP
+ *                 specified.
+ * @jobslot:       Job slot to use when BASE_JD_REQ_JOB_SLOT is specified.
+ * @core_req:      Core requirements.
+ * @renderpass_id: Renderpass identifier used to associate an atom that has
+ *                 BASE_JD_REQ_START_RENDERPASS set in its core requirements
+ *                 with an atom that has BASE_JD_REQ_END_RENDERPASS set.
+ * @padding:       Unused. Must be zero.
+ */
+typedef struct base_jd_atom {
+	u64 seq_nr;
+	u64 jc;
+	struct base_jd_udata udata;
+	u64 extres_list;
+	u16 nr_extres;
+	u8 jit_id[2];
+	struct base_dependency pre_dep[2];
+	base_atom_id atom_number;
+	base_jd_prio prio;
+	u8 device_nr;
+	u8 jobslot;
+	base_jd_core_req core_req;
+	u8 renderpass_id;
+	u8 padding[7];
+} base_jd_atom;
+
 /* Job chain event code bits
  * Defines the bits used to create ::base_jd_event_code
  */
@@ -982,7 +1060,7 @@ struct base_jd_event_v2 {
  *                                     jobs.
  *
  * This structure is stored into the memory pointed to by the @jc field
- * of &struct base_jd_atom_v2.
+ * of &struct base_jd_atom.
  *
  * It must not occupy the same CPU cache line(s) as any neighboring data.
  * This is to avoid cases where access to pages containing the structure
diff --git a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h
index 5f767109a511..4fb5d1d9c410 100644
--- a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h
+++ b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_defs.h
@@ -496,9 +496,9 @@ struct kbase_jd_atom {
 	struct list_head jd_item;
 	bool in_jd_list;
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	u8 jit_ids[2];
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	u16 nr_extres;
 	struct kbase_ext_res *extres;
@@ -608,6 +608,9 @@ struct kbase_jd_atom {
 
 	atomic_t blocked;
 
+	/* user-space sequence number, to order atoms in some temporal order */
+	u64 seq_nr;
+
 	struct kbase_jd_atom *pre_dep;
 	struct kbase_jd_atom *post_dep;
 
@@ -815,4 +818,27 @@ struct jsctx_queue {
 	struct list_head x_dep_head;
 };
 
+/**
+ * struct kbase_as   - Object representing an address space of GPU.
+ * @number:            Index at which this address space structure is present
+ *                     in an array of address space structures embedded inside
+ *                     the &struct kbase_device.
+ * @pf_wq:             Workqueue for processing work items related to
+ *                     Page fault and Bus fault handling.
+ * @work_pagefault:    Work item for the Page fault handling.
+ * @work_busfault:     Work item for the Bus fault handling.
+ * @pf_data:           Data relating to Page fault.
+ * @bf_data:           Data relating to Bus fault.
+ * @current_setup:     Stores the MMU configuration for this address space.
+ */
+struct kbase_as {
+	int number;
+	struct workqueue_struct *pf_wq;
+	struct work_struct work_pagefault;
+	struct work_struct work_busfault;
+	struct kbase_fault pf_data;
+	struct kbase_fault bf_data;
+	struct kbase_mmu_setup current_setup;
+};
+
 #endif /* _KBASE_JM_DEFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h
index 408e98e33519..305a9eb221ae 100644
--- a/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h
+++ b/drivers/gpu/arm/bifrost/jm/mali_kbase_jm_ioctl.h
@@ -94,16 +94,54 @@
  * - The above changes are checked for safe values in usual builds
  * 11.21:
  * - v2.0 of mali_trace debugfs file, which now versions the file separately
+ * 11.22:
+ * - Added base_jd_atom (v3), which is seq_nr + base_jd_atom_v2.
+ *   KBASE_IOCTL_JOB_SUBMIT supports both in parallel.
+ * 11.23:
+ * - Modified KBASE_IOCTL_MEM_COMMIT behavior to reject requests to modify
+ *   the physical memory backing of JIT allocations. This was not supposed
+ *   to be a valid use case, but it was allowed by the previous implementation.
+ * 11.24:
+ * - Added a sysfs file 'serialize_jobs' inside a new sub-directory
+ *   'scheduling'.
+ * 11.25:
+ * - Enabled JIT pressure limit in base/kbase by default
+ * 11.26:
+ * - Added kinstr_jm API
+ * 11.27:
+ * - Backwards compatible extension to HWC ioctl.
+ * 11.28:
+ * - Added kernel side cache ops needed hint
+ * 11.29:
+ * - Reserve ioctl 52
  */
 #define BASE_UK_VERSION_MAJOR 11
-#define BASE_UK_VERSION_MINOR 21
+#define BASE_UK_VERSION_MINOR 29
+
+/**
+ * struct kbase_ioctl_version_check - Check version compatibility between
+ * kernel and userspace
+ *
+ * @major: Major version number
+ * @minor: Minor version number
+ */
+struct kbase_ioctl_version_check {
+	__u16 major;
+	__u16 minor;
+};
+
+#define KBASE_IOCTL_VERSION_CHECK \
+	_IOWR(KBASE_IOCTL_TYPE, 0, struct kbase_ioctl_version_check)
+
+#define KBASE_IOCTL_VERSION_CHECK_RESERVED \
+	_IOWR(KBASE_IOCTL_TYPE, 52, struct kbase_ioctl_version_check)
 
 /**
  * struct kbase_ioctl_job_submit - Submit jobs/atoms to the kernel
  *
- * @addr: Memory address of an array of struct base_jd_atom_v2
+ * @addr: Memory address of an array of struct base_jd_atom_v2 or v3
  * @nr_atoms: Number of entries in the array
- * @stride: sizeof(struct base_jd_atom_v2)
+ * @stride: sizeof(struct base_jd_atom_v2) or sizeof(struct base_jd_atom)
  */
 struct kbase_ioctl_job_submit {
 	__u64 addr;
@@ -132,5 +170,47 @@ struct kbase_ioctl_soft_event_update {
 #define KBASE_IOCTL_SOFT_EVENT_UPDATE \
 	_IOW(KBASE_IOCTL_TYPE, 28, struct kbase_ioctl_soft_event_update)
 
+/**
+ * struct kbase_kinstr_jm_fd_out - Explains the compatibility information for
+ * the `struct kbase_kinstr_jm_atom_state_change` structure returned from the
+ * kernel
+ *
+ * @size:    The size of the `struct kbase_kinstr_jm_atom_state_change`
+ * @version: Represents a breaking change in the
+ *           `struct kbase_kinstr_jm_atom_state_change`
+ * @padding: Explicit padding to get the structure up to 64bits. See
+ * https://www.kernel.org/doc/Documentation/ioctl/botching-up-ioctls.rst
+ *
+ * The `struct kbase_kinstr_jm_atom_state_change` may have extra members at the
+ * end of the structure that older user space might not understand. If the
+ * `version` is the same, the structure is still compatible with newer kernels.
+ * The `size` can be used to cast the opaque memory returned from the kernel.
+ */
+struct kbase_kinstr_jm_fd_out {
+	__u16 size;
+	__u8 version;
+	__u8 padding[5];
+};
+
+/**
+ * struct kbase_kinstr_jm_fd_in - Options when creating the file descriptor
+ *
+ * @count: Number of atom states that can be stored in the kernel circular
+ *         buffer. Must be a power of two
+ * @padding: Explicit padding to get the structure up to 64bits. See
+ * https://www.kernel.org/doc/Documentation/ioctl/botching-up-ioctls.rst
+ */
+struct kbase_kinstr_jm_fd_in {
+	__u16 count;
+	__u8 padding[6];
+};
+
+union kbase_kinstr_jm_fd {
+	struct kbase_kinstr_jm_fd_in in;
+	struct kbase_kinstr_jm_fd_out out;
+};
+
+#define KBASE_IOCTL_KINSTR_JM_FD \
+	_IOWR(KBASE_IOCTL_TYPE, 51, union kbase_kinstr_jm_fd)
 
 #endif /* _KBASE_JM_IOCTL_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h b/drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h
index 6885f8d58066..0dc08381bee6 100644
--- a/drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h
+++ b/drivers/gpu/arm/bifrost/mali_base_hwconfig_features.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -313,6 +313,35 @@ static const enum base_hw_feature base_hw_features_tBEx[] = {
 	BASE_HW_FEATURE_END
 };
 
+static const enum base_hw_feature base_hw_features_tBAx[] = {
+	BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION,
+	BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS,
+	BASE_HW_FEATURE_XAFFINITY,
+	BASE_HW_FEATURE_WARPING,
+	BASE_HW_FEATURE_INTERPIPE_REG_ALIASING,
+	BASE_HW_FEATURE_32_BIT_UNIFORM_ADDRESS,
+	BASE_HW_FEATURE_ATTR_AUTO_TYPE_INFERRAL,
+	BASE_HW_FEATURE_BRNDOUT_CC,
+	BASE_HW_FEATURE_BRNDOUT_KILL,
+	BASE_HW_FEATURE_LD_ST_LEA_TEX,
+	BASE_HW_FEATURE_LD_ST_TILEBUFFER,
+	BASE_HW_FEATURE_LINEAR_FILTER_FLOAT,
+	BASE_HW_FEATURE_MRT,
+	BASE_HW_FEATURE_MSAA_16X,
+	BASE_HW_FEATURE_NEXT_INSTRUCTION_TYPE,
+	BASE_HW_FEATURE_OUT_OF_ORDER_EXEC,
+	BASE_HW_FEATURE_T7XX_PAIRING_RULES,
+	BASE_HW_FEATURE_TEST4_DATUM_MODE,
+	BASE_HW_FEATURE_FLUSH_REDUCTION,
+	BASE_HW_FEATURE_PROTECTED_DEBUG_MODE,
+	BASE_HW_FEATURE_COHERENCY_REG,
+	BASE_HW_FEATURE_AARCH64_MMU,
+	BASE_HW_FEATURE_IDVS_GROUP_SIZE,
+	BASE_HW_FEATURE_L2_CONFIG,
+	BASE_HW_FEATURE_CLEAN_ONLY_SAFE,
+	BASE_HW_FEATURE_END
+};
+
 static const enum base_hw_feature base_hw_features_tDUx[] = {
 	BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION,
 	BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS,
diff --git a/drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h b/drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h
index 3966069178c1..c1ad3ac40705 100644
--- a/drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h
+++ b/drivers/gpu/arm/bifrost/mali_base_hwconfig_issues.h
@@ -59,6 +59,7 @@ enum base_hw_issue {
 	BASE_HW_ISSUE_TTRX_3470,
 	BASE_HW_ISSUE_TTRX_3464,
 	BASE_HW_ISSUE_TTRX_3485,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -532,6 +533,44 @@ static const enum base_hw_issue base_hw_issues_lBEx_r1p1[] = {
 	BASE_HW_ISSUE_END
 };
 
+static const enum base_hw_issue base_hw_issues_tBAx_r0p0[] = {
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_END
+};
+
+static const enum base_hw_issue base_hw_issues_tBAx_r1p0[] = {
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_TTRX_2968_TTRX_3162,
+	BASE_HW_ISSUE_TTRX_921,
+	BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_END
+};
+
+static const enum base_hw_issue base_hw_issues_model_tBAx[] = {
+	BASE_HW_ISSUE_5736,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_TSIX_2033,
+	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_TTRX_3414,
+	BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_TTRX_3470,
+	BASE_HW_ISSUE_TTRX_3464,
+	BASE_HW_ISSUE_END
+};
+
 static const enum base_hw_issue base_hw_issues_tDUx_r0p0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
@@ -556,6 +595,7 @@ static const enum base_hw_issue base_hw_issues_tODx_r0p0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -564,6 +604,7 @@ static const enum base_hw_issue base_hw_issues_model_tODx[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -571,6 +612,7 @@ static const enum base_hw_issue base_hw_issues_tGRx_r0p0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -579,6 +621,7 @@ static const enum base_hw_issue base_hw_issues_model_tGRx[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -586,6 +629,7 @@ static const enum base_hw_issue base_hw_issues_tVAx_r0p0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -594,6 +638,7 @@ static const enum base_hw_issue base_hw_issues_model_tVAx[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -601,6 +646,7 @@ static const enum base_hw_issue base_hw_issues_tTUx_r0p0[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -609,6 +655,7 @@ static const enum base_hw_issue base_hw_issues_model_tTUx[] = {
 	BASE_HW_ISSUE_9435,
 	BASE_HW_ISSUE_TSIX_2033,
 	BASE_HW_ISSUE_TTRX_1337,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -619,6 +666,7 @@ static const enum base_hw_issue base_hw_issues_tE2x_r0p0[] = {
 	BASE_HW_ISSUE_TTRX_921,
 	BASE_HW_ISSUE_TTRX_3414,
 	BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
@@ -629,6 +677,7 @@ static const enum base_hw_issue base_hw_issues_model_tE2x[] = {
 	BASE_HW_ISSUE_TTRX_1337,
 	BASE_HW_ISSUE_TTRX_3414,
 	BASE_HW_ISSUE_TTRX_3083,
+	BASE_HW_ISSUE_GPU2019_3212,
 	BASE_HW_ISSUE_END
 };
 
diff --git a/drivers/gpu/arm/bifrost/mali_base_kernel.h b/drivers/gpu/arm/bifrost/mali_base_kernel.h
index 1e2744d0fe8a..086171adb6e5 100644
--- a/drivers/gpu/arm/bifrost/mali_base_kernel.h
+++ b/drivers/gpu/arm/bifrost/mali_base_kernel.h
@@ -213,28 +213,6 @@ struct base_mem_aliasing_info {
  */
 #define BASE_JIT_ALLOC_COUNT (255)
 
-/* Similar to BASE_MEM_TILER_ALIGN_TOP, memory starting from the end of the
- * initial commit is aligned to 'extent' pages, where 'extent' must be a power
- * of 2 and no more than BASE_MEM_TILER_ALIGN_TOP_EXTENT_MAX_PAGES
- */
-#define BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP  (1 << 0)
-
-/**
- * If set, the heap info address points to a u32 holding the used size in bytes;
- * otherwise it points to a u64 holding the lowest address of unused memory.
- */
-#define BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE  (1 << 1)
-
-/**
- * Valid set of just-in-time memory allocation flags
- *
- * Note: BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE cannot be set if heap_info_gpu_addr
- * in %base_jit_alloc_info is 0 (atom with BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE set
- * and heap_info_gpu_addr being 0 will be rejected).
- */
-#define BASE_JIT_ALLOC_VALID_FLAGS \
-	(BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP | BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE)
-
 /* base_jit_alloc_info in use for kernel driver versions 10.2 to early 11.5
  *
  * jit_version is 1
@@ -719,7 +697,11 @@ struct base_gpu_props {
 	struct mali_base_gpu_coherent_group_info coherency_info;
 };
 
+#if MALI_USE_CSF
+#include "csf/mali_base_csf_kernel.h"
+#else
 #include "jm/mali_base_jm_kernel.h"
+#endif
 
 /**
  * base_mem_group_id_get() - Get group ID from flags
@@ -751,8 +733,10 @@ static inline int base_mem_group_id_get(base_mem_alloc_flags flags)
  */
 static inline base_mem_alloc_flags base_mem_group_id_set(int id)
 {
-	LOCAL_ASSERT(id >= 0);
-	LOCAL_ASSERT(id < BASE_MEM_GROUP_COUNT);
+	if ((id < 0) || (id >= BASE_MEM_GROUP_COUNT)) {
+		/* Set to default value when id is out of range. */
+		id = BASE_MEM_GROUP_DEFAULT;
+	}
 
 	return ((base_mem_alloc_flags)id << BASEP_MEM_GROUP_ID_SHIFT) &
 		BASE_MEM_GROUP_ID_MASK;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase.h b/drivers/gpu/arm/bifrost/mali_kbase.h
index dd49495c7133..8189d02ab910 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase.h
@@ -46,6 +46,7 @@
 #include <linux/vmalloc.h>
 #include <linux/wait.h>
 #include <linux/workqueue.h>
+#include <linux/interrupt.h>
 
 #include "mali_base_kernel.h"
 #include <mali_kbase_linux.h>
@@ -67,10 +68,12 @@
 #include "mali_kbase_mem_profile_debugfs.h"
 #include "mali_kbase_gpuprops.h"
 #include "mali_kbase_ioctl.h"
+#if !MALI_USE_CSF
 #include "mali_kbase_debug_job_fault.h"
 #include "mali_kbase_jd_debugfs.h"
 #include "mali_kbase_jm.h"
 #include "mali_kbase_js.h"
+#endif /* !MALI_USE_CSF */
 
 #include "ipa/mali_kbase_ipa.h"
 
@@ -80,12 +83,24 @@
 
 #include "mali_linux_trace.h"
 
+#if MALI_USE_CSF
+#include "csf/mali_kbase_csf.h"
+#endif
 
 #ifndef u64_to_user_ptr
 /* Introduced in Linux v4.6 */
 #define u64_to_user_ptr(x) ((void __user *)(uintptr_t)x)
 #endif
 
+#if MALI_USE_CSF
+/* Physical memory group ID for command stream frontend user I/O.
+ */
+#define KBASE_MEM_GROUP_CSF_IO BASE_MEM_GROUP_DEFAULT
+
+/* Physical memory group ID for command stream frontend firmware.
+ */
+#define KBASE_MEM_GROUP_CSF_FW BASE_MEM_GROUP_DEFAULT
+#endif
 
 /* Physical memory group ID for a special page which can alias several regions.
  */
@@ -206,6 +221,7 @@ int buslog_init(struct kbase_device *kbdev);
 void buslog_term(struct kbase_device *kbdev);
 #endif
 
+#if !MALI_USE_CSF
 int kbase_jd_init(struct kbase_context *kctx);
 void kbase_jd_exit(struct kbase_context *kctx);
 
@@ -213,9 +229,9 @@ void kbase_jd_exit(struct kbase_context *kctx);
  * kbase_jd_submit - Submit atoms to the job dispatcher
  *
  * @kctx: The kbase context to submit to
- * @user_addr: The address in user space of the struct base_jd_atom_v2 array
+ * @user_addr: The address in user space of the struct base_jd_atom array
  * @nr_atoms: The number of atoms in the array
- * @stride: sizeof(struct base_jd_atom_v2)
+ * @stride: sizeof(struct base_jd_atom)
  * @uk6_atom: true if the atoms are legacy atoms (struct base_jd_atom_v2_uk6)
  *
  * Return: 0 on success or error code
@@ -306,9 +322,12 @@ void kbase_job_check_enter_disjoint(struct kbase_device *kbdev, u32 action,
 void kbase_job_check_leave_disjoint(struct kbase_device *kbdev,
 		struct kbase_jd_atom *target_katom);
 
+#endif /* !MALI_USE_CSF */
 
 void kbase_event_post(struct kbase_context *ctx, struct kbase_jd_atom *event);
+#if !MALI_USE_CSF
 int kbase_event_dequeue(struct kbase_context *ctx, struct base_jd_event_v2 *uevent);
+#endif /* !MALI_USE_CSF */
 int kbase_event_pending(struct kbase_context *ctx);
 int kbase_event_init(struct kbase_context *kctx);
 void kbase_event_close(struct kbase_context *kctx);
@@ -372,6 +391,7 @@ static inline void kbase_free_user_buffer(
  */
 int kbase_mem_copy_from_extres(struct kbase_context *kctx,
 		struct kbase_debug_copy_buffer *buf_data);
+#if !MALI_USE_CSF
 int kbase_process_soft_job(struct kbase_jd_atom *katom);
 int kbase_prepare_soft_job(struct kbase_jd_atom *katom);
 void kbase_finish_soft_job(struct kbase_jd_atom *katom);
@@ -387,6 +407,7 @@ int kbase_soft_event_update(struct kbase_context *kctx,
 
 void kbasep_soft_job_timeout_worker(struct timer_list *timer);
 void kbasep_complete_triggered_soft_events(struct kbase_context *kctx, u64 evt);
+#endif /* !MALI_USE_CSF */
 
 void kbasep_as_do_poke(struct work_struct *work);
 
@@ -419,7 +440,23 @@ static inline bool kbase_pm_is_suspending(struct kbase_device *kbdev)
  */
 static inline bool kbase_pm_is_gpu_lost(struct kbase_device *kbdev)
 {
-	return kbdev->pm.gpu_lost;
+	return (atomic_read(&kbdev->pm.gpu_lost) == 0 ? false : true);
+}
+
+/*
+ * Set or clear gpu lost state
+ *
+ * @kbdev: The kbase device structure for the device (must be a valid pointer)
+ * @gpu_lost: true to activate GPU lost state, FALSE is deactive it
+ *
+ * Puts power management code into gpu lost state or takes it out of the
+ * state.  Once in gpu lost state new GPU jobs will no longer be
+ * scheduled.
+ */
+static inline void kbase_pm_set_gpu_lost(struct kbase_device *kbdev,
+	bool gpu_lost)
+{
+	atomic_set(&kbdev->pm.gpu_lost, (gpu_lost ? 1 : 0));
 }
 #endif
 
@@ -455,9 +492,10 @@ void kbase_pm_metrics_start(struct kbase_device *kbdev);
  */
 void kbase_pm_metrics_stop(struct kbase_device *kbdev);
 
+#if !MALI_USE_CSF
 /**
  * Return the atom's ID, as was originally supplied by userspace in
- * base_jd_atom_v2::atom_number
+ * base_jd_atom::atom_number
  */
 static inline int kbase_jd_atom_id(struct kbase_context *kctx, struct kbase_jd_atom *katom)
 {
@@ -484,6 +522,7 @@ static inline struct kbase_jd_atom *kbase_jd_atom_from_id(
 {
 	return &kctx->jctx.atoms[id];
 }
+#endif /* !MALI_USE_CSF */
 
 /**
  * Initialize the disjoint state
@@ -572,53 +611,4 @@ void kbase_disjoint_state_down(struct kbase_device *kbdev);
 	#define UINT64_MAX ((uint64_t)0xFFFFFFFFFFFFFFFFULL)
 #endif
 
-#if defined(CONFIG_DEBUG_FS) && !defined(CONFIG_MALI_BIFROST_NO_MALI)
-
-/* kbase_io_history_init - initialize data struct for register access history
- *
- * @kbdev The register history to initialize
- * @n The number of register accesses that the buffer could hold
- *
- * @return 0 if successfully initialized, failure otherwise
- */
-int kbase_io_history_init(struct kbase_io_history *h, u16 n);
-
-/* kbase_io_history_term - uninit all resources for the register access history
- *
- * @h The register history to terminate
- */
-void kbase_io_history_term(struct kbase_io_history *h);
-
-/* kbase_io_history_dump - print the register history to the kernel ring buffer
- *
- * @kbdev Pointer to kbase_device containing the register history to dump
- */
-void kbase_io_history_dump(struct kbase_device *kbdev);
-
-/**
- * kbase_io_history_resize - resize the register access history buffer.
- *
- * @h: Pointer to a valid register history to resize
- * @new_size: Number of accesses the buffer could hold
- *
- * A successful resize will clear all recent register accesses.
- * If resizing fails for any reason (e.g., could not allocate memory, invalid
- * buffer size) then the original buffer will be kept intact.
- *
- * @return 0 if the buffer was resized, failure otherwise
- */
-int kbase_io_history_resize(struct kbase_io_history *h, u16 new_size);
-
-#else /* CONFIG_DEBUG_FS */
-
-#define kbase_io_history_init(...) ((int)0)
-
-#define kbase_io_history_term CSTD_NOP
-
-#define kbase_io_history_dump CSTD_NOP
-
-#define kbase_io_history_resize CSTD_NOP
-
-#endif /* CONFIG_DEBUG_FS */
-
 #endif
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_caps.h b/drivers/gpu/arm/bifrost/mali_kbase_caps.h
new file mode 100644
index 000000000000..b201a60fa6e3
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_caps.h
@@ -0,0 +1,65 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+
+/**
+ * @file mali_kbase_caps.h
+ *
+ * Driver Capability Queries.
+ */
+
+#ifndef _KBASE_CAPS_H_
+#define _KBASE_CAPS_H_
+
+#include <linux/types.h>
+
+typedef enum mali_kbase_cap {
+	MALI_KBASE_CAP_SYSTEM_MONITOR = 0,
+	MALI_KBASE_CAP_JIT_PRESSURE_LIMIT,
+	MALI_KBASE_CAP_MEM_GROW_ON_GPF,
+	MALI_KBASE_CAP_MEM_PROTECTED,
+	MALI_KBASE_NUM_CAPS
+} mali_kbase_cap;
+
+extern bool mali_kbase_supports_cap(unsigned long api_version, mali_kbase_cap cap);
+
+static inline bool mali_kbase_supports_system_monitor(unsigned long api_version)
+{
+	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_SYSTEM_MONITOR);
+}
+
+static inline bool mali_kbase_supports_jit_pressure_limit(unsigned long api_version)
+{
+	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_JIT_PRESSURE_LIMIT);
+}
+
+static inline bool mali_kbase_supports_mem_grow_on_gpf(unsigned long api_version)
+{
+	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_MEM_GROW_ON_GPF);
+}
+
+static inline bool mali_kbase_supports_mem_protected(unsigned long api_version)
+{
+	return mali_kbase_supports_cap(api_version, MALI_KBASE_CAP_MEM_PROTECTED);
+}
+
+#endif	/* __KBASE_CAPS_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ccswe.c b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.c
new file mode 100644
index 000000000000..87d5aaa6bb5d
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.c
@@ -0,0 +1,105 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_ccswe.h"
+#include "mali_kbase_linux.h"
+
+#include <linux/math64.h>
+#include <linux/time.h>
+
+static u64 kbasep_ccswe_cycle_at_no_lock(
+	struct kbase_ccswe *self, u64 timestamp_ns)
+{
+	s64 diff_s, diff_ns;
+	u32 gpu_freq;
+
+	lockdep_assert_held(&self->access);
+
+	diff_ns = timestamp_ns - self->timestamp_ns;
+	gpu_freq = diff_ns > 0 ? self->gpu_freq : self->prev_gpu_freq;
+
+	diff_s = div_s64(diff_ns, NSEC_PER_SEC);
+	diff_ns -= diff_s * NSEC_PER_SEC;
+
+	return self->cycles_elapsed + diff_s * gpu_freq
+		+ div_s64(diff_ns * gpu_freq, NSEC_PER_SEC);
+}
+
+void kbase_ccswe_init(struct kbase_ccswe *self)
+{
+	memset(self, 0, sizeof(*self));
+
+	spin_lock_init(&self->access);
+}
+KBASE_EXPORT_TEST_API(kbase_ccswe_init);
+
+u64 kbase_ccswe_cycle_at(struct kbase_ccswe *self, u64 timestamp_ns)
+{
+	unsigned long flags;
+	u64 result;
+
+	spin_lock_irqsave(&self->access, flags);
+	result = kbasep_ccswe_cycle_at_no_lock(self, timestamp_ns);
+	spin_unlock_irqrestore(&self->access, flags);
+
+	return result;
+}
+KBASE_EXPORT_TEST_API(kbase_ccswe_cycle_at);
+
+void kbase_ccswe_freq_change(
+	struct kbase_ccswe *self, u64 timestamp_ns, u32 gpu_freq)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&self->access, flags);
+
+	/* The time must go only forward. */
+	if (WARN_ON(timestamp_ns < self->timestamp_ns))
+		goto exit;
+
+	/* If this is the first frequency change, cycles_elapsed is zero. */
+	if (self->timestamp_ns)
+		self->cycles_elapsed = kbasep_ccswe_cycle_at_no_lock(
+			self, timestamp_ns);
+
+	self->timestamp_ns = timestamp_ns;
+	self->prev_gpu_freq = self->gpu_freq;
+	self->gpu_freq = gpu_freq;
+exit:
+	spin_unlock_irqrestore(&self->access, flags);
+}
+KBASE_EXPORT_TEST_API(kbase_ccswe_freq_change);
+
+void kbase_ccswe_reset(struct kbase_ccswe *self)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&self->access, flags);
+
+	self->timestamp_ns = 0;
+	self->cycles_elapsed = 0;
+	self->gpu_freq = 0;
+	self->prev_gpu_freq = 0;
+
+	spin_unlock_irqrestore(&self->access, flags);
+}
+
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ccswe.h b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.h
new file mode 100644
index 000000000000..3a7cf73d9eac
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ccswe.h
@@ -0,0 +1,97 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_CCSWE_H_
+#define _KBASE_CCSWE_H_
+
+#include <linux/spinlock.h>
+
+/**
+ * struct kbase_ccswe - Cycle count software estimator.
+ *
+ * @access:         Spinlock protecting this structure access.
+ * @timestamp_ns:   Timestamp(ns) when the last frequency change
+ *                  occurred.
+ * @cycles_elapsed: Number of cycles elapsed before the last frequency
+ *                  change
+ * @gpu_freq:       Current GPU frequency(Hz) value.
+ * @prev_gpu_freq:  Previous GPU frequency(Hz) before the last frequency
+ *                  change.
+ */
+struct kbase_ccswe {
+	spinlock_t access;
+	u64 timestamp_ns;
+	u64 cycles_elapsed;
+	u32 gpu_freq;
+	u32 prev_gpu_freq;
+};
+
+/**
+ * kbase_ccswe_init() - initialize the cycle count estimator.
+ *
+ * @self: Cycles count software estimator instance.
+ */
+void kbase_ccswe_init(struct kbase_ccswe *self);
+
+
+/**
+ * kbase_ccswe_cycle_at() - Estimate cycle count at given timestamp.
+ *
+ * @self: Cycles count software estimator instance.
+ * @timestamp_ns: The timestamp(ns) for cycle count estimation.
+ *
+ * The timestamp must be bigger than the timestamp of the penultimate
+ * frequency change. If only one frequency change occurred, the
+ * timestamp must be bigger than the timestamp of the frequency change.
+ * This is to allow the following code to be executed w/o synchronization.
+ * If lines below executed atomically, it is safe to assume that only
+ * one frequency change may happen in between.
+ *
+ *     u64 ts = ktime_get_raw_ns();
+ *     u64 cycle = kbase_ccswe_cycle_at(&ccswe, ts)
+ *
+ * Returns: estimated value of cycle count at a given time.
+ */
+u64 kbase_ccswe_cycle_at(struct kbase_ccswe *self, u64 timestamp_ns);
+
+/**
+ * kbase_ccswe_freq_change() - update GPU frequency.
+ *
+ * @self:         Cycles count software estimator instance.
+ * @timestamp_ns: Timestamp(ns) when frequency change occurred.
+ * @gpu_freq:     New GPU frequency value.
+ *
+ * The timestamp must be bigger than the timestamp of the previous
+ * frequency change. The function is to be called at the frequency
+ * change moment (not later).
+ */
+void kbase_ccswe_freq_change(
+	struct kbase_ccswe *self, u64 timestamp_ns, u32 gpu_freq);
+
+/**
+ * kbase_ccswe_reset() - reset estimator state
+ *
+ * @self:    Cycles count software estimator instance.
+ */
+void kbase_ccswe_reset(struct kbase_ccswe *self);
+
+#endif /* _KBASE_CCSWE_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_config.h b/drivers/gpu/arm/bifrost/mali_kbase_config.h
index 69723eaf188e..57456e2b90db 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_config.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_config.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017, 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2017, 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -223,6 +223,88 @@ struct kbase_pm_callback_conf {
 	int (*soft_reset_callback)(struct kbase_device *kbdev);
 };
 
+/* struct kbase_gpu_clk_notifier_data - Data for clock rate change notifier.
+ *
+ * Pointer to this structure is supposed to be passed to the gpu clock rate
+ * change notifier function. This structure is deliberately aligned with the
+ * common clock framework notification structure 'struct clk_notifier_data'
+ * and such alignment should be maintained.
+ *
+ * @gpu_clk_handle: Handle of the GPU clock for which notifier was registered.
+ * @old_rate:       Previous rate of this GPU clock.
+ * @new_rate:       New rate of this GPU clock.
+ */
+struct kbase_gpu_clk_notifier_data {
+	void *gpu_clk_handle;
+	unsigned long old_rate;
+	unsigned long new_rate;
+};
+
+/**
+ * kbase_clk_rate_trace_op_conf - Specifies GPU clock rate trace operations.
+ *
+ * Specifies the functions pointers for platform specific GPU clock rate trace
+ * operations. By default no functions are required.
+ */
+struct kbase_clk_rate_trace_op_conf {
+	/**
+	 * enumerate_gpu_clk - Enumerate a GPU clock on the given index
+	 * @kbdev - kbase_device pointer
+	 * @index - GPU clock index
+	 *
+	 * Returns a handle unique to the given GPU clock, or NULL if the clock
+	 * array has been exhausted at the given index value.
+	 *
+	 * Kbase will use this function pointer to enumerate the existence of a
+	 * GPU clock on the given index.
+	 */
+	void *(*enumerate_gpu_clk)(struct kbase_device *kbdev,
+		unsigned int index);
+
+	/**
+	 * get_gpu_clk_rate - Get the current rate for an enumerated clock.
+	 * @kbdev          - kbase_device pointer
+	 * @gpu_clk_handle - Handle unique to the enumerated GPU clock
+	 *
+	 * Returns current rate of the GPU clock in unit of Hz.
+	 */
+	unsigned long (*get_gpu_clk_rate)(struct kbase_device *kbdev,
+		void *gpu_clk_handle);
+
+	/**
+	 * gpu_clk_notifier_register - Register a clock rate change notifier.
+	 * @kbdev          - kbase_device pointer
+	 * @gpu_clk_handle - Handle unique to the enumerated GPU clock
+	 * @nb             - notifier block containing the callback function
+	 *                   pointer
+	 *
+	 * Returns 0 on success, negative error code otherwise.
+	 *
+	 * This function pointer is used to register a callback function that
+	 * is supposed to be invoked whenever the rate of clock corresponding
+	 * to @gpu_clk_handle changes.
+	 * @nb contains the pointer to callback function.
+	 * The callback function expects the pointer of type
+	 * 'struct kbase_gpu_clk_notifier_data' as the third argument.
+	 */
+	int (*gpu_clk_notifier_register)(struct kbase_device *kbdev,
+		void *gpu_clk_handle, struct notifier_block *nb);
+
+	/**
+	 * gpu_clk_notifier_unregister - Unregister clock rate change notifier
+	 * @kbdev          - kbase_device pointer
+	 * @gpu_clk_handle - Handle unique to the enumerated GPU clock
+	 * @nb             - notifier block containing the callback function
+	 *                   pointer
+	 *
+	 * This function pointer is used to unregister a callback function that
+	 * was previously registered to get notified of the change in rate
+	 * of clock corresponding to @gpu_clk_handle.
+	 */
+	void (*gpu_clk_notifier_unregister)(struct kbase_device *kbdev,
+		void *gpu_clk_handle, struct notifier_block *nb);
+};
+
 #ifdef CONFIG_OF
 struct kbase_platform_config {
 };
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c b/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c
index 05fcb239a6fd..4b84baba48d7 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_core_linux.c
@@ -47,18 +47,28 @@
 #include "mali_kbase_regs_history_debugfs.h"
 #include <mali_kbase_hwaccess_backend.h>
 #include <mali_kbase_hwaccess_time.h>
+#if !MALI_USE_CSF
 #include <mali_kbase_hwaccess_jm.h>
+#endif /* !MALI_USE_CSF */
 #ifdef CONFIG_MALI_PRFCNT_SET_SECONDARY_VIA_DEBUG_FS
 #include <mali_kbase_hwaccess_instr.h>
 #endif
 #include <mali_kbase_ctx_sched.h>
 #include <mali_kbase_reset_gpu.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
 #include "mali_kbase_ioctl.h"
+#if !MALI_USE_CSF
+#include "mali_kbase_kinstr_jm.h"
+#endif
 #include "mali_kbase_hwcnt_context.h"
 #include "mali_kbase_hwcnt_virtualizer.h"
 #include "mali_kbase_hwcnt_legacy.h"
 #include "mali_kbase_vinstr.h"
+#if MALI_USE_CSF
+#include "csf/mali_kbase_csf_firmware.h"
+#include "csf/mali_kbase_csf_tiler_heap.h"
+#include "csf/mali_kbase_csf_kcpu_debugfs.h"
+#include "csf/mali_kbase_csf_csg_debugfs.h"
+#endif
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
 #include "arbiter/mali_kbase_arbiter_pm.h"
 #endif
@@ -115,6 +125,8 @@
 #include <device/mali_kbase_device.h>
 #include <context/mali_kbase_context.h>
 
+#include <mali_kbase_caps.h>
+
 /* GPU IRQ Tags */
 #define	JOB_IRQ_TAG	0
 #define MMU_IRQ_TAG	1
@@ -122,6 +134,89 @@
 
 #define KERNEL_SIDE_DDK_VERSION_STRING "K:" MALI_RELEASE_NAME "(GPL)"
 
+/**
+ * Kernel min/maj <=> API Version
+ */
+#define KBASE_API_VERSION(major, minor) ((((major) & 0xFFF) << 20)  | \
+					 (((minor) & 0xFFF) << 8) | \
+					 ((0 & 0xFF) << 0))
+
+#define KBASE_API_MIN(api_version) ((api_version >> 8) & 0xFFF)
+#define KBASE_API_MAJ(api_version) ((api_version >> 20) & 0xFFF)
+
+/**
+ * mali_kbase_api_version_to_maj_min - convert an api_version to a min/maj pair
+ *
+ * @api_version: API version to convert
+ * @major:  Major version number (must not exceed 12 bits)
+ * @minor:  Major version number (must not exceed 12 bits)
+ */
+void mali_kbase_api_version_to_maj_min(unsigned long api_version, u16 *maj, u16 *min)
+{
+	if (WARN_ON(!maj))
+		return;
+
+	if (WARN_ON(!min))
+		return;
+
+	*maj = KBASE_API_MAJ(api_version);
+	*min = KBASE_API_MIN(api_version);
+}
+
+/**
+ * kbase capabilities table
+ */
+typedef struct mali_kbase_capability_def {
+	u16 required_major;
+	u16 required_minor;
+} mali_kbase_capability_def;
+
+/**
+ * This must be kept in-sync with mali_kbase_cap
+ *
+ * TODO: The alternative approach would be to embed the cap enum values
+ * in the table. Less efficient but potentially safer.
+ */
+static mali_kbase_capability_def kbase_caps_table[MALI_KBASE_NUM_CAPS] = {
+#if MALI_USE_CSF
+	{ 1, 0 },               /* SYSTEM_MONITOR 	*/
+	{ 1, 0 },               /* JIT_PRESSURE_LIMIT	*/
+	{ 1, 0 },               /* MEM_GROW_ON_GPF	*/
+	{ 1, 0 }                /* MEM_PROTECTED	*/
+#else
+	{ 11, 15 },             /* SYSTEM_MONITOR 	*/
+	{ 11, 25 },             /* JIT_PRESSURE_LIMIT	*/
+	{ 11,  2 },             /* MEM_GROW_ON_GPF	*/
+	{ 11,  2 }              /* MEM_PROTECTED	*/
+#endif
+};
+
+/**
+ * mali_kbase_supports_cap - Query whether a kbase capability is supported
+ *
+ * @api_version: 	API version to convert
+ * @cap:		Capability to query for - see mali_kbase_caps.h
+ */
+bool mali_kbase_supports_cap(unsigned long api_version, mali_kbase_cap cap)
+{
+	bool supported = false;
+	unsigned long required_ver;
+
+	mali_kbase_capability_def const *cap_def;
+
+	if (WARN_ON(cap < 0))
+		return false;
+
+	if (WARN_ON(cap >= MALI_KBASE_NUM_CAPS))
+		return false;
+
+	cap_def = &kbase_caps_table[(int)cap];
+	required_ver = KBASE_API_VERSION(cap_def->required_major, cap_def->required_minor);
+	supported = (api_version >= required_ver);
+
+	return supported;
+}
+
 /**
  * kbase_file_new - Create an object representing a device file
  *
@@ -153,7 +248,7 @@ static struct kbase_file *kbase_file_new(struct kbase_device *const kbdev,
 }
 
 /**
- * kbase_file_get_api_version - Set the application programmer interface version
+ * kbase_file_set_api_version - Set the application programmer interface version
  *
  * @kfile:  A device file created by kbase_file_new()
  * @major:  Major version number (must not exceed 12 bits)
@@ -327,13 +422,19 @@ static int kbase_api_handshake(struct kbase_file *kfile,
 	 * the flags have been set. Originally it was created on file open
 	 * (with job submission disabled) but we don't support that usage.
 	 */
-	if (kbase_file_get_api_version(kfile) < KBASE_API_VERSION(11, 15))
+	if (!mali_kbase_supports_system_monitor(kbase_file_get_api_version(kfile)))
 		err = kbase_file_create_kctx(kfile,
 			BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED);
 
 	return err;
 }
 
+static int kbase_api_handshake_dummy(struct kbase_file *kfile,
+		struct kbase_ioctl_version_check *version)
+{
+	return -EPERM;
+}
+
 /**
  * enum mali_error - Mali error codes shared with userspace
  *
@@ -580,6 +681,8 @@ static int kbase_file_create_kctx(struct kbase_file *const kfile,
 #ifdef CONFIG_DEBUG_FS
 	snprintf(kctx_name, 64, "%d_%d", kctx->tgid, kctx->id);
 
+	mutex_init(&kctx->mem_profile_lock);
+
 	kctx->kctx_dentry = debugfs_create_dir(kctx_name,
 			kbdev->debugfs_ctx_directory);
 
@@ -600,8 +703,6 @@ static int kbase_file_create_kctx(struct kbase_file *const kfile,
 		debugfs_create_file("force_same_va", 0600, kctx->kctx_dentry,
 			kctx, &kbase_force_same_va_fops);
 
-		mutex_init(&kctx->mem_profile_lock);
-
 		kbase_context_debugfs_init(kctx);
 	}
 #endif /* CONFIG_DEBUG_FS */
@@ -664,11 +765,13 @@ static int kbase_api_set_flags(struct kbase_file *kfile,
 	/* For backward compatibility, the context may have been created before
 	 * the flags were set.
 	 */
-	if (api_version >= KBASE_API_VERSION(11, 15)) {
+	if (mali_kbase_supports_system_monitor(api_version)) {
 		err = kbase_file_create_kctx(kfile, flags->create_flags);
 	} else {
+#if !MALI_USE_CSF
 		struct kbasep_js_kctx_info *js_kctx_info = NULL;
 		unsigned long irq_flags = 0;
+#endif
 
 		/* If setup is incomplete (e.g. because the API version
 		 * wasn't set) then we have to give up.
@@ -677,6 +780,12 @@ static int kbase_api_set_flags(struct kbase_file *kfile,
 		if (unlikely(!kctx))
 			return -EPERM;
 
+#if MALI_USE_CSF
+		/* On CSF GPUs Job Manager interface isn't used to submit jobs
+		 * (there are no job slots). So the legacy job manager path to
+		 * submit jobs needs to remain disabled for CSF GPUs.
+		 */
+#else
 		js_kctx_info = &kctx->jctx.sched_info;
 		mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
 		spin_lock_irqsave(&kctx->kbdev->hwaccess_lock, irq_flags);
@@ -688,11 +797,13 @@ static int kbase_api_set_flags(struct kbase_file *kfile,
 
 		spin_unlock_irqrestore(&kctx->kbdev->hwaccess_lock, irq_flags);
 		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+#endif
 	}
 
 	return err;
 }
 
+#if !MALI_USE_CSF
 static int kbase_api_job_submit(struct kbase_context *kctx,
 		struct kbase_ioctl_job_submit *submit)
 {
@@ -700,6 +811,7 @@ static int kbase_api_job_submit(struct kbase_context *kctx,
 			submit->nr_atoms,
 			submit->stride, false);
 }
+#endif /* !MALI_USE_CSF */
 
 static int kbase_api_get_gpuprops(struct kbase_context *kctx,
 		struct kbase_ioctl_get_gpuprops *get_props)
@@ -725,11 +837,13 @@ static int kbase_api_get_gpuprops(struct kbase_context *kctx,
 	return kprops->prop_buffer_size;
 }
 
+#if !MALI_USE_CSF
 static int kbase_api_post_term(struct kbase_context *kctx)
 {
 	kbase_event_close(kctx);
 	return 0;
 }
+#endif /* !MALI_USE_CSF */
 
 static int kbase_api_mem_alloc(struct kbase_context *kctx,
 		union kbase_ioctl_mem_alloc *alloc)
@@ -763,6 +877,20 @@ static int kbase_api_mem_alloc(struct kbase_context *kctx,
 			flags |= BASE_MEM_SAME_VA;
 	}
 
+#if MALI_USE_CSF
+	/* If CSF event memory allocation, need to force certain flags.
+	 * SAME_VA - GPU address needs to be used as a CPU address, explicit
+	 * mmap has to be avoided.
+	 * CACHED_CPU - Frequent access to the event memory by CPU.
+	 * COHERENT_SYSTEM - No explicit cache maintenance around the access
+	 * to event memory so need to leverage the coherency support.
+	 */
+	if (flags & BASE_MEM_CSF_EVENT) {
+		flags |= (BASE_MEM_SAME_VA |
+			  BASE_MEM_CACHED_CPU |
+			  BASE_MEM_COHERENT_SYSTEM);
+	}
+#endif
 
 	reg = kbase_mem_alloc(kctx, alloc->in.va_pages,
 			alloc->in.commit_pages,
@@ -791,6 +919,14 @@ static int kbase_api_mem_free(struct kbase_context *kctx,
 	return kbase_mem_free(kctx, free->gpu_addr);
 }
 
+#if !MALI_USE_CSF
+static int kbase_api_kinstr_jm_fd(struct kbase_context *kctx,
+				  union kbase_kinstr_jm_fd *arg)
+{
+	return kbase_kinstr_jm_get_fd(kctx->kinstr_jm, arg);
+}
+#endif
+
 static int kbase_api_hwcnt_reader_setup(struct kbase_context *kctx,
 		struct kbase_ioctl_hwcnt_reader_setup *setup)
 {
@@ -1186,6 +1322,7 @@ static int kbase_api_mem_profile_add(struct kbase_context *kctx,
 	return kbasep_mem_profile_debugfs_insert(kctx, buf, data->len);
 }
 
+#if !MALI_USE_CSF
 static int kbase_api_soft_event_update(struct kbase_context *kctx,
 		struct kbase_ioctl_soft_event_update *update)
 {
@@ -1194,6 +1331,7 @@ static int kbase_api_soft_event_update(struct kbase_context *kctx,
 
 	return kbase_soft_event_update(kctx, update->event, update->new_status);
 }
+#endif /* !MALI_USE_CSF */
 
 static int kbase_api_sticky_resource_map(struct kbase_context *kctx,
 		struct kbase_ioctl_sticky_resource_map *map)
@@ -1288,6 +1426,159 @@ static int kbase_api_tlstream_stats(struct kbase_context *kctx,
 }
 #endif /* MALI_UNIT_TEST */
 
+#if MALI_USE_CSF
+static int kbasep_cs_event_signal(struct kbase_context *kctx)
+{
+	kbase_csf_event_signal_notify_gpu(kctx);
+	return 0;
+}
+
+static int kbasep_cs_queue_register(struct kbase_context *kctx,
+			      struct kbase_ioctl_cs_queue_register *reg)
+{
+	kctx->jit_group_id = BASE_MEM_GROUP_DEFAULT;
+
+	return kbase_csf_queue_register(kctx, reg);
+}
+
+static int kbasep_cs_queue_terminate(struct kbase_context *kctx,
+			       struct kbase_ioctl_cs_queue_terminate *term)
+{
+	kbase_csf_queue_terminate(kctx, term);
+
+	return 0;
+}
+
+static int kbasep_cs_queue_bind(struct kbase_context *kctx,
+				union kbase_ioctl_cs_queue_bind *bind)
+{
+	return kbase_csf_queue_bind(kctx, bind);
+}
+
+static int kbasep_cs_queue_kick(struct kbase_context *kctx,
+				struct kbase_ioctl_cs_queue_kick *kick)
+{
+	return kbase_csf_queue_kick(kctx, kick);
+}
+
+static int kbasep_cs_queue_group_create(struct kbase_context *kctx,
+			     union kbase_ioctl_cs_queue_group_create *create)
+{
+	return kbase_csf_queue_group_create(kctx, create);
+}
+
+static int kbasep_cs_queue_group_terminate(struct kbase_context *kctx,
+		struct kbase_ioctl_cs_queue_group_term *term)
+{
+	kbase_csf_queue_group_terminate(kctx, term->group_handle);
+
+	return 0;
+}
+
+static int kbasep_kcpu_queue_new(struct kbase_context *kctx,
+		struct kbase_ioctl_kcpu_queue_new *new)
+{
+	return kbase_csf_kcpu_queue_new(kctx, new);
+}
+
+static int kbasep_kcpu_queue_delete(struct kbase_context *kctx,
+		struct kbase_ioctl_kcpu_queue_delete *delete)
+{
+	return kbase_csf_kcpu_queue_delete(kctx, delete);
+}
+
+static int kbasep_kcpu_queue_enqueue(struct kbase_context *kctx,
+		struct kbase_ioctl_kcpu_queue_enqueue *enqueue)
+{
+	return kbase_csf_kcpu_queue_enqueue(kctx, enqueue);
+}
+
+static int kbasep_cs_tiler_heap_init(struct kbase_context *kctx,
+		union kbase_ioctl_cs_tiler_heap_init *heap_init)
+{
+	kctx->jit_group_id = heap_init->in.group_id;
+
+	return kbase_csf_tiler_heap_init(kctx, heap_init->in.chunk_size,
+		heap_init->in.initial_chunks, heap_init->in.max_chunks,
+		heap_init->in.target_in_flight,
+		&heap_init->out.gpu_heap_va, &heap_init->out.first_chunk_va);
+}
+
+static int kbasep_cs_tiler_heap_term(struct kbase_context *kctx,
+		struct kbase_ioctl_cs_tiler_heap_term *heap_term)
+{
+	return kbase_csf_tiler_heap_term(kctx, heap_term->gpu_heap_va);
+}
+
+static int kbase_ioctl_cs_get_glb_iface(struct kbase_context *kctx,
+		union kbase_ioctl_cs_get_glb_iface *param)
+{
+	struct basep_cs_stream_control *stream_data = NULL;
+	struct basep_cs_group_control *group_data = NULL;
+	void __user *user_groups, *user_streams;
+	int err = 0;
+	u32 const max_group_num = param->in.max_group_num;
+	u32 const max_total_stream_num = param->in.max_total_stream_num;
+
+	if (max_group_num > MAX_SUPPORTED_CSGS)
+		return -EINVAL;
+
+	if (max_total_stream_num >
+		MAX_SUPPORTED_CSGS * MAX_SUPPORTED_STREAMS_PER_GROUP)
+		return -EINVAL;
+
+	user_groups = u64_to_user_ptr(param->in.groups_ptr);
+	user_streams = u64_to_user_ptr(param->in.streams_ptr);
+
+	if (max_group_num > 0) {
+		if (!user_groups)
+			err = -EINVAL;
+		else {
+			group_data = kcalloc(max_group_num,
+				sizeof(*group_data), GFP_KERNEL);
+			if (!group_data)
+				err = -ENOMEM;
+		}
+	}
+
+	if (max_total_stream_num > 0) {
+		if (!user_streams)
+			err = -EINVAL;
+		else {
+			stream_data = kcalloc(max_total_stream_num,
+				sizeof(*stream_data), GFP_KERNEL);
+			if (!stream_data)
+				err = -ENOMEM;
+		}
+	}
+
+	if (!err) {
+		param->out.total_stream_num =
+			kbase_csf_firmware_get_glb_iface(kctx->kbdev,
+				group_data, max_group_num,
+				stream_data, max_total_stream_num,
+				&param->out.glb_version, &param->out.features,
+				&param->out.group_num, &param->out.prfcnt_size);
+
+		param->out.padding = 0;
+
+		if (copy_to_user(user_groups, group_data,
+			MIN(max_group_num, param->out.group_num) *
+				sizeof(*group_data)))
+			err = -EFAULT;
+	}
+
+	if (!err)
+		if (copy_to_user(user_streams, stream_data,
+			MIN(max_total_stream_num, param->out.total_stream_num) *
+				sizeof(*stream_data)))
+			err = -EFAULT;
+
+	kfree(group_data);
+	kfree(stream_data);
+	return err;
+}
+#endif /* MALI_USE_CSF */
 
 #define KBASE_HANDLE_IOCTL(cmd, function, arg)    \
 	do {                                          \
@@ -1353,6 +1644,13 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 				kfile);
 		break;
 
+	case KBASE_IOCTL_VERSION_CHECK_RESERVED:
+		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_VERSION_CHECK_RESERVED,
+				kbase_api_handshake_dummy,
+				struct kbase_ioctl_version_check,
+				kfile);
+		break;
+
 	case KBASE_IOCTL_SET_FLAGS:
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_SET_FLAGS,
 				kbase_api_set_flags,
@@ -1367,23 +1665,27 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 
 	/* Normal ioctls */
 	switch (cmd) {
+#if !MALI_USE_CSF
 	case KBASE_IOCTL_JOB_SUBMIT:
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_JOB_SUBMIT,
 				kbase_api_job_submit,
 				struct kbase_ioctl_job_submit,
 				kctx);
 		break;
+#endif /* !MALI_USE_CSF */
 	case KBASE_IOCTL_GET_GPUPROPS:
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_GET_GPUPROPS,
 				kbase_api_get_gpuprops,
 				struct kbase_ioctl_get_gpuprops,
 				kctx);
 		break;
+#if !MALI_USE_CSF
 	case KBASE_IOCTL_POST_TERM:
 		KBASE_HANDLE_IOCTL(KBASE_IOCTL_POST_TERM,
 				kbase_api_post_term,
 				kctx);
 		break;
+#endif /* !MALI_USE_CSF */
 	case KBASE_IOCTL_MEM_ALLOC:
 		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_MEM_ALLOC,
 				kbase_api_mem_alloc,
@@ -1516,12 +1818,14 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 				kctx);
 		break;
 
+#if !MALI_USE_CSF
 	case KBASE_IOCTL_SOFT_EVENT_UPDATE:
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_SOFT_EVENT_UPDATE,
 				kbase_api_soft_event_update,
 				struct kbase_ioctl_soft_event_update,
 				kctx);
 		break;
+#endif /* !MALI_USE_CSF */
 
 	case KBASE_IOCTL_STICKY_RESOURCE_MAP:
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_STICKY_RESOURCE_MAP,
@@ -1537,6 +1841,14 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 		break;
 
 	/* Instrumentation. */
+#if !MALI_USE_CSF
+	case KBASE_IOCTL_KINSTR_JM_FD:
+		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_KINSTR_JM_FD,
+				kbase_api_kinstr_jm_fd,
+				union kbase_kinstr_jm_fd,
+				kctx);
+		break;
+#endif
 	case KBASE_IOCTL_HWCNT_READER_SETUP:
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_HWCNT_READER_SETUP,
 				kbase_api_hwcnt_reader_setup,
@@ -1591,6 +1903,85 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 				kctx);
 		break;
 #endif
+#if MALI_USE_CSF
+	case KBASE_IOCTL_CS_EVENT_SIGNAL:
+		KBASE_HANDLE_IOCTL(KBASE_IOCTL_CS_EVENT_SIGNAL,
+				kbasep_cs_event_signal,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_QUEUE_REGISTER:
+		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_CS_QUEUE_REGISTER,
+				kbasep_cs_queue_register,
+				struct kbase_ioctl_cs_queue_register,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_QUEUE_TERMINATE:
+		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_CS_QUEUE_TERMINATE,
+				kbasep_cs_queue_terminate,
+				struct kbase_ioctl_cs_queue_terminate,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_QUEUE_BIND:
+		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_CS_QUEUE_BIND,
+				kbasep_cs_queue_bind,
+				union kbase_ioctl_cs_queue_bind,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_QUEUE_KICK:
+		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_CS_QUEUE_KICK,
+				kbasep_cs_queue_kick,
+				struct kbase_ioctl_cs_queue_kick,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_QUEUE_GROUP_CREATE:
+		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_CS_QUEUE_GROUP_CREATE,
+				kbasep_cs_queue_group_create,
+				union kbase_ioctl_cs_queue_group_create,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE:
+		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_CS_QUEUE_GROUP_TERMINATE,
+				kbasep_cs_queue_group_terminate,
+				struct kbase_ioctl_cs_queue_group_term,
+				kctx);
+		break;
+	case KBASE_IOCTL_KCPU_QUEUE_CREATE:
+		KBASE_HANDLE_IOCTL_OUT(KBASE_IOCTL_KCPU_QUEUE_CREATE,
+				kbasep_kcpu_queue_new,
+				struct kbase_ioctl_kcpu_queue_new,
+				kctx);
+		break;
+	case KBASE_IOCTL_KCPU_QUEUE_DELETE:
+		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_KCPU_QUEUE_DELETE,
+				kbasep_kcpu_queue_delete,
+				struct kbase_ioctl_kcpu_queue_delete,
+				kctx);
+		break;
+	case KBASE_IOCTL_KCPU_QUEUE_ENQUEUE:
+		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_KCPU_QUEUE_ENQUEUE,
+				kbasep_kcpu_queue_enqueue,
+				struct kbase_ioctl_kcpu_queue_enqueue,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_TILER_HEAP_INIT:
+		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_CS_TILER_HEAP_INIT,
+				kbasep_cs_tiler_heap_init,
+				union kbase_ioctl_cs_tiler_heap_init,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_TILER_HEAP_TERM:
+		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_CS_TILER_HEAP_TERM,
+				kbasep_cs_tiler_heap_term,
+				struct kbase_ioctl_cs_tiler_heap_term,
+				kctx);
+		break;
+	case KBASE_IOCTL_CS_GET_GLB_IFACE:
+		KBASE_HANDLE_IOCTL_INOUT(KBASE_IOCTL_CS_GET_GLB_IFACE,
+				kbase_ioctl_cs_get_glb_iface,
+				union kbase_ioctl_cs_get_glb_iface,
+				kctx);
+		break;
+#endif /* MALI_USE_CSF */
 #if MALI_UNIT_TEST
 	case KBASE_IOCTL_TLSTREAM_TEST:
 		KBASE_HANDLE_IOCTL_IN(KBASE_IOCTL_TLSTREAM_TEST,
@@ -1612,6 +2003,47 @@ static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 	return -ENOIOCTLCMD;
 }
 
+#if MALI_USE_CSF
+static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, loff_t *f_pos)
+{
+	struct kbase_file *const kfile = filp->private_data;
+	struct kbase_context *const kctx =
+		kbase_file_get_kctx_if_setup_complete(kfile);
+	struct base_csf_notification event_data = {
+		.type = BASE_CSF_NOTIFICATION_EVENT };
+	const size_t data_size = sizeof(event_data);
+	bool read_event = false, read_error = false;
+
+	if (unlikely(!kctx))
+		return -EPERM;
+
+	if (atomic_read(&kctx->event_count))
+		read_event = true;
+	else
+		read_error = kbase_csf_read_error(kctx, &event_data);
+
+	if (!read_event && !read_error) {
+		/* This condition is not treated as an error.
+		 * It is possible that event handling thread was woken up due
+		 * to a fault/error that occurred for a queue group, but before
+		 * the corresponding fault data was read by the thread the
+		 * queue group was already terminated by the userspace.
+		 */
+		dev_dbg(kctx->kbdev->dev, "Neither event nor error signaled");
+	}
+
+	if (copy_to_user(buf, &event_data, data_size) != 0) {
+		dev_warn(kctx->kbdev->dev,
+			"Failed to copy data\n");
+		return -EFAULT;
+	}
+
+	if (read_event)
+		atomic_set(&kctx->event_count, 0);
+
+	return data_size;
+}
+#else /* MALI_USE_CSF */
 static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, loff_t *f_pos)
 {
 	struct kbase_file *const kfile = filp->private_data;
@@ -1655,6 +2087,7 @@ static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, lof
  out:
 	return out_count * sizeof(uevent);
 }
+#endif /* MALI_USE_CSF */
 
 static unsigned int kbase_poll(struct file *filp, poll_table *wait)
 {
@@ -1681,13 +2114,23 @@ void kbase_event_wakeup(struct kbase_context *kctx)
 
 KBASE_EXPORT_TEST_API(kbase_event_wakeup);
 
+#if MALI_USE_CSF
+int kbase_event_pending(struct kbase_context *ctx)
+{
+	WARN_ON_ONCE(!ctx);
+
+	return (atomic_read(&ctx->event_count) != 0) ||
+		kbase_csf_error_pending(ctx);
+}
+#else
 int kbase_event_pending(struct kbase_context *ctx)
 {
 	KBASE_DEBUG_ASSERT(ctx);
 
 	return (atomic_read(&ctx->event_count) != 0) ||
-			(atomic_read(&ctx->event_closed) != 0);
+		(atomic_read(&ctx->event_closed) != 0);
 }
+#endif
 
 KBASE_EXPORT_TEST_API(kbase_event_pending);
 
@@ -1801,7 +2244,7 @@ static ssize_t show_policy(struct device *dev, struct device_attribute *attr, ch
  * @dev:	The device with sysfs file is for
  * @attr:	The attributes of the sysfs file
  * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @count:	The number of bytes to write to the sysfs file
  *
  * Return: @count if the function succeeded. An error code on failure.
  */
@@ -1891,7 +2334,7 @@ static ssize_t show_core_mask(struct device *dev, struct device_attribute *attr,
  * @dev:	The device with sysfs file is for
  * @attr:	The attributes of the sysfs file
  * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @count:	The number of bytes to write to the sysfs file
  *
  * Return: @count if the function succeeded. An error code on failure.
  */
@@ -1979,6 +2422,7 @@ static ssize_t set_core_mask(struct device *dev, struct device_attribute *attr,
  */
 static DEVICE_ATTR(core_mask, S_IRUGO | S_IWUSR, show_core_mask, set_core_mask);
 
+#if !MALI_USE_CSF
 /**
  * set_soft_job_timeout - Store callback for the soft_job_timeout sysfs
  * file.
@@ -1986,7 +2430,7 @@ static DEVICE_ATTR(core_mask, S_IRUGO | S_IWUSR, show_core_mask, set_core_mask);
  * @dev: The device this sysfs file is for.
  * @attr: The attributes of the sysfs file.
  * @buf: The value written to the sysfs file.
- * @count: The number of bytes written to the sysfs file.
+ * @count: The number of bytes to write to the sysfs file.
  *
  * This allows setting the timeout for software jobs. Waiting soft event wait
  * jobs will be cancelled after this period expires, while soft fence wait jobs
@@ -2079,7 +2523,7 @@ static u32 timeout_ms_to_ticks(struct kbase_device *kbdev, long timeout_ms,
  * @dev:	The device with sysfs file is for
  * @attr:	The attributes of the sysfs file
  * @buf:	The value written to the sysfs file
- * @count:	The number of bytes written to the sysfs file
+ * @count:	The number of bytes to write to the sysfs file
  *
  * Return: @count if the function succeeded. An error code on failure.
  */
@@ -2256,7 +2700,7 @@ static u32 get_new_js_timeout(
  * @dev:   The device the sysfs file is for
  * @attr:  The attributes of the sysfs file
  * @buf:   The value written to the sysfs file
- * @count: The number of bytes written to the sysfs file
+ * @count: The number of bytes to write to the sysfs file
  *
  * This function is called when the js_scheduling_period sysfs file is written
  * to. It checks the data written, and if valid updates the js_scheduling_period
@@ -2421,6 +2865,7 @@ static ssize_t show_js_softstop_always(struct device *dev,
  */
 static DEVICE_ATTR(js_softstop_always, S_IRUGO | S_IWUSR, show_js_softstop_always, set_js_softstop_always);
 #endif /* CONFIG_MALI_BIFROST_DEBUG */
+#endif /* !MALI_USE_CSF */
 
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 typedef void (kbasep_debug_command_func) (struct kbase_device *);
@@ -2569,6 +3014,8 @@ static ssize_t kbase_show_gpuinfo(struct device *dev,
 		  .name = "Mali-G77" },
 		{ .id = GPU_ID2_PRODUCT_TBEX >> GPU_ID_VERSION_PRODUCT_ID_SHIFT,
 		  .name = "Mali-G78" },
+		{ .id = GPU_ID2_PRODUCT_TBAX >> GPU_ID_VERSION_PRODUCT_ID_SHIFT,
+		  .name = "Mali-TBAX" },
 		{ .id = GPU_ID2_PRODUCT_LBEX >> GPU_ID_VERSION_PRODUCT_ID_SHIFT,
 		  .name = "Mali-G68" },
 		{ .id = GPU_ID2_PRODUCT_TNAX >> GPU_ID_VERSION_PRODUCT_ID_SHIFT,
@@ -3013,6 +3460,7 @@ static ssize_t set_lp_mem_pool_max_size(struct device *dev,
 static DEVICE_ATTR(lp_mem_pool_max_size, S_IRUGO | S_IWUSR, show_lp_mem_pool_max_size,
 		set_lp_mem_pool_max_size);
 
+#if !MALI_USE_CSF
 /**
  * show_js_ctx_scheduling_mode - Show callback for js_ctx_scheduling_mode sysfs
  *                               entry.
@@ -3097,7 +3545,6 @@ static DEVICE_ATTR(js_ctx_scheduling_mode, S_IRUGO | S_IWUSR,
 		set_js_ctx_scheduling_mode);
 
 #ifdef MALI_KBASE_BUILD
-#ifdef CONFIG_DEBUG_FS
 
 /* Number of entries in serialize_jobs_settings[] */
 #define NR_SERIALIZE_JOBS_SETTINGS 5
@@ -3118,8 +3565,47 @@ static struct
 };
 
 /**
- * kbasep_serialize_jobs_seq_show - Show callback for the serialize_jobs debugfs
- *                                  file
+ * update_serialize_jobs_setting - Update the serialization setting for the
+ *                                 submission of GPU jobs.
+ *
+ * This function is called when the serialize_jobs sysfs/debugfs file is
+ * written to. It matches the requested setting against the available settings
+ * and if a matching setting is found updates kbdev->serialize_jobs.
+ *
+ * @kbdev:  An instance of the GPU platform device, allocated from the probe
+ *          method of the driver.
+ * @buf:    Buffer containing the value written to the sysfs/debugfs file.
+ * @count:  The number of bytes to write to the sysfs/debugfs file.
+ *
+ * Return: @count if the function succeeded. An error code on failure.
+ */
+static ssize_t update_serialize_jobs_setting(struct kbase_device *kbdev,
+					     const char *buf, size_t count)
+{
+	int i;
+	bool valid = false;
+
+	for (i = 0; i < NR_SERIALIZE_JOBS_SETTINGS; i++) {
+		if (sysfs_streq(serialize_jobs_settings[i].name, buf)) {
+			kbdev->serialize_jobs =
+				serialize_jobs_settings[i].setting;
+			valid = true;
+			break;
+		}
+	}
+
+	if (!valid) {
+		dev_err(kbdev->dev, "serialize_jobs: invalid setting");
+		return -EINVAL;
+	}
+
+	return count;
+}
+
+#ifdef CONFIG_DEBUG_FS
+/**
+ * kbasep_serialize_jobs_seq_debugfs_show - Show callback for the serialize_jobs
+ *					    debugfs file
  * @sfile: seq_file pointer
  * @data:  Private callback data
  *
@@ -3129,7 +3615,8 @@ static struct
  *
  * Return: 0 on success, or an error code on error
  */
-static int kbasep_serialize_jobs_seq_show(struct seq_file *sfile, void *data)
+static int kbasep_serialize_jobs_seq_debugfs_show(struct seq_file *sfile,
+						  void *data)
 {
 	struct kbase_device *kbdev = sfile->private;
 	int i;
@@ -3170,8 +3657,6 @@ static ssize_t kbasep_serialize_jobs_debugfs_write(struct file *file,
 	struct seq_file *s = file->private_data;
 	struct kbase_device *kbdev = s->private;
 	char buf[MAX_SERIALIZE_JOBS_NAME_LEN];
-	int i;
-	bool valid = false;
 
 	CSTD_UNUSED(ppos);
 
@@ -3181,21 +3666,7 @@ static ssize_t kbasep_serialize_jobs_debugfs_write(struct file *file,
 
 	buf[count] = 0;
 
-	for (i = 0; i < NR_SERIALIZE_JOBS_SETTINGS; i++) {
-		if (sysfs_streq(serialize_jobs_settings[i].name, buf)) {
-			kbdev->serialize_jobs =
-					serialize_jobs_settings[i].setting;
-			valid = true;
-			break;
-		}
-	}
-
-	if (!valid) {
-		dev_err(kbdev->dev, "serialize_jobs: invalid setting\n");
-		return -EINVAL;
-	}
-
-	return count;
+	return update_serialize_jobs_setting(kbdev, buf, count);
 }
 
 /**
@@ -3209,7 +3680,8 @@ static ssize_t kbasep_serialize_jobs_debugfs_write(struct file *file,
 static int kbasep_serialize_jobs_debugfs_open(struct inode *in,
 		struct file *file)
 {
-	return single_open(file, kbasep_serialize_jobs_seq_show, in->i_private);
+	return single_open(file, kbasep_serialize_jobs_seq_debugfs_show,
+			   in->i_private);
 }
 
 static const struct file_operations kbasep_serialize_jobs_debugfs_fops = {
@@ -3222,7 +3694,74 @@ static const struct file_operations kbasep_serialize_jobs_debugfs_fops = {
 };
 
 #endif /* CONFIG_DEBUG_FS */
+
+/**
+ * show_serialize_jobs_sysfs - Show callback for serialize_jobs sysfs file.
+ *
+ * This function is called to get the contents of the serialize_jobs sysfs
+ * file. This is a list of the available settings with the currently active
+ * one surrounded by square brackets.
+ *
+ * @dev:	The device this sysfs file is for
+ * @attr:	The attributes of the sysfs file
+ * @buf:	The output buffer for the sysfs file contents
+ *
+ * Return: The number of bytes output to @buf.
+ */
+static ssize_t show_serialize_jobs_sysfs(struct device *dev,
+					 struct device_attribute *attr,
+					 char *buf)
+{
+	struct kbase_device *kbdev = to_kbase_device(dev);
+	ssize_t ret = 0;
+	int i;
+
+	for (i = 0; i < NR_SERIALIZE_JOBS_SETTINGS; i++) {
+		if (kbdev->serialize_jobs ==
+				serialize_jobs_settings[i].setting)
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "[%s]",
+					 serialize_jobs_settings[i].name);
+		else
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s ",
+					 serialize_jobs_settings[i].name);
+	}
+
+	if (ret < PAGE_SIZE - 1) {
+		ret += scnprintf(buf + ret, PAGE_SIZE - ret, "\n");
+	} else {
+		buf[PAGE_SIZE - 2] = '\n';
+		buf[PAGE_SIZE - 1] = '\0';
+		ret = PAGE_SIZE - 1;
+	}
+
+	return ret;
+}
+
+/**
+ * store_serialize_jobs_sysfs - Store callback for serialize_jobs sysfs file.
+ *
+ * This function is called when the serialize_jobs sysfs file is written to.
+ * It matches the requested setting against the available settings and if a
+ * matching setting is found updates kbdev->serialize_jobs.
+ *
+ * @dev:	The device this sysfs file is for
+ * @attr:	The attributes of the sysfs file
+ * @buf:	The value written to the sysfs file
+ * @count:	The number of bytes to write to the sysfs file
+ *
+ * Return: @count if the function succeeded. An error code on failure.
+ */
+static ssize_t store_serialize_jobs_sysfs(struct device *dev,
+					  struct device_attribute *attr,
+					  const char *buf, size_t count)
+{
+	return update_serialize_jobs_setting(to_kbase_device(dev), buf, count);
+}
+
+static DEVICE_ATTR(serialize_jobs, 0600, show_serialize_jobs_sysfs,
+		   store_serialize_jobs_sysfs);
 #endif /* MALI_KBASE_BUILD */
+#endif /* !MALI_USE_CSF */
 
 static void kbasep_protected_mode_hwcnt_disable_worker(struct work_struct *data)
 {
@@ -3252,7 +3791,9 @@ static void kbasep_protected_mode_hwcnt_disable_worker(struct work_struct *data)
 		 * the state machine.
 		 */
 		kbdev->protected_mode_hwcnt_disabled = true;
+#if !MALI_USE_CSF
 		kbase_backend_slot_update(kbdev);
+#endif /* !MALI_USE_CSF */
 	} else {
 		/* Protected mode state was updated while we were doing the
 		 * disable, so we need to undo the disable we just performed.
@@ -3368,6 +3909,15 @@ int registers_map(struct kbase_device * const kbdev)
 	kbdev->reg_start = reg_res->start;
 	kbdev->reg_size = resource_size(reg_res);
 
+#if MALI_USE_CSF
+	if (kbdev->reg_size <
+		(CSF_HW_DOORBELL_PAGE_OFFSET +
+		 CSF_NUM_DOORBELL * CSF_HW_DOORBELL_PAGE_SIZE)) {
+		dev_err(kbdev->dev, "Insufficient register space, will override to the required size\n");
+		kbdev->reg_size = CSF_HW_DOORBELL_PAGE_OFFSET +
+				CSF_NUM_DOORBELL * CSF_HW_DOORBELL_PAGE_SIZE;
+	}
+#endif
 
 	err = kbase_common_reg_map(kbdev);
 	if (err) {
@@ -3832,6 +4382,14 @@ int kbase_device_debugfs_init(struct kbase_device *kbdev)
 		goto out;
 	}
 
+	kbdev->debugfs_instr_directory = debugfs_create_dir("instrumentation",
+			kbdev->mali_debugfs_directory);
+	if (!kbdev->debugfs_instr_directory) {
+		dev_err(kbdev->dev, "Couldn't create mali debugfs instrumentation directory\n");
+		err = -ENOMEM;
+		goto out;
+	}
+
 	debugfs_ctx_defaults_directory = debugfs_create_dir("defaults",
 			kbdev->debugfs_ctx_directory);
 	if (!debugfs_ctx_defaults_directory) {
@@ -3845,7 +4403,9 @@ int kbase_device_debugfs_init(struct kbase_device *kbdev)
 #endif /* !MALI_CUSTOMER_RELEASE */
 	kbasep_regs_history_debugfs_init(kbdev);
 
+#if !MALI_USE_CSF
 	kbase_debug_job_fault_debugfs_init(kbdev);
+#endif /* !MALI_USE_CSF */
 
 	kbasep_gpu_memory_debugfs_init(kbdev);
 	kbase_as_fault_debugfs_init(kbdev);
@@ -3900,9 +4460,13 @@ int kbase_device_debugfs_init(struct kbase_device *kbdev)
 #endif /* CONFIG_DEVFREQ_THERMAL */
 #endif /* CONFIG_MALI_BIFROST_DEVFREQ */
 
+#if MALI_USE_CSF
+	kbase_csf_debugfs_init(kbdev);
+#else
 	debugfs_create_file("serialize_jobs", S_IRUGO | S_IWUSR,
 			kbdev->mali_debugfs_directory, kbdev,
 			&kbasep_serialize_jobs_debugfs_fops);
+#endif
 
 	return 0;
 
@@ -4013,28 +4577,49 @@ void buslog_term(struct kbase_device *kbdev)
 }
 #endif
 
+static struct attribute *kbase_scheduling_attrs[] = {
+#if !MALI_USE_CSF
+	&dev_attr_serialize_jobs.attr,
+#endif /* !MALI_USE_CSF */
+	NULL
+};
+
 static struct attribute *kbase_attrs[] = {
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 	&dev_attr_debug_command.attr,
+#if !MALI_USE_CSF
 	&dev_attr_js_softstop_always.attr,
+#endif /* !MALI_USE_CSF */
 #endif
+#if !MALI_USE_CSF
 	&dev_attr_js_timeouts.attr,
 	&dev_attr_soft_job_timeout.attr,
+#endif /* !MALI_USE_CSF */
 	&dev_attr_gpuinfo.attr,
 	&dev_attr_dvfs_period.attr,
 	&dev_attr_pm_poweroff.attr,
 	&dev_attr_reset_timeout.attr,
+#if !MALI_USE_CSF
 	&dev_attr_js_scheduling_period.attr,
+#endif /* !MALI_USE_CSF */
 	&dev_attr_power_policy.attr,
 	&dev_attr_core_mask.attr,
 	&dev_attr_mem_pool_size.attr,
 	&dev_attr_mem_pool_max_size.attr,
 	&dev_attr_lp_mem_pool_size.attr,
 	&dev_attr_lp_mem_pool_max_size.attr,
+#if !MALI_USE_CSF
 	&dev_attr_js_ctx_scheduling_mode.attr,
+#endif /* !MALI_USE_CSF */
 	NULL
 };
 
+#define SYSFS_SCHEDULING_GROUP "scheduling"
+static const struct attribute_group kbase_scheduling_attr_group = {
+	.name = SYSFS_SCHEDULING_GROUP,
+	.attrs = kbase_scheduling_attrs,
+};
+
 static const struct attribute_group kbase_attr_group = {
 	.attrs = kbase_attrs,
 };
@@ -4050,11 +4635,23 @@ int kbase_sysfs_init(struct kbase_device *kbdev)
 	kbdev->mdev.mode = 0666;
 
 	err = sysfs_create_group(&kbdev->dev->kobj, &kbase_attr_group);
+	if (!err) {
+		err = sysfs_create_group(&kbdev->dev->kobj,
+					 &kbase_scheduling_attr_group);
+		if (err) {
+			dev_err(kbdev->dev, "Creation of %s sysfs group failed",
+				SYSFS_SCHEDULING_GROUP);
+			sysfs_remove_group(&kbdev->dev->kobj,
+					   &kbase_attr_group);
+		}
+	}
+
 	return err;
 }
 
 void kbase_sysfs_term(struct kbase_device *kbdev)
 {
+	sysfs_remove_group(&kbdev->dev->kobj, &kbase_scheduling_attr_group);
 	sysfs_remove_group(&kbdev->dev->kobj, &kbase_attr_group);
 	put_device(kbdev->dev);
 }
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h b/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h
index e1fffc3bd8b7..caba2cd7a0e3 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_cs_experimental.h
@@ -41,9 +41,6 @@
  */
 static inline void mali_kbase_print_cs_experimental(void)
 {
-#if MALI_JIT_PRESSURE_LIMIT
-	pr_info("mali_kbase: JIT_PRESSURE_LIMIT (experimental) enabled");
-#endif /* MALI_JIT_PRESSURE_LIMIT */
 #if MALI_INCREMENTAL_RENDERING
 	pr_info("mali_kbase: INCREMENTAL_RENDERING (experimental) enabled");
 #endif /* MALI_INCREMENTAL_RENDERING */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c b/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c
index cea91bcaf02e..750dbd8c3924 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ctx_sched.c
@@ -212,6 +212,13 @@ void kbase_ctx_sched_restore_all_as(struct kbase_device *kbdev)
 	for (i = 0; i != kbdev->nr_hw_address_spaces; ++i) {
 		struct kbase_context *kctx;
 
+#if MALI_USE_CSF
+		if ((i == MCU_AS_NR) && kbdev->csf.firmware_inited) {
+			kbase_mmu_update(kbdev, &kbdev->csf.mcu_mmu,
+					 MCU_AS_NR);
+			continue;
+		}
+#endif
 		kctx = kbdev->as_to_kctx[i];
 		if (kctx) {
 			if (atomic_read(&kctx->refcount)) {
@@ -265,6 +272,7 @@ struct kbase_context *kbase_ctx_sched_as_to_ctx_refcount(
 struct kbase_context *kbase_ctx_sched_as_to_ctx(struct kbase_device *kbdev,
 		size_t as_nr)
 {
+	unsigned long flags;
 	struct kbase_context *found_kctx;
 
 	if (WARN_ON(kbdev == NULL))
@@ -273,13 +281,16 @@ struct kbase_context *kbase_ctx_sched_as_to_ctx(struct kbase_device *kbdev,
 	if (WARN_ON(as_nr >= BASE_MAX_NR_AS))
 		return NULL;
 
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
 	found_kctx = kbdev->as_to_kctx[as_nr];
 
-	if (WARN_ON(!found_kctx))
-		return NULL;
+	if (found_kctx) {
+		if (WARN_ON(atomic_read(&found_kctx->refcount) <= 0))
+			found_kctx = NULL;
+	}
 
-	if (WARN_ON(atomic_read(&found_kctx->refcount) <= 0))
-		return NULL;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
 
 	return found_kctx;
 }
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_defs.h b/drivers/gpu/arm/bifrost/mali_kbase_defs.h
index 958defbfbe3b..19bff5aa989d 100755
--- a/drivers/gpu/arm/bifrost/mali_kbase_defs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_defs.h
@@ -40,7 +40,7 @@
 #include <mali_kbase_instr_defs.h>
 #include <mali_kbase_pm.h>
 #include <mali_kbase_gpuprops_types.h>
-#include <mali_kbase_hwcnt_backend_gpu.h>
+#include <mali_kbase_hwcnt_backend_jm.h>
 #include <protected_mode_switcher.h>
 
 #include <linux/atomic.h>
@@ -156,6 +156,7 @@ struct kbase_device;
 struct kbase_as;
 struct kbase_mmu_setup;
 struct kbase_ipa_model_vinstr_data;
+struct kbase_kinstr_jm;
 
 /**
  * struct kbase_io_access - holds information about 1 register access
@@ -242,29 +243,6 @@ struct kbase_fault {
 	bool protected_mode;
 };
 
-/**
- * struct kbase_as   - object representing an address space of GPU.
- * @number:            Index at which this address space structure is present
- *                     in an array of address space structures embedded inside the
- *                     struct kbase_device.
- * @pf_wq:             Workqueue for processing work items related to Bus fault
- *                     and Page fault handling.
- * @work_pagefault:    Work item for the Page fault handling.
- * @work_busfault:     Work item for the Bus fault handling.
- * @pf_data:           Data relating to page fault.
- * @bf_data:           Data relating to bus fault.
- * @current_setup:     Stores the MMU configuration for this address space.
- */
-struct kbase_as {
-	int number;
-	struct workqueue_struct *pf_wq;
-	struct work_struct work_pagefault;
-	struct work_struct work_busfault;
-	struct kbase_fault pf_data;
-	struct kbase_fault bf_data;
-	struct kbase_mmu_setup current_setup;
-};
-
 /**
  * struct kbase_mmu_table  - object representing a set of GPU page tables
  * @mmu_teardown_pages:   Buffer of 4 Pages in size, used to cache the entries
@@ -291,7 +269,11 @@ struct kbase_mmu_table {
 	struct kbase_context *kctx;
 };
 
+#if MALI_USE_CSF
+#include "csf/mali_kbase_csf_defs.h"
+#else
 #include "jm/mali_kbase_jm_defs.h"
+#endif
 
 static inline int kbase_as_has_bus_fault(struct kbase_as *as,
 	struct kbase_fault *fault)
@@ -320,6 +302,58 @@ struct kbasep_mem_device {
 	atomic_t ir_threshold;
 };
 
+struct kbase_clk_rate_listener;
+
+/**
+ * kbase_clk_rate_listener_on_change_t() - Frequency change callback
+ *
+ * @listener:     Clock frequency change listener.
+ * @clk_index:    Index of the clock for which the change has occurred.
+ * @clk_rate_hz:  Clock frequency(Hz).
+ *
+ * A callback to call when clock rate changes. The function must not
+ * sleep. No clock rate manager functions must be called from here, as
+ * its lock is taken.
+ */
+typedef void (*kbase_clk_rate_listener_on_change_t)(
+	struct kbase_clk_rate_listener *listener,
+	u32 clk_index,
+	u32 clk_rate_hz);
+
+/**
+ * struct kbase_clk_rate_listener - Clock frequency listener
+ *
+ * @node:        List node.
+ * @notify:    Callback to be called when GPU frequency changes.
+ */
+struct kbase_clk_rate_listener {
+	struct list_head node;
+	kbase_clk_rate_listener_on_change_t notify;
+};
+
+/**
+ * struct kbase_clk_rate_trace_manager - Data stored per device for GPU clock
+ *                                       rate trace manager.
+ *
+ * @gpu_idle:           Tracks the idle state of GPU.
+ * @clks:               Array of pointer to structures storing data for every
+ *                      enumerated GPU clock.
+ * @clk_rate_trace_ops: Pointer to the platform specific GPU clock rate trace
+ *                      operations.
+ * @gpu_clk_rate_trace_write: Pointer to the function that would emit the
+ *                            tracepoint for the clock rate change.
+ * @listeners:          List of listener attached.
+ * @lock:               Lock to serialize the actions of GPU clock rate trace
+ *                      manager.
+ */
+struct kbase_clk_rate_trace_manager {
+	bool gpu_idle;
+	struct kbase_clk_data *clks[BASE_MAX_NR_CLOCKS_REGULATORS];
+	struct kbase_clk_rate_trace_op_conf *clk_rate_trace_ops;
+	struct list_head listeners;
+	spinlock_t lock;
+};
+
 /**
  * Data stored per device for power management.
  *
@@ -346,7 +380,7 @@ struct kbase_pm_device_data {
 	bool suspending;
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
 	/* Flag indicating gpu lost */
-	bool gpu_lost;
+	atomic_t gpu_lost;
 #endif /* CONFIG_MALI_ARBITER_SUPPORT */
 	/* Wait queue set when active_count == 0 */
 	wait_queue_head_t zero_active_count_wait;
@@ -385,6 +419,11 @@ struct kbase_pm_device_data {
 	 */
 	struct kbase_arbiter_vm_state *arb_vm_state;
 #endif /* CONFIG_MALI_ARBITER_SUPPORT */
+
+	/**
+	 * The state of the GPU clock rate trace manager
+	 */
+	struct kbase_clk_rate_trace_manager clk_rtm;
 };
 
 /**
@@ -559,6 +598,32 @@ struct kbase_devfreq_queue_info {
 	enum kbase_devfreq_work_type acted_type;
 };
 
+/**
+ * struct kbase_process - Representing an object of a kbase process instantiated
+ *                        when the first kbase context is created under it.
+ * @tgid:               Thread group ID.
+ * @total_gpu_pages:    Total gpu pages allocated across all the contexts
+ *                      of this process, it accounts for both native allocations
+ *                      and dma_buf imported allocations.
+ * @kctx_list:          List of kbase contexts created for the process.
+ * @kprcs_node:         Node to a rb_tree, kbase_device will maintain a rb_tree
+ *                      based on key tgid, kprcs_node is the node link to
+ *                      &struct_kbase_device.process_root.
+ * @dma_buf_root:       RB tree of the dma-buf imported allocations, imported
+ *                      across all the contexts created for this process.
+ *                      Used to ensure that pages of allocation are accounted
+ *                      only once for the process, even if the allocation gets
+ *                      imported multiple times for the process.
+ */
+struct kbase_process {
+	pid_t tgid;
+	size_t total_gpu_pages;
+	struct list_head kctx_list;
+
+	struct rb_node kprcs_node;
+	struct rb_root dma_buf_root;
+};
+
 /**
  * struct kbase_device   - Object representing an instance of GPU platform device,
  *                         allocated from the probe method of mali driver.
@@ -806,6 +871,20 @@ struct kbase_devfreq_queue_info {
  *                          Job Scheduler
  * @l2_size_override:       Used to set L2 cache size via device tree blob
  * @l2_hash_override:       Used to set L2 cache hash via device tree blob
+ * @process_root:           rb_tree root node for maintaining a rb_tree of
+ *                          kbase_process based on key tgid(thread group ID).
+ * @dma_buf_root:           rb_tree root node for maintaining a rb_tree of
+ *                          &struct kbase_dma_buf based on key dma_buf.
+ *                          We maintain a rb_tree of dma_buf mappings under
+ *                          kbase_device and kbase_process, one indicates a
+ *                          mapping and gpu memory usage at device level and
+ *                          other one at process level.
+ * @total_gpu_pages:        Total GPU pages used for the complete GPU device.
+ * @dma_buf_lock:           This mutex should be held while accounting for
+ *                          @total_gpu_pages from imported dma buffers.
+ * @gpu_mem_usage_lock:     This spinlock should be held while accounting
+ *                          @total_gpu_pages for both native and dma-buf imported
+ *                          allocations.
  */
 struct kbase_device {
 	u32 hw_quirks_sc;
@@ -955,6 +1034,7 @@ struct kbase_device {
 #ifdef CONFIG_DEBUG_FS
 	struct dentry *mali_debugfs_directory;
 	struct dentry *debugfs_ctx_directory;
+	struct dentry *debugfs_instr_directory;
 
 #ifdef CONFIG_MALI_BIFROST_DEBUG
 	u64 debugfs_as_read_bitmap;
@@ -1030,6 +1110,10 @@ struct kbase_device {
 	u8 l2_size_override;
 	u8 l2_hash_override;
 
+#if MALI_USE_CSF
+	/* Command-stream front-end for the device. */
+	struct kbase_csf_device csf;
+#else
 	struct kbasep_js_device_data js_data;
 
 	/* See KBASE_JS_*_PRIORITY_MODE for details. */
@@ -1042,6 +1126,14 @@ struct kbase_device {
 	u8 backup_serialize_jobs;
 #endif /* CONFIG_MALI_CINSTR_GWT */
 
+#endif /* MALI_USE_CSF */
+
+	struct rb_root process_root;
+	struct rb_root dma_buf_root;
+
+	size_t total_gpu_pages;
+	struct mutex dma_buf_lock;
+	spinlock_t gpu_mem_usage_lock;
 
 	struct {
 		struct kbase_context *ctx;
@@ -1056,10 +1148,6 @@ struct kbase_device {
 #endif
 };
 
-#define KBASE_API_VERSION(major, minor) ((((major) & 0xFFF) << 20)  | \
-					 (((minor) & 0xFFF) << 8) | \
-					 ((0 & 0xFF) << 0))
-
 /**
  * enum kbase_file_state - Initialization state of a file opened by @kbase_open
  *
@@ -1189,6 +1277,13 @@ enum kbase_context_flags {
 	KCTX_PULLED_SINCE_ACTIVE_JS1 = 1U << 13,
 	KCTX_PULLED_SINCE_ACTIVE_JS2 = 1U << 14,
 	KCTX_AS_DISABLED_ON_FAULT = 1U << 15,
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	/*
+	 * Set when JIT physical page limit is less than JIT virtual address
+	 * page limit, so we must take care to not exceed the physical limit
+	 */
+	KCTX_JPL_ENABLED = 1U << 16,
+#endif /* !MALI_JIT_PRESSURE_LIMIT_BASE */
 };
 
 struct kbase_sub_alloc {
@@ -1315,7 +1410,8 @@ struct kbase_sub_alloc {
  *                        at any point.
  *                        Generally the reference count is incremented when the context
  *                        is scheduled in and an atom is pulled from the context's per
- *                        slot runnable tree.
+ *                        slot runnable tree in JM GPU or GPU command queue
+ *                        group is programmed on CSG slot in CSF GPU.
  * @mm_update_lock:       lock used for handling of special tracking page.
  * @process_mm:           Pointer to the memory descriptor of the process which
  *                        created the context. Used for accounting the physical
@@ -1399,6 +1495,16 @@ struct kbase_sub_alloc {
  *                             that were used (i.e. the
  *                             &struct_kbase_va_region.used_pages for regions
  *                             that have had a usage report).
+ * @jit_phys_pages_to_be_allocated: Count of the physical pages that are being
+ *                                  now allocated for just-in-time memory
+ *                                  allocations of a context (across all the
+ *                                  threads). This is supposed to be updated
+ *                                  with @reg_lock held before allocating
+ *                                  the backing pages. This helps ensure that
+ *                                  total physical memory usage for just in
+ *                                  time memory allocation remains within the
+ *                                  @jit_phys_pages_limit in multi-threaded
+ *                                  scenarios.
  * @jit_active_head:      List containing the just-in-time memory allocations
  *                        which are in use.
  * @jit_pool_head:        List containing the just-in-time memory allocations
@@ -1425,6 +1531,10 @@ struct kbase_sub_alloc {
  *                        is used to determine the atom's age when it is added to
  *                        the runnable RB-tree.
  * @trim_level:           Level of JIT allocation trimming to perform on free (0-100%)
+ * @kprcs:                Reference to @struct kbase_process that the current
+ *                        kbase_context belongs to.
+ * @kprcs_link:           List link for the list of kbase context maintained
+ *                        under kbase_process.
  * @gwt_enabled:          Indicates if tracking of GPU writes is enabled, protected by
  *                        kbase_context.reg_lock.
  * @gwt_was_enabled:      Simple sticky bit flag to know if GWT was ever enabled.
@@ -1435,6 +1545,7 @@ struct kbase_sub_alloc {
  *                        for context scheduling, protected by hwaccess_lock.
  * @atoms_count:          Number of GPU atoms currently in use, per priority
  * @create_flags:         Flags used in context creation.
+ * @kinstr_jm:            Kernel job manager instrumentation context handle
  *
  * A kernel base context is an entity among which the GPU is scheduled.
  * Each context has its own GPU address space.
@@ -1453,7 +1564,9 @@ struct kbase_context {
 	struct list_head event_list;
 	struct list_head event_coalesce_list;
 	struct mutex event_mutex;
+#if !MALI_USE_CSF
 	atomic_t event_closed;
+#endif
 	struct workqueue_struct *event_workq;
 	atomic_t event_count;
 	int event_coalesce_count;
@@ -1471,6 +1584,9 @@ struct kbase_context {
 	struct rb_root reg_rbtree_custom;
 	struct rb_root reg_rbtree_exec;
 
+#if MALI_USE_CSF
+	struct kbase_csf_context csf;
+#else
 	struct kbase_jd_context jctx;
 	struct jsctx_queue jsctx_queue
 		[KBASE_JS_ATOM_SCHED_PRIO_COUNT][BASE_JM_MAX_NR_SLOTS];
@@ -1488,6 +1604,7 @@ struct kbase_context {
 	s16 atoms_count[KBASE_JS_ATOM_SCHED_PRIO_COUNT];
 	u32 slots_pullable;
 	u32 age_count;
+#endif /* MALI_USE_CSF */
 
 	DECLARE_BITMAP(cookies, BITS_PER_LONG);
 	struct kbase_va_region *pending_regions[BITS_PER_LONG];
@@ -1545,10 +1662,11 @@ struct kbase_context {
 	u8 jit_current_allocations_per_bin[256];
 	u8 jit_version;
 	u8 jit_group_id;
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	u64 jit_phys_pages_limit;
 	u64 jit_current_phys_pressure;
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+	u64 jit_phys_pages_to_be_allocated;
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 	struct list_head jit_active_head;
 	struct list_head jit_pool_head;
 	struct list_head jit_destroy_head;
@@ -1559,6 +1677,9 @@ struct kbase_context {
 
 	u8 trim_level;
 
+	struct kbase_process *kprcs;
+	struct list_head kprcs_link;
+
 #ifdef CONFIG_MALI_CINSTR_GWT
 	bool gwt_enabled;
 	bool gwt_was_enabled;
@@ -1567,6 +1688,10 @@ struct kbase_context {
 #endif
 
 	base_context_create_flags create_flags;
+
+#if !MALI_USE_CSF
+	struct kbase_kinstr_jm *kinstr_jm;
+#endif
 };
 
 #ifdef CONFIG_MALI_CINSTR_GWT
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.c b/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.c
index 6a872be1b165..1fac5e3e68f1 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2011-2017,2020 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2011-2016, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -20,12 +20,10 @@
  *
  */
 
-
 /* Include mali_kbase_dma_fence.h before checking for CONFIG_MALI_BIFROST_DMA_FENCE as
  * it will be set there.
  */
 #include "mali_kbase_dma_fence.h"
-
 #include <linux/atomic.h>
 #include <linux/list.h>
 #include <linux/lockdep.h>
@@ -35,7 +33,6 @@
 #include <linux/spinlock.h>
 #include <linux/workqueue.h>
 #include <linux/ww_mutex.h>
-
 #include <mali_kbase.h>
 
 static void
@@ -59,7 +56,11 @@ static int
 kbase_dma_fence_lock_reservations(struct kbase_dma_fence_resv_info *info,
 				  struct ww_acquire_ctx *ctx)
 {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 	struct reservation_object *content_res = NULL;
+#else
+	struct dma_resv *content_res = NULL;
+#endif
 	unsigned int content_res_idx = 0;
 	unsigned int r;
 	int err = 0;
@@ -225,10 +226,17 @@ kbase_dma_fence_cb(struct dma_fence *fence, struct dma_fence_cb *cb)
 		kbase_dma_fence_queue_work(katom);
 }
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 static int
 kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 					 struct reservation_object *resv,
 					 bool exclusive)
+#else
+static int
+kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
+					 struct dma_resv *resv,
+					 bool exclusive)
+#endif
 {
 #if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
 	struct fence *excl_fence = NULL;
@@ -294,9 +302,15 @@ kbase_dma_fence_add_reservation_callback(struct kbase_jd_atom *katom,
 	return err;
 }
 
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 void kbase_dma_fence_add_reservation(struct reservation_object *resv,
 				     struct kbase_dma_fence_resv_info *info,
 				     bool exclusive)
+#else
+void kbase_dma_fence_add_reservation(struct dma_resv *resv,
+				     struct kbase_dma_fence_resv_info *info,
+				     bool exclusive)
+#endif
 {
 	unsigned int i;
 
@@ -346,8 +360,11 @@ int kbase_dma_fence_wait(struct kbase_jd_atom *katom,
 	}
 
 	for (i = 0; i < info->dma_fence_resv_count; i++) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 		struct reservation_object *obj = info->resv_objs[i];
-
+#else
+		struct dma_resv *obj = info->resv_objs[i];
+#endif
 		if (!test_bit(i, info->dma_fence_excl_bitmap)) {
 			err = reservation_object_reserve_shared(obj);
 			if (err) {
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.h b/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.h
index 1adb4fa00469..3ac8186328a1 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_dma_fence.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2016, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,7 +29,6 @@
 #include <linux/reservation.h>
 #include <mali_kbase_fence.h>
 
-
 /* Forward declaration from mali_kbase_defs.h */
 struct kbase_jd_atom;
 struct kbase_context;
@@ -45,7 +44,11 @@ struct kbase_context;
  * reservation objects.
  */
 struct kbase_dma_fence_resv_info {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 	struct reservation_object **resv_objs;
+#else
+	struct dma_resv **resv_objs;
+#endif
 	unsigned int dma_fence_resv_count;
 	unsigned long *dma_fence_excl_bitmap;
 };
@@ -60,9 +63,15 @@ struct kbase_dma_fence_resv_info {
  * reservation_objects. At the same time keeps track of which objects require
  * exclusive access in dma_fence_excl_bitmap.
  */
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 void kbase_dma_fence_add_reservation(struct reservation_object *resv,
 				     struct kbase_dma_fence_resv_info *info,
 				     bool exclusive);
+#else
+void kbase_dma_fence_add_reservation(struct dma_resv *resv,
+				     struct kbase_dma_fence_resv_info *info,
+				     bool exclusive);
+#endif
 
 /**
  * kbase_dma_fence_wait() - Creates a new fence and attaches it to the resv_objs
@@ -122,7 +131,6 @@ void kbase_dma_fence_term(struct kbase_context *kctx);
  */
 int kbase_dma_fence_init(struct kbase_context *kctx);
 
-
 #else /* CONFIG_MALI_BIFROST_DMA_FENCE */
 /* Dummy functions for when dma-buf fence isn't enabled. */
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c b/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c
index 188e53bf1abe..a5a7ad744a8e 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -25,7 +25,7 @@
  */
 
 #include <mali_kbase.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <mali_kbase_dummy_job_wa.h>
 
 #include <linux/firmware.h>
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.h b/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.h
index 5bbe37df7ed6..e19495055b48 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_dummy_job_wa.h
@@ -31,6 +31,34 @@
 				  KBASE_DUMMY_JOB_WA_FLAG_WAIT_POWERUP | \
 				  KBASE_DUMMY_JOB_WA_FLAG_LOGICAL_SHADER_POWER)
 
+#if MALI_USE_CSF
+
+static inline int kbase_dummy_job_wa_load(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+	return 0;
+}
+
+static inline void kbase_dummy_job_wa_cleanup(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+static inline int kbase_dummy_job_wa_execute(struct kbase_device *kbdev,
+		u64 cores)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(cores);
+	return 0;
+}
+
+static inline bool kbase_dummy_job_wa_enabled(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+	return false;
+}
+
+#else
 
 int kbase_dummy_job_wa_load(struct kbase_device *kbdev);
 void kbase_dummy_job_wa_cleanup(struct kbase_device *kbdev);
@@ -41,5 +69,6 @@ static inline bool kbase_dummy_job_wa_enabled(struct kbase_device *kbdev)
 	return (kbdev->dummy_job_wa.ctx != NULL);
 }
 
+#endif /* MALI_USE_CSF */
 
 #endif /* _KBASE_DUMMY_JOB_WORKAROUND_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_event.c b/drivers/gpu/arm/bifrost/mali_kbase_event.c
index c8b8f22d14f7..5adb80f9bbd2 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_event.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_event.c
@@ -168,6 +168,16 @@ void kbase_event_post(struct kbase_context *ctx, struct kbase_jd_atom *atom)
 
 	dev_dbg(kbdev->dev, "Posting event for atom %p\n", (void *)atom);
 
+	if (WARN_ON(atom->status != KBASE_JD_ATOM_STATE_COMPLETED)) {
+		dev_warn(kbdev->dev,
+				"%s: Atom %d (%p) not completed (status %d)\n",
+				__func__,
+				kbase_jd_atom_id(atom->kctx, atom),
+				atom->kctx,
+				atom->status);
+		return;
+	}
+
 	if (atom->core_req & BASE_JD_REQ_EVENT_ONLY_ON_FAILURE) {
 		if (atom->event_code == BASE_JD_EVENT_DONE) {
 			dev_dbg(kbdev->dev, "Suppressing event (atom done)\n");
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_fence.c b/drivers/gpu/arm/bifrost/mali_kbase_fence.c
index 7a715b3354be..5e04acf87892 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_fence.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_fence.c
@@ -23,71 +23,12 @@
 #include <linux/atomic.h>
 #include <linux/list.h>
 #include <linux/spinlock.h>
-#include <mali_kbase_fence_defs.h>
 #include <mali_kbase_fence.h>
 #include <mali_kbase.h>
 
 /* Spin lock protecting all Mali fences as fence->lock. */
 static DEFINE_SPINLOCK(kbase_fence_lock);
 
-static const char *
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_get_driver_name(struct fence *fence)
-#else
-kbase_fence_get_driver_name(struct dma_fence *fence)
-#endif
-{
-	return kbase_drv_name;
-}
-
-static const char *
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_get_timeline_name(struct fence *fence)
-#else
-kbase_fence_get_timeline_name(struct dma_fence *fence)
-#endif
-{
-	return kbase_timeline_name;
-}
-
-static bool
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_enable_signaling(struct fence *fence)
-#else
-kbase_fence_enable_signaling(struct dma_fence *fence)
-#endif
-{
-	return true;
-}
-
-static void
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-kbase_fence_fence_value_str(struct fence *fence, char *str, int size)
-#else
-kbase_fence_fence_value_str(struct dma_fence *fence, char *str, int size)
-#endif
-{
-#if (KERNEL_VERSION(5, 1, 0) > LINUX_VERSION_CODE)
-	snprintf(str, size, "%u", fence->seqno);
-#else
-	snprintf(str, size, "%llu", fence->seqno);
-#endif
-}
-
-#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
-const struct fence_ops kbase_fence_ops = {
-	.wait = fence_default_wait,
-#else
-const struct dma_fence_ops kbase_fence_ops = {
-	.wait = dma_fence_default_wait,
-#endif
-	.get_driver_name = kbase_fence_get_driver_name,
-	.get_timeline_name = kbase_fence_get_timeline_name,
-	.enable_signaling = kbase_fence_enable_signaling,
-	.fence_value_str = kbase_fence_fence_value_str
-};
-
-
 #if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
 struct fence *
 kbase_fence_out_new(struct kbase_jd_atom *katom)
@@ -211,4 +152,3 @@ kbase_fence_add_callback(struct kbase_jd_atom *katom,
 
 	return err;
 }
-
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_fence.h b/drivers/gpu/arm/bifrost/mali_kbase_fence.h
index 6079a7dfb2ef..f319d9e1dce6 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_fence.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_fence.h
@@ -88,6 +88,7 @@ struct dma_fence *kbase_fence_out_new(struct kbase_jd_atom *katom);
 #endif
 
 
+#if !MALI_USE_CSF
 /**
  * kbase_fence_out_remove() - Removes the output fence from atom
  * @katom: Atom to remove output fence for
@@ -269,6 +270,7 @@ bool kbase_fence_free_callbacks(struct kbase_jd_atom *katom);
  */
 #define kbase_fence_out_get(katom) dma_fence_get((katom)->dma_fence.fence)
 
+#endif /* !MALI_USE_CSF */
 
 /**
  * kbase_fence_put() - Releases a reference to a fence
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_fence_defs.h b/drivers/gpu/arm/bifrost/mali_kbase_fence_defs.h
index 475136430649..303029639d38 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_fence_defs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_fence_defs.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2010-2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2010-2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -28,8 +28,6 @@
  * This file hides the compatibility issues with this for the rest the driver
  */
 
-#if defined(CONFIG_MALI_BIFROST_DMA_FENCE) || defined(CONFIG_SYNC_FILE)
-
 #include <linux/version.h>
 
 #if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
@@ -63,6 +61,4 @@
 
 #endif /* < 4.10.0 */
 
-#endif /* CONFIG_MALI_BIFROST_DMA_FENCE || CONFIG_SYNC_FILE */
-
 #endif /* _KBASE_FENCE_DEFS_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c b/drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c
new file mode 100644
index 000000000000..c4703748bec6
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_fence_ops.c
@@ -0,0 +1,84 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <linux/atomic.h>
+#include <linux/list.h>
+#include <mali_kbase_fence_defs.h>
+#include <mali_kbase.h>
+
+static const char *
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
+kbase_fence_get_driver_name(struct fence *fence)
+#else
+kbase_fence_get_driver_name(struct dma_fence *fence)
+#endif
+{
+	return kbase_drv_name;
+}
+
+static const char *
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
+kbase_fence_get_timeline_name(struct fence *fence)
+#else
+kbase_fence_get_timeline_name(struct dma_fence *fence)
+#endif
+{
+	return kbase_timeline_name;
+}
+
+static bool
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
+kbase_fence_enable_signaling(struct fence *fence)
+#else
+kbase_fence_enable_signaling(struct dma_fence *fence)
+#endif
+{
+	return true;
+}
+
+static void
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
+kbase_fence_fence_value_str(struct fence *fence, char *str, int size)
+#else
+kbase_fence_fence_value_str(struct dma_fence *fence, char *str, int size)
+#endif
+{
+#if (KERNEL_VERSION(5, 1, 0) > LINUX_VERSION_CODE)
+	snprintf(str, size, "%u", fence->seqno);
+#else
+	snprintf(str, size, "%llu", fence->seqno);
+#endif
+}
+
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
+const struct fence_ops kbase_fence_ops = {
+	.wait = fence_default_wait,
+#else
+const struct dma_fence_ops kbase_fence_ops = {
+	.wait = dma_fence_default_wait,
+#endif
+	.get_driver_name = kbase_fence_get_driver_name,
+	.get_timeline_name = kbase_fence_get_timeline_name,
+	.enable_signaling = kbase_fence_enable_signaling,
+	.fence_value_str = kbase_fence_fence_value_str
+};
+
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h b/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h
index 28a871a0da4f..a45dabbb680f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gpu_memory_debugfs.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2014, 2016 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2014, 2016, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -34,8 +34,20 @@
 #include <linux/debugfs.h>
 #include <linux/seq_file.h>
 
+/* kbase_io_history_add - add new entry to the register access history
+ *
+ * @h: Pointer to the history data structure
+ * @addr: Register address
+ * @value: The value that is either read from or written to the register
+ * @write: 1 if it's a register write, 0 if it's a read
+ */
+void kbase_io_history_add(struct kbase_io_history *h, void __iomem const *addr,
+		u32 value, u8 write);
+
 /**
- * @brief Initialize gpu_memory debugfs entry
+ * kbasep_gpu_memory_debugfs_init - Initialize gpu_memory debugfs entry
+ *
+ * @kbdev: Device pointer
  */
 void kbasep_gpu_memory_debugfs_init(struct kbase_device *kbdev);
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c b/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c
index ae2458f497da..020b5d853608 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gpuprops.c
@@ -254,6 +254,18 @@ static void kbase_gpuprops_calculate_props(
 	product_id = gpu_id & GPU_ID_VERSION_PRODUCT_ID;
 	product_id >>= GPU_ID_VERSION_PRODUCT_ID_SHIFT;
 
+#if MALI_USE_CSF
+	gpu_props->thread_props.max_registers =
+		KBASE_UBFX32(gpu_props->raw_props.thread_features,
+			     0U, 22);
+	gpu_props->thread_props.impl_tech =
+		KBASE_UBFX32(gpu_props->raw_props.thread_features,
+			     22U, 2);
+	gpu_props->thread_props.max_task_queue =
+		KBASE_UBFX32(gpu_props->raw_props.thread_features,
+			     24U, 8);
+	gpu_props->thread_props.max_thread_group_split = 0;
+#else
 	if ((gpu_id & GPU_ID2_PRODUCT_MODEL) == GPU_ID2_PRODUCT_TDUX) {
 		gpu_props->thread_props.max_registers =
 			KBASE_UBFX32(gpu_props->raw_props.thread_features,
@@ -279,6 +291,7 @@ static void kbase_gpuprops_calculate_props(
 			KBASE_UBFX32(gpu_props->raw_props.thread_features,
 				     30U, 2);
 	}
+#endif
 
 	/* If values are not specified, then use defaults */
 	if (gpu_props->thread_props.max_registers == 0) {
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_gwt.c b/drivers/gpu/arm/bifrost/mali_kbase_gwt.c
index 6a47c9dd3610..91dc4dbc0800 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_gwt.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_gwt.c
@@ -71,6 +71,7 @@ int kbase_gpu_gwt_start(struct kbase_context *kctx)
 	INIT_LIST_HEAD(&kctx->gwt_current_list);
 	INIT_LIST_HEAD(&kctx->gwt_snapshot_list);
 
+#if !MALI_USE_CSF
 	/* If GWT is enabled using new vector dumping format
 	 * from user space, back up status of the job serialization flag and
 	 * use full serialisation of jobs for dumping.
@@ -80,6 +81,7 @@ int kbase_gpu_gwt_start(struct kbase_context *kctx)
 	kctx->kbdev->serialize_jobs = KBASE_SERIALIZE_INTRA_SLOT |
 						KBASE_SERIALIZE_INTER_SLOT;
 
+#endif
 	/* Mark gwt enabled before making pages read only in case a
 	   write page fault is triggered while we're still in this loop.
 	   (kbase_gpu_vm_lock() doesn't prevent this!)
@@ -113,7 +115,9 @@ int kbase_gpu_gwt_stop(struct kbase_context *kctx)
 		kfree(pos);
 	}
 
+#if !MALI_USE_CSF
 	kctx->kbdev->serialize_jobs = kctx->kbdev->backup_serialize_jobs;
+#endif
 
 	kbase_gpu_gwt_setup_pages(kctx, ~0UL);
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hw.c b/drivers/gpu/arm/bifrost/mali_kbase_hw.c
index f8a9248e3c06..dc58ffb931be 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hw.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hw.c
@@ -68,6 +68,9 @@ void kbase_hw_set_features_mask(struct kbase_device *kbdev)
 	case GPU_ID2_PRODUCT_TBEX:
 		features = base_hw_features_tBEx;
 		break;
+	case GPU_ID2_PRODUCT_TBAX:
+		features = base_hw_features_tBAx;
+		break;
 	case GPU_ID2_PRODUCT_TDUX:
 		features = base_hw_features_tDUx;
 		break;
@@ -206,6 +209,12 @@ static const enum base_hw_issue *kbase_hw_get_issues_for_new_id(
 		  {GPU_ID2_VERSION_MAKE(1, 0, 0), base_hw_issues_tBEx_r1p0},
 		  {U32_MAX, NULL} } },
 
+		{GPU_ID2_PRODUCT_TBAX,
+		 {{GPU_ID2_VERSION_MAKE(0, 0, 0), base_hw_issues_tBAx_r0p0},
+		  {GPU_ID2_VERSION_MAKE(0, 0, 3), base_hw_issues_tBAx_r0p0},
+		  {GPU_ID2_VERSION_MAKE(1, 0, 0), base_hw_issues_tBAx_r1p0},
+		  {U32_MAX, NULL} } },
+
 		{GPU_ID2_PRODUCT_TDUX,
 		 {{GPU_ID2_VERSION_MAKE(0, 0, 0), base_hw_issues_tDUx_r0p0},
 		  {U32_MAX, NULL} } },
@@ -227,7 +236,7 @@ static const enum base_hw_issue *kbase_hw_get_issues_for_new_id(
 		  {U32_MAX, NULL} } },
 
 		{GPU_ID2_PRODUCT_TTUX,
-		 {{GPU_ID2_VERSION_MAKE(2, 0, 0), base_hw_issues_tTUx_r0p0},
+		 {{GPU_ID2_VERSION_MAKE(0, 0, 0), base_hw_issues_tTUx_r0p0},
 		  {U32_MAX, NULL} } },
 
 		{GPU_ID2_PRODUCT_LTUX,
@@ -374,6 +383,9 @@ int kbase_hw_set_issues_mask(struct kbase_device *kbdev)
 		case GPU_ID2_PRODUCT_TBEX:
 			issues = base_hw_issues_model_tBEx;
 			break;
+		case GPU_ID2_PRODUCT_TBAX:
+			issues = base_hw_issues_model_tBAx;
+			break;
 		case GPU_ID2_PRODUCT_TDUX:
 			issues = base_hw_issues_model_tDUx;
 			break;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_instr.h b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_instr.h
index be85491c18d9..4fd2e3549268 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_instr.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_instr.h
@@ -35,7 +35,7 @@
  * struct kbase_instr_hwcnt_enable - Enable hardware counter collection.
  * @dump_buffer:       GPU address to write counters to.
  * @dump_buffer_bytes: Size in bytes of the buffer pointed to by dump_buffer.
- * @jm_bm:             counters selection bitmask (JM).
+ * @fe_bm:             counters selection bitmask (Front End).
  * @shader_bm:         counters selection bitmask (Shader).
  * @tiler_bm:          counters selection bitmask (Tiler).
  * @mmu_l2_bm:         counters selection bitmask (MMU_L2).
@@ -45,7 +45,7 @@
 struct kbase_instr_hwcnt_enable {
 	u64 dump_buffer;
 	u64 dump_buffer_bytes;
-	u32 jm_bm;
+	u32 fe_bm;
 	u32 shader_bm;
 	u32 tiler_bm;
 	u32 mmu_l2_bm;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h
index 3d5934e0e0a1..f6ce17e4180f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_jm.h
@@ -154,6 +154,7 @@ void kbase_backend_cache_clean(struct kbase_device *kbdev,
 void kbase_backend_complete_wq(struct kbase_device *kbdev,
 				struct kbase_jd_atom *katom);
 
+#if !MALI_USE_CSF
 /**
  * kbase_backend_complete_wq_post_sched - Perform backend-specific actions
  *                                        required on completing an atom, after
@@ -166,6 +167,7 @@ void kbase_backend_complete_wq(struct kbase_device *kbdev,
  */
 void kbase_backend_complete_wq_post_sched(struct kbase_device *kbdev,
 		base_jd_core_req core_req);
+#endif /* !MALI_USE_CSF */
 
 /**
  * kbase_backend_reset() - The GPU is being reset. Cancel all jobs on the GPU
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h
index a61e5b9b3e0a..94b7551b865e 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwaccess_time.h
@@ -39,4 +39,18 @@
 void kbase_backend_get_gpu_time(struct kbase_device *kbdev, u64 *cycle_counter,
 				u64 *system_time, struct timespec64 *ts);
 
+/**
+ * kbase_backend_get_gpu_time_norequest() - Get current GPU time without
+ *                                          request/release cycle counter
+ * @kbdev:		Device pointer
+ * @cycle_counter:	Pointer to u64 to store cycle counter in
+ * @system_time:	Pointer to u64 to store system time in
+ * @ts:			Pointer to struct timespec to store current monotonic
+ *			time in
+ */
+void kbase_backend_get_gpu_time_norequest(struct kbase_device *kbdev,
+					  u64 *cycle_counter,
+					  u64 *system_time,
+					  struct timespec64 *ts);
+
 #endif /* _KBASE_BACKEND_TIME_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt.c b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt.c
index 14ec5cb1c0d3..2708af78b292 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt.c
@@ -242,6 +242,7 @@ static void kbasep_hwcnt_accumulator_disable(
 	bool backend_enabled = false;
 	struct kbase_hwcnt_accumulator *accum;
 	unsigned long flags;
+	u64 dump_time_ns;
 
 	WARN_ON(!hctx);
 	lockdep_assert_held(&hctx->accum_lock);
@@ -271,7 +272,7 @@ static void kbasep_hwcnt_accumulator_disable(
 		goto disable;
 
 	/* Try and accumulate before disabling */
-	errcode = hctx->iface->dump_request(accum->backend);
+	errcode = hctx->iface->dump_request(accum->backend, &dump_time_ns);
 	if (errcode)
 		goto disable;
 
@@ -419,23 +420,16 @@ static int kbasep_hwcnt_accumulator_dump(
 
 	/* Initiate the dump if the backend is enabled. */
 	if ((state == ACCUM_STATE_ENABLED) && cur_map_any_enabled) {
-		/* Disable pre-emption, to make the timestamp as accurate as
-		 * possible.
-		 */
-		preempt_disable();
-		{
+		if (dump_buf) {
+			errcode = hctx->iface->dump_request(
+					accum->backend, &dump_time_ns);
+			dump_requested = true;
+		} else {
 			dump_time_ns = hctx->iface->timestamp_ns(
-				accum->backend);
-			if (dump_buf) {
-				errcode = hctx->iface->dump_request(
 					accum->backend);
-				dump_requested = true;
-			} else {
-				errcode = hctx->iface->dump_clear(
-					accum->backend);
-			}
+			errcode = hctx->iface->dump_clear(accum->backend);
 		}
-		preempt_enable();
+
 		if (errcode)
 			goto error;
 	} else {
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend.h b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend.h
index b7aa0e1fa8e9..3a921b754b55 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -137,6 +137,8 @@ typedef int (*kbase_hwcnt_backend_dump_clear_fn)(
  * typedef kbase_hwcnt_backend_dump_request_fn - Request an asynchronous counter
  *                                               dump.
  * @backend: Non-NULL pointer to backend.
+ * @dump_time_ns: Non-NULL pointer where the timestamp of when the dump was
+ *                requested will be written out to on success.
  *
  * If the backend is not enabled or another dump is already in progress,
  * returns an error.
@@ -144,7 +146,8 @@ typedef int (*kbase_hwcnt_backend_dump_clear_fn)(
  * Return: 0 on success, else error code.
  */
 typedef int (*kbase_hwcnt_backend_dump_request_fn)(
-	struct kbase_hwcnt_backend *backend);
+	struct kbase_hwcnt_backend *backend,
+	u64 *dump_time_ns);
 
 /**
  * typedef kbase_hwcnt_backend_dump_wait_fn - Wait until the last requested
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_gpu.c b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_gpu.c
deleted file mode 100644
index 1fcff38b48dd..000000000000
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_gpu.c
+++ /dev/null
@@ -1,510 +0,0 @@
-/*
- *
- * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
- *
- * This program is free software and is provided to you under the terms of the
- * GNU General Public License version 2 as published by the Free Software
- * Foundation, and any use by you of this program is subject to the terms
- * of such GNU licence.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
- * SPDX-License-Identifier: GPL-2.0
- *
- */
-
-#include "mali_kbase_hwcnt_backend_gpu.h"
-#include "mali_kbase_hwcnt_gpu.h"
-#include "mali_kbase_hwcnt_types.h"
-#include "mali_kbase.h"
-#include "mali_kbase_pm_ca.h"
-#include "mali_kbase_hwaccess_instr.h"
-#ifdef CONFIG_MALI_BIFROST_NO_MALI
-#include "backend/gpu/mali_kbase_model_dummy.h"
-#endif
-
-
-/**
- * struct kbase_hwcnt_backend_gpu_info - Information used to create an instance
- *                                       of a GPU hardware counter backend.
- * @kbdev:         KBase device.
- * @use_secondary: True if secondary performance counters should be used,
- *                 else false. Ignored if secondary counters are not supported.
- * @metadata:      Hardware counter metadata.
- * @dump_bytes:    Bytes of GPU memory required to perform a
- *                 hardware counter dump.
- */
-struct kbase_hwcnt_backend_gpu_info {
-	struct kbase_device *kbdev;
-	bool use_secondary;
-	const struct kbase_hwcnt_metadata *metadata;
-	size_t dump_bytes;
-};
-
-/**
- * struct kbase_hwcnt_backend_gpu - Instance of a GPU hardware counter backend.
- * @info:         Info used to create the backend.
- * @kctx:         KBase context used for GPU memory allocation and
- *                counter dumping.
- * @gpu_dump_va:  GPU hardware counter dump buffer virtual address.
- * @cpu_dump_va:  CPU mapping of gpu_dump_va.
- * @vmap:         Dump buffer vmap.
- * @enabled:      True if dumping has been enabled, else false.
- * @pm_core_mask:  PM state sync-ed shaders core mask for the enabled dumping.
- */
-struct kbase_hwcnt_backend_gpu {
-	const struct kbase_hwcnt_backend_gpu_info *info;
-	struct kbase_context *kctx;
-	u64 gpu_dump_va;
-	void *cpu_dump_va;
-	struct kbase_vmap_struct *vmap;
-	bool enabled;
-	u64 pm_core_mask;
-};
-
-/* GPU backend implementation of kbase_hwcnt_backend_timestamp_ns_fn */
-static u64 kbasep_hwcnt_backend_gpu_timestamp_ns(
-	struct kbase_hwcnt_backend *backend)
-{
-	(void)backend;
-	return ktime_get_raw_ns();
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_dump_enable_nolock_fn */
-static int kbasep_hwcnt_backend_gpu_dump_enable_nolock(
-	struct kbase_hwcnt_backend *backend,
-	const struct kbase_hwcnt_enable_map *enable_map)
-{
-	int errcode;
-	struct kbase_hwcnt_backend_gpu *backend_gpu =
-		(struct kbase_hwcnt_backend_gpu *)backend;
-	struct kbase_context *kctx;
-	struct kbase_device *kbdev;
-	struct kbase_hwcnt_physical_enable_map phys;
-	struct kbase_instr_hwcnt_enable enable;
-
-	if (!backend_gpu || !enable_map || backend_gpu->enabled ||
-	    (enable_map->metadata != backend_gpu->info->metadata))
-		return -EINVAL;
-
-	kctx = backend_gpu->kctx;
-	kbdev = backend_gpu->kctx->kbdev;
-
-	lockdep_assert_held(&kbdev->hwaccess_lock);
-
-	kbase_hwcnt_gpu_enable_map_to_physical(&phys, enable_map);
-
-	enable.jm_bm = phys.jm_bm;
-	enable.shader_bm = phys.shader_bm;
-	enable.tiler_bm = phys.tiler_bm;
-	enable.mmu_l2_bm = phys.mmu_l2_bm;
-	enable.use_secondary = backend_gpu->info->use_secondary;
-	enable.dump_buffer = backend_gpu->gpu_dump_va;
-	enable.dump_buffer_bytes = backend_gpu->info->dump_bytes;
-
-	errcode = kbase_instr_hwcnt_enable_internal(kbdev, kctx, &enable);
-	if (errcode)
-		goto error;
-
-	backend_gpu->pm_core_mask = kbase_pm_ca_get_instr_core_mask(kbdev);
-	backend_gpu->enabled = true;
-
-	return 0;
-error:
-	return errcode;
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_dump_enable_fn */
-static int kbasep_hwcnt_backend_gpu_dump_enable(
-	struct kbase_hwcnt_backend *backend,
-	const struct kbase_hwcnt_enable_map *enable_map)
-{
-	unsigned long flags;
-	int errcode;
-	struct kbase_hwcnt_backend_gpu *backend_gpu =
-		(struct kbase_hwcnt_backend_gpu *)backend;
-	struct kbase_device *kbdev;
-
-	if (!backend_gpu)
-		return -EINVAL;
-
-	kbdev = backend_gpu->kctx->kbdev;
-
-	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
-
-	errcode = kbasep_hwcnt_backend_gpu_dump_enable_nolock(
-		backend, enable_map);
-
-	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
-
-	return errcode;
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_dump_disable_fn */
-static void kbasep_hwcnt_backend_gpu_dump_disable(
-	struct kbase_hwcnt_backend *backend)
-{
-	int errcode;
-	struct kbase_hwcnt_backend_gpu *backend_gpu =
-		(struct kbase_hwcnt_backend_gpu *)backend;
-
-	if (WARN_ON(!backend_gpu) || !backend_gpu->enabled)
-		return;
-
-	errcode = kbase_instr_hwcnt_disable_internal(backend_gpu->kctx);
-	WARN_ON(errcode);
-
-	backend_gpu->enabled = false;
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_dump_clear_fn */
-static int kbasep_hwcnt_backend_gpu_dump_clear(
-	struct kbase_hwcnt_backend *backend)
-{
-	struct kbase_hwcnt_backend_gpu *backend_gpu =
-		(struct kbase_hwcnt_backend_gpu *)backend;
-
-	if (!backend_gpu || !backend_gpu->enabled)
-		return -EINVAL;
-
-	return kbase_instr_hwcnt_clear(backend_gpu->kctx);
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_dump_request_fn */
-static int kbasep_hwcnt_backend_gpu_dump_request(
-	struct kbase_hwcnt_backend *backend)
-{
-	struct kbase_hwcnt_backend_gpu *backend_gpu =
-		(struct kbase_hwcnt_backend_gpu *)backend;
-
-	if (!backend_gpu || !backend_gpu->enabled)
-		return -EINVAL;
-
-	return kbase_instr_hwcnt_request_dump(backend_gpu->kctx);
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_dump_wait_fn */
-static int kbasep_hwcnt_backend_gpu_dump_wait(
-	struct kbase_hwcnt_backend *backend)
-{
-	struct kbase_hwcnt_backend_gpu *backend_gpu =
-		(struct kbase_hwcnt_backend_gpu *)backend;
-
-	if (!backend_gpu || !backend_gpu->enabled)
-		return -EINVAL;
-
-	return kbase_instr_hwcnt_wait_for_dump(backend_gpu->kctx);
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_dump_get_fn */
-static int kbasep_hwcnt_backend_gpu_dump_get(
-	struct kbase_hwcnt_backend *backend,
-	struct kbase_hwcnt_dump_buffer *dst,
-	const struct kbase_hwcnt_enable_map *dst_enable_map,
-	bool accumulate)
-{
-	struct kbase_hwcnt_backend_gpu *backend_gpu =
-		(struct kbase_hwcnt_backend_gpu *)backend;
-
-	if (!backend_gpu || !dst || !dst_enable_map ||
-	    (backend_gpu->info->metadata != dst->metadata) ||
-	    (dst_enable_map->metadata != dst->metadata))
-		return -EINVAL;
-
-	/* Invalidate the kernel buffer before reading from it. */
-	kbase_sync_mem_regions(
-		backend_gpu->kctx, backend_gpu->vmap, KBASE_SYNC_TO_CPU);
-
-	return kbase_hwcnt_gpu_dump_get(
-		dst, backend_gpu->cpu_dump_va, dst_enable_map,
-		backend_gpu->pm_core_mask, accumulate);
-}
-
-/**
- * kbasep_hwcnt_backend_gpu_dump_alloc() - Allocate a GPU dump buffer.
- * @info:        Non-NULL pointer to GPU backend info.
- * @kctx:        Non-NULL pointer to kbase context.
- * @gpu_dump_va: Non-NULL pointer to where GPU dump buffer virtual address
- *               is stored on success.
- *
- * Return: 0 on success, else error code.
- */
-static int kbasep_hwcnt_backend_gpu_dump_alloc(
-	const struct kbase_hwcnt_backend_gpu_info *info,
-	struct kbase_context *kctx,
-	u64 *gpu_dump_va)
-{
-	struct kbase_va_region *reg;
-	u64 flags;
-	u64 nr_pages;
-
-	WARN_ON(!info);
-	WARN_ON(!kctx);
-	WARN_ON(!gpu_dump_va);
-
-	flags = BASE_MEM_PROT_CPU_RD |
-		BASE_MEM_PROT_GPU_WR |
-		BASEP_MEM_PERMANENT_KERNEL_MAPPING |
-		BASE_MEM_CACHED_CPU;
-
-	if (kctx->kbdev->mmu_mode->flags & KBASE_MMU_MODE_HAS_NON_CACHEABLE)
-		flags |= BASE_MEM_UNCACHED_GPU;
-
-	nr_pages = PFN_UP(info->dump_bytes);
-
-	reg = kbase_mem_alloc(kctx, nr_pages, nr_pages, 0, &flags, gpu_dump_va);
-
-	if (!reg)
-		return -ENOMEM;
-
-	return 0;
-}
-
-/**
- * kbasep_hwcnt_backend_gpu_dump_free() - Free an allocated GPU dump buffer.
- * @kctx:        Non-NULL pointer to kbase context.
- * @gpu_dump_va: GPU dump buffer virtual address.
- */
-static void kbasep_hwcnt_backend_gpu_dump_free(
-	struct kbase_context *kctx,
-	u64 gpu_dump_va)
-{
-	WARN_ON(!kctx);
-	if (gpu_dump_va)
-		kbase_mem_free(kctx, gpu_dump_va);
-}
-
-/**
- * kbasep_hwcnt_backend_gpu_destroy() - Destroy a GPU backend.
- * @backend: Pointer to GPU backend to destroy.
- *
- * Can be safely called on a backend in any state of partial construction.
- */
-static void kbasep_hwcnt_backend_gpu_destroy(
-	struct kbase_hwcnt_backend_gpu *backend)
-{
-	if (!backend)
-		return;
-
-	if (backend->kctx) {
-		struct kbase_context *kctx = backend->kctx;
-		struct kbase_device *kbdev = kctx->kbdev;
-
-		if (backend->cpu_dump_va)
-			kbase_phy_alloc_mapping_put(kctx, backend->vmap);
-
-		if (backend->gpu_dump_va)
-			kbasep_hwcnt_backend_gpu_dump_free(
-				kctx, backend->gpu_dump_va);
-
-		kbasep_js_release_privileged_ctx(kbdev, kctx);
-		kbase_destroy_context(kctx);
-	}
-
-	kfree(backend);
-}
-
-/**
- * kbasep_hwcnt_backend_gpu_create() - Create a GPU backend.
- * @info:        Non-NULL pointer to backend info.
- * @out_backend: Non-NULL pointer to where backend is stored on success.
- *
- * Return: 0 on success, else error code.
- */
-static int kbasep_hwcnt_backend_gpu_create(
-	const struct kbase_hwcnt_backend_gpu_info *info,
-	struct kbase_hwcnt_backend_gpu **out_backend)
-{
-
-	int errcode;
-	struct kbase_device *kbdev;
-	struct kbase_hwcnt_backend_gpu *backend = NULL;
-
-	WARN_ON(!info);
-	WARN_ON(!out_backend);
-
-	kbdev = info->kbdev;
-
-	backend = kzalloc(sizeof(*backend), GFP_KERNEL);
-	if (!backend)
-		goto alloc_error;
-
-	backend->info = info;
-
-	backend->kctx = kbase_create_context(kbdev, true,
-		BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED, 0, NULL);
-	if (!backend->kctx)
-		goto alloc_error;
-
-	kbasep_js_schedule_privileged_ctx(kbdev, backend->kctx);
-
-	errcode = kbasep_hwcnt_backend_gpu_dump_alloc(
-		info, backend->kctx, &backend->gpu_dump_va);
-	if (errcode)
-		goto error;
-
-	backend->cpu_dump_va = kbase_phy_alloc_mapping_get(backend->kctx,
-		backend->gpu_dump_va, &backend->vmap);
-	if (!backend->cpu_dump_va)
-		goto alloc_error;
-
-#ifdef CONFIG_MALI_BIFROST_NO_MALI
-	/* The dummy model needs the CPU mapping. */
-	gpu_model_set_dummy_prfcnt_base_cpu(backend->cpu_dump_va);
-#endif
-
-	*out_backend = backend;
-	return 0;
-
-alloc_error:
-	errcode = -ENOMEM;
-error:
-	kbasep_hwcnt_backend_gpu_destroy(backend);
-	return errcode;
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_init_fn */
-static int kbasep_hwcnt_backend_gpu_init(
-	const struct kbase_hwcnt_backend_info *info,
-	struct kbase_hwcnt_backend **out_backend)
-{
-	int errcode;
-	struct kbase_hwcnt_backend_gpu *backend = NULL;
-
-	if (!info || !out_backend)
-		return -EINVAL;
-
-	errcode = kbasep_hwcnt_backend_gpu_create(
-		(const struct kbase_hwcnt_backend_gpu_info *) info, &backend);
-	if (errcode)
-		return errcode;
-
-	*out_backend = (struct kbase_hwcnt_backend *)backend;
-
-	return 0;
-}
-
-/* GPU backend implementation of kbase_hwcnt_backend_term_fn */
-static void kbasep_hwcnt_backend_gpu_term(struct kbase_hwcnt_backend *backend)
-{
-	if (!backend)
-		return;
-
-	kbasep_hwcnt_backend_gpu_dump_disable(backend);
-	kbasep_hwcnt_backend_gpu_destroy(
-		(struct kbase_hwcnt_backend_gpu *)backend);
-}
-
-/**
- * kbasep_hwcnt_backend_gpu_info_destroy() - Destroy a GPU backend info.
- * @info: Pointer to info to destroy.
- *
- * Can be safely called on a backend info in any state of partial construction.
- */
-static void kbasep_hwcnt_backend_gpu_info_destroy(
-	const struct kbase_hwcnt_backend_gpu_info *info)
-{
-	if (!info)
-		return;
-
-	kbase_hwcnt_gpu_metadata_destroy(info->metadata);
-	kfree(info);
-}
-
-/**
- * kbasep_hwcnt_backend_gpu_info_create() - Create a GPU backend info.
- * @kbdev: Non_NULL pointer to kbase device.
- * @out_info: Non-NULL pointer to where info is stored on success.
- *
- * Return 0 on success, else error code.
- */
-static int kbasep_hwcnt_backend_gpu_info_create(
-	struct kbase_device *kbdev,
-	const struct kbase_hwcnt_backend_gpu_info **out_info)
-{
-	int errcode = -ENOMEM;
-	struct kbase_hwcnt_gpu_info hwcnt_gpu_info;
-	struct kbase_hwcnt_backend_gpu_info *info = NULL;
-
-	WARN_ON(!kbdev);
-	WARN_ON(!out_info);
-
-	errcode = kbase_hwcnt_gpu_info_init(kbdev, &hwcnt_gpu_info);
-	if (errcode)
-		return errcode;
-
-	info = kzalloc(sizeof(*info), GFP_KERNEL);
-	if (!info)
-		goto error;
-
-	info->kbdev = kbdev;
-
-#ifdef CONFIG_MALI_BIFROST_PRFCNT_SET_SECONDARY
-	info->use_secondary = true;
-#else
-	info->use_secondary = false;
-#endif
-
-	errcode = kbase_hwcnt_gpu_metadata_create(
-		&hwcnt_gpu_info, info->use_secondary,
-		&info->metadata,
-		&info->dump_bytes);
-	if (errcode)
-		goto error;
-
-	*out_info = info;
-
-	return 0;
-error:
-	kbasep_hwcnt_backend_gpu_info_destroy(info);
-	return errcode;
-}
-
-int kbase_hwcnt_backend_gpu_create(
-	struct kbase_device *kbdev,
-	struct kbase_hwcnt_backend_interface *iface)
-{
-	int errcode;
-	const struct kbase_hwcnt_backend_gpu_info *info = NULL;
-
-	if (!kbdev || !iface)
-		return -EINVAL;
-
-	errcode = kbasep_hwcnt_backend_gpu_info_create(kbdev, &info);
-
-	if (errcode)
-		return errcode;
-
-	iface->metadata = info->metadata;
-	iface->info = (struct kbase_hwcnt_backend_info *)info;
-	iface->init = kbasep_hwcnt_backend_gpu_init;
-	iface->term = kbasep_hwcnt_backend_gpu_term;
-	iface->timestamp_ns = kbasep_hwcnt_backend_gpu_timestamp_ns;
-	iface->dump_enable = kbasep_hwcnt_backend_gpu_dump_enable;
-	iface->dump_enable_nolock = kbasep_hwcnt_backend_gpu_dump_enable_nolock;
-	iface->dump_disable = kbasep_hwcnt_backend_gpu_dump_disable;
-	iface->dump_clear = kbasep_hwcnt_backend_gpu_dump_clear;
-	iface->dump_request = kbasep_hwcnt_backend_gpu_dump_request;
-	iface->dump_wait = kbasep_hwcnt_backend_gpu_dump_wait;
-	iface->dump_get = kbasep_hwcnt_backend_gpu_dump_get;
-
-	return 0;
-}
-
-void kbase_hwcnt_backend_gpu_destroy(
-	struct kbase_hwcnt_backend_interface *iface)
-{
-	if (!iface)
-		return;
-
-	kbasep_hwcnt_backend_gpu_info_destroy(
-		(const struct kbase_hwcnt_backend_gpu_info *)iface->info);
-	memset(iface, 0, sizeof(*iface));
-}
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_jm.c b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_jm.c
new file mode 100644
index 000000000000..9f65de41694f
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_jm.c
@@ -0,0 +1,736 @@
+/*
+ *
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "mali_kbase_hwcnt_backend_jm.h"
+#include "mali_kbase_hwcnt_gpu.h"
+#include "mali_kbase_hwcnt_types.h"
+#include "mali_kbase.h"
+#include "mali_kbase_pm_ca.h"
+#include "mali_kbase_hwaccess_instr.h"
+#include "mali_kbase_hwaccess_time.h"
+#include "mali_kbase_ccswe.h"
+
+#ifdef CONFIG_MALI_BIFROST_NO_MALI
+#include "backend/gpu/mali_kbase_model_dummy.h"
+#endif
+#include "backend/gpu/mali_kbase_clk_rate_trace_mgr.h"
+
+#if MALI_USE_CSF
+#include "mali_kbase_ctx_sched.h"
+#else
+#include "backend/gpu/mali_kbase_pm_internal.h"
+#endif
+
+/**
+ * struct kbase_hwcnt_backend_jm_info - Information used to create an instance
+ *                                      of a JM hardware counter backend.
+ * @kbdev:         KBase device.
+ * @use_secondary: True if secondary performance counters should be used,
+ *                 else false. Ignored if secondary counters are not supported.
+ * @metadata:      Hardware counter metadata.
+ * @dump_bytes:    Bytes of GPU memory required to perform a
+ *                 hardware counter dump.
+ */
+struct kbase_hwcnt_backend_jm_info {
+	struct kbase_device *kbdev;
+	bool use_secondary;
+	const struct kbase_hwcnt_metadata *metadata;
+	size_t dump_bytes;
+};
+
+/**
+ * struct kbase_hwcnt_backend_jm - Instance of a JM hardware counter backend.
+ * @info:             Info used to create the backend.
+ * @kctx:             KBase context used for GPU memory allocation and
+ *                    counter dumping.
+ * @gpu_dump_va:      GPU hardware counter dump buffer virtual address.
+ * @cpu_dump_va:      CPU mapping of gpu_dump_va.
+ * @vmap:             Dump buffer vmap.
+ * @enabled:          True if dumping has been enabled, else false.
+ * @pm_core_mask:     PM state sync-ed shaders core mask for the enabled
+ *                    dumping.
+ * @clk_enable_map:   The enable map specifying enabled clock domains.
+ * @cycle_count_elapsed:
+ *                    Cycle count elapsed for a given sample period.
+ *                    The top clock cycle, index 0, is read directly from
+ *                    hardware, but the other clock domains need to be
+ *                    calculated with software estimation.
+ * @prev_cycle_count: Previous cycle count to calculate the cycle count for
+ *                    sample period.
+ * @rate_listener:    Clock rate listener callback state.
+ * @ccswe_shader_cores: Shader cores cycle count software estimator.
+ */
+struct kbase_hwcnt_backend_jm {
+	const struct kbase_hwcnt_backend_jm_info *info;
+	struct kbase_context *kctx;
+	u64 gpu_dump_va;
+	void *cpu_dump_va;
+	struct kbase_vmap_struct *vmap;
+	bool enabled;
+	u64 pm_core_mask;
+	u64 clk_enable_map;
+	u64 cycle_count_elapsed[BASE_MAX_NR_CLOCKS_REGULATORS];
+	u64 prev_cycle_count[BASE_MAX_NR_CLOCKS_REGULATORS];
+	struct kbase_clk_rate_listener rate_listener;
+	struct kbase_ccswe ccswe_shader_cores;
+};
+
+/**
+ * kbasep_hwcnt_backend_jm_on_freq_change() - On freq change callback
+ *
+ * @rate_listener:    Callback state
+ * @clk_index:        Clock index
+ * @clk_rate_hz:      Clock frequency(hz)
+ */
+static void kbasep_hwcnt_backend_jm_on_freq_change(
+	struct kbase_clk_rate_listener *rate_listener,
+	u32 clk_index,
+	u32 clk_rate_hz)
+{
+	struct kbase_hwcnt_backend_jm *backend_jm = container_of(
+		rate_listener, struct kbase_hwcnt_backend_jm, rate_listener);
+	u64 timestamp_ns;
+
+	if (clk_index != KBASE_CLOCK_DOMAIN_SHADER_CORES)
+		return;
+
+	timestamp_ns = ktime_get_raw_ns();
+	kbase_ccswe_freq_change(
+		&backend_jm->ccswe_shader_cores, timestamp_ns, clk_rate_hz);
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_cc_enable() - Enable cycle count tracking
+ *
+ * @backend:      Non-NULL pointer to backend.
+ * @enable_map:   Non-NULL pointer to enable map specifying enabled counters.
+ * @timestamp_ns: Timestamp(ns) when HWCNT were enabled.
+ */
+static void kbasep_hwcnt_backend_jm_cc_enable(
+	struct kbase_hwcnt_backend_jm *backend_jm,
+	const struct kbase_hwcnt_enable_map *enable_map,
+	u64 timestamp_ns)
+{
+	struct kbase_device *kbdev = backend_jm->kctx->kbdev;
+	u64 clk_enable_map = enable_map->clk_enable_map;
+	u64 cycle_count;
+
+	if (kbase_hwcnt_clk_enable_map_enabled(
+		    clk_enable_map, KBASE_CLOCK_DOMAIN_TOP)) {
+#if !MALI_USE_CSF
+		/* turn on the cycle counter */
+		kbase_pm_request_gpu_cycle_counter_l2_is_on(kbdev);
+#endif
+		/* Read cycle count for top clock domain. */
+		kbase_backend_get_gpu_time_norequest(
+			kbdev, &cycle_count, NULL, NULL);
+
+		backend_jm->prev_cycle_count[KBASE_CLOCK_DOMAIN_TOP] =
+			cycle_count;
+	}
+
+	if (kbase_hwcnt_clk_enable_map_enabled(
+		    clk_enable_map, KBASE_CLOCK_DOMAIN_SHADER_CORES)) {
+		/* software estimation for non-top clock domains */
+		struct kbase_clk_rate_trace_manager *rtm = &kbdev->pm.clk_rtm;
+		const struct kbase_clk_data *clk_data =
+			rtm->clks[KBASE_CLOCK_DOMAIN_SHADER_CORES];
+		u32 cur_freq;
+		unsigned long flags;
+
+		spin_lock_irqsave(&rtm->lock, flags);
+
+		cur_freq = (u32) clk_data->clock_val;
+		kbase_ccswe_reset(&backend_jm->ccswe_shader_cores);
+		kbase_ccswe_freq_change(
+			&backend_jm->ccswe_shader_cores,
+			timestamp_ns,
+			cur_freq);
+
+		kbase_clk_rate_trace_manager_subscribe_no_lock(
+			rtm, &backend_jm->rate_listener);
+
+		spin_unlock_irqrestore(&rtm->lock, flags);
+
+		/* ccswe was reset. The estimated cycle is zero. */
+		backend_jm->prev_cycle_count[
+			KBASE_CLOCK_DOMAIN_SHADER_CORES] = 0;
+	}
+
+	/* Keep clk_enable_map for dump_request. */
+	backend_jm->clk_enable_map = clk_enable_map;
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_cc_disable() - Disable cycle count tracking
+ *
+ * @backend:      Non-NULL pointer to backend.
+ */
+static void kbasep_hwcnt_backend_jm_cc_disable(
+	struct kbase_hwcnt_backend_jm *backend_jm)
+{
+	struct kbase_device *kbdev = backend_jm->kctx->kbdev;
+	struct kbase_clk_rate_trace_manager *rtm = &kbdev->pm.clk_rtm;
+	u64 clk_enable_map = backend_jm->clk_enable_map;
+
+#if !MALI_USE_CSF
+	if (kbase_hwcnt_clk_enable_map_enabled(
+		clk_enable_map, KBASE_CLOCK_DOMAIN_TOP)) {
+		/* turn off the cycle counter */
+		kbase_pm_release_gpu_cycle_counter(kbdev);
+	}
+#endif
+	if (kbase_hwcnt_clk_enable_map_enabled(
+		clk_enable_map, KBASE_CLOCK_DOMAIN_SHADER_CORES)) {
+
+		kbase_clk_rate_trace_manager_unsubscribe(
+			rtm, &backend_jm->rate_listener);
+	}
+}
+
+
+/* JM backend implementation of kbase_hwcnt_backend_timestamp_ns_fn */
+static u64 kbasep_hwcnt_backend_jm_timestamp_ns(
+	struct kbase_hwcnt_backend *backend)
+{
+	(void)backend;
+	return ktime_get_raw_ns();
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_dump_enable_nolock_fn */
+static int kbasep_hwcnt_backend_jm_dump_enable_nolock(
+	struct kbase_hwcnt_backend *backend,
+	const struct kbase_hwcnt_enable_map *enable_map)
+{
+	int errcode;
+	struct kbase_hwcnt_backend_jm *backend_jm =
+		(struct kbase_hwcnt_backend_jm *)backend;
+	struct kbase_context *kctx;
+	struct kbase_device *kbdev;
+	struct kbase_hwcnt_physical_enable_map phys;
+	struct kbase_instr_hwcnt_enable enable;
+	u64 timestamp_ns;
+
+	if (!backend_jm || !enable_map || backend_jm->enabled ||
+	    (enable_map->metadata != backend_jm->info->metadata))
+		return -EINVAL;
+
+	kctx = backend_jm->kctx;
+	kbdev = backend_jm->kctx->kbdev;
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	kbase_hwcnt_gpu_enable_map_to_physical(&phys, enable_map);
+
+	enable.fe_bm = phys.fe_bm;
+	enable.shader_bm = phys.shader_bm;
+	enable.tiler_bm = phys.tiler_bm;
+	enable.mmu_l2_bm = phys.mmu_l2_bm;
+	enable.use_secondary = backend_jm->info->use_secondary;
+	enable.dump_buffer = backend_jm->gpu_dump_va;
+	enable.dump_buffer_bytes = backend_jm->info->dump_bytes;
+
+	timestamp_ns = kbasep_hwcnt_backend_jm_timestamp_ns(backend);
+
+	errcode = kbase_instr_hwcnt_enable_internal(kbdev, kctx, &enable);
+	if (errcode)
+		goto error;
+
+	backend_jm->pm_core_mask = kbase_pm_ca_get_instr_core_mask(kbdev);
+	backend_jm->enabled = true;
+
+	kbasep_hwcnt_backend_jm_cc_enable(backend_jm, enable_map, timestamp_ns);
+
+	return 0;
+error:
+	return errcode;
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_dump_enable_fn */
+static int kbasep_hwcnt_backend_jm_dump_enable(
+	struct kbase_hwcnt_backend *backend,
+	const struct kbase_hwcnt_enable_map *enable_map)
+{
+	unsigned long flags;
+	int errcode;
+	struct kbase_hwcnt_backend_jm *backend_jm =
+		(struct kbase_hwcnt_backend_jm *)backend;
+	struct kbase_device *kbdev;
+
+	if (!backend_jm)
+		return -EINVAL;
+
+	kbdev = backend_jm->kctx->kbdev;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+
+	errcode = kbasep_hwcnt_backend_jm_dump_enable_nolock(
+		backend, enable_map);
+
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return errcode;
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_dump_disable_fn */
+static void kbasep_hwcnt_backend_jm_dump_disable(
+	struct kbase_hwcnt_backend *backend)
+{
+	int errcode;
+	struct kbase_hwcnt_backend_jm *backend_jm =
+		(struct kbase_hwcnt_backend_jm *)backend;
+
+	if (WARN_ON(!backend_jm) || !backend_jm->enabled)
+		return;
+
+	kbasep_hwcnt_backend_jm_cc_disable(backend_jm);
+
+	errcode = kbase_instr_hwcnt_disable_internal(backend_jm->kctx);
+	WARN_ON(errcode);
+
+	backend_jm->enabled = false;
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_dump_clear_fn */
+static int kbasep_hwcnt_backend_jm_dump_clear(
+	struct kbase_hwcnt_backend *backend)
+{
+	struct kbase_hwcnt_backend_jm *backend_jm =
+		(struct kbase_hwcnt_backend_jm *)backend;
+
+	if (!backend_jm || !backend_jm->enabled)
+		return -EINVAL;
+
+	return kbase_instr_hwcnt_clear(backend_jm->kctx);
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_dump_request_fn */
+static int kbasep_hwcnt_backend_jm_dump_request(
+	struct kbase_hwcnt_backend *backend,
+	u64 *dump_time_ns)
+{
+	struct kbase_hwcnt_backend_jm *backend_jm =
+		(struct kbase_hwcnt_backend_jm *)backend;
+	struct kbase_device *kbdev;
+	const struct kbase_hwcnt_metadata *metadata;
+	u64 current_cycle_count;
+	size_t clk;
+	int ret;
+
+	if (!backend_jm || !backend_jm->enabled)
+		return -EINVAL;
+
+	kbdev = backend_jm->kctx->kbdev;
+	metadata = backend_jm->info->metadata;
+
+	/* Disable pre-emption, to make the timestamp as accurate as possible */
+	preempt_disable();
+	{
+		*dump_time_ns = kbasep_hwcnt_backend_jm_timestamp_ns(backend);
+		ret = kbase_instr_hwcnt_request_dump(backend_jm->kctx);
+
+		kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+			if (!kbase_hwcnt_clk_enable_map_enabled(
+				backend_jm->clk_enable_map, clk))
+				continue;
+
+			if (clk == KBASE_CLOCK_DOMAIN_TOP) {
+				/* Read cycle count for top clock domain. */
+				kbase_backend_get_gpu_time_norequest(
+					kbdev, &current_cycle_count,
+					NULL, NULL);
+			} else {
+				/*
+				 * Estimate cycle count for non-top clock
+				 * domain.
+				 */
+				current_cycle_count = kbase_ccswe_cycle_at(
+					&backend_jm->ccswe_shader_cores,
+					*dump_time_ns);
+			}
+			backend_jm->cycle_count_elapsed[clk] =
+				current_cycle_count -
+				backend_jm->prev_cycle_count[clk];
+
+			/*
+			 * Keep the current cycle count for later calculation.
+			 */
+			backend_jm->prev_cycle_count[clk] = current_cycle_count;
+		}
+	}
+	preempt_enable();
+
+	return ret;
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_dump_wait_fn */
+static int kbasep_hwcnt_backend_jm_dump_wait(
+	struct kbase_hwcnt_backend *backend)
+{
+	struct kbase_hwcnt_backend_jm *backend_jm =
+		(struct kbase_hwcnt_backend_jm *)backend;
+
+	if (!backend_jm || !backend_jm->enabled)
+		return -EINVAL;
+
+	return kbase_instr_hwcnt_wait_for_dump(backend_jm->kctx);
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_dump_get_fn */
+static int kbasep_hwcnt_backend_jm_dump_get(
+	struct kbase_hwcnt_backend *backend,
+	struct kbase_hwcnt_dump_buffer *dst,
+	const struct kbase_hwcnt_enable_map *dst_enable_map,
+	bool accumulate)
+{
+	struct kbase_hwcnt_backend_jm *backend_jm =
+		(struct kbase_hwcnt_backend_jm *)backend;
+	size_t clk;
+
+	if (!backend_jm || !dst || !dst_enable_map ||
+	    (backend_jm->info->metadata != dst->metadata) ||
+	    (dst_enable_map->metadata != dst->metadata))
+		return -EINVAL;
+
+	/* Invalidate the kernel buffer before reading from it. */
+	kbase_sync_mem_regions(
+		backend_jm->kctx, backend_jm->vmap, KBASE_SYNC_TO_CPU);
+
+	kbase_hwcnt_metadata_for_each_clock(dst_enable_map->metadata, clk) {
+		if (!kbase_hwcnt_clk_enable_map_enabled(
+			dst_enable_map->clk_enable_map, clk))
+			continue;
+
+		/* Extract elapsed cycle count for each clock domain. */
+		dst->clk_cnt_buf[clk] = backend_jm->cycle_count_elapsed[clk];
+	}
+
+	return kbase_hwcnt_gpu_dump_get(
+		dst, backend_jm->cpu_dump_va, dst_enable_map,
+		backend_jm->pm_core_mask, accumulate);
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_dump_alloc() - Allocate a GPU dump buffer.
+ * @info:        Non-NULL pointer to JM backend info.
+ * @kctx:        Non-NULL pointer to kbase context.
+ * @gpu_dump_va: Non-NULL pointer to where GPU dump buffer virtual address
+ *               is stored on success.
+ *
+ * Return: 0 on success, else error code.
+ */
+static int kbasep_hwcnt_backend_jm_dump_alloc(
+	const struct kbase_hwcnt_backend_jm_info *info,
+	struct kbase_context *kctx,
+	u64 *gpu_dump_va)
+{
+	struct kbase_va_region *reg;
+	u64 flags;
+	u64 nr_pages;
+
+	WARN_ON(!info);
+	WARN_ON(!kctx);
+	WARN_ON(!gpu_dump_va);
+
+	flags = BASE_MEM_PROT_CPU_RD |
+		BASE_MEM_PROT_GPU_WR |
+		BASEP_MEM_PERMANENT_KERNEL_MAPPING |
+		BASE_MEM_CACHED_CPU;
+
+	if (kctx->kbdev->mmu_mode->flags & KBASE_MMU_MODE_HAS_NON_CACHEABLE)
+		flags |= BASE_MEM_UNCACHED_GPU;
+
+	nr_pages = PFN_UP(info->dump_bytes);
+
+	reg = kbase_mem_alloc(kctx, nr_pages, nr_pages, 0, &flags, gpu_dump_va);
+
+	if (!reg)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_dump_free() - Free an allocated GPU dump buffer.
+ * @kctx:        Non-NULL pointer to kbase context.
+ * @gpu_dump_va: GPU dump buffer virtual address.
+ */
+static void kbasep_hwcnt_backend_jm_dump_free(
+	struct kbase_context *kctx,
+	u64 gpu_dump_va)
+{
+	WARN_ON(!kctx);
+	if (gpu_dump_va)
+		kbase_mem_free(kctx, gpu_dump_va);
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_destroy() - Destroy a JM backend.
+ * @backend: Pointer to JM backend to destroy.
+ *
+ * Can be safely called on a backend in any state of partial construction.
+ */
+static void kbasep_hwcnt_backend_jm_destroy(
+	struct kbase_hwcnt_backend_jm *backend)
+{
+	if (!backend)
+		return;
+
+	if (backend->kctx) {
+#if MALI_USE_CSF
+		unsigned long flags;
+#endif
+		struct kbase_context *kctx = backend->kctx;
+		struct kbase_device *kbdev = kctx->kbdev;
+
+		if (backend->cpu_dump_va)
+			kbase_phy_alloc_mapping_put(kctx, backend->vmap);
+
+		if (backend->gpu_dump_va)
+			kbasep_hwcnt_backend_jm_dump_free(
+				kctx, backend->gpu_dump_va);
+
+#if MALI_USE_CSF
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbase_ctx_sched_release_ctx(kctx);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+#else
+		kbasep_js_release_privileged_ctx(kbdev, kctx);
+#endif
+		kbase_destroy_context(kctx);
+	}
+
+	kfree(backend);
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_create() - Create a JM backend.
+ * @info:        Non-NULL pointer to backend info.
+ * @out_backend: Non-NULL pointer to where backend is stored on success.
+ *
+ * Return: 0 on success, else error code.
+ */
+static int kbasep_hwcnt_backend_jm_create(
+	const struct kbase_hwcnt_backend_jm_info *info,
+	struct kbase_hwcnt_backend_jm **out_backend)
+{
+#if MALI_USE_CSF
+	unsigned long flags;
+#endif
+	int errcode;
+	struct kbase_device *kbdev;
+	struct kbase_hwcnt_backend_jm *backend = NULL;
+
+	WARN_ON(!info);
+	WARN_ON(!out_backend);
+
+	kbdev = info->kbdev;
+
+	backend = kzalloc(sizeof(*backend), GFP_KERNEL);
+	if (!backend)
+		goto alloc_error;
+
+	backend->info = info;
+
+	backend->kctx = kbase_create_context(kbdev, true,
+		BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED, 0, NULL);
+	if (!backend->kctx)
+		goto alloc_error;
+
+#if MALI_USE_CSF
+	kbase_pm_context_active(kbdev);
+	mutex_lock(&kbdev->mmu_hw_mutex);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_ctx_sched_retain_ctx(backend->kctx);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
+	kbase_pm_context_idle(kbdev);
+#else
+	kbasep_js_schedule_privileged_ctx(kbdev, backend->kctx);
+#endif
+
+	errcode = kbasep_hwcnt_backend_jm_dump_alloc(
+		info, backend->kctx, &backend->gpu_dump_va);
+	if (errcode)
+		goto error;
+
+	backend->cpu_dump_va = kbase_phy_alloc_mapping_get(backend->kctx,
+		backend->gpu_dump_va, &backend->vmap);
+	if (!backend->cpu_dump_va)
+		goto alloc_error;
+
+	kbase_ccswe_init(&backend->ccswe_shader_cores);
+	backend->rate_listener.notify = kbasep_hwcnt_backend_jm_on_freq_change;
+
+#ifdef CONFIG_MALI_BIFROST_NO_MALI
+	/* The dummy model needs the CPU mapping. */
+	gpu_model_set_dummy_prfcnt_base_cpu(backend->cpu_dump_va);
+#endif
+
+	*out_backend = backend;
+	return 0;
+
+alloc_error:
+	errcode = -ENOMEM;
+error:
+	kbasep_hwcnt_backend_jm_destroy(backend);
+	return errcode;
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_init_fn */
+static int kbasep_hwcnt_backend_jm_init(
+	const struct kbase_hwcnt_backend_info *info,
+	struct kbase_hwcnt_backend **out_backend)
+{
+	int errcode;
+	struct kbase_hwcnt_backend_jm *backend = NULL;
+
+	if (!info || !out_backend)
+		return -EINVAL;
+
+	errcode = kbasep_hwcnt_backend_jm_create(
+		(const struct kbase_hwcnt_backend_jm_info *) info, &backend);
+	if (errcode)
+		return errcode;
+
+	*out_backend = (struct kbase_hwcnt_backend *)backend;
+
+	return 0;
+}
+
+/* JM backend implementation of kbase_hwcnt_backend_term_fn */
+static void kbasep_hwcnt_backend_jm_term(struct kbase_hwcnt_backend *backend)
+{
+	if (!backend)
+		return;
+
+	kbasep_hwcnt_backend_jm_dump_disable(backend);
+	kbasep_hwcnt_backend_jm_destroy(
+		(struct kbase_hwcnt_backend_jm *)backend);
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_info_destroy() - Destroy a JM backend info.
+ * @info: Pointer to info to destroy.
+ *
+ * Can be safely called on a backend info in any state of partial construction.
+ */
+static void kbasep_hwcnt_backend_jm_info_destroy(
+	const struct kbase_hwcnt_backend_jm_info *info)
+{
+	if (!info)
+		return;
+
+	kbase_hwcnt_gpu_metadata_destroy(info->metadata);
+	kfree(info);
+}
+
+/**
+ * kbasep_hwcnt_backend_jm_info_create() - Create a JM backend info.
+ * @kbdev: Non_NULL pointer to kbase device.
+ * @out_info: Non-NULL pointer to where info is stored on success.
+ *
+ * Return 0 on success, else error code.
+ */
+static int kbasep_hwcnt_backend_jm_info_create(
+	struct kbase_device *kbdev,
+	const struct kbase_hwcnt_backend_jm_info **out_info)
+{
+	int errcode = -ENOMEM;
+	struct kbase_hwcnt_gpu_info hwcnt_gpu_info;
+	struct kbase_hwcnt_backend_jm_info *info = NULL;
+
+	WARN_ON(!kbdev);
+	WARN_ON(!out_info);
+
+	errcode = kbase_hwcnt_gpu_info_init(kbdev, &hwcnt_gpu_info);
+	if (errcode)
+		return errcode;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info)
+		goto error;
+
+	info->kbdev = kbdev;
+
+#ifdef CONFIG_MALI_BIFROST_PRFCNT_SET_SECONDARY
+	info->use_secondary = true;
+#else
+	info->use_secondary = false;
+#endif
+
+	errcode = kbase_hwcnt_gpu_metadata_create(
+		&hwcnt_gpu_info, info->use_secondary,
+		&info->metadata,
+		&info->dump_bytes);
+	if (errcode)
+		goto error;
+
+	*out_info = info;
+
+	return 0;
+error:
+	kbasep_hwcnt_backend_jm_info_destroy(info);
+	return errcode;
+}
+
+int kbase_hwcnt_backend_jm_create(
+	struct kbase_device *kbdev,
+	struct kbase_hwcnt_backend_interface *iface)
+{
+	int errcode;
+	const struct kbase_hwcnt_backend_jm_info *info = NULL;
+
+	if (!kbdev || !iface)
+		return -EINVAL;
+
+	errcode = kbasep_hwcnt_backend_jm_info_create(kbdev, &info);
+
+	if (errcode)
+		return errcode;
+
+	iface->metadata = info->metadata;
+	iface->info = (struct kbase_hwcnt_backend_info *)info;
+	iface->init = kbasep_hwcnt_backend_jm_init;
+	iface->term = kbasep_hwcnt_backend_jm_term;
+	iface->timestamp_ns = kbasep_hwcnt_backend_jm_timestamp_ns;
+	iface->dump_enable = kbasep_hwcnt_backend_jm_dump_enable;
+	iface->dump_enable_nolock = kbasep_hwcnt_backend_jm_dump_enable_nolock;
+	iface->dump_disable = kbasep_hwcnt_backend_jm_dump_disable;
+	iface->dump_clear = kbasep_hwcnt_backend_jm_dump_clear;
+	iface->dump_request = kbasep_hwcnt_backend_jm_dump_request;
+	iface->dump_wait = kbasep_hwcnt_backend_jm_dump_wait;
+	iface->dump_get = kbasep_hwcnt_backend_jm_dump_get;
+
+	return 0;
+}
+
+void kbase_hwcnt_backend_jm_destroy(
+	struct kbase_hwcnt_backend_interface *iface)
+{
+	if (!iface)
+		return;
+
+	kbasep_hwcnt_backend_jm_info_destroy(
+		(const struct kbase_hwcnt_backend_jm_info *)iface->info);
+	memset(iface, 0, sizeof(*iface));
+}
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_gpu.h b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_jm.h
similarity index 79%
rename from drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_gpu.h
rename to drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_jm.h
index 7712f1424a8b..f15faeba704a 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_gpu.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_backend_jm.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -21,19 +21,19 @@
  */
 
 /**
- * Concrete implementation of mali_kbase_hwcnt_backend interface for GPU
+ * Concrete implementation of mali_kbase_hwcnt_backend interface for JM
  * backend.
  */
 
-#ifndef _KBASE_HWCNT_BACKEND_GPU_H_
-#define _KBASE_HWCNT_BACKEND_GPU_H_
+#ifndef _KBASE_HWCNT_BACKEND_JM_H_
+#define _KBASE_HWCNT_BACKEND_JM_H_
 
 #include "mali_kbase_hwcnt_backend.h"
 
 struct kbase_device;
 
 /**
- * kbase_hwcnt_backend_gpu_create() - Create a GPU hardware counter backend
+ * kbase_hwcnt_backend_jm_create() - Create a JM hardware counter backend
  *                                    interface.
  * @kbdev: Non-NULL pointer to kbase device.
  * @iface: Non-NULL pointer to backend interface structure that is filled in
@@ -43,19 +43,19 @@ struct kbase_device;
  *
  * Return: 0 on success, else error code.
  */
-int kbase_hwcnt_backend_gpu_create(
+int kbase_hwcnt_backend_jm_create(
 	struct kbase_device *kbdev,
 	struct kbase_hwcnt_backend_interface *iface);
 
 /**
- * kbase_hwcnt_backend_gpu_destroy() - Destroy a GPU hardware counter backend
+ * kbase_hwcnt_backend_jm_destroy() - Destroy a JM hardware counter backend
  *                                     interface.
  * @iface: Pointer to interface to destroy.
  *
  * Can be safely called on an all-zeroed interface, or on an already destroyed
  * interface.
  */
-void kbase_hwcnt_backend_gpu_destroy(
+void kbase_hwcnt_backend_jm_destroy(
 	struct kbase_hwcnt_backend_interface *iface);
 
-#endif /* _KBASE_HWCNT_BACKEND_GPU_H_ */
+#endif /* _KBASE_HWCNT_BACKEND_JM_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.c b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.c
index 73d1802c70bf..499f3bc23bec 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2018-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -27,17 +27,6 @@
 #include "backend/gpu/mali_kbase_model_dummy.h"
 #endif
 
-#define KBASE_HWCNT_V4_BLOCKS_PER_GROUP 8
-#define KBASE_HWCNT_V4_SC_BLOCKS_PER_GROUP 4
-#define KBASE_HWCNT_V4_MAX_GROUPS \
-	(KBASE_HWCNT_AVAIL_MASK_BITS / KBASE_HWCNT_V4_BLOCKS_PER_GROUP)
-#define KBASE_HWCNT_V4_HEADERS_PER_BLOCK 4
-#define KBASE_HWCNT_V4_COUNTERS_PER_BLOCK 60
-#define KBASE_HWCNT_V4_VALUES_PER_BLOCK \
-	(KBASE_HWCNT_V4_HEADERS_PER_BLOCK + KBASE_HWCNT_V4_COUNTERS_PER_BLOCK)
-/* Index of the PRFCNT_EN header into a V4 counter block */
-#define KBASE_HWCNT_V4_PRFCNT_EN_HEADER 2
-
 #define KBASE_HWCNT_V5_BLOCK_TYPE_COUNT 4
 #define KBASE_HWCNT_V5_HEADERS_PER_BLOCK 4
 #define KBASE_HWCNT_V5_COUNTERS_PER_BLOCK 60
@@ -46,126 +35,6 @@
 /* Index of the PRFCNT_EN header into a V5 counter block */
 #define KBASE_HWCNT_V5_PRFCNT_EN_HEADER 2
 
-/**
- * kbasep_hwcnt_backend_gpu_metadata_v4_create() - Create hardware counter
- *                                                 metadata for a v4 GPU.
- * @v4_info:  Non-NULL pointer to hwcnt info for a v4 GPU.
- * @metadata: Non-NULL pointer to where created metadata is stored on success.
- *
- * Return: 0 on success, else error code.
- */
-static int kbasep_hwcnt_backend_gpu_metadata_v4_create(
-	const struct kbase_hwcnt_gpu_v4_info *v4_info,
-	const struct kbase_hwcnt_metadata **metadata)
-{
-	size_t grp;
-	int errcode = -ENOMEM;
-	struct kbase_hwcnt_description desc;
-	struct kbase_hwcnt_group_description *grps;
-	size_t avail_mask_bit;
-
-	WARN_ON(!v4_info);
-	WARN_ON(!metadata);
-
-	/* Check if there are enough bits in the availability mask to represent
-	 * all the hardware counter blocks in the system.
-	 */
-	if (v4_info->cg_count > KBASE_HWCNT_V4_MAX_GROUPS)
-		return -EINVAL;
-
-	grps = kcalloc(v4_info->cg_count, sizeof(*grps), GFP_KERNEL);
-	if (!grps)
-		goto clean_up;
-
-	desc.grp_cnt = v4_info->cg_count;
-	desc.grps = grps;
-
-	for (grp = 0; grp < v4_info->cg_count; grp++) {
-		size_t blk;
-		size_t sc;
-		const u64 core_mask = v4_info->cgs[grp].core_mask;
-		struct kbase_hwcnt_block_description *blks = kcalloc(
-			KBASE_HWCNT_V4_BLOCKS_PER_GROUP,
-			sizeof(*blks),
-			GFP_KERNEL);
-
-		if (!blks)
-			goto clean_up;
-
-		grps[grp].type = KBASE_HWCNT_GPU_GROUP_TYPE_V4;
-		grps[grp].blk_cnt = KBASE_HWCNT_V4_BLOCKS_PER_GROUP;
-		grps[grp].blks = blks;
-
-		for (blk = 0; blk < KBASE_HWCNT_V4_BLOCKS_PER_GROUP; blk++) {
-			blks[blk].inst_cnt = 1;
-			blks[blk].hdr_cnt =
-				KBASE_HWCNT_V4_HEADERS_PER_BLOCK;
-			blks[blk].ctr_cnt =
-				KBASE_HWCNT_V4_COUNTERS_PER_BLOCK;
-		}
-
-		for (sc = 0; sc < KBASE_HWCNT_V4_SC_BLOCKS_PER_GROUP; sc++) {
-			blks[sc].type = core_mask & (1ull << sc) ?
-				KBASE_HWCNT_GPU_V4_BLOCK_TYPE_SHADER :
-				KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED;
-		}
-
-		blks[4].type = KBASE_HWCNT_GPU_V4_BLOCK_TYPE_TILER;
-		blks[5].type = KBASE_HWCNT_GPU_V4_BLOCK_TYPE_MMU_L2;
-		blks[6].type = KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED;
-		blks[7].type = (grp == 0) ?
-			KBASE_HWCNT_GPU_V4_BLOCK_TYPE_JM :
-			KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED;
-
-		WARN_ON(KBASE_HWCNT_V4_BLOCKS_PER_GROUP != 8);
-	}
-
-	/* Initialise the availability mask */
-	desc.avail_mask = 0;
-	avail_mask_bit = 0;
-
-	for (grp = 0; grp < desc.grp_cnt; grp++) {
-		size_t blk;
-		const struct kbase_hwcnt_block_description *blks =
-			desc.grps[grp].blks;
-		for (blk = 0; blk < desc.grps[grp].blk_cnt; blk++) {
-			WARN_ON(blks[blk].inst_cnt != 1);
-			if (blks[blk].type !=
-			    KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED)
-				desc.avail_mask |= (1ull << avail_mask_bit);
-
-			avail_mask_bit++;
-		}
-	}
-
-	errcode = kbase_hwcnt_metadata_create(&desc, metadata);
-
-	/* Always clean up, as metadata will make a copy of the input args */
-clean_up:
-	if (grps) {
-		for (grp = 0; grp < v4_info->cg_count; grp++)
-			kfree(grps[grp].blks);
-		kfree(grps);
-	}
-	return errcode;
-}
-
-/**
- * kbasep_hwcnt_backend_gpu_v4_dump_bytes() - Get the raw dump buffer size for a
- *                                            V4 GPU.
- * @v4_info: Non-NULL pointer to hwcnt info for a v4 GPU.
- *
- * Return: Size of buffer the V4 GPU needs to perform a counter dump.
- */
-static size_t kbasep_hwcnt_backend_gpu_v4_dump_bytes(
-	const struct kbase_hwcnt_gpu_v4_info *v4_info)
-{
-	return v4_info->cg_count *
-		KBASE_HWCNT_V4_BLOCKS_PER_GROUP *
-		KBASE_HWCNT_V4_VALUES_PER_BLOCK *
-		KBASE_HWCNT_VALUE_BYTES;
-}
-
 /**
  * kbasep_hwcnt_backend_gpu_metadata_v5_create() - Create hardware counter
  *                                                 metadata for a v5 GPU.
@@ -258,6 +127,7 @@ static int kbasep_hwcnt_backend_gpu_metadata_v5_create(
 
 	desc.grp_cnt = 1;
 	desc.grps = &group;
+	desc.clk_cnt = v5_info->clk_cnt;
 
 	/* The JM, Tiler, and L2s are always available, and are before cores */
 	desc.avail_mask = (1ull << non_sc_block_count) - 1;
@@ -287,6 +157,8 @@ int kbase_hwcnt_gpu_info_init(
 	struct kbase_device *kbdev,
 	struct kbase_hwcnt_gpu_info *info)
 {
+	size_t clk;
+
 	if (!kbdev || !info)
 		return -EINVAL;
 
@@ -307,6 +179,14 @@ int kbase_hwcnt_gpu_info_init(
 		info->v5.core_mask = core_mask;
 	}
 #endif
+
+	/* Determine the number of available clock domains. */
+	for (clk = 0; clk < BASE_MAX_NR_CLOCKS_REGULATORS; clk++) {
+		if (kbdev->pm.clk_rtm.clks[clk] == NULL)
+			break;
+	}
+	info->v5.clk_cnt = clk;
+
 	return 0;
 }
 
@@ -323,18 +203,11 @@ int kbase_hwcnt_gpu_metadata_create(
 	if (!info || !out_metadata || !out_dump_bytes)
 		return -EINVAL;
 
-	switch (info->type) {
-	case KBASE_HWCNT_GPU_GROUP_TYPE_V4:
-		dump_bytes = kbasep_hwcnt_backend_gpu_v4_dump_bytes(&info->v4);
-		errcode = kbasep_hwcnt_backend_gpu_metadata_v4_create(
-			&info->v4, &metadata);
-		break;
-	case KBASE_HWCNT_GPU_GROUP_TYPE_V5:
+	if (info->type == KBASE_HWCNT_GPU_GROUP_TYPE_V5) {
 		dump_bytes = kbasep_hwcnt_backend_gpu_v5_dump_bytes(&info->v5);
 		errcode = kbasep_hwcnt_backend_gpu_metadata_v5_create(
 			&info->v5, use_secondary, &metadata);
-		break;
-	default:
+	} else {
 		return -EINVAL;
 	}
 	if (errcode)
@@ -370,27 +243,13 @@ static bool is_block_type_shader(
 {
 	bool is_shader = false;
 
-	switch (grp_type) {
-	case KBASE_HWCNT_GPU_GROUP_TYPE_V4:
-		/* blk-value in [0, KBASE_HWCNT_V4_SC_BLOCKS_PER_GROUP-1]
-		 * corresponds to a shader, or its implementation
-		 * reserved. As such, here we use the blk index value to
-		 * tell the reserved case.
-		 */
-		if (blk_type == KBASE_HWCNT_GPU_V4_BLOCK_TYPE_SHADER ||
-		    (blk < KBASE_HWCNT_V4_SC_BLOCKS_PER_GROUP &&
-		     blk_type == KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED))
-			is_shader = true;
-		break;
-	case KBASE_HWCNT_GPU_GROUP_TYPE_V5:
-		if (blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC ||
-		    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC2)
-			is_shader = true;
-		break;
-	default:
-		/* Warn on unknown group type */
-		WARN_ON(true);
-	}
+	/* Warn on unknown group type */
+	if (WARN_ON(grp_type != KBASE_HWCNT_GPU_GROUP_TYPE_V5))
+		return false;
+
+	if (blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC ||
+	    blk_type == KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_SC2)
+		is_shader = true;
 
 	return is_shader;
 }
@@ -405,7 +264,6 @@ int kbase_hwcnt_gpu_dump_get(
 	const struct kbase_hwcnt_metadata *metadata;
 	const u32 *dump_src;
 	size_t src_offset, grp, blk, blk_inst;
-	size_t grp_prev = 0;
 	u64 core_mask = pm_core_mask;
 
 	if (!dst || !src || !dst_enable_map ||
@@ -430,18 +288,6 @@ int kbase_hwcnt_gpu_dump_get(
 			kbase_hwcnt_metadata_group_type(metadata, grp),
 			blk_type, blk);
 
-		if (grp != grp_prev) {
-			/* grp change would only happen with V4. V5 and
-			 * further are envisaged to be single group
-			 * scenario only. Here needs to drop the lower
-			 * group core-mask by shifting right with
-			 * KBASE_HWCNT_V4_SC_BLOCKS_PER_GROUP.
-			 */
-			core_mask = pm_core_mask >>
-				KBASE_HWCNT_V4_SC_BLOCKS_PER_GROUP;
-			grp_prev = grp;
-		}
-
 		/* Early out if no values in the dest block are enabled */
 		if (kbase_hwcnt_enable_map_block_enabled(
 			dst_enable_map, grp, blk, blk_inst)) {
@@ -563,7 +409,7 @@ void kbase_hwcnt_gpu_enable_map_to_physical(
 {
 	const struct kbase_hwcnt_metadata *metadata;
 
-	u64 jm_bm = 0;
+	u64 fe_bm = 0;
 	u64 shader_bm = 0;
 	u64 tiler_bm = 0;
 	u64 mmu_l2_bm = 0;
@@ -587,33 +433,12 @@ void kbase_hwcnt_gpu_enable_map_to_physical(
 		const u64 *blk_map = kbase_hwcnt_enable_map_block_instance(
 			src, grp, blk, blk_inst);
 
-		switch ((enum kbase_hwcnt_gpu_group_type)grp_type) {
-		case KBASE_HWCNT_GPU_GROUP_TYPE_V4:
-			WARN_ON(blk_val_cnt != KBASE_HWCNT_V4_VALUES_PER_BLOCK);
-			switch ((enum kbase_hwcnt_gpu_v4_block_type)blk_type) {
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_SHADER:
-				shader_bm |= *blk_map;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_TILER:
-				tiler_bm |= *blk_map;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_MMU_L2:
-				mmu_l2_bm |= *blk_map;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_JM:
-				jm_bm |= *blk_map;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED:
-				break;
-			default:
-				WARN_ON(true);
-			}
-			break;
-		case KBASE_HWCNT_GPU_GROUP_TYPE_V5:
+		if ((enum kbase_hwcnt_gpu_group_type)grp_type ==
+		    KBASE_HWCNT_GPU_GROUP_TYPE_V5) {
 			WARN_ON(blk_val_cnt != KBASE_HWCNT_V5_VALUES_PER_BLOCK);
 			switch ((enum kbase_hwcnt_gpu_v5_block_type)blk_type) {
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_JM:
-				jm_bm |= *blk_map;
+				fe_bm |= *blk_map;
 				break;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_TILER:
 				tiler_bm |= *blk_map;
@@ -629,14 +454,13 @@ void kbase_hwcnt_gpu_enable_map_to_physical(
 			default:
 				WARN_ON(true);
 			}
-			break;
-		default:
+		} else {
 			WARN_ON(true);
 		}
 	}
 
-	dst->jm_bm =
-		kbasep_hwcnt_backend_gpu_block_map_to_physical(jm_bm, 0);
+	dst->fe_bm =
+		kbasep_hwcnt_backend_gpu_block_map_to_physical(fe_bm, 0);
 	dst->shader_bm =
 		kbasep_hwcnt_backend_gpu_block_map_to_physical(shader_bm, 0);
 	dst->tiler_bm =
@@ -653,7 +477,7 @@ void kbase_hwcnt_gpu_enable_map_from_physical(
 	const struct kbase_hwcnt_metadata *metadata;
 
 	u64 ignored_hi;
-	u64 jm_bm;
+	u64 fe_bm;
 	u64 shader_bm;
 	u64 tiler_bm;
 	u64 mmu_l2_bm;
@@ -665,7 +489,7 @@ void kbase_hwcnt_gpu_enable_map_from_physical(
 	metadata = dst->metadata;
 
 	kbasep_hwcnt_backend_gpu_block_map_from_physical(
-		src->jm_bm, &jm_bm, &ignored_hi);
+		src->fe_bm, &fe_bm, &ignored_hi);
 	kbasep_hwcnt_backend_gpu_block_map_from_physical(
 		src->shader_bm, &shader_bm, &ignored_hi);
 	kbasep_hwcnt_backend_gpu_block_map_from_physical(
@@ -684,33 +508,12 @@ void kbase_hwcnt_gpu_enable_map_from_physical(
 		u64 *blk_map = kbase_hwcnt_enable_map_block_instance(
 			dst, grp, blk, blk_inst);
 
-		switch ((enum kbase_hwcnt_gpu_group_type)grp_type) {
-		case KBASE_HWCNT_GPU_GROUP_TYPE_V4:
-			WARN_ON(blk_val_cnt != KBASE_HWCNT_V4_VALUES_PER_BLOCK);
-			switch ((enum kbase_hwcnt_gpu_v4_block_type)blk_type) {
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_SHADER:
-				*blk_map = shader_bm;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_TILER:
-				*blk_map = tiler_bm;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_MMU_L2:
-				*blk_map = mmu_l2_bm;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_JM:
-				*blk_map = jm_bm;
-				break;
-			case KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED:
-				break;
-			default:
-				WARN_ON(true);
-			}
-			break;
-		case KBASE_HWCNT_GPU_GROUP_TYPE_V5:
+		if ((enum kbase_hwcnt_gpu_group_type)grp_type ==
+		    KBASE_HWCNT_GPU_GROUP_TYPE_V5) {
 			WARN_ON(blk_val_cnt != KBASE_HWCNT_V5_VALUES_PER_BLOCK);
 			switch ((enum kbase_hwcnt_gpu_v5_block_type)blk_type) {
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_JM:
-				*blk_map = jm_bm;
+				*blk_map = fe_bm;
 				break;
 			case KBASE_HWCNT_GPU_V5_BLOCK_TYPE_PERF_TILER:
 				*blk_map = tiler_bm;
@@ -726,8 +529,7 @@ void kbase_hwcnt_gpu_enable_map_from_physical(
 			default:
 				WARN_ON(true);
 			}
-			break;
-		default:
+		} else {
 			WARN_ON(true);
 		}
 	}
@@ -758,14 +560,10 @@ void kbase_hwcnt_gpu_patch_dump_headers(
 			kbasep_hwcnt_backend_gpu_block_map_to_physical(
 				blk_map[0], 0);
 
-		switch ((enum kbase_hwcnt_gpu_group_type)grp_type) {
-		case KBASE_HWCNT_GPU_GROUP_TYPE_V4:
-			buf_blk[KBASE_HWCNT_V4_PRFCNT_EN_HEADER] = prfcnt_en;
-			break;
-		case KBASE_HWCNT_GPU_GROUP_TYPE_V5:
+		if ((enum kbase_hwcnt_gpu_group_type)grp_type ==
+		    KBASE_HWCNT_GPU_GROUP_TYPE_V5) {
 			buf_blk[KBASE_HWCNT_V5_PRFCNT_EN_HEADER] = prfcnt_en;
-			break;
-		default:
+		} else {
 			WARN_ON(true);
 		}
 	}
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.h b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.h
index 12891e036a80..f0d51763f7f7 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_gpu.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -33,29 +33,10 @@ struct kbase_hwcnt_dump_buffer;
 /**
  * enum kbase_hwcnt_gpu_group_type - GPU hardware counter group types, used to
  *                                   identify metadata groups.
- * @KBASE_HWCNT_GPU_GROUP_TYPE_V4: GPU V4 group type.
  * @KBASE_HWCNT_GPU_GROUP_TYPE_V5: GPU V5 group type.
  */
 enum kbase_hwcnt_gpu_group_type {
-	KBASE_HWCNT_GPU_GROUP_TYPE_V4 = 0x10,
-	KBASE_HWCNT_GPU_GROUP_TYPE_V5,
-};
-
-/**
- * enum kbase_hwcnt_gpu_v4_block_type - GPU V4 hardware counter block types,
- *                                      used to identify metadata blocks.
- * @KBASE_HWCNT_GPU_V4_BLOCK_TYPE_SHADER:   Shader block.
- * @KBASE_HWCNT_GPU_V4_BLOCK_TYPE_TILER:    Tiler block.
- * @KBASE_HWCNT_GPU_V4_BLOCK_TYPE_MMU_L2:   MMU/L2 block.
- * @KBASE_HWCNT_GPU_V4_BLOCK_TYPE_JM:       Job Manager block.
- * @KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED: Reserved block.
- */
-enum kbase_hwcnt_gpu_v4_block_type {
-	KBASE_HWCNT_GPU_V4_BLOCK_TYPE_SHADER = 0x20,
-	KBASE_HWCNT_GPU_V4_BLOCK_TYPE_TILER,
-	KBASE_HWCNT_GPU_V4_BLOCK_TYPE_MMU_L2,
-	KBASE_HWCNT_GPU_V4_BLOCK_TYPE_JM,
-	KBASE_HWCNT_GPU_V4_BLOCK_TYPE_RESERVED,
+	KBASE_HWCNT_GPU_GROUP_TYPE_V5 = 0x10,
 };
 
 /**
@@ -80,54 +61,39 @@ enum kbase_hwcnt_gpu_v5_block_type {
 /**
  * struct kbase_hwcnt_physical_enable_map - Representation of enable map
  *                                          directly used by GPU.
- * @jm_bm:     Job Manager counters selection bitmask.
+ * @fe_bm:     Front end (JM/CSHW) counters selection bitmask.
  * @shader_bm: Shader counters selection bitmask.
  * @tiler_bm:  Tiler counters selection bitmask.
  * @mmu_l2_bm: MMU_L2 counters selection bitmask.
  */
 struct kbase_hwcnt_physical_enable_map {
-	u32 jm_bm;
+	u32 fe_bm;
 	u32 shader_bm;
 	u32 tiler_bm;
 	u32 mmu_l2_bm;
 };
 
-/**
- * struct kbase_hwcnt_gpu_v4_info - Information about hwcnt blocks on v4 GPUs.
- * @cg_count: Core group count.
- * @cgs:      Non-NULL pointer to array of cg_count coherent group structures.
- *
- * V4 devices are Mali-T6xx or Mali-T72x, and have one or more core groups,
- * where each core group may have a physically different layout.
- */
-struct kbase_hwcnt_gpu_v4_info {
-	size_t cg_count;
-	const struct mali_base_gpu_coherent_group *cgs;
-};
-
 /**
  * struct kbase_hwcnt_gpu_v5_info - Information about hwcnt blocks on v5 GPUs.
  * @l2_count:   L2 cache count.
  * @core_mask:  Shader core mask. May be sparse.
+ * @clk_cnt:    Number of clock domains available.
  */
 struct kbase_hwcnt_gpu_v5_info {
 	size_t l2_count;
 	u64 core_mask;
+	u8 clk_cnt;
 };
 
 /**
  * struct kbase_hwcnt_gpu_info - Tagged union with information about the current
  *                               GPU's hwcnt blocks.
  * @type: GPU type.
- * @v4:   Info filled in if a v4 GPU.
  * @v5:   Info filled in if a v5 GPU.
  */
 struct kbase_hwcnt_gpu_info {
 	enum kbase_hwcnt_gpu_group_type type;
-	union {
-		struct kbase_hwcnt_gpu_v4_info v4;
-		struct kbase_hwcnt_gpu_v5_info v5;
-	};
+	struct kbase_hwcnt_gpu_v5_info v5;
 };
 
 /**
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_legacy.c b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_legacy.c
index b0e6aee1b135..794ef39e365c 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_legacy.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_legacy.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -69,7 +69,7 @@ int kbase_hwcnt_legacy_client_create(
 		goto error;
 
 	/* Translate from the ioctl enable map to the internal one */
-	phys_em.jm_bm = enable->jm_bm;
+	phys_em.fe_bm = enable->fe_bm;
 	phys_em.shader_bm = enable->shader_bm;
 	phys_em.tiler_bm = enable->tiler_bm;
 	phys_em.mmu_l2_bm = enable->mmu_l2_bm;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_reader.h b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_reader.h
index 10706b8d2548..8cd3835595f7 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_reader.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_reader.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2015 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2015, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,31 +23,53 @@
 #ifndef _KBASE_HWCNT_READER_H_
 #define _KBASE_HWCNT_READER_H_
 
+#include <stddef.h>
+
 /* The ids of ioctl commands. */
 #define KBASE_HWCNT_READER 0xBE
 #define KBASE_HWCNT_READER_GET_HWVER       _IOR(KBASE_HWCNT_READER, 0x00, u32)
 #define KBASE_HWCNT_READER_GET_BUFFER_SIZE _IOR(KBASE_HWCNT_READER, 0x01, u32)
 #define KBASE_HWCNT_READER_DUMP            _IOW(KBASE_HWCNT_READER, 0x10, u32)
 #define KBASE_HWCNT_READER_CLEAR           _IOW(KBASE_HWCNT_READER, 0x11, u32)
-#define KBASE_HWCNT_READER_GET_BUFFER      _IOR(KBASE_HWCNT_READER, 0x20,\
+#define KBASE_HWCNT_READER_GET_BUFFER      _IOC(_IOC_READ, KBASE_HWCNT_READER, 0x20,\
+		offsetof(struct kbase_hwcnt_reader_metadata, cycles))
+#define KBASE_HWCNT_READER_GET_BUFFER_WITH_CYCLES      _IOR(KBASE_HWCNT_READER, 0x20,\
 		struct kbase_hwcnt_reader_metadata)
-#define KBASE_HWCNT_READER_PUT_BUFFER      _IOW(KBASE_HWCNT_READER, 0x21,\
+#define KBASE_HWCNT_READER_PUT_BUFFER      _IOC(_IOC_WRITE, KBASE_HWCNT_READER, 0x21,\
+		offsetof(struct kbase_hwcnt_reader_metadata, cycles))
+#define KBASE_HWCNT_READER_PUT_BUFFER_WITH_CYCLES      _IOW(KBASE_HWCNT_READER, 0x21,\
 		struct kbase_hwcnt_reader_metadata)
 #define KBASE_HWCNT_READER_SET_INTERVAL    _IOW(KBASE_HWCNT_READER, 0x30, u32)
 #define KBASE_HWCNT_READER_ENABLE_EVENT    _IOW(KBASE_HWCNT_READER, 0x40, u32)
 #define KBASE_HWCNT_READER_DISABLE_EVENT   _IOW(KBASE_HWCNT_READER, 0x41, u32)
 #define KBASE_HWCNT_READER_GET_API_VERSION _IOW(KBASE_HWCNT_READER, 0xFF, u32)
+#define KBASE_HWCNT_READER_GET_API_VERSION_WITH_FEATURES \
+		_IOW(KBASE_HWCNT_READER, 0xFF, \
+		     struct kbase_hwcnt_reader_api_version)
+
+/**
+ * struct kbase_hwcnt_reader_metadata_cycles - GPU clock cycles
+ * @top:           the number of cycles associated with the main clock for the
+ *                 GPU
+ * @shader_cores:  the cycles that have elapsed on the GPU shader cores
+ */
+struct kbase_hwcnt_reader_metadata_cycles {
+	u64 top;
+	u64 shader_cores;
+};
 
 /**
  * struct kbase_hwcnt_reader_metadata - hwcnt reader sample buffer metadata
  * @timestamp:  time when sample was collected
  * @event_id:   id of an event that triggered sample collection
  * @buffer_idx: position in sampling area where sample buffer was stored
+ * @cycles:     the GPU cycles that occurred since the last sample
  */
 struct kbase_hwcnt_reader_metadata {
 	u64 timestamp;
 	u32 event_id;
 	u32 buffer_idx;
+	struct kbase_hwcnt_reader_metadata_cycles cycles;
 };
 
 /**
@@ -67,5 +89,18 @@ enum base_hwcnt_reader_event {
 	BASE_HWCNT_READER_EVENT_COUNT
 };
 
+/**
+ * struct kbase_hwcnt_reader_api_version - hwcnt reader API version
+ * @versoin:  API version
+ * @features: available features in this API version
+ */
+#define KBASE_HWCNT_READER_API_VERSION_NO_FEATURE                  (0)
+#define KBASE_HWCNT_READER_API_VERSION_FEATURE_CYCLES_TOP          (1 << 0)
+#define KBASE_HWCNT_READER_API_VERSION_FEATURE_CYCLES_SHADER_CORES (1 << 1)
+struct kbase_hwcnt_reader_api_version {
+	u32 version;
+	u32 features;
+};
+
 #endif /* _KBASE_HWCNT_READER_H_ */
 
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.c b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.c
index 1e9efde97c59..2b9fe02acd75 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -55,6 +55,10 @@ int kbase_hwcnt_metadata_create(
 	if (!desc || !out_metadata)
 		return -EINVAL;
 
+	/* The maximum number of clock domains is 64. */
+	if (desc->clk_cnt > (sizeof(u64) * BITS_PER_BYTE))
+		return -EINVAL;
+
 	/* Calculate the bytes needed to tightly pack the metadata */
 
 	/* Top level metadata */
@@ -158,6 +162,7 @@ int kbase_hwcnt_metadata_create(
 		enable_map_count * KBASE_HWCNT_BITFIELD_BYTES;
 	metadata->dump_buf_bytes = dump_buf_count * KBASE_HWCNT_VALUE_BYTES;
 	metadata->avail_mask = desc->avail_mask;
+	metadata->clk_cnt = desc->clk_cnt;
 
 	WARN_ON(size != offset);
 	/* Due to the block alignment, there should be exactly one enable map
@@ -187,12 +192,17 @@ int kbase_hwcnt_enable_map_alloc(
 	if (!metadata || !enable_map)
 		return -EINVAL;
 
-	enable_map_buf = kzalloc(metadata->enable_map_bytes, GFP_KERNEL);
-	if (!enable_map_buf)
-		return -ENOMEM;
+	if (metadata->enable_map_bytes > 0) {
+		enable_map_buf =
+			kzalloc(metadata->enable_map_bytes, GFP_KERNEL);
+		if (!enable_map_buf)
+			return -ENOMEM;
+	} else {
+		enable_map_buf = NULL;
+	}
 
 	enable_map->metadata = metadata;
-	enable_map->enable_map = enable_map_buf;
+	enable_map->hwcnt_enable_map = enable_map_buf;
 	return 0;
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_enable_map_alloc);
@@ -202,8 +212,8 @@ void kbase_hwcnt_enable_map_free(struct kbase_hwcnt_enable_map *enable_map)
 	if (!enable_map)
 		return;
 
-	kfree(enable_map->enable_map);
-	enable_map->enable_map = NULL;
+	kfree(enable_map->hwcnt_enable_map);
+	enable_map->hwcnt_enable_map = NULL;
 	enable_map->metadata = NULL;
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_enable_map_free);
@@ -212,17 +222,25 @@ int kbase_hwcnt_dump_buffer_alloc(
 	const struct kbase_hwcnt_metadata *metadata,
 	struct kbase_hwcnt_dump_buffer *dump_buf)
 {
-	u32 *buf;
+	size_t dump_buf_bytes;
+	size_t clk_cnt_buf_bytes;
+	u8 *buf;
 
 	if (!metadata || !dump_buf)
 		return -EINVAL;
 
-	buf = kmalloc(metadata->dump_buf_bytes, GFP_KERNEL);
+	dump_buf_bytes = metadata->dump_buf_bytes;
+	clk_cnt_buf_bytes = sizeof(*dump_buf->clk_cnt_buf) * metadata->clk_cnt;
+
+	/* Make a single allocation for both dump_buf and clk_cnt_buf. */
+	buf = kmalloc(dump_buf_bytes + clk_cnt_buf_bytes, GFP_KERNEL);
 	if (!buf)
 		return -ENOMEM;
 
 	dump_buf->metadata = metadata;
-	dump_buf->dump_buf = buf;
+	dump_buf->dump_buf = (u32 *)buf;
+	dump_buf->clk_cnt_buf = (u64 *)(buf + dump_buf_bytes);
+
 	return 0;
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_dump_buffer_alloc);
@@ -246,10 +264,16 @@ int kbase_hwcnt_dump_buffer_array_alloc(
 	size_t buf_idx;
 	unsigned int order;
 	unsigned long addr;
+	size_t dump_buf_bytes;
+	size_t clk_cnt_buf_bytes;
 
 	if (!metadata || !dump_bufs)
 		return -EINVAL;
 
+	dump_buf_bytes = metadata->dump_buf_bytes;
+	clk_cnt_buf_bytes =
+		sizeof(*dump_bufs->bufs->clk_cnt_buf) * metadata->clk_cnt;
+
 	/* Allocate memory for the dump buffer struct array */
 	buffers = kmalloc_array(n, sizeof(*buffers), GFP_KERNEL);
 	if (!buffers)
@@ -258,8 +282,8 @@ int kbase_hwcnt_dump_buffer_array_alloc(
 	/* Allocate pages for the actual dump buffers, as they tend to be fairly
 	 * large.
 	 */
-	order = get_order(metadata->dump_buf_bytes * n);
-	addr = __get_free_pages(GFP_KERNEL, order);
+	order = get_order((dump_buf_bytes + clk_cnt_buf_bytes) * n);
+	addr = __get_free_pages(GFP_KERNEL | __GFP_ZERO, order);
 
 	if (!addr) {
 		kfree(buffers);
@@ -273,10 +297,14 @@ int kbase_hwcnt_dump_buffer_array_alloc(
 
 	/* Set the buffer of each dump buf */
 	for (buf_idx = 0; buf_idx < n; buf_idx++) {
-		const size_t offset = metadata->dump_buf_bytes * buf_idx;
+		const size_t dump_buf_offset = dump_buf_bytes * buf_idx;
+		const size_t clk_cnt_buf_offset =
+			(dump_buf_bytes * n) + (clk_cnt_buf_bytes * buf_idx);
 
 		buffers[buf_idx].metadata = metadata;
-		buffers[buf_idx].dump_buf = (u32 *)(addr + offset);
+		buffers[buf_idx].dump_buf = (u32 *)(addr + dump_buf_offset);
+		buffers[buf_idx].clk_cnt_buf =
+			(u64 *)(addr + clk_cnt_buf_offset);
 	}
 
 	return 0;
@@ -324,6 +352,9 @@ void kbase_hwcnt_dump_buffer_zero(
 
 		kbase_hwcnt_dump_buffer_block_zero(dst_blk, val_cnt);
 	}
+
+	memset(dst->clk_cnt_buf, 0,
+		sizeof(*dst->clk_cnt_buf) * metadata->clk_cnt);
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_dump_buffer_zero);
 
@@ -334,6 +365,9 @@ void kbase_hwcnt_dump_buffer_zero_strict(
 		return;
 
 	memset(dst->dump_buf, 0, dst->metadata->dump_buf_bytes);
+
+	memset(dst->clk_cnt_buf, 0,
+		sizeof(*dst->clk_cnt_buf) * dst->metadata->clk_cnt);
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_dump_buffer_zero_strict);
 
@@ -384,6 +418,7 @@ void kbase_hwcnt_dump_buffer_copy(
 {
 	const struct kbase_hwcnt_metadata *metadata;
 	size_t grp, blk, blk_inst;
+	size_t clk;
 
 	if (WARN_ON(!dst) ||
 	    WARN_ON(!src) ||
@@ -413,6 +448,12 @@ void kbase_hwcnt_dump_buffer_copy(
 
 		kbase_hwcnt_dump_buffer_block_copy(dst_blk, src_blk, val_cnt);
 	}
+
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+		if (kbase_hwcnt_clk_enable_map_enabled(
+			dst_enable_map->clk_enable_map, clk))
+			dst->clk_cnt_buf[clk] = src->clk_cnt_buf[clk];
+	}
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_dump_buffer_copy);
 
@@ -423,6 +464,7 @@ void kbase_hwcnt_dump_buffer_copy_strict(
 {
 	const struct kbase_hwcnt_metadata *metadata;
 	size_t grp, blk, blk_inst;
+	size_t clk;
 
 	if (WARN_ON(!dst) ||
 	    WARN_ON(!src) ||
@@ -451,6 +493,14 @@ void kbase_hwcnt_dump_buffer_copy_strict(
 		kbase_hwcnt_dump_buffer_block_copy_strict(
 			dst_blk, src_blk, blk_em, val_cnt);
 	}
+
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+		bool clk_enabled =
+			kbase_hwcnt_clk_enable_map_enabled(
+				dst_enable_map->clk_enable_map, clk);
+
+		dst->clk_cnt_buf[clk] = clk_enabled ? src->clk_cnt_buf[clk] : 0;
+	}
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_dump_buffer_copy_strict);
 
@@ -461,6 +511,7 @@ void kbase_hwcnt_dump_buffer_accumulate(
 {
 	const struct kbase_hwcnt_metadata *metadata;
 	size_t grp, blk, blk_inst;
+	size_t clk;
 
 	if (WARN_ON(!dst) ||
 	    WARN_ON(!src) ||
@@ -494,6 +545,12 @@ void kbase_hwcnt_dump_buffer_accumulate(
 		kbase_hwcnt_dump_buffer_block_accumulate(
 			dst_blk, src_blk, hdr_cnt, ctr_cnt);
 	}
+
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+		if (kbase_hwcnt_clk_enable_map_enabled(
+			dst_enable_map->clk_enable_map, clk))
+			dst->clk_cnt_buf[clk] += src->clk_cnt_buf[clk];
+	}
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_dump_buffer_accumulate);
 
@@ -504,6 +561,7 @@ void kbase_hwcnt_dump_buffer_accumulate_strict(
 {
 	const struct kbase_hwcnt_metadata *metadata;
 	size_t grp, blk, blk_inst;
+	size_t clk;
 
 	if (WARN_ON(!dst) ||
 	    WARN_ON(!src) ||
@@ -534,5 +592,13 @@ void kbase_hwcnt_dump_buffer_accumulate_strict(
 		kbase_hwcnt_dump_buffer_block_accumulate_strict(
 			dst_blk, src_blk, blk_em, hdr_cnt, ctr_cnt);
 	}
+
+	kbase_hwcnt_metadata_for_each_clock(metadata, clk) {
+		if (kbase_hwcnt_clk_enable_map_enabled(
+			dst_enable_map->clk_enable_map, clk))
+			dst->clk_cnt_buf[clk] += src->clk_cnt_buf[clk];
+		else
+			dst->clk_cnt_buf[clk] = 0;
+	}
 }
 KBASE_EXPORT_TEST_API(kbase_hwcnt_dump_buffer_accumulate_strict);
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.h b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.h
index 4d78c8457574..3394b1271cc8 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_hwcnt_types.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2018 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -136,11 +136,13 @@ struct kbase_hwcnt_group_description {
  * @grps:       Non-NULL pointer to an array of grp_cnt group descriptions,
  *              describing each Hardware Counter Group in the system.
  * @avail_mask: Flat Availability Mask for all block instances in the system.
+ * @clk_cnt:    The number of clock domains in the system. The maximum is 64.
  */
 struct kbase_hwcnt_description {
 	size_t grp_cnt;
 	const struct kbase_hwcnt_group_description *grps;
 	u64 avail_mask;
+	u8 clk_cnt;
 };
 
 /**
@@ -220,6 +222,7 @@ struct kbase_hwcnt_group_metadata {
  * @enable_map_bytes: The size in bytes of an Enable Map needed for the system.
  * @dump_buf_bytes:   The size in bytes of a Dump Buffer needed for the system.
  * @avail_mask:       The Availability Mask for the system.
+ * @clk_cnt:          The number of clock domains in the system.
  */
 struct kbase_hwcnt_metadata {
 	size_t grp_cnt;
@@ -227,6 +230,7 @@ struct kbase_hwcnt_metadata {
 	size_t enable_map_bytes;
 	size_t dump_buf_bytes;
 	u64 avail_mask;
+	u8 clk_cnt;
 };
 
 /**
@@ -234,13 +238,16 @@ struct kbase_hwcnt_metadata {
  *                                 bitfields.
  * @metadata:   Non-NULL pointer to metadata used to identify, and to describe
  *              the layout of the enable map.
- * @enable_map: Non-NULL pointer of size metadata->enable_map_bytes to an array
- *              of u64 bitfields, each bit of which enables one hardware
+ * @hwcnt_enable_map: Non-NULL pointer of size metadata->enable_map_bytes to an
+ *              array of u64 bitfields, each bit of which enables one hardware
  *              counter.
+ * @clk_enable_map: An array of u64 bitfields, each bit of which enables cycle
+ *              counter for a given clock domain.
  */
 struct kbase_hwcnt_enable_map {
 	const struct kbase_hwcnt_metadata *metadata;
-	u64 *enable_map;
+	u64 *hwcnt_enable_map;
+	u64 clk_enable_map;
 };
 
 /**
@@ -250,10 +257,13 @@ struct kbase_hwcnt_enable_map {
  *            the layout of the Dump Buffer.
  * @dump_buf: Non-NULL pointer of size metadata->dump_buf_bytes to an array
  *            of u32 values.
+ * @clk_cnt_buf: A pointer to an array of u64 values for cycle count elapsed
+ *               for each clock domain.
  */
 struct kbase_hwcnt_dump_buffer {
 	const struct kbase_hwcnt_metadata *metadata;
 	u32 *dump_buf;
+	u64 *clk_cnt_buf;
 };
 
 /**
@@ -473,7 +483,7 @@ void kbase_hwcnt_enable_map_free(struct kbase_hwcnt_enable_map *enable_map);
  *         block instance.
  */
 #define kbase_hwcnt_enable_map_block_instance(map, grp, blk, blk_inst) \
-	((map)->enable_map + \
+	((map)->hwcnt_enable_map + \
 	 (map)->metadata->grp_metadata[(grp)].enable_map_index + \
 	 (map)->metadata->grp_metadata[(grp)].blk_metadata[(blk)].enable_map_index + \
 	 (map)->metadata->grp_metadata[(grp)].blk_metadata[(blk)].enable_map_stride * (blk_inst))
@@ -520,7 +530,11 @@ static inline void kbase_hwcnt_enable_map_block_disable_all(
 static inline void kbase_hwcnt_enable_map_disable_all(
 	struct kbase_hwcnt_enable_map *dst)
 {
-	memset(dst->enable_map, 0, dst->metadata->enable_map_bytes);
+	if (dst->hwcnt_enable_map != NULL)
+		memset(dst->hwcnt_enable_map, 0,
+		       dst->metadata->enable_map_bytes);
+
+	dst->clk_enable_map = 0;
 }
 
 /**
@@ -569,6 +583,8 @@ static inline void kbase_hwcnt_enable_map_enable_all(
 	kbase_hwcnt_metadata_for_each_block(dst->metadata, grp, blk, blk_inst)
 		kbase_hwcnt_enable_map_block_enable_all(
 			dst, grp, blk, blk_inst);
+
+	dst->clk_enable_map = (1ull << dst->metadata->clk_cnt) - 1;
 }
 
 /**
@@ -582,9 +598,13 @@ static inline void kbase_hwcnt_enable_map_copy(
 	struct kbase_hwcnt_enable_map *dst,
 	const struct kbase_hwcnt_enable_map *src)
 {
-	memcpy(dst->enable_map,
-	       src->enable_map,
-	       dst->metadata->enable_map_bytes);
+	if (dst->hwcnt_enable_map != NULL) {
+		memcpy(dst->hwcnt_enable_map,
+		       src->hwcnt_enable_map,
+		       dst->metadata->enable_map_bytes);
+	}
+
+	dst->clk_enable_map = src->clk_enable_map;
 }
 
 /**
@@ -602,8 +622,12 @@ static inline void kbase_hwcnt_enable_map_union(
 		dst->metadata->enable_map_bytes / KBASE_HWCNT_BITFIELD_BYTES;
 	size_t i;
 
-	for (i = 0; i < bitfld_count; i++)
-		dst->enable_map[i] |= src->enable_map[i];
+	if (dst->hwcnt_enable_map != NULL) {
+		for (i = 0; i < bitfld_count; i++)
+			dst->hwcnt_enable_map[i] |= src->hwcnt_enable_map[i];
+	}
+
+	dst->clk_enable_map |= src->clk_enable_map;
 }
 
 /**
@@ -656,6 +680,12 @@ static inline bool kbase_hwcnt_enable_map_any_enabled(
 	const struct kbase_hwcnt_enable_map *enable_map)
 {
 	size_t grp, blk, blk_inst;
+	const u64 clk_enable_map_mask =
+		(1ull << enable_map->metadata->clk_cnt) - 1;
+
+	if (enable_map->metadata->clk_cnt > 0 &&
+		(enable_map->clk_enable_map & clk_enable_map_mask))
+		return true;
 
 	kbase_hwcnt_metadata_for_each_block(
 		enable_map->metadata, grp, blk, blk_inst) {
@@ -753,8 +783,8 @@ void kbase_hwcnt_dump_buffer_free(struct kbase_hwcnt_dump_buffer *dump_buf);
  *             dump buffer in the array will be initialised to undefined values,
  *             so must be used as a copy dest, or cleared before use.
  *
- * A single contiguous page allocation will be used for all of the buffers
- * inside the array, where:
+ * A single zeroed contiguous page allocation will be used for all of the
+ * buffers inside the array, where:
  * dump_bufs[n].dump_buf == page_addr + n * metadata.dump_buf_bytes
  *
  * Return: 0 on success, else error code.
@@ -1084,4 +1114,29 @@ static inline void kbase_hwcnt_dump_buffer_block_accumulate_strict(
 	}
 }
 
+/**
+ * @brief Iterate over each clock domain in the metadata.
+ *
+ * @param[in] md          Non-NULL pointer to metadata.
+ * @param[in] clk         size_t variable used as clock iterator.
+ */
+#define kbase_hwcnt_metadata_for_each_clock(md, clk)    \
+	for ((clk) = 0; (clk) < (md)->clk_cnt; (clk)++)
+
+/**
+ * kbase_hwcnt_clk_enable_map_enabled() - Check if the given index is enabled
+ *                                        in clk_enable_map.
+ * @clk_enable_map: An enable map for clock domains.
+ * @index:          Index of the enable map for clock domain.
+ *
+ * Return: true if the index of the clock domain is enabled, else false.
+ */
+static inline bool kbase_hwcnt_clk_enable_map_enabled(
+	const u64 clk_enable_map, const size_t index)
+{
+	if (clk_enable_map & (1ull << index))
+		return true;
+	return false;
+}
+
 #endif /* _KBASE_HWCNT_TYPES_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_ioctl.h b/drivers/gpu/arm/bifrost/mali_kbase_ioctl.h
index 977b194eb9c4..fed45100b4be 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_ioctl.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_ioctl.h
@@ -30,24 +30,14 @@ extern "C" {
 #include <asm-generic/ioctl.h>
 #include <linux/types.h>
 
+#if MALI_USE_CSF
+#include "csf/mali_kbase_csf_ioctl.h"
+#else
 #include "jm/mali_kbase_jm_ioctl.h"
+#endif /* MALI_USE_CSF */
 
 #define KBASE_IOCTL_TYPE 0x80
 
-/**
- * struct kbase_ioctl_version_check - Check version compatibility with kernel
- *
- * @major: Major version number
- * @minor: Minor version number
- */
-struct kbase_ioctl_version_check {
-	__u16 major;
-	__u16 minor;
-};
-
-#define KBASE_IOCTL_VERSION_CHECK \
-	_IOWR(KBASE_IOCTL_TYPE, 0, struct kbase_ioctl_version_check)
-
 /**
  * struct kbase_ioctl_set_flags - Set kernel context creation flags
  *
@@ -166,7 +156,7 @@ struct kbase_ioctl_mem_free {
 /**
  * struct kbase_ioctl_hwcnt_reader_setup - Setup HWC dumper/reader
  * @buffer_count: requested number of dumping buffers
- * @jm_bm:        counters selection bitmask (JM)
+ * @fe_bm:        counters selection bitmask (Front end)
  * @shader_bm:    counters selection bitmask (Shader)
  * @tiler_bm:     counters selection bitmask (Tiler)
  * @mmu_l2_bm:    counters selection bitmask (MMU_L2)
@@ -175,7 +165,7 @@ struct kbase_ioctl_mem_free {
  */
 struct kbase_ioctl_hwcnt_reader_setup {
 	__u32 buffer_count;
-	__u32 jm_bm;
+	__u32 fe_bm;
 	__u32 shader_bm;
 	__u32 tiler_bm;
 	__u32 mmu_l2_bm;
@@ -187,14 +177,14 @@ struct kbase_ioctl_hwcnt_reader_setup {
 /**
  * struct kbase_ioctl_hwcnt_enable - Enable hardware counter collection
  * @dump_buffer:  GPU address to write counters to
- * @jm_bm:        counters selection bitmask (JM)
+ * @fe_bm:        counters selection bitmask (Front end)
  * @shader_bm:    counters selection bitmask (Shader)
  * @tiler_bm:     counters selection bitmask (Tiler)
  * @mmu_l2_bm:    counters selection bitmask (MMU_L2)
  */
 struct kbase_ioctl_hwcnt_enable {
 	__u64 dump_buffer;
-	__u32 jm_bm;
+	__u32 fe_bm;
 	__u32 shader_bm;
 	__u32 tiler_bm;
 	__u32 mmu_l2_bm;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_jd.c b/drivers/gpu/arm/bifrost/mali_kbase_jd.c
index 935a7bf9a1e9..fb187208ffe5 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_jd.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_jd.c
@@ -32,6 +32,7 @@
 #include <linux/ratelimit.h>
 
 #include <mali_kbase_jm.h>
+#include <mali_kbase_kinstr_jm.h>
 #include <mali_kbase_hwaccess_jm.h>
 #include <tl/mali_kbase_tracepoints.h>
 #include <mali_linux_trace.h>
@@ -39,6 +40,8 @@
 #include "mali_kbase_dma_fence.h"
 #include <mali_kbase_cs_experimental.h>
 
+#include <mali_kbase_caps.h>
+
 #define beenthere(kctx, f, a...)  dev_dbg(kctx->kbdev->dev, "%s:" f, __func__, ##a)
 
 #if LINUX_VERSION_CODE < KERNEL_VERSION(3, 8, 0)
@@ -52,11 +55,6 @@
 			((katom->core_req & BASE_JD_REQ_ATOM_TYPE) ==    \
 							BASE_JD_REQ_DEP)))
 
-/* Minimum API version that supports the just-in-time memory allocation pressure
- * limit feature.
- */
-#define MIN_API_VERSION_WITH_JPL KBASE_API_VERSION(11, 20)
-
 /*
  * This is the kernel side of the API. Only entry points are:
  * - kbase_jd_submit(): Called from userspace to submit a single bag
@@ -77,6 +75,15 @@ get_compat_pointer(struct kbase_context *kctx, const u64 p)
 	return u64_to_user_ptr(p);
 }
 
+/* Mark an atom as complete, and trace it in kinstr_jm */
+static void jd_mark_atom_complete(struct kbase_jd_atom *katom)
+{
+	katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+	kbase_kinstr_jm_atom_complete(katom);
+	dev_dbg(katom->kctx->kbdev->dev, "Atom %p status to completed\n",
+		(void *)katom);
+}
+
 /* Runs an atom, either by handing to the JS or by immediately running it in the case of soft-jobs
  *
  * Returns whether the JS needs a reschedule.
@@ -97,24 +104,18 @@ static bool jd_run_atom(struct kbase_jd_atom *katom)
 		/* Dependency only atom */
 		trace_sysgraph(SGR_SUBMIT, kctx->id,
 				kbase_jd_atom_id(katom->kctx, katom));
-		katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
-		dev_dbg(kctx->kbdev->dev, "Atom %p status to completed\n",
-			(void *)katom);
+		jd_mark_atom_complete(katom);
 		return 0;
 	} else if (katom->core_req & BASE_JD_REQ_SOFT_JOB) {
 		/* Soft-job */
 		if (katom->will_fail_event_code) {
 			kbase_finish_soft_job(katom);
-			katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
-			dev_dbg(kctx->kbdev->dev,
-				"Atom %p status to completed\n", (void *)katom);
+			jd_mark_atom_complete(katom);
 			return 0;
 		}
 		if (kbase_process_soft_job(katom) == 0) {
 			kbase_finish_soft_job(katom);
-			katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
-			dev_dbg(kctx->kbdev->dev,
-				"Atom %p status to completed\n", (void *)katom);
+			jd_mark_atom_complete(katom);
 		}
 		return 0;
 	}
@@ -205,7 +206,7 @@ static void kbase_jd_post_external_resources(struct kbase_jd_atom *katom)
  * jctx.lock must be held when this is called.
  */
 
-static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const struct base_jd_atom_v2 *user_atom)
+static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const struct base_jd_atom *user_atom)
 {
 	int err_ret_val = -EINVAL;
 	u32 res_no;
@@ -259,7 +260,11 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 #ifdef CONFIG_MALI_BIFROST_DMA_FENCE
 	if (implicit_sync) {
 		info.resv_objs = kmalloc_array(katom->nr_extres,
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 					sizeof(struct reservation_object *),
+#else
+					sizeof(struct dma_resv *),
+#endif
 					GFP_KERNEL);
 		if (!info.resv_objs) {
 			err_ret_val = -ENOMEM;
@@ -277,7 +282,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 #endif /* CONFIG_MALI_BIFROST_DMA_FENCE */
 
 	/* Take the processes mmap lock */
-	down_read(&current->mm->mmap_sem);
+	down_read(kbase_mem_get_process_mmap_lock());
 
 	/* need to keep the GPU VM locked while we set up UMM buffers */
 	kbase_gpu_vm_lock(katom->kctx);
@@ -314,8 +319,11 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 #ifdef CONFIG_MALI_BIFROST_DMA_FENCE
 		if (implicit_sync &&
 		    reg->gpu_alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM) {
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(5, 4, 0))
 			struct reservation_object *resv;
-
+#else
+			struct dma_resv *resv;
+#endif
 			resv = reg->gpu_alloc->imported.umm.dma_buf->resv;
 			if (resv)
 				kbase_dma_fence_add_reservation(resv, &info,
@@ -337,7 +345,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 	kbase_gpu_vm_unlock(katom->kctx);
 
 	/* Release the processes mmap lock */
-	up_read(&current->mm->mmap_sem);
+	up_read(kbase_mem_get_process_mmap_lock());
 
 #ifdef CONFIG_MALI_BIFROST_DMA_FENCE
 	if (implicit_sync) {
@@ -362,7 +370,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 #ifdef CONFIG_MALI_BIFROST_DMA_FENCE
 failed_dma_fence_setup:
 	/* Lock the processes mmap lock */
-	down_read(&current->mm->mmap_sem);
+	down_read(kbase_mem_get_process_mmap_lock());
 
 	/* lock before we unmap */
 	kbase_gpu_vm_lock(katom->kctx);
@@ -378,7 +386,7 @@ static int kbase_jd_pre_external_resources(struct kbase_jd_atom *katom, const st
 	kbase_gpu_vm_unlock(katom->kctx);
 
 	/* Release the processes mmap lock */
-	up_read(&current->mm->mmap_sem);
+	up_read(kbase_mem_get_process_mmap_lock());
 
  early_err_out:
 	kfree(katom->extres);
@@ -556,7 +564,7 @@ static void jd_try_submitting_deps(struct list_head *out_list,
 	}
 }
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 /**
  * jd_update_jit_usage - Update just-in-time physical memory usage for an atom.
  *
@@ -696,7 +704,7 @@ static void jd_update_jit_usage(struct kbase_jd_atom *katom)
 
 	kbase_jit_retry_pending_alloc(kctx);
 }
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 /*
  * Perform the necessary handling of an atom that has finished running
@@ -721,9 +729,10 @@ bool jd_done_nolock(struct kbase_jd_atom *katom,
 
 	KBASE_DEBUG_ASSERT(katom->status != KBASE_JD_ATOM_STATE_UNUSED);
 
-#if MALI_JIT_PRESSURE_LIMIT
-	jd_update_jit_usage(katom);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	if (kbase_ctx_flag(kctx, KCTX_JPL_ENABLED))
+		jd_update_jit_usage(katom);
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	/* This is needed in case an atom is failed due to being invalid, this
 	 * can happen *before* the jobs that the atom depends on have completed */
@@ -734,9 +743,7 @@ bool jd_done_nolock(struct kbase_jd_atom *katom,
 		}
 	}
 
-	katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
-	dev_dbg(kctx->kbdev->dev, "Atom %p status to completed\n",
-		(void *)katom);
+	jd_mark_atom_complete(katom);
 	list_add_tail(&katom->jd_item, &completed_jobs);
 
 	while (!list_empty(&completed_jobs)) {
@@ -868,8 +875,23 @@ static const char *kbasep_map_core_reqs_to_string(base_jd_core_req core_req)
 }
 #endif
 
+/* Trace an atom submission. */
+static void jd_trace_atom_submit(struct kbase_context *const kctx,
+				 struct kbase_jd_atom *const katom,
+				 int *priority)
+{
+	struct kbase_device *const kbdev = kctx->kbdev;
+
+	KBASE_TLSTREAM_TL_NEW_ATOM(kbdev, katom, kbase_jd_atom_id(kctx, katom));
+	KBASE_TLSTREAM_TL_RET_ATOM_CTX(kbdev, katom, kctx);
+	if (priority)
+		KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY(kbdev, katom, *priority);
+	KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(kbdev, katom, TL_ATOM_STATE_IDLE);
+	kbase_kinstr_jm_atom_queue(katom);
+}
+
 static bool jd_submit_atom(struct kbase_context *const kctx,
-	const struct base_jd_atom_v2 *const user_atom,
+	const struct base_jd_atom *const user_atom,
 	const struct base_jd_fragment *const user_jc_incr,
 	struct kbase_jd_atom *const katom)
 {
@@ -879,6 +901,8 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 	int i;
 	int sched_prio;
 	bool will_fail = false;
+	unsigned long flags;
+	enum kbase_jd_atom_state status;
 
 	dev_dbg(kbdev->dev, "User did JD submit atom %p\n", (void *)katom);
 
@@ -899,6 +923,7 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 	katom->jc = user_atom->jc;
 	katom->core_req = user_atom->core_req;
 	katom->jobslot = user_atom->jobslot;
+	katom->seq_nr = user_atom->seq_nr;
 	katom->atom_flags = 0;
 	katom->retry_count = 0;
 	katom->need_cache_flush_cores_retained = 0;
@@ -911,19 +936,19 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 
 	trace_sysgraph(SGR_ARRIVE, kctx->id, user_atom->atom_number);
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	/* Older API version atoms might have random values where jit_id now
 	 * lives, but we must maintain backwards compatibility - handle the
 	 * issue.
 	 */
-	if (kctx->api_version < MIN_API_VERSION_WITH_JPL) {
+	if (!mali_kbase_supports_jit_pressure_limit(kctx->api_version)) {
 		katom->jit_ids[0] = 0;
 		katom->jit_ids[1] = 0;
 	} else {
 		katom->jit_ids[0] = user_atom->jit_id[0];
 		katom->jit_ids[1] = user_atom->jit_id[1];
 	}
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	katom->renderpass_id = user_atom->renderpass_id;
 
@@ -959,17 +984,7 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 				/* Wrong dependency setup. Atom will be sent
 				 * back to user space. Do not record any
 				 * dependencies. */
-				KBASE_TLSTREAM_TL_NEW_ATOM(
-						kbdev,
-						katom,
-						kbase_jd_atom_id(kctx, katom));
-				KBASE_TLSTREAM_TL_RET_ATOM_CTX(
-						kbdev,
-						katom, kctx);
-				KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(
-						kbdev,
-						katom,
-						TL_ATOM_STATE_IDLE);
+				jd_trace_atom_submit(kctx, katom, NULL);
 
 				return jd_done_nolock(katom, NULL);
 			}
@@ -1011,13 +1026,7 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 			/* This atom will be sent back to user space.
 			 * Do not record any dependencies.
 			 */
-			KBASE_TLSTREAM_TL_NEW_ATOM(
-					kbdev,
-					katom,
-					kbase_jd_atom_id(kctx, katom));
-			KBASE_TLSTREAM_TL_RET_ATOM_CTX(kbdev, katom, kctx);
-			KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(kbdev, katom,
-					TL_ATOM_STATE_IDLE);
+			jd_trace_atom_submit(kctx, katom, NULL);
 
 			will_fail = true;
 
@@ -1076,13 +1085,7 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 	katom->sched_priority = sched_prio;
 
 	/* Create a new atom. */
-	KBASE_TLSTREAM_TL_NEW_ATOM(
-			kbdev,
-			katom,
-			kbase_jd_atom_id(kctx, katom));
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_STATE(kbdev, katom, TL_ATOM_STATE_IDLE);
-	KBASE_TLSTREAM_TL_ATTRIB_ATOM_PRIORITY(kbdev, katom, katom->sched_priority);
-	KBASE_TLSTREAM_TL_RET_ATOM_CTX(kbdev, katom, kctx);
+	jd_trace_atom_submit(kctx, katom, &katom->sched_priority);
 
 #if !MALI_INCREMENTAL_RENDERING
 	/* Reject atoms for incremental rendering if not supported */
@@ -1149,8 +1152,8 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 		}
 	}
 
-#if !MALI_JIT_PRESSURE_LIMIT
-	if ((kctx->api_version >= MIN_API_VERSION_WITH_JPL) &&
+#if !MALI_JIT_PRESSURE_LIMIT_BASE
+	if (mali_kbase_supports_jit_pressure_limit(kctx->api_version) &&
 		(user_atom->jit_id[0] || user_atom->jit_id[1])) {
 		/* JIT pressure limit is disabled, but we are receiving non-0
 		 * JIT IDs - atom is invalid.
@@ -1158,7 +1161,7 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 		katom->event_code = BASE_JD_EVENT_JOB_INVALID;
 		return jd_done_nolock(katom, NULL);
 	}
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	/* Validate the atom. Function will return error if the atom is
 	 * malformed.
@@ -1214,6 +1217,17 @@ static bool jd_submit_atom(struct kbase_context *const kctx,
 		/* If job was cancelled then resolve immediately */
 		if (katom->event_code != BASE_JD_EVENT_JOB_CANCELLED)
 			return need_to_try_schedule_context;
+
+		/* Synchronize with backend reset */
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		status = katom->status;
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+		if (status == KBASE_JD_ATOM_STATE_HW_COMPLETED) {
+			dev_dbg(kctx->kbdev->dev,
+					"Atom %d cancelled on HW\n",
+					kbase_jd_atom_id(katom->kctx, katom));
+			return need_to_try_schedule_context;
+		}
 	}
 
 	/* This is a pure dependency. Resolve it immediately */
@@ -1231,6 +1245,9 @@ int kbase_jd_submit(struct kbase_context *kctx,
 	struct kbase_device *kbdev;
 	u32 latest_flush;
 
+	bool jd_atom_is_v2 = (stride == sizeof(struct base_jd_atom_v2) ||
+	                      stride == offsetof(struct base_jd_atom_v2, renderpass_id));
+
 	/*
 	 * kbase_jd_submit isn't expected to fail and so all errors with the
 	 * jobs are reported by immediately failing them (through event system)
@@ -1245,7 +1262,9 @@ int kbase_jd_submit(struct kbase_context *kctx,
 	}
 
 	if (stride != offsetof(struct base_jd_atom_v2, renderpass_id) &&
-		stride != sizeof(struct base_jd_atom_v2)) {
+		stride != sizeof(struct base_jd_atom_v2) &&
+		stride != offsetof(struct base_jd_atom, renderpass_id) &&
+		stride != sizeof(struct base_jd_atom)) {
 		dev_err(kbdev->dev,
 			"Stride %u passed to job_submit isn't supported by the kernel\n",
 			stride);
@@ -1256,16 +1275,29 @@ int kbase_jd_submit(struct kbase_context *kctx,
 	latest_flush = kbase_backend_get_current_flush_id(kbdev);
 
 	for (i = 0; i < nr_atoms; i++) {
-		struct base_jd_atom_v2 user_atom;
+		struct base_jd_atom user_atom;
 		struct base_jd_fragment user_jc_incr;
 		struct kbase_jd_atom *katom;
 
-		if (copy_from_user(&user_atom, user_addr, stride) != 0) {
-			dev_err(kbdev->dev,
-				"Invalid atom address %p passed to job_submit\n",
-				user_addr);
-			err = -EFAULT;
-			break;
+		if (unlikely(jd_atom_is_v2)) {
+			if (copy_from_user(&user_atom.jc, user_addr, sizeof(struct base_jd_atom_v2)) != 0) {
+				dev_err(kbdev->dev,
+					"Invalid atom address %p passed to job_submit\n",
+					user_addr);
+				err = -EFAULT;
+				break;
+			}
+
+			/* no seq_nr in v2 */
+			user_atom.seq_nr = 0;
+		} else {
+			if (copy_from_user(&user_atom, user_addr, stride) != 0) {
+				dev_err(kbdev->dev,
+					"Invalid atom address %p passed to job_submit\n",
+					user_addr);
+				err = -EFAULT;
+				break;
+			}
 		}
 
 		if (stride == offsetof(struct base_jd_atom_v2, renderpass_id)) {
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_jd_debugfs.c b/drivers/gpu/arm/bifrost/mali_kbase_jd_debugfs.c
index fc6baebec8d4..6b0c36d6b93f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_jd_debugfs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_jd_debugfs.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -228,6 +228,12 @@ static const struct file_operations kbasep_jd_debugfs_atoms_fops = {
 
 void kbasep_jd_debugfs_ctx_init(struct kbase_context *kctx)
 {
+#if (KERNEL_VERSION(4, 7, 0) <= LINUX_VERSION_CODE)
+	const mode_t mode = S_IRUGO;
+#else
+	const mode_t mode = S_IRUSR;
+#endif
+
 	/* Caller already ensures this, but we keep the pattern for
 	 * maintenance safety.
 	 */
@@ -236,7 +242,7 @@ void kbasep_jd_debugfs_ctx_init(struct kbase_context *kctx)
 		return;
 
 	/* Expose all atoms */
-	debugfs_create_file("atoms", S_IRUGO, kctx->kctx_dentry, kctx,
+	debugfs_create_file("atoms", mode, kctx->kctx_dentry, kctx,
 			&kbasep_jd_debugfs_atoms_fops);
 
 }
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_jm.c b/drivers/gpu/arm/bifrost/mali_kbase_jm.c
index 3f17dd763b97..fb15a8c1727a 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_jm.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_jm.c
@@ -29,6 +29,7 @@
 #include "mali_kbase_hwaccess_jm.h"
 #include "mali_kbase_jm.h"
 
+#if !MALI_USE_CSF
 /**
  * kbase_jm_next_job() - Attempt to run the next @nr_jobs_to_submit jobs on slot
  *			 @js on the active context.
@@ -109,6 +110,7 @@ void kbase_jm_try_kick_all(struct kbase_device *kbdev)
 		up(&js_devdata->schedule_sem);
 	}
 }
+#endif /* !MALI_USE_CSF */
 
 void kbase_jm_idle_ctx(struct kbase_device *kbdev, struct kbase_context *kctx)
 {
@@ -125,6 +127,7 @@ void kbase_jm_idle_ctx(struct kbase_device *kbdev, struct kbase_context *kctx)
 	}
 }
 
+#if !MALI_USE_CSF
 struct kbase_jd_atom *kbase_jm_return_atom_to_js(struct kbase_device *kbdev,
 				struct kbase_jd_atom *katom)
 {
@@ -149,3 +152,4 @@ struct kbase_jd_atom *kbase_jm_complete(struct kbase_device *kbdev,
 
 	return kbase_js_complete_atom(katom, end_timestamp);
 }
+#endif /* !MALI_USE_CSF */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_jm.h b/drivers/gpu/arm/bifrost/mali_kbase_jm.h
index a3c774483256..b3fd421a1ff3 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_jm.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_jm.h
@@ -28,6 +28,7 @@
 #ifndef _KBASE_JM_H_
 #define _KBASE_JM_H_
 
+#if !MALI_USE_CSF
 /**
  * kbase_jm_kick() - Indicate that there are jobs ready to run.
  * @kbdev:	Device pointer
@@ -73,6 +74,7 @@ void kbase_jm_try_kick(struct kbase_device *kbdev, u32 js_mask);
  * kbase_jm_kick_all() otherwise it will do nothing.
  */
 void kbase_jm_try_kick_all(struct kbase_device *kbdev);
+#endif /* !MALI_USE_CSF */
 
 /**
  * kbase_jm_idle_ctx() - Mark a context as idle.
@@ -89,6 +91,7 @@ void kbase_jm_try_kick_all(struct kbase_device *kbdev);
  */
 void kbase_jm_idle_ctx(struct kbase_device *kbdev, struct kbase_context *kctx);
 
+#if !MALI_USE_CSF
 /**
  * kbase_jm_return_atom_to_js() - Return an atom to the job scheduler that has
  *				  been soft-stopped or will fail due to a
@@ -111,5 +114,6 @@ struct kbase_jd_atom *kbase_jm_return_atom_to_js(struct kbase_device *kbdev,
  */
 struct kbase_jd_atom *kbase_jm_complete(struct kbase_device *kbdev,
 		struct kbase_jd_atom *katom, ktime_t *end_timestamp);
+#endif /* !MALI_USE_CSF */
 
 #endif /* _KBASE_JM_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c
new file mode 100644
index 000000000000..8457d6cf9ede
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.c
@@ -0,0 +1,911 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * mali_kbase_kinstr_jm.c
+ * Kernel driver public interface to job manager atom tracing
+ */
+
+#include "mali_kbase_kinstr_jm.h"
+#include "mali_kbase_kinstr_jm_reader.h"
+
+#include "mali_kbase.h"
+#include "mali_kbase_linux.h"
+
+#include <mali_kbase_jm_rb.h>
+
+#include <asm/barrier.h>
+#include <linux/anon_inodes.h>
+#include <linux/circ_buf.h>
+#include <linux/fs.h>
+#include <linux/kref.h>
+#include <linux/log2.h>
+#include <linux/mutex.h>
+#include <linux/rculist_bl.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/version.h>
+#include <linux/wait.h>
+
+#if KERNEL_VERSION(5, 1, 0) <= LINUX_VERSION_CODE
+#include <linux/build_bug.h>
+#else
+// Stringify the expression if no message is given.
+#define static_assert(e, ...)  __static_assert(e, #__VA_ARGS__, #e)
+#define __static_assert(e, msg, ...) _Static_assert(e, msg)
+#endif
+
+#if KERNEL_VERSION(4, 16, 0) >= LINUX_VERSION_CODE
+typedef unsigned int __poll_t;
+#endif
+
+#ifndef ENOTSUP
+#define ENOTSUP EOPNOTSUPP
+#endif
+
+/* The module printing prefix */
+#define PR_ "mali_kbase_kinstr_jm: "
+
+/* Allows us to perform ASM goto for the tracing
+ * https://www.kernel.org/doc/Documentation/static-keys.txt
+ */
+#if KERNEL_VERSION(4, 3, 0) <= LINUX_VERSION_CODE
+DEFINE_STATIC_KEY_FALSE(basep_kinstr_jm_reader_static_key);
+#else
+struct static_key basep_kinstr_jm_reader_static_key = STATIC_KEY_INIT_FALSE;
+#define static_branch_inc(key) static_key_slow_inc(key)
+#define static_branch_dec(key) static_key_slow_dec(key)
+#endif /* KERNEL_VERSION(4 ,3, 0) <= LINUX_VERSION_CODE */
+
+#define KBASE_KINSTR_JM_VERSION 1
+
+/**
+ * struct kbase_kinstr_jm - The context for the kernel job manager atom tracing
+ * @readers: a bitlocked list of opened readers. Readers are attached to the
+ *           private data of a file descriptor that the user opens with the
+ *           KBASE_IOCTL_KINSTR_JM_FD IO control call.
+ * @refcount: reference count for the context. Any reader will have a link
+ *            back to the context so that they can remove themselves from the
+ *            list.
+ *
+ * This is opaque outside this compilation unit
+ */
+struct kbase_kinstr_jm {
+	struct hlist_bl_head readers;
+	struct kref refcount;
+};
+
+/**
+ * struct kbase_kinstr_jm_atom_state_change - Represents an atom changing to a
+ *                                            new state
+ * @timestamp: Raw monotonic nanoseconds of the state change
+ * @state:     The state that the atom has moved to
+ * @atom:      The atom number that has changed state
+ * @flags:     Flags associated with the state change. See
+ *             KBASE_KINSTR_JM_ATOM_STATE_FLAG_* defines.
+ * @reserved:  Reserved for future use.
+ * @data:      Extra data for the state change. Active member depends on state.
+ *
+ * We can add new fields to the structure and old user code will gracefully
+ * ignore the new fields.
+ *
+ * We can change the size of the structure and old user code will gracefully
+ * skip over the new size via `struct kbase_kinstr_jm_fd_out->size`.
+ *
+ * If we remove fields, the version field in `struct
+ * kbase_kinstr_jm_fd_out->version` will be incremented and old user code will
+ * gracefully fail and tell the user that the kernel API is too new and has
+ * backwards-incompatible changes. Note that one userspace can opt to handle
+ * multiple kernel major versions of the structure.
+ *
+ * If we need to change the _meaning_ of one of the fields, i.e. the state
+ * machine has had a incompatible change, we can keep the same members in the
+ * structure and update the version as above. User code will no longer
+ * recognise that it has the supported field and can gracefully explain to the
+ * user that the kernel API is no longer supported.
+ *
+ * When making changes to this structure, make sure they are either:
+ *  - additions to the end (for minor version bumps (i.e. only a size increase))
+ *  such that the layout of existing fields doesn't change, or;
+ *  - update the version reported to userspace so that it can fail explicitly.
+ */
+struct kbase_kinstr_jm_atom_state_change {
+	u64 timestamp;
+	s8 state; /* enum kbase_kinstr_jm_reader_atom_state */
+	u8 atom;
+	u8 flags;
+	u8 reserved[1];
+	/* Tagged union based on state. Ensure members are aligned correctly! */
+	union {
+		struct {
+			u8 slot;
+		} start;
+		u8 padding[4];
+	} data;
+};
+static_assert(
+	((1 << 8 * sizeof(((struct kbase_kinstr_jm_atom_state_change *)0)->state)) - 1) >=
+	KBASE_KINSTR_JM_READER_ATOM_STATE_COUNT);
+
+#define KBASE_KINSTR_JM_ATOM_STATE_FLAG_OVERFLOW BIT(0)
+
+/**
+ * struct reader_changes - The circular buffer of kernel atom state changes
+ * @data:      The allocated buffer. This is allocated when the user requests
+ *             the reader file descriptor. It is released when the user calls
+ *             close() on the fd. When accessing this, lock the producer spin
+ *             lock to prevent races on the allocated memory. The consume lock
+ *             does not need to be held because newly-inserted data will always
+ *             be outside the currenly-read range.
+ * @producer:  The producing spinlock which allows us to push changes into the
+ *             buffer at the same time as a user read occurring. This needs to
+ *             be locked when saving/restoring the IRQ because we can receive an
+ *             interrupt from the GPU when an atom completes. The CPU could have
+ *             a task preempted that is holding this lock.
+ * @consumer:  The consuming mutex which locks around the user read().
+ *             Must be held when updating the tail of the circular buffer.
+ * @head:      The head of the circular buffer. Can be used with Linux @c CIRC_
+ *             helpers. The producer should lock and update this with an SMP
+ *             store when a new change lands. The consumer can read with an
+ *             SMP load. This allows the producer to safely insert new changes
+ *             into the circular buffer.
+ * @tail:      The tail of the circular buffer. Can be used with Linux @c CIRC_
+ *             helpers. The producer should do a READ_ONCE load and the consumer
+ *             should SMP store.
+ * @size:      The number of changes that are allowed in @c data. Can be used
+ *             with Linux @c CIRC_ helpers. Will always be a power of two. The
+ *             producer lock should be held when updating this and stored with
+ *             an SMP release memory barrier. This means that the consumer can
+ *             do an SMP load.
+ * @threshold: The number of changes above which threads polling on the reader
+ *             file descriptor will be woken up.
+ */
+struct reader_changes {
+	struct kbase_kinstr_jm_atom_state_change *data;
+	spinlock_t producer;
+	struct mutex consumer;
+	u32 head;
+	u32 tail;
+	u32 size;
+	u32 threshold;
+};
+
+/**
+ * reader_changes_is_valid_size() - Determines if requested changes buffer size
+ *                                  is valid.
+ * @size: The requested memory size
+ *
+ * We have a constraint that the underlying physical buffer must be a
+ * power of two so that we can use the efficient circular buffer helpers that
+ * the kernel provides. It also needs to be representable within a u32.
+ *
+ * Return:
+ * * true  - the size is valid
+ * * false - the size is invalid
+ */
+static inline bool reader_changes_is_valid_size(const size_t size)
+{
+	typedef struct reader_changes changes_t;
+	const size_t elem_size = sizeof(*((changes_t *)0)->data);
+	const size_t size_size = sizeof(((changes_t *)0)->size);
+	const size_t size_max = (1ull << (size_size * 8)) - 1;
+
+	return is_power_of_2(size) && /* Is a power of two */
+	       ((size / elem_size) <= size_max); /* Small enough */
+}
+
+/**
+ * reader_changes_init() - Initializes the reader changes and allocates the
+ *                         changes buffer
+ * @changes: The context pointer, must point to a zero-inited allocated reader
+ *           changes structure. We may support allocating the structure in the
+ *           future.
+ * @size: The requested changes buffer size
+ *
+ * Return:
+ * (0, U16_MAX] - the number of data elements allocated
+ * -EINVAL - a pointer was invalid
+ * -ENOTSUP - we do not support allocation of the context
+ * -ERANGE - the requested memory size was invalid
+ * -ENOMEM - could not allocate the memory
+ * -EADDRINUSE - the buffer memory was already allocated
+ */
+static int reader_changes_init(struct reader_changes *const changes,
+			       const size_t size)
+{
+	BUILD_BUG_ON((PAGE_SIZE % sizeof(*changes->data)) != 0);
+
+	if (!reader_changes_is_valid_size(size)) {
+		pr_warn(PR_ "invalid size %zu\n", size);
+		return -ERANGE;
+	}
+
+	changes->data = vmalloc(size);
+	if (!changes->data)
+		return -ENOMEM;
+
+	spin_lock_init(&changes->producer);
+	mutex_init(&changes->consumer);
+
+	changes->size = size / sizeof(*changes->data);
+	changes->threshold = min(((size_t)(changes->size)) / 4,
+			     ((size_t)(PAGE_SIZE)) / sizeof(*changes->data));
+
+	return changes->size;
+}
+
+/**
+ * reader_changes_term() - Cleans up a reader changes structure
+ * @changes: The context to clean up
+ *
+ * Releases the allocated state changes memory
+ */
+static void reader_changes_term(struct reader_changes *const changes)
+{
+	struct kbase_kinstr_jm_atom_state_change *data = NULL;
+	unsigned long irq;
+
+	/*
+	 * Although changes->data is used on the consumer side, too, no active
+	 * consumer is possible by the time we clean up the reader changes, so
+	 * no need to take the consumer lock. However, we do need the producer
+	 * lock because the list removal can race with list traversal.
+	 */
+	spin_lock_irqsave(&changes->producer, irq);
+	swap(changes->data, data);
+	spin_unlock_irqrestore(&changes->producer, irq);
+
+	mutex_destroy(&changes->consumer);
+	vfree(data);
+}
+
+/**
+ * reader_changes_count_locked() - Retrieves the count of state changes from the
+ * tail to the physical end of the buffer
+ * @changes: The state changes context
+ *
+ * The consumer mutex must be held. Uses the CIRC_CNT_TO_END macro to
+ * determine the count, so there may be more items. However, that's the maximum
+ * number that can be read in one contiguous read.
+ *
+ * Return: the number of changes in the circular buffer until the end of the
+ * allocation
+ */
+static u32 reader_changes_count_locked(struct reader_changes *const changes)
+{
+	u32 head;
+
+	lockdep_assert_held_once(&changes->consumer);
+
+	head = smp_load_acquire(&changes->head);
+
+	return CIRC_CNT_TO_END(head, changes->tail, changes->size);
+}
+
+/**
+ * reader_changes_count() - Retrieves the count of state changes from the
+ * tail to the physical end of the buffer
+ * @changes: The state changes context
+ *
+ * Return: the number of changes in the circular buffer until the end of the
+ * allocation
+ */
+static u32 reader_changes_count(struct reader_changes *const changes)
+{
+	u32 ret;
+
+	mutex_lock(&changes->consumer);
+	ret = reader_changes_count_locked(changes);
+	mutex_unlock(&changes->consumer);
+	return ret;
+}
+
+/**
+ * reader_changes_push() - Pushes a change into the reader circular buffer.
+ * @changes:    The buffer to insert the change into
+ * @change:     Kernel atom change to insert
+ * @wait_queue: The queue to be kicked when changes should be read from
+ *              userspace. Kicked when a threshold is reached or there is
+ *              overflow.
+ */
+static void reader_changes_push(
+	struct reader_changes *const changes,
+	const struct kbase_kinstr_jm_atom_state_change *const change,
+	wait_queue_head_t *const wait_queue)
+{
+	u32 head, tail, size, space;
+	unsigned long irq;
+	struct kbase_kinstr_jm_atom_state_change *data;
+
+	spin_lock_irqsave(&changes->producer, irq);
+
+	/* We may be called for a reader_changes that's awaiting cleanup. */
+	data = changes->data;
+	if (!data)
+		goto unlock;
+
+	size = changes->size;
+	head = changes->head;
+	tail = smp_load_acquire(&changes->tail);
+
+	space = CIRC_SPACE(head, tail, size);
+	if (space >= 1) {
+		data[head] = *change;
+		if (space == 1) {
+			data[head].flags |=
+				KBASE_KINSTR_JM_ATOM_STATE_FLAG_OVERFLOW;
+			pr_warn(PR_ "overflow of circular buffer\n");
+		}
+		smp_store_release(&changes->head, (head + 1) & (size - 1));
+	}
+
+	/* Wake for either overflow or over-threshold cases. */
+	if (CIRC_CNT(head + 1, tail, size) >= changes->threshold)
+		wake_up_interruptible(wait_queue);
+
+unlock:
+	spin_unlock_irqrestore(&changes->producer, irq);
+}
+
+/**
+ * struct reader - Allows the kernel state changes to be read by user space.
+ * @node: The node in the @c readers locked list
+ * @rcu_head: storage for the RCU callback to free this reader (see kfree_rcu)
+ * @changes: The circular buffer of user changes
+ * @wait_queue: A wait queue for poll
+ * @context: a pointer to the parent context that created this reader. Can be
+ *           used to remove the reader from the list of readers. Reference
+ *           counted.
+ *
+ * The reader is a circular buffer in kernel space. State changes are pushed
+ * into the buffer. The flow from user space is:
+ *
+ *   * Request file descriptor with KBASE_IOCTL_KINSTR_JM_FD. This will
+ *     allocate the kernel side circular buffer with a size specified in the
+ *     ioctl argument.
+ *   * The user will then poll the file descriptor for data
+ *   * Upon receiving POLLIN, perform a read() on the file descriptor to get
+ *     the data out.
+ *   * The buffer memory will be freed when the file descriptor is closed
+ */
+struct reader {
+	struct hlist_bl_node node;
+	struct rcu_head rcu_head;
+	struct reader_changes changes;
+	wait_queue_head_t wait_queue;
+	struct kbase_kinstr_jm *context;
+};
+
+static struct kbase_kinstr_jm *
+kbase_kinstr_jm_ref_get(struct kbase_kinstr_jm *const ctx);
+static void kbase_kinstr_jm_ref_put(struct kbase_kinstr_jm *const ctx);
+static int kbase_kinstr_jm_readers_add(struct kbase_kinstr_jm *const ctx,
+					struct reader *const reader);
+static void kbase_kinstr_jm_readers_del(struct kbase_kinstr_jm *const ctx,
+					struct reader *const reader);
+
+/**
+ * reader_term() - Terminate a instrumentation job manager reader context.
+ * @reader: Pointer to context to be terminated.
+ */
+static void reader_term(struct reader *const reader)
+{
+	if (!reader)
+		return;
+
+	kbase_kinstr_jm_readers_del(reader->context, reader);
+	reader_changes_term(&reader->changes);
+	kbase_kinstr_jm_ref_put(reader->context);
+
+	kfree_rcu(reader, rcu_head);
+}
+
+/**
+ * reader_init() - Initialise a instrumentation job manager reader context.
+ * @out_reader:  Non-NULL pointer to where the pointer to the created context
+ *               will be stored on success.
+ * @ctx:         the pointer to the parent context. Reference count will be
+ *               increased if initialization is successful
+ * @num_changes: The number of changes to allocate a buffer for
+ *
+ * Return: 0 on success, else error code.
+ */
+static int reader_init(struct reader **const out_reader,
+		       struct kbase_kinstr_jm *const ctx,
+		       size_t const num_changes)
+{
+	struct reader *reader = NULL;
+	const size_t change_size = sizeof(struct kbase_kinstr_jm_atom_state_change);
+	int status;
+
+	if (!out_reader || !ctx || !num_changes)
+		return -EINVAL;
+
+	reader = kzalloc(sizeof(*reader), GFP_KERNEL);
+	if (!reader)
+		return -ENOMEM;
+
+	INIT_HLIST_BL_NODE(&reader->node);
+	init_waitqueue_head(&reader->wait_queue);
+
+	reader->context = kbase_kinstr_jm_ref_get(ctx);
+
+	status = reader_changes_init(&reader->changes, num_changes * change_size);
+	if (status < 0)
+		goto fail;
+
+	status = kbase_kinstr_jm_readers_add(ctx, reader);
+	if (status < 0)
+		goto fail;
+
+	*out_reader = reader;
+
+	return 0;
+
+fail:
+	kbase_kinstr_jm_ref_put(reader->context);
+	kfree(reader);
+	return status;
+}
+
+/**
+ * reader_release() - Invoked when the reader file descriptor is released
+ * @node: The inode that the file descriptor that the file corresponds to. In
+ *        our case our reader file descriptor is backed by an anonymous node so
+ *        not much is in this.
+ * @file: the file data. Our reader context is held in the private data
+ * Return: zero on success
+ */
+static int reader_release(struct inode *const node, struct file *const file)
+{
+	struct reader *const reader = file->private_data;
+
+	reader_term(reader);
+	file->private_data = NULL;
+
+	return 0;
+}
+
+/**
+ * reader_changes_copy_to_user() - Copy any changes from a changes structure to
+ * the user-provided buffer.
+ * @changes: The changes structure from which to copy.
+ * @buffer: The user buffer to copy the data to.
+ * @buffer_size: The number of bytes in the buffer.
+ * Return: The number of bytes copied or negative errno on failure.
+ */
+static ssize_t reader_changes_copy_to_user(struct reader_changes *const changes,
+					   char __user *buffer,
+					   size_t buffer_size)
+{
+	ssize_t ret = 0;
+	struct kbase_kinstr_jm_atom_state_change const *src_buf = READ_ONCE(
+		changes->data);
+	size_t const entry_size = sizeof(*src_buf);
+	size_t changes_tail, changes_count, read_size;
+
+	/* Needed for the quick buffer capacity calculation below.
+	 * Note that we can't use is_power_of_2() since old compilers don't
+	 * understand it's a constant expression.
+	 */
+#define is_power_of_two(x) ((x) && !((x) & ((x) - 1)))
+	static_assert(is_power_of_two(
+			sizeof(struct kbase_kinstr_jm_atom_state_change)));
+#undef is_power_of_two
+
+	lockdep_assert_held_once(&changes->consumer);
+
+	/* Read continuously until either:
+	 * - we've filled the output buffer, or
+	 * - there are no changes when we check.
+	 *
+	 * If more changes arrive while we're copying to the user, we can copy
+	 * those as well, space permitting.
+	 */
+	do {
+		changes_tail = changes->tail;
+		changes_count = reader_changes_count_locked(changes);
+		read_size = min(changes_count * entry_size,
+				buffer_size & ~(entry_size - 1));
+
+		if (!read_size)
+			break;
+
+		if (copy_to_user(buffer, &(src_buf[changes_tail]), read_size))
+			return -EFAULT;
+
+		buffer += read_size;
+		buffer_size -= read_size;
+		ret += read_size;
+		changes_tail = (changes_tail + read_size / entry_size) &
+			(changes->size - 1);
+		smp_store_release(&changes->tail, changes_tail);
+	} while (read_size);
+
+	return ret;
+}
+
+/**
+ * reader_read() - Handles a read call on the reader file descriptor
+ *
+ * @filp: The file that the read was performed on
+ * @buffer: The destination buffer
+ * @buffer_size: The maximum number of bytes to read
+ * @offset: The offset into the 'file' to read from.
+ *
+ * Note the destination buffer needs to be fully mapped in userspace or the read
+ * will fault.
+ *
+ * Return:
+ * * The number of bytes read or:
+ * * -EBADF - the file descriptor did not have an attached reader
+ * * -EFAULT - memory access fault
+ * * -EAGAIN - if the file is set to nonblocking reads with O_NONBLOCK and there
+ *             is no data available
+ *
+ * Note: The number of bytes read will always be a multiple of the size of an
+ * entry.
+ */
+static ssize_t reader_read(struct file *const filp,
+			   char __user *const buffer,
+			   size_t const buffer_size,
+			   loff_t *const offset)
+{
+	struct reader *const reader = filp->private_data;
+	struct reader_changes *changes;
+	ssize_t ret;
+
+	if (!reader)
+		return -EBADF;
+
+	if (buffer_size < sizeof(struct kbase_kinstr_jm_atom_state_change))
+		return -ENOBUFS;
+
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE
+	if (!access_ok(buffer, buffer_size))
+		return -EIO;
+#else
+	if (!access_ok(VERIFY_WRITE, buffer, buffer_size))
+		return -EIO;
+#endif
+
+	changes = &reader->changes;
+
+	mutex_lock(&changes->consumer);
+	if (!reader_changes_count_locked(changes)) {
+		if (filp->f_flags & O_NONBLOCK) {
+			ret = -EAGAIN;
+			goto exit;
+		}
+
+		if (wait_event_interruptible(
+				reader->wait_queue,
+				!!reader_changes_count_locked(changes))) {
+			ret = -EINTR;
+			goto exit;
+		}
+	}
+
+	ret = reader_changes_copy_to_user(changes, buffer, buffer_size);
+
+exit:
+	mutex_unlock(&changes->consumer);
+	return ret;
+}
+
+/**
+ * reader_poll() - Handles a poll call on the reader file descriptor
+ * @file: The file that the poll was performed on
+ * @wait: The poll table
+ *
+ * The results of the poll will be unreliable if there is no mapped memory as
+ * there is no circular buffer to push atom state changes into.
+ *
+ * Return:
+ * * 0 - no data ready
+ * * POLLIN - state changes have been buffered
+ * * -EBADF - the file descriptor did not have an attached reader
+ * * -EINVAL - the IO control arguments were invalid
+ */
+static __poll_t reader_poll(struct file *const file,
+			    struct poll_table_struct *const wait)
+{
+	struct reader *reader;
+	struct reader_changes *changes;
+
+	if (unlikely(!file || !wait))
+		return -EINVAL;
+
+	reader = file->private_data;
+	if (unlikely(!reader))
+		return -EBADF;
+
+	changes = &reader->changes;
+
+	if (reader_changes_count(changes) >= changes->threshold)
+		return POLLIN;
+
+	poll_wait(file, &reader->wait_queue, wait);
+
+	return (reader_changes_count(changes) > 0) ? POLLIN : 0;
+}
+
+/* The file operations virtual function table */
+static const struct file_operations file_operations = {
+	.owner = THIS_MODULE,
+	.llseek = no_llseek,
+	.read = reader_read,
+	.poll = reader_poll,
+	.release = reader_release
+};
+
+/* The maximum amount of readers that can be created on a context. */
+static const size_t kbase_kinstr_jm_readers_max = 16;
+
+/**
+ * kbasep_kinstr_jm_release() - Invoked when the reference count is dropped
+ * @ref: the context reference count
+ */
+static void kbase_kinstr_jm_release(struct kref *const ref)
+{
+	struct kbase_kinstr_jm *const ctx =
+		container_of(ref, struct kbase_kinstr_jm, refcount);
+
+	kfree(ctx);
+}
+
+/**
+ * kbase_kinstr_jm_ref_get() - Reference counts the instrumentation context
+ * @ctx: the context to reference count
+ * Return: the reference counted context
+ */
+static struct kbase_kinstr_jm *
+kbase_kinstr_jm_ref_get(struct kbase_kinstr_jm *const ctx)
+{
+	if (likely(ctx))
+		kref_get(&ctx->refcount);
+	return ctx;
+}
+
+/**
+ * kbase_kinstr_jm_ref_put() - Dereferences the instrumentation context
+ * @ctx: the context to lower the reference count on
+ */
+static void kbase_kinstr_jm_ref_put(struct kbase_kinstr_jm *const ctx)
+{
+	if (likely(ctx))
+		kref_put(&ctx->refcount, kbase_kinstr_jm_release);
+}
+
+/**
+ * kbase_kinstr_jm_readers_add() - Adds a reader to the list of readers
+ * @ctx: the instrumentation context
+ * @reader: the reader to add
+ *
+ * Return:
+ * 0 - success
+ * -ENOMEM - too many readers already added.
+ */
+static int kbase_kinstr_jm_readers_add(struct kbase_kinstr_jm *const ctx,
+					struct reader *const reader)
+{
+	struct hlist_bl_head *const readers = &ctx->readers;
+	struct hlist_bl_node *node;
+	struct reader *temp;
+	size_t count = 0;
+
+	hlist_bl_lock(readers);
+
+	hlist_bl_for_each_entry_rcu(temp, node, readers, node)
+		++count;
+
+	if (kbase_kinstr_jm_readers_max < count) {
+		hlist_bl_unlock(readers);
+		return -ENOMEM;
+	}
+
+	hlist_bl_add_head_rcu(&reader->node, readers);
+
+	hlist_bl_unlock(readers);
+
+	static_branch_inc(&basep_kinstr_jm_reader_static_key);
+
+	return 0;
+}
+
+/**
+ * readers_del() - Deletes a reader from the list of readers
+ * @ctx: the instrumentation context
+ * @reader: the reader to delete
+ */
+static void kbase_kinstr_jm_readers_del(struct kbase_kinstr_jm *const ctx,
+					struct reader *const reader)
+{
+	struct hlist_bl_head *const readers = &ctx->readers;
+
+	hlist_bl_lock(readers);
+	hlist_bl_del_rcu(&reader->node);
+	hlist_bl_unlock(readers);
+
+	static_branch_dec(&basep_kinstr_jm_reader_static_key);
+}
+
+int kbase_kinstr_jm_get_fd(struct kbase_kinstr_jm *const ctx,
+			   union kbase_kinstr_jm_fd *jm_fd_arg)
+{
+	struct kbase_kinstr_jm_fd_in const *in;
+	struct reader *reader;
+	size_t const change_size = sizeof(struct
+					  kbase_kinstr_jm_atom_state_change);
+	int status;
+	int fd;
+	int i;
+
+	if (!ctx || !jm_fd_arg)
+		return -EINVAL;
+
+	in = &jm_fd_arg->in;
+
+	if (!is_power_of_2(in->count))
+		return -EINVAL;
+
+	for (i = 0; i < sizeof(in->padding); ++i)
+		if (in->padding[i])
+			return -EINVAL;
+
+	status = reader_init(&reader, ctx, in->count);
+	if (status < 0)
+		return status;
+
+	jm_fd_arg->out.version = KBASE_KINSTR_JM_VERSION;
+	jm_fd_arg->out.size = change_size;
+	memset(&jm_fd_arg->out.padding, 0, sizeof(jm_fd_arg->out.padding));
+
+	fd = anon_inode_getfd("[mali_kinstr_jm]", &file_operations, reader,
+			      O_CLOEXEC);
+	if (fd < 0)
+		reader_term(reader);
+
+	return fd;
+}
+
+int kbase_kinstr_jm_init(struct kbase_kinstr_jm **const out_ctx)
+{
+	struct kbase_kinstr_jm *ctx = NULL;
+
+	if (!out_ctx)
+		return -EINVAL;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	INIT_HLIST_BL_HEAD(&ctx->readers);
+	kref_init(&ctx->refcount);
+
+	*out_ctx = ctx;
+
+	return 0;
+}
+
+void kbase_kinstr_jm_term(struct kbase_kinstr_jm *const ctx)
+{
+	kbase_kinstr_jm_ref_put(ctx);
+}
+
+/**
+ * timestamp() - Retrieves the current monotonic nanoseconds
+ * Return: monotonic nanoseconds timestamp.
+ */
+static u64 timestamp(void)
+{
+	struct timespec ts;
+	long ns;
+
+	getrawmonotonic(&ts);
+	ns = ((long)(ts.tv_sec) * NSEC_PER_SEC) + ts.tv_nsec;
+	if (unlikely(ns < 0))
+		return 0;
+	return ((u64)(ns));
+}
+
+void kbasep_kinstr_jm_atom_state(
+	struct kbase_jd_atom *const katom,
+	const enum kbase_kinstr_jm_reader_atom_state state)
+{
+	struct kbase_context *const kctx = katom->kctx;
+	struct kbase_kinstr_jm *const ctx = kctx->kinstr_jm;
+	const u8 id = kbase_jd_atom_id(kctx, katom);
+	struct kbase_kinstr_jm_atom_state_change change = {
+		.timestamp = timestamp(), .atom = id, .state = state
+	};
+	struct reader *reader;
+	struct hlist_bl_node *node;
+
+	WARN(KBASE_KINSTR_JM_READER_ATOM_STATE_COUNT < state || 0 > state,
+	     PR_ "unsupported katom (%u) state (%i)", id, state);
+
+	switch (state) {
+	case KBASE_KINSTR_JM_READER_ATOM_STATE_START:
+		change.data.start.slot = katom->jobslot;
+		break;
+	default:
+		break;
+	}
+
+	rcu_read_lock();
+	hlist_bl_for_each_entry_rcu(reader, node, &ctx->readers, node)
+		reader_changes_push(
+			&reader->changes, &change, &reader->wait_queue);
+	rcu_read_unlock();
+}
+
+KBASE_EXPORT_TEST_API(kbasep_kinstr_jm_atom_state);
+
+void kbasep_kinstr_jm_atom_hw_submit(struct kbase_jd_atom *const katom)
+{
+	struct kbase_context *const kctx = katom->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	const int slot = katom->slot_nr;
+	struct kbase_jd_atom *const submitted = kbase_gpu_inspect(kbdev, slot, 0);
+
+	BUILD_BUG_ON(SLOT_RB_SIZE != 2);
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (WARN_ON(slot < 0 || slot >= GPU_MAX_JOB_SLOTS))
+		return;
+	if (WARN_ON(!submitted))
+		return;
+
+	if (submitted == katom)
+		kbase_kinstr_jm_atom_state_start(katom);
+}
+
+void kbasep_kinstr_jm_atom_hw_release(struct kbase_jd_atom *const katom)
+{
+	struct kbase_context *const kctx = katom->kctx;
+	struct kbase_device *const kbdev = kctx->kbdev;
+	const int slot = katom->slot_nr;
+	struct kbase_jd_atom *const submitted = kbase_gpu_inspect(kbdev, slot, 0);
+	struct kbase_jd_atom *const queued = kbase_gpu_inspect(kbdev, slot, 1);
+
+	BUILD_BUG_ON(SLOT_RB_SIZE != 2);
+
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (WARN_ON(slot < 0 || slot >= GPU_MAX_JOB_SLOTS))
+		return;
+	if (WARN_ON(!submitted))
+		return;
+	if (WARN_ON((submitted != katom) && (queued != katom)))
+		return;
+
+	if (queued == katom)
+		return;
+
+	if (katom->gpu_rb_state == KBASE_ATOM_GPU_RB_SUBMITTED)
+		kbase_kinstr_jm_atom_state_stop(katom);
+	if (queued && queued->gpu_rb_state == KBASE_ATOM_GPU_RB_SUBMITTED)
+		kbase_kinstr_jm_atom_state_start(queued);
+}
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.h b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.h
new file mode 100644
index 000000000000..555edfeef77c
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm.h
@@ -0,0 +1,283 @@
+/*
+ *
+ * (C) COPYRIGHT 2019,2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * mali_kbase_kinstr_jm.h
+ * Kernel driver public interface to job manager atom tracing. This API provides
+ * a method to get the atom state changes into user space.
+ *
+ * The flow of operation is:
+ *
+ * | kernel                              | user                                |
+ * | ----------------------------------- | ----------------------------------- |
+ * | Initialize API with                 |                                     |
+ * | kbase_kinstr_jm_init()              |                                     |
+ * |                                     |                                     |
+ * | Kernel code injects states with     |                                     |
+ * | kbase_kinstr_jm_atom_state_*() APIs |                                     |
+ * |                                     | Call ioctl() to get file descriptor |
+ * |                                     | via KBASE_IOCTL_KINSTR_JM_FD        |
+ * | Allocates a reader attached to FD   |                                     |
+ * | Allocates circular buffer and       |                                     |
+ * | patches, via ASM goto, the          |                                     |
+ * | kbase_kinstr_jm_atom_state_*()      |                                     |
+ * |                                     | loop:                               |
+ * |                                     |   Call poll() on FD for POLLIN      |
+ * |   When threshold of changes is hit, |                                     |
+ * |   the poll is interrupted with      |                                     |
+ * |   POLLIN. If circular buffer is     |                                     |
+ * |   full then store the missed count  |                                     |
+ * |   and interrupt poll                |   Call read() to get data from      |
+ * |                                     |   circular buffer via the fd        |
+ * |   Kernel advances tail of circular  |                                     |
+ * |   buffer                            |                                     |
+ * |                                     | Close file descriptor               |
+ * | Deallocates circular buffer         |                                     |
+ * |                                     |                                     |
+ * | Terminate API with                  |                                     |
+ * | kbase_kinstr_jm_term()              |                                     |
+ *
+ * All tracepoints are guarded on a static key. The static key is activated when
+ * a user space reader gets created. This means that there is negligible cost
+ * inserting the tracepoints into code when there are no readers.
+ */
+
+#ifndef _KBASE_KINSTR_JM_H_
+#define _KBASE_KINSTR_JM_H_
+
+#include "mali_kbase_kinstr_jm_reader.h"
+
+#ifdef __KERNEL__
+#include <linux/version.h>
+#include <linux/static_key.h>
+#else
+/* empty wrapper macros for userspace */
+#define static_branch_unlikely(key) (1)
+#define KERNEL_VERSION(a, b, c) (0)
+#define LINUX_VERSION_CODE (1)
+#endif /* __KERNEL__ */
+
+/* Forward declarations */
+struct kbase_context;
+struct kbase_kinstr_jm;
+struct kbase_jd_atom;
+union kbase_kinstr_jm_fd;
+
+/**
+ * kbase_kinstr_jm_init() - Initialise an instrumentation job manager context.
+ * @ctx: Non-NULL pointer to where the pointer to the created context will
+ *       be stored on success.
+ *
+ * Return: 0 on success, else error code.
+ */
+int kbase_kinstr_jm_init(struct kbase_kinstr_jm **ctx);
+
+/**
+ * kbase_kinstr_jm_term() - Terminate an instrumentation job manager context.
+ * @ctx: Pointer to context to be terminated.
+ */
+void kbase_kinstr_jm_term(struct kbase_kinstr_jm *ctx);
+
+/**
+ * kbase_kinstr_jm_get_fd() - Retrieves a file descriptor that can be used to
+ * read the atom state changes from userspace
+ *
+ * @ctx: Pointer to the initialized context
+ * @jm_fd_arg: Pointer to the union containing the in/out params
+ * Return: -1 on failure, valid file descriptor on success
+ */
+int kbase_kinstr_jm_get_fd(struct kbase_kinstr_jm *const ctx,
+			   union kbase_kinstr_jm_fd *jm_fd_arg);
+
+/**
+ * kbasep_kinstr_jm_atom_state() - Signifies that an atom has changed state
+ * @atom: The atom that has changed state
+ * @state: The new state of the atom
+ *
+ * This performs the actual storage of the state ready for user space to
+ * read the data. It is only called when the static key is enabled from
+ * kbase_kinstr_jm_atom_state(). There is almost never a need to invoke this
+ * function directly.
+ */
+void kbasep_kinstr_jm_atom_state(
+	struct kbase_jd_atom *const atom,
+	const enum kbase_kinstr_jm_reader_atom_state state);
+
+/* Allows ASM goto patching to reduce tracing overhead. This is
+ * incremented/decremented when readers are created and terminated. This really
+ * shouldn't be changed externally, but if you do, make sure you use
+ * a static_key_inc()/static_key_dec() pair.
+ */
+#if KERNEL_VERSION(4, 3, 0) <= LINUX_VERSION_CODE
+extern struct static_key_false basep_kinstr_jm_reader_static_key;
+#else
+/* Pre-4.3 kernels have a different API for static keys, but work
+ * mostly the same with less type safety. */
+extern struct static_key basep_kinstr_jm_reader_static_key;
+#define static_branch_unlikely(key) static_key_false(key)
+#endif /* KERNEL_VERSION(4, 3, 0) <= LINUX_VERSION_CODE */
+
+/**
+ * kbase_kinstr_jm_atom_state() - Signifies that an atom has changed state
+ * @atom: The atom that has changed state
+ * @state: The new state of the atom
+ *
+ * This uses a static key to reduce overhead when tracing is disabled
+ */
+static inline void kbase_kinstr_jm_atom_state(
+	struct kbase_jd_atom *const atom,
+	const enum kbase_kinstr_jm_reader_atom_state state)
+{
+	if (static_branch_unlikely(&basep_kinstr_jm_reader_static_key))
+		kbasep_kinstr_jm_atom_state(atom, state);
+}
+
+/**
+ * kbase_kinstr_jm_atom_state_queue() - Signifies that an atom has entered a
+ *                                      hardware or software queue.
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_state_queue(
+	struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state(
+		atom, KBASE_KINSTR_JM_READER_ATOM_STATE_QUEUE);
+}
+
+/**
+ * kbase_kinstr_jm_atom_state_start() - Signifies that work has started on an
+ *                                      atom
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_state_start(
+	struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state(
+		atom, KBASE_KINSTR_JM_READER_ATOM_STATE_START);
+}
+
+/**
+ * kbase_kinstr_jm_atom_state_stop() - Signifies that work has stopped on an
+ *                                     atom
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_state_stop(
+	struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state(
+		atom, KBASE_KINSTR_JM_READER_ATOM_STATE_STOP);
+}
+
+/**
+ * kbase_kinstr_jm_atom_state_complete() - Signifies that all work has completed
+ *                                         on an atom
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_state_complete(
+	struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state(
+		atom, KBASE_KINSTR_JM_READER_ATOM_STATE_COMPLETE);
+}
+
+/**
+ * kbase_kinstr_jm_atom_queue() - A software *or* hardware atom is queued for
+ *                                execution
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_queue(struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state_queue(atom);
+}
+
+/**
+ * kbase_kinstr_jm_atom_complete() - A software *or* hardware atom is fully
+ *                                   completed
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_complete(
+	struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state_complete(atom);
+}
+
+/**
+ * kbase_kinstr_jm_atom_sw_start() - A software atom has started work
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_sw_start(
+	struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state_start(atom);
+}
+
+/**
+ * kbase_kinstr_jm_atom_sw_stop() - A software atom has stopped work
+ * @atom: The atom that has changed state
+ */
+static inline void kbase_kinstr_jm_atom_sw_stop(
+	struct kbase_jd_atom *const atom)
+{
+	kbase_kinstr_jm_atom_state_stop(atom);
+}
+
+/**
+ * kbasep_kinstr_jm_atom_hw_submit() - A hardware atom has been submitted
+ * @atom: The atom that has been submitted
+ *
+ * This private implementation should not be called directly, it is protected
+ * by a static key in kbase_kinstr_jm_atom_hw_submit(). Use that instead.
+ */
+void kbasep_kinstr_jm_atom_hw_submit(struct kbase_jd_atom *const atom);
+
+/**
+ * kbase_kinstr_jm_atom_hw_submit() - A hardware atom has been submitted
+ * @atom: The atom that has been submitted
+ */
+static inline void kbase_kinstr_jm_atom_hw_submit(
+	struct kbase_jd_atom *const atom)
+{
+	if (static_branch_unlikely(&basep_kinstr_jm_reader_static_key))
+		kbasep_kinstr_jm_atom_hw_submit(atom);
+}
+
+/**
+ * kbasep_kinstr_jm_atom_hw_release() - A hardware atom has been released
+ * @atom: The atom that has been released
+ *
+ * This private implementation should not be called directly, it is protected
+ * by a static key in kbase_kinstr_jm_atom_hw_release(). Use that instead.
+ */
+void kbasep_kinstr_jm_atom_hw_release(struct kbase_jd_atom *const atom);
+
+/**
+ * kbase_kinstr_jm_atom_hw_release() - A hardware atom has been released
+ * @atom: The atom that has been released
+ */
+static inline void kbase_kinstr_jm_atom_hw_release(
+	struct kbase_jd_atom *const atom)
+{
+	if (static_branch_unlikely(&basep_kinstr_jm_reader_static_key))
+		kbasep_kinstr_jm_atom_hw_release(atom);
+}
+
+#endif /* _KBASE_KINSTR_JM_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm_reader.h b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm_reader.h
new file mode 100644
index 000000000000..e267e6bc44de
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_kinstr_jm_reader.h
@@ -0,0 +1,70 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/*
+ * mali_kbase_kinstr_jm_reader.h
+ * Provides an ioctl API to read kernel atom state changes. The flow of the
+ * API is:
+ *    1. Obtain the file descriptor with ``KBASE_IOCTL_KINSTR_JM_FD``
+ *    2. Determine the buffer structure layout via the above ioctl's returned
+ *       size and version fields in ``struct kbase_kinstr_jm_fd_out``
+ *    4. Poll the file descriptor for ``POLLIN``
+ *    5. Get data with read() on the fd
+ *    6. Use the structure version to understand how to read the data from the
+ *       buffer
+ *    7. Repeat 4-6
+ *    8. Close the file descriptor
+ */
+
+#ifndef _KBASE_KINSTR_JM_READER_H_
+#define _KBASE_KINSTR_JM_READER_H_
+
+/**
+ * enum kbase_kinstr_jm_reader_atom_state - Determines the work state of an atom
+ * @KBASE_KINSTR_JM_READER_ATOM_STATE_QUEUE:    Signifies that an atom has
+ *                                              entered a hardware queue
+ * @KBASE_KINSTR_JM_READER_ATOM_STATE_START:    Signifies that work has started
+ *                                              on an atom
+ * @KBASE_KINSTR_JM_READER_ATOM_STATE_STOP:     Signifies that work has stopped
+ *                                              on an atom
+ * @KBASE_KINSTR_JM_READER_ATOM_STATE_COMPLETE: Signifies that work has
+ *                                              completed on an atom
+ * @KBASE_KINSTR_JM_READER_ATOM_STATE_COUNT:    The number of state enumerations
+ *
+ * We can add new states to the end of this if they do not break the existing
+ * state machine. Old user mode code can gracefully ignore states they do not
+ * understand.
+ *
+ * If we need to make a breaking change to the state machine, we can do that by
+ * changing the version reported by KBASE_IOCTL_KINSTR_JM_FD. This will
+ * mean that old user mode code will fail to understand the new state field in
+ * the structure and gracefully not use the state change API.
+ */
+enum kbase_kinstr_jm_reader_atom_state {
+	KBASE_KINSTR_JM_READER_ATOM_STATE_QUEUE,
+	KBASE_KINSTR_JM_READER_ATOM_STATE_START,
+	KBASE_KINSTR_JM_READER_ATOM_STATE_STOP,
+	KBASE_KINSTR_JM_READER_ATOM_STATE_COMPLETE,
+	KBASE_KINSTR_JM_READER_ATOM_STATE_COUNT
+};
+
+#endif /* _KBASE_KINSTR_JM_READER_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem.c b/drivers/gpu/arm/bifrost/mali_kbase_mem.c
index 4a1004b6a45c..ce52f6a78fcd 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem.c
@@ -43,6 +43,7 @@
 #include <mali_kbase_mem_pool_group.h>
 #include <mmu/mali_kbase_mmu.h>
 #include <mali_kbase_config_defaults.h>
+#include <mali_kbase_trace_gpu_mem.h>
 
 /*
  * Alignment of objects allocated by the GPU inside a just-in-time memory
@@ -614,6 +615,7 @@ int kbase_add_va_region_rbtree(struct kbase_device *kbdev,
 		size_t align_offset = align;
 		size_t align_mask = align - 1;
 
+#if !MALI_USE_CSF
 		if ((reg->flags & KBASE_REG_TILER_ALIGN_TOP)) {
 			WARN(align > 1, "%s with align %lx might not be honored for KBASE_REG_TILER_ALIGN_TOP memory",
 					__func__,
@@ -621,6 +623,7 @@ int kbase_add_va_region_rbtree(struct kbase_device *kbdev,
 			align_mask  = reg->extent - 1;
 			align_offset = reg->extent - reg->initial_commit;
 		}
+#endif /* !MALI_USE_CSF */
 
 		tmp = kbase_region_tracker_find_region_meeting_reqs(reg,
 				nr_pages, align_offset, align_mask,
@@ -698,6 +701,9 @@ void kbase_region_tracker_term(struct kbase_context *kctx)
 	kbase_region_tracker_erase_rbtree(&kctx->reg_rbtree_same);
 	kbase_region_tracker_erase_rbtree(&kctx->reg_rbtree_custom);
 	kbase_region_tracker_erase_rbtree(&kctx->reg_rbtree_exec);
+#if MALI_USE_CSF
+	WARN_ON(!list_empty(&kctx->csf.event_pages_head));
+#endif
 	kbase_gpu_vm_unlock(kctx);
 }
 
@@ -773,6 +779,9 @@ int kbase_region_tracker_init(struct kbase_context *kctx)
 	kctx->exec_va_start = U64_MAX;
 	kctx->jit_va = false;
 
+#if MALI_USE_CSF
+	INIT_LIST_HEAD(&kctx->csf.event_pages_head);
+#endif
 
 	kbase_gpu_vm_unlock(kctx);
 	return 0;
@@ -847,13 +856,14 @@ int kbase_region_tracker_init_jit(struct kbase_context *kctx, u64 jit_va_pages,
 	if (group_id < 0 || group_id >= MEMORY_GROUP_MANAGER_NR_GROUPS)
 		return -EINVAL;
 
-#if MALI_JIT_PRESSURE_LIMIT
 	if (phys_pages_limit > jit_va_pages)
-#else
-	if (phys_pages_limit != jit_va_pages)
-#endif /* MALI_JIT_PRESSURE_LIMIT */
 		return -EINVAL;
 
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	if (phys_pages_limit != jit_va_pages)
+		kbase_ctx_flag_set(kctx, KCTX_JPL_ENABLED);
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
+
 	kbase_gpu_vm_lock(kctx);
 
 #ifdef CONFIG_64BIT
@@ -870,11 +880,11 @@ int kbase_region_tracker_init_jit(struct kbase_context *kctx, u64 jit_va_pages,
 		kctx->trim_level = trim_level;
 		kctx->jit_va = true;
 		kctx->jit_group_id = group_id;
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 		kctx->jit_phys_pages_limit = phys_pages_limit;
 		dev_dbg(kctx->kbdev->dev, "phys_pages_limit set to %llu\n",
 				phys_pages_limit);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 	}
 
 	kbase_gpu_vm_unlock(kctx);
@@ -957,6 +967,34 @@ int kbase_region_tracker_init_exec(struct kbase_context *kctx, u64 exec_va_pages
 	return err;
 }
 
+#if MALI_USE_CSF
+void kbase_mcu_shared_interface_region_tracker_term(struct kbase_device *kbdev)
+{
+	kbase_region_tracker_term_rbtree(&kbdev->csf.shared_reg_rbtree);
+}
+
+int kbase_mcu_shared_interface_region_tracker_init(struct kbase_device *kbdev)
+{
+	struct kbase_va_region *shared_reg;
+	u64 shared_reg_start_pfn;
+	u64 shared_reg_size;
+
+	shared_reg_start_pfn = KBASE_REG_ZONE_MCU_SHARED_BASE;
+	shared_reg_size = KBASE_REG_ZONE_MCU_SHARED_SIZE;
+
+	kbdev->csf.shared_reg_rbtree = RB_ROOT;
+
+	shared_reg = kbase_alloc_free_region(&kbdev->csf.shared_reg_rbtree,
+					shared_reg_start_pfn,
+					shared_reg_size,
+					KBASE_REG_ZONE_MCU_SHARED);
+	if (!shared_reg)
+		return -ENOMEM;
+
+	kbase_region_tracker_insert(shared_reg);
+	return 0;
+}
+#endif
 
 int kbase_mem_init(struct kbase_device *kbdev)
 {
@@ -976,6 +1014,12 @@ int kbase_mem_init(struct kbase_device *kbdev)
 	/* Initialize memory usage */
 	atomic_set(&memdev->used_pages, 0);
 
+	spin_lock_init(&kbdev->gpu_mem_usage_lock);
+	kbdev->total_gpu_pages = 0;
+	kbdev->process_root = RB_ROOT;
+	kbdev->dma_buf_root = RB_ROOT;
+	mutex_init(&kbdev->dma_buf_lock);
+
 #ifdef IR_THRESHOLD
 	atomic_set(&memdev->ir_threshold, IR_THRESHOLD);
 #else
@@ -1053,6 +1097,11 @@ void kbase_mem_term(struct kbase_device *kbdev)
 
 	kbase_mem_pool_group_term(&kbdev->mem_pools);
 
+	WARN_ON(kbdev->total_gpu_pages);
+	WARN_ON(!RB_EMPTY_ROOT(&kbdev->process_root));
+	WARN_ON(!RB_EMPTY_ROOT(&kbdev->dma_buf_root));
+	mutex_destroy(&kbdev->dma_buf_lock);
+
 	if (kbdev->mgm_dev)
 		module_put(kbdev->mgm_dev->owner);
 }
@@ -1143,6 +1192,13 @@ static struct kbase_context *kbase_reg_flags_to_kctx(
  */
 void kbase_free_alloced_region(struct kbase_va_region *reg)
 {
+#if MALI_USE_CSF
+	if ((reg->flags & KBASE_REG_ZONE_MASK) ==
+			KBASE_REG_ZONE_MCU_SHARED) {
+		kfree(reg);
+		return;
+	}
+#endif
 	if (!(reg->flags & KBASE_REG_FREE)) {
 		struct kbase_context *kctx = kbase_reg_flags_to_kctx(reg);
 
@@ -1154,6 +1210,10 @@ void kbase_free_alloced_region(struct kbase_va_region *reg)
 
 		dev_dbg(kctx->kbdev->dev, "Freeing memory region %p\n",
 			(void *)reg);
+#if MALI_USE_CSF
+		if (reg->flags & KBASE_REG_CSF_EVENT)
+			kbase_unlink_event_mem_page(kctx, reg);
+#endif
 
 		mutex_lock(&kctx->jit_evict_lock);
 
@@ -1404,7 +1464,7 @@ static struct kbase_cpu_mapping *kbasep_find_enclosing_cpu_mapping(
 	unsigned long map_start;
 	size_t map_size;
 
-	lockdep_assert_held(&current->mm->mmap_sem);
+	lockdep_assert_held(kbase_mem_get_process_mmap_lock());
 
 	if ((uintptr_t) uaddr + size < (uintptr_t) uaddr) /* overflow check */
 		return NULL;
@@ -1835,9 +1895,25 @@ int kbase_update_region_flags(struct kbase_context *kctx,
 		reg->flags |= KBASE_REG_SHARE_IN;
 	}
 
+#if !MALI_USE_CSF
 	if (flags & BASE_MEM_TILER_ALIGN_TOP)
 		reg->flags |= KBASE_REG_TILER_ALIGN_TOP;
+#endif /* !MALI_USE_CSF */
 
+#if MALI_USE_CSF
+	if (flags & BASE_MEM_CSF_EVENT) {
+		reg->flags |= KBASE_REG_CSF_EVENT;
+		reg->flags |= KBASE_REG_PERMANENT_KERNEL_MAPPING;
+
+		if (!(reg->flags & KBASE_REG_SHARE_BOTH)) {
+			/* On non coherent platforms need to map as uncached on
+			 * both sides.
+			 */
+			reg->flags &= ~KBASE_REG_CPU_CACHED;
+			reg->flags &= ~KBASE_REG_GPU_CACHED;
+		}
+	}
+#endif
 
 	/* Set up default MEMATTR usage */
 	if (!(reg->flags & KBASE_REG_GPU_CACHED)) {
@@ -1851,6 +1927,13 @@ int kbase_update_region_flags(struct kbase_context *kctx,
 				"Can't allocate GPU uncached memory due to MMU in Legacy Mode\n");
 			return -EINVAL;
 		}
+#if MALI_USE_CSF
+	} else if (reg->flags & KBASE_REG_CSF_EVENT) {
+		WARN_ON(!(reg->flags & KBASE_REG_SHARE_BOTH));
+
+		reg->flags |=
+			KBASE_REG_MEMATTR_INDEX(AS_MEMATTR_INDEX_SHARED);
+#endif
 	} else if (kctx->kbdev->system_coherency == COHERENCY_ACE &&
 		(reg->flags & KBASE_REG_SHARE_BOTH)) {
 		reg->flags |=
@@ -2033,6 +2116,9 @@ int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc *alloc,
 			(u64)new_page_count);
 
 	alloc->nents += nr_pages_requested;
+
+	kbase_trace_gpu_mem_usage_inc(kctx->kbdev, kctx, nr_pages_requested);
+
 done:
 	return 0;
 
@@ -2209,6 +2295,9 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(
 			(u64)new_page_count);
 
 	alloc->nents += nr_pages_requested;
+
+	kbase_trace_gpu_mem_usage_inc(kctx->kbdev, kctx, nr_pages_requested);
+
 done:
 	return new_pages;
 
@@ -2374,6 +2463,8 @@ int kbase_free_phy_pages_helper(
 			kbdev,
 			kctx->id,
 			(u64)new_page_count);
+
+		kbase_trace_gpu_mem_usage_dec(kctx->kbdev, kctx, freed);
 	}
 
 	return 0;
@@ -2496,9 +2587,18 @@ void kbase_free_phy_pages_helper_locked(struct kbase_mem_phy_alloc *alloc,
 				kbdev,
 				kctx->id,
 				(u64)new_page_count);
+
+		kbase_trace_gpu_mem_usage_dec(kctx->kbdev, kctx, freed);
 	}
 }
 
+#if MALI_USE_CSF
+/**
+ * kbase_jd_user_buf_unpin_pages - Release the pinned pages of a user buffer.
+ * @alloc: The allocation for the imported user buffer.
+ */
+static void kbase_jd_user_buf_unpin_pages(struct kbase_mem_phy_alloc *alloc);
+#endif
 
 void kbase_mem_kref_free(struct kref *kref)
 {
@@ -2558,12 +2658,17 @@ void kbase_mem_kref_free(struct kref *kref)
 					alloc->imported.umm.dma_attachment,
 					alloc->imported.umm.sgt,
 					DMA_BIDIRECTIONAL);
+			kbase_remove_dma_buf_usage(alloc->imported.umm.kctx,
+						   alloc);
 		}
 		dma_buf_detach(alloc->imported.umm.dma_buf,
 			       alloc->imported.umm.dma_attachment);
 		dma_buf_put(alloc->imported.umm.dma_buf);
 		break;
 	case KBASE_MEM_TYPE_IMPORTED_USER_BUF:
+#if MALI_USE_CSF
+		kbase_jd_user_buf_unpin_pages(alloc);
+#endif
 		if (alloc->imported.user_buf.mm)
 			mmdrop(alloc->imported.user_buf.mm);
 		if (alloc->properties & KBASE_MEM_PHY_ALLOC_LARGE)
@@ -2643,19 +2748,33 @@ bool kbase_check_alloc_flags(unsigned long flags)
 	/* GPU executable memory cannot:
 	 * - Be written by the GPU
 	 * - Be grown on GPU page fault
-	 * - Have the top of its initial commit aligned to 'extent' */
+	 */
+	if ((flags & BASE_MEM_PROT_GPU_EX) && (flags &
+			(BASE_MEM_PROT_GPU_WR | BASE_MEM_GROW_ON_GPF)))
+		return false;
+
+#if !MALI_USE_CSF
+	/* GPU executable memory also cannot have the top of its initial
+	 * commit aligned to 'extent'
+	 */
 	if ((flags & BASE_MEM_PROT_GPU_EX) && (flags &
-			(BASE_MEM_PROT_GPU_WR | BASE_MEM_GROW_ON_GPF |
-			BASE_MEM_TILER_ALIGN_TOP)))
+			BASE_MEM_TILER_ALIGN_TOP))
 		return false;
+#endif /* !MALI_USE_CSF */
 
 	/* To have an allocation lie within a 4GB chunk is required only for
-	 * TLS memory, which will never be used to contain executable code
-	 * and also used for Tiler heap.
+	 * TLS memory, which will never be used to contain executable code.
 	 */
 	if ((flags & BASE_MEM_GPU_VA_SAME_4GB_PAGE) && (flags &
-			(BASE_MEM_PROT_GPU_EX | BASE_MEM_TILER_ALIGN_TOP)))
+			BASE_MEM_PROT_GPU_EX))
+		return false;
+
+#if !MALI_USE_CSF
+	/* TLS memory should also not be used for tiler heap */
+	if ((flags & BASE_MEM_GPU_VA_SAME_4GB_PAGE) && (flags &
+			BASE_MEM_TILER_ALIGN_TOP))
 		return false;
+#endif /* !MALI_USE_CSF */
 
 	/* GPU should have at least read or write access otherwise there is no
 	   reason for allocating. */
@@ -2699,9 +2818,11 @@ bool kbase_check_import_flags(unsigned long flags)
 	if (flags & BASE_MEM_GROW_ON_GPF)
 		return false;
 
+#if !MALI_USE_CSF
 	/* Imported memory cannot be aligned to the end of its initial commit */
 	if (flags & BASE_MEM_TILER_ALIGN_TOP)
 		return false;
+#endif /* !MALI_USE_CSF */
 
 	/* GPU should have at least read or write access otherwise there is no
 	   reason for importing. */
@@ -2751,9 +2872,14 @@ int kbase_check_alloc_sizes(struct kbase_context *kctx, unsigned long flags,
 		return -EINVAL;
 	}
 
-	if ((flags & (BASE_MEM_GROW_ON_GPF | BASE_MEM_TILER_ALIGN_TOP)) &&
-			test_reg.extent == 0) {
-		dev_warn(dev, KBASE_MSG_PRE "BASE_MEM_GROW_ON_GPF or BASE_MEM_TILER_ALIGN_TOP but extent == 0\n");
+	if ((flags & BASE_MEM_GROW_ON_GPF) && (test_reg.extent == 0)) {
+		dev_warn(dev, KBASE_MSG_PRE "BASE_MEM_GROW_ON_GPF but extent == 0\n");
+		return -EINVAL;
+	}
+
+#if !MALI_USE_CSF
+	if ((flags & BASE_MEM_TILER_ALIGN_TOP) && (test_reg.extent == 0)) {
+		dev_warn(dev, KBASE_MSG_PRE "BASE_MEM_TILER_ALIGN_TOP but extent == 0\n");
 		return -EINVAL;
 	}
 
@@ -2762,7 +2888,14 @@ int kbase_check_alloc_sizes(struct kbase_context *kctx, unsigned long flags,
 		dev_warn(dev, KBASE_MSG_PRE "neither BASE_MEM_GROW_ON_GPF nor BASE_MEM_TILER_ALIGN_TOP set but extent != 0\n");
 		return -EINVAL;
 	}
+#else
+	if (!(flags & BASE_MEM_GROW_ON_GPF) && test_reg.extent != 0) {
+		dev_warn(dev, KBASE_MSG_PRE "BASE_MEM_GROW_ON_GPF not set but extent != 0\n");
+		return -EINVAL;
+	}
+#endif /* !MALI_USE_CSF */
 
+#if !MALI_USE_CSF
 	/* BASE_MEM_TILER_ALIGN_TOP memory has a number of restrictions */
 	if (flags & BASE_MEM_TILER_ALIGN_TOP) {
 #define KBASE_MSG_PRE_FLAG KBASE_MSG_PRE "BASE_MEM_TILER_ALIGN_TOP and "
@@ -2792,6 +2925,7 @@ int kbase_check_alloc_sizes(struct kbase_context *kctx, unsigned long flags,
 		}
 #undef KBASE_MSG_PRE_FLAG
 	}
+#endif /* !MALI_USE_CSF */
 
 	if ((flags & BASE_MEM_GPU_VA_SAME_4GB_PAGE) &&
 	    (va_pages > (BASE_MEM_PFN_MASK_4GB + 1))) {
@@ -2983,19 +3117,23 @@ static int kbase_jit_debugfs_phys_get(struct kbase_jit_debugfs_data *data)
 KBASE_JIT_DEBUGFS_DECLARE(kbase_jit_debugfs_phys_fops,
 		kbase_jit_debugfs_phys_get);
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 static int kbase_jit_debugfs_used_get(struct kbase_jit_debugfs_data *data)
 {
 	struct kbase_context *kctx = data->kctx;
 	struct kbase_va_region *reg;
 
+#if !MALI_USE_CSF
 	mutex_lock(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
 	mutex_lock(&kctx->jit_evict_lock);
 	list_for_each_entry(reg, &kctx->jit_active_head, jit_node) {
 		data->active_value += reg->used_pages;
 	}
 	mutex_unlock(&kctx->jit_evict_lock);
+#if !MALI_USE_CSF
 	mutex_unlock(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
 
 	return 0;
 }
@@ -3012,7 +3150,9 @@ static int kbase_jit_debugfs_trim_get(struct kbase_jit_debugfs_data *data)
 	struct kbase_context *kctx = data->kctx;
 	struct kbase_va_region *reg;
 
+#if !MALI_USE_CSF
 	mutex_lock(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
 	kbase_gpu_vm_lock(kctx);
 	mutex_lock(&kctx->jit_evict_lock);
 	list_for_each_entry(reg, &kctx->jit_active_head, jit_node) {
@@ -3031,14 +3171,16 @@ static int kbase_jit_debugfs_trim_get(struct kbase_jit_debugfs_data *data)
 	}
 	mutex_unlock(&kctx->jit_evict_lock);
 	kbase_gpu_vm_unlock(kctx);
+#if !MALI_USE_CSF
 	mutex_unlock(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
 
 	return 0;
 }
 
 KBASE_JIT_DEBUGFS_DECLARE(kbase_jit_debugfs_trim_fops,
 		kbase_jit_debugfs_trim_get);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 void kbase_jit_debugfs_init(struct kbase_context *kctx)
 {
@@ -3078,7 +3220,7 @@ void kbase_jit_debugfs_init(struct kbase_context *kctx)
 	 */
 	debugfs_create_file("mem_jit_phys", mode, kctx->kctx_dentry,
 			kctx, &kbase_jit_debugfs_phys_fops);
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	/*
 	 * Debugfs entry for getting the number of pages used
 	 * by JIT allocations for estimating the physical pressure
@@ -3093,7 +3235,7 @@ void kbase_jit_debugfs_init(struct kbase_context *kctx)
 	 */
 	debugfs_create_file("mem_jit_trim", mode, kctx->kctx_dentry,
 			kctx, &kbase_jit_debugfs_trim_fops);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 }
 #endif /* CONFIG_DEBUG_FS */
 
@@ -3138,8 +3280,13 @@ int kbase_jit_init(struct kbase_context *kctx)
 	INIT_LIST_HEAD(&kctx->jit_destroy_head);
 	INIT_WORK(&kctx->jit_work, kbase_jit_destroy_worker);
 
+#if MALI_USE_CSF
+	INIT_LIST_HEAD(&kctx->csf.kcpu_queues.jit_cmds_head);
+	INIT_LIST_HEAD(&kctx->csf.kcpu_queues.jit_blocked_queues);
+#else /* !MALI_USE_CSF */
 	INIT_LIST_HEAD(&kctx->jctx.jit_atoms_head);
 	INIT_LIST_HEAD(&kctx->jctx.jit_pending_alloc);
+#endif /* MALI_USE_CSF */
 	mutex_unlock(&kctx->jit_evict_lock);
 
 	kctx->jit_max_allocations = 0;
@@ -3153,25 +3300,29 @@ int kbase_jit_init(struct kbase_context *kctx)
  * allocation and also, if BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP is set, meets
  * the alignment requirements.
  */
-static bool meet_size_and_tiler_align_top_requirements(struct kbase_context *kctx,
-	struct kbase_va_region *walker, const struct base_jit_alloc_info *info)
+static bool meet_size_and_tiler_align_top_requirements(
+	const struct kbase_va_region *walker,
+	const struct base_jit_alloc_info *info)
 {
 	bool meet_reqs = true;
 
 	if (walker->nr_pages != info->va_pages)
 		meet_reqs = false;
-	else if (info->flags & BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP) {
+
+#if !MALI_USE_CSF
+	if (meet_reqs && (info->flags & BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP)) {
 		size_t align = info->extent;
 		size_t align_mask = align - 1;
 
 		if ((walker->start_pfn + info->commit_pages) & align_mask)
 			meet_reqs = false;
 	}
+#endif /* !MALI_USE_CSF */
 
 	return meet_reqs;
 }
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 /* Function will guarantee *@freed will not exceed @pages_needed
  */
 static int kbase_mem_jit_trim_pages_from_region(struct kbase_context *kctx,
@@ -3185,7 +3336,9 @@ static int kbase_mem_jit_trim_pages_from_region(struct kbase_context *kctx,
 	size_t to_free = 0u;
 	size_t max_allowed_pages = old_pages;
 
+#if !MALI_USE_CSF
 	lockdep_assert_held(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
 	lockdep_assert_held(&kctx->reg_lock);
 
 	/* Is this a JIT allocation that has been reported on? */
@@ -3308,8 +3461,12 @@ static size_t kbase_mem_jit_trim_pages(struct kbase_context *kctx,
 	struct kbase_va_region *reg, *tmp;
 	size_t total_freed = 0;
 
-	kbase_gpu_vm_lock(kctx);
-	mutex_lock(&kctx->jit_evict_lock);
+#if !MALI_USE_CSF
+	lockdep_assert_held(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
+	lockdep_assert_held(&kctx->reg_lock);
+	lockdep_assert_held(&kctx->jit_evict_lock);
+
 	list_for_each_entry_safe(reg, tmp, &kctx->jit_active_head, jit_node) {
 		int err;
 		size_t freed = 0u;
@@ -3328,18 +3485,17 @@ static size_t kbase_mem_jit_trim_pages(struct kbase_context *kctx,
 		if (!pages_needed)
 			break;
 	}
-	mutex_unlock(&kctx->jit_evict_lock);
-	kbase_gpu_vm_unlock(kctx);
 
 	trace_mali_jit_trim(total_freed);
 
 	return total_freed;
 }
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 static int kbase_jit_grow(struct kbase_context *kctx,
-		const struct base_jit_alloc_info *info,
-		struct kbase_va_region *reg)
+			  const struct base_jit_alloc_info *info,
+			  struct kbase_va_region *reg,
+			  struct kbase_sub_alloc **prealloc_sas)
 {
 	size_t delta;
 	size_t pages_required;
@@ -3347,15 +3503,13 @@ static int kbase_jit_grow(struct kbase_context *kctx,
 	struct kbase_mem_pool *pool;
 	int ret = -ENOMEM;
 	struct tagged_addr *gpu_pages;
-	struct kbase_sub_alloc *prealloc_sas[2] = { NULL, NULL };
-	int i;
 
 	if (info->commit_pages > reg->nr_pages) {
 		/* Attempted to grow larger than maximum size */
 		return -EINVAL;
 	}
 
-	kbase_gpu_vm_lock(kctx);
+	lockdep_assert_held(&kctx->reg_lock);
 
 	/* Make the physical backing no longer reclaimable */
 	if (!kbase_mem_evictable_unmake(reg->gpu_alloc))
@@ -3372,14 +3526,6 @@ static int kbase_jit_grow(struct kbase_context *kctx,
 	pages_required = delta;
 
 #ifdef CONFIG_MALI_2MB_ALLOC
-	/* Preallocate memory for the sub-allocation structs */
-	for (i = 0; i != ARRAY_SIZE(prealloc_sas); ++i) {
-		prealloc_sas[i] = kmalloc(sizeof(*prealloc_sas[i]),
-				GFP_KERNEL);
-		if (!prealloc_sas[i])
-			goto update_failed;
-	}
-
 	if (pages_required >= (SZ_2M / SZ_4K)) {
 		pool = &kctx->mem_pools.large[kctx->jit_group_id];
 		/* Round up to number of 2 MB pages required */
@@ -3405,15 +3551,18 @@ static int kbase_jit_grow(struct kbase_context *kctx,
 	 */
 	while (kbase_mem_pool_size(pool) < pages_required) {
 		int pool_delta = pages_required - kbase_mem_pool_size(pool);
+		int ret;
 
 		kbase_mem_pool_unlock(pool);
 		spin_unlock(&kctx->mem_partials_lock);
+
 		kbase_gpu_vm_unlock(kctx);
+		ret = kbase_mem_pool_grow(pool, pool_delta);
+		kbase_gpu_vm_lock(kctx);
 
-		if (kbase_mem_pool_grow(pool, pool_delta))
-			goto update_failed_unlocked;
+		if (ret)
+			goto update_failed;
 
-		kbase_gpu_vm_lock(kctx);
 		spin_lock(&kctx->mem_partials_lock);
 		kbase_mem_pool_lock(pool);
 	}
@@ -3459,11 +3608,6 @@ static int kbase_jit_grow(struct kbase_context *kctx,
 	reg->extent = info->extent;
 
 update_failed:
-	kbase_gpu_vm_unlock(kctx);
-update_failed_unlocked:
-	for (i = 0; i != ARRAY_SIZE(prealloc_sas); ++i)
-		kfree(prealloc_sas[i]);
-
 	return ret;
 }
 
@@ -3492,9 +3636,9 @@ static void trace_jit_stats(struct kbase_context *kctx,
 		max_allocations, alloc_count, va_pages, ph_pages);
 }
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 /**
- * get_jit_backed_pressure() - calculate the physical backing of all JIT
+ * get_jit_phys_backing() - calculate the physical backing of all JIT
  * allocations
  *
  * @kctx: Pointer to the kbase context whose active JIT allocations will be
@@ -3502,83 +3646,50 @@ static void trace_jit_stats(struct kbase_context *kctx,
  *
  * Return: number of pages that are committed by JIT allocations
  */
-static size_t get_jit_backed_pressure(struct kbase_context *kctx)
+static size_t get_jit_phys_backing(struct kbase_context *kctx)
 {
-	size_t backed_pressure = 0;
-	int jit_id;
-
-	lockdep_assert_held(&kctx->jctx.lock);
+	struct kbase_va_region *walker;
+	size_t backing = 0;
 
-	kbase_gpu_vm_lock(kctx);
-	for (jit_id = 0; jit_id <= BASE_JIT_ALLOC_COUNT; jit_id++) {
-		struct kbase_va_region *reg = kctx->jit_alloc[jit_id];
+	lockdep_assert_held(&kctx->jit_evict_lock);
 
-		if (reg && (reg != KBASE_RESERVED_REG_JIT_ALLOC)) {
-			/* If region has no report, be pessimistic */
-			if (reg->used_pages == reg->nr_pages) {
-				backed_pressure += reg->nr_pages;
-			} else {
-				backed_pressure +=
-					kbase_reg_current_backed_size(reg);
-			}
-		}
+	list_for_each_entry(walker, &kctx->jit_active_head, jit_node) {
+		backing += kbase_reg_current_backed_size(walker);
 	}
-	kbase_gpu_vm_unlock(kctx);
 
-	return backed_pressure;
+	return backing;
 }
 
-/**
- * jit_trim_necessary_pages() - calculate and trim the least pages possible to
- * satisfy a new JIT allocation
- *
- * @kctx: Pointer to the kbase context
- * @info: Pointer to JIT allocation information for the new allocation
- *
- * Before allocating a new just-in-time memory region or reusing a previous
- * one, ensure that the total JIT physical page usage also will not exceed the
- * pressure limit.
- *
- * If there are no reported-on allocations, then we already guarantee this will
- * be the case - because our current pressure then only comes from the va_pages
- * of each JIT region, hence JIT physical page usage is guaranteed to be
- * bounded by this.
- *
- * However as soon as JIT allocations become "reported on", the pressure is
- * lowered to allow new JIT regions to be allocated. It is after such a point
- * that the total JIT physical page usage could (either now or in the future on
- * a grow-on-GPU-page-fault) exceed the pressure limit, but only on newly
- * allocated JIT regions. Hence, trim any "reported on" regions.
- *
- * Any pages freed will go into the pool and be allocated from there in
- * kbase_mem_alloc().
- */
-static void jit_trim_necessary_pages(struct kbase_context *kctx,
-		const struct base_jit_alloc_info *info)
+void kbase_jit_trim_necessary_pages(struct kbase_context *kctx,
+				    size_t needed_pages)
 {
-	size_t backed_pressure = 0;
-	size_t needed_pages = 0;
+	size_t jit_backing = 0;
+	size_t pages_to_trim = 0;
 
-	backed_pressure = get_jit_backed_pressure(kctx);
+#if !MALI_USE_CSF
+	lockdep_assert_held(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
+	lockdep_assert_held(&kctx->reg_lock);
+	lockdep_assert_held(&kctx->jit_evict_lock);
+
+	jit_backing = get_jit_phys_backing(kctx);
 
 	/* It is possible that this is the case - if this is the first
 	 * allocation after "ignore_pressure_limit" allocation.
 	 */
-	if (backed_pressure > kctx->jit_phys_pages_limit) {
-		needed_pages +=
-			(backed_pressure - kctx->jit_phys_pages_limit)
-			+ info->va_pages;
+	if (jit_backing > kctx->jit_phys_pages_limit) {
+		pages_to_trim += (jit_backing - kctx->jit_phys_pages_limit) +
+				 needed_pages;
 	} else {
-		size_t backed_diff =
-			kctx->jit_phys_pages_limit - backed_pressure;
+		size_t backed_diff = kctx->jit_phys_pages_limit - jit_backing;
 
-		if (info->va_pages > backed_diff)
-			needed_pages += info->va_pages - backed_diff;
+		if (needed_pages > backed_diff)
+			pages_to_trim += needed_pages - backed_diff;
 	}
 
-	if (needed_pages) {
-		size_t trimmed_pages = kbase_mem_jit_trim_pages(kctx,
-			needed_pages);
+	if (pages_to_trim) {
+		size_t trimmed_pages =
+			kbase_mem_jit_trim_pages(kctx, pages_to_trim);
 
 		/* This should never happen - we already asserted that
 		 * we are not violating JIT pressure limit in earlier
@@ -3586,10 +3697,10 @@ static void jit_trim_necessary_pages(struct kbase_context *kctx,
 		 * must have enough unused pages to satisfy the new
 		 * allocation
 		 */
-		WARN_ON(trimmed_pages < needed_pages);
+		WARN_ON(trimmed_pages < pages_to_trim);
 	}
 }
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 /**
  * jit_allow_allocate() - check whether basic conditions are satisfied to allow
@@ -3606,10 +3717,14 @@ static bool jit_allow_allocate(struct kbase_context *kctx,
 		const struct base_jit_alloc_info *info,
 		bool ignore_pressure_limit)
 {
+#if MALI_USE_CSF
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+#else
 	lockdep_assert_held(&kctx->jctx.lock);
+#endif
 
-#if MALI_JIT_PRESSURE_LIMIT
-	if (likely(!ignore_pressure_limit) &&
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	if (!ignore_pressure_limit &&
 			((kctx->jit_phys_pages_limit <= kctx->jit_current_phys_pressure) ||
 			(info->va_pages > (kctx->jit_phys_pages_limit - kctx->jit_current_phys_pressure)))) {
 		dev_dbg(kctx->kbdev->dev,
@@ -3618,7 +3733,7 @@ static bool jit_allow_allocate(struct kbase_context *kctx,
 			kctx->jit_phys_pages_limit);
 		return false;
 	}
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	if (kctx->jit_current_allocations >= kctx->jit_max_allocations) {
 		/* Too many current allocations */
@@ -3644,123 +3759,156 @@ static bool jit_allow_allocate(struct kbase_context *kctx,
 	return true;
 }
 
+static struct kbase_va_region *
+find_reasonable_region(const struct base_jit_alloc_info *info,
+		       struct list_head *pool_head, bool ignore_usage_id)
+{
+	struct kbase_va_region *closest_reg = NULL;
+	struct kbase_va_region *walker;
+	size_t current_diff = SIZE_MAX;
+
+	list_for_each_entry(walker, pool_head, jit_node) {
+		if ((ignore_usage_id ||
+		     walker->jit_usage_id == info->usage_id) &&
+		    walker->jit_bin_id == info->bin_id &&
+		    meet_size_and_tiler_align_top_requirements(walker, info)) {
+			size_t min_size, max_size, diff;
+
+			/*
+			 * The JIT allocations VA requirements have been met,
+			 * it's suitable but other allocations might be a
+			 * better fit.
+			 */
+			min_size = min_t(size_t, walker->gpu_alloc->nents,
+					 info->commit_pages);
+			max_size = max_t(size_t, walker->gpu_alloc->nents,
+					 info->commit_pages);
+			diff = max_size - min_size;
+
+			if (current_diff > diff) {
+				current_diff = diff;
+				closest_reg = walker;
+			}
+
+			/* The allocation is an exact match */
+			if (current_diff == 0)
+				break;
+		}
+	}
+
+	return closest_reg;
+}
+
 struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 		const struct base_jit_alloc_info *info,
 		bool ignore_pressure_limit)
 {
 	struct kbase_va_region *reg = NULL;
+	struct kbase_sub_alloc *prealloc_sas[2] = { NULL, NULL };
+	int i;
 
+#if MALI_USE_CSF
+	lockdep_assert_held(&kctx->csf.kcpu_queues.lock);
+#else
 	lockdep_assert_held(&kctx->jctx.lock);
+#endif
 
 	if (!jit_allow_allocate(kctx, info, ignore_pressure_limit))
 		return NULL;
 
-#if MALI_JIT_PRESSURE_LIMIT
-	if (!ignore_pressure_limit)
-		jit_trim_necessary_pages(kctx, info);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#ifdef CONFIG_MALI_2MB_ALLOC
+	/* Preallocate memory for the sub-allocation structs */
+	for (i = 0; i != ARRAY_SIZE(prealloc_sas); ++i) {
+		prealloc_sas[i] = kmalloc(sizeof(*prealloc_sas[i]), GFP_KERNEL);
+		if (!prealloc_sas[i])
+			goto end;
+	}
+#endif
 
+	kbase_gpu_vm_lock(kctx);
 	mutex_lock(&kctx->jit_evict_lock);
 
 	/*
 	 * Scan the pool for an existing allocation which meets our
 	 * requirements and remove it.
 	 */
-	if (info->usage_id != 0) {
+	if (info->usage_id != 0)
 		/* First scan for an allocation with the same usage ID */
-		struct kbase_va_region *walker;
-		size_t current_diff = SIZE_MAX;
-
-		list_for_each_entry(walker, &kctx->jit_pool_head, jit_node) {
-
-			if (walker->jit_usage_id == info->usage_id &&
-					walker->jit_bin_id == info->bin_id &&
-					meet_size_and_tiler_align_top_requirements(
-							kctx, walker, info)) {
-				size_t min_size, max_size, diff;
+		reg = find_reasonable_region(info, &kctx->jit_pool_head, false);
 
-				/*
-				 * The JIT allocations VA requirements have been
-				 * met, it's suitable but other allocations
-				 * might be a better fit.
-				 */
-				min_size = min_t(size_t,
-						walker->gpu_alloc->nents,
-						info->commit_pages);
-				max_size = max_t(size_t,
-						walker->gpu_alloc->nents,
-						info->commit_pages);
-				diff = max_size - min_size;
-
-				if (current_diff > diff) {
-					current_diff = diff;
-					reg = walker;
-				}
-
-				/* The allocation is an exact match */
-				if (current_diff == 0)
-					break;
-			}
-		}
-	}
-
-	if (!reg) {
+	if (!reg)
 		/* No allocation with the same usage ID, or usage IDs not in
 		 * use. Search for an allocation we can reuse.
 		 */
-		struct kbase_va_region *walker;
-		size_t current_diff = SIZE_MAX;
-
-		list_for_each_entry(walker, &kctx->jit_pool_head, jit_node) {
-
-			if (walker->jit_bin_id == info->bin_id &&
-					meet_size_and_tiler_align_top_requirements(
-							kctx, walker, info)) {
-				size_t min_size, max_size, diff;
-
-				/*
-				 * The JIT allocations VA requirements have been
-				 * met, it's suitable but other allocations
-				 * might be a better fit.
-				 */
-				min_size = min_t(size_t,
-						walker->gpu_alloc->nents,
-						info->commit_pages);
-				max_size = max_t(size_t,
-						walker->gpu_alloc->nents,
-						info->commit_pages);
-				diff = max_size - min_size;
-
-				if (current_diff > diff) {
-					current_diff = diff;
-					reg = walker;
-				}
-
-				/* The allocation is an exact match, so stop
-				 * looking.
-				 */
-				if (current_diff == 0)
-					break;
-			}
-		}
-	}
+		reg = find_reasonable_region(info, &kctx->jit_pool_head, true);
 
 	if (reg) {
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+		size_t needed_pages = 0;
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
+		int ret;
+
 		/*
 		 * Remove the found region from the pool and add it to the
 		 * active list.
 		 */
 		list_move(&reg->jit_node, &kctx->jit_active_head);
 
+		WARN_ON(reg->gpu_alloc->evicted);
+
 		/*
 		 * Remove the allocation from the eviction list as it's no
 		 * longer eligible for eviction. This must be done before
 		 * dropping the jit_evict_lock
 		 */
 		list_del_init(&reg->gpu_alloc->evict_node);
+
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+		if (!ignore_pressure_limit) {
+			if (info->commit_pages > reg->gpu_alloc->nents)
+				needed_pages = info->commit_pages -
+					       reg->gpu_alloc->nents;
+
+			/* Update early the recycled JIT region's estimate of
+			 * used_pages to ensure it doesn't get trimmed
+			 * undesirably. This is needed as the recycled JIT
+			 * region has been added to the active list but the
+			 * number of used pages for it would be zero, so it
+			 * could get trimmed instead of other allocations only
+			 * to be regrown later resulting in a breach of the JIT
+			 * physical pressure limit.
+			 * Also that trimming would disturb the accounting of
+			 * physical pages, i.e. the VM stats, as the number of
+			 * backing pages would have changed when the call to
+			 * kbase_mem_evictable_unmark_reclaim is made.
+			 *
+			 * The second call to update pressure at the end of
+			 * this function would effectively be a nop.
+			 */
+			kbase_jit_report_update_pressure(
+				kctx, reg, info->va_pages,
+				KBASE_JIT_REPORT_ON_ALLOC_OR_FREE);
+
+			kbase_jit_request_phys_increase_locked(kctx,
+							       needed_pages);
+		}
+#endif
 		mutex_unlock(&kctx->jit_evict_lock);
 
-		if (kbase_jit_grow(kctx, info, reg) < 0) {
+		/* kbase_jit_grow() can release & reacquire 'kctx->reg_lock',
+		 * so any state protected by that lock might need to be
+		 * re-evaluated if more code is added here in future.
+		 */
+		ret = kbase_jit_grow(kctx, info, reg, prealloc_sas);
+
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+		if (!ignore_pressure_limit)
+			kbase_jit_done_phys_increase(kctx, needed_pages);
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
+
+		kbase_gpu_vm_unlock(kctx);
+
+		if (ret < 0) {
 			/*
 			 * An update to an allocation from the pool failed,
 			 * chances are slim a new allocation would fair any
@@ -3770,10 +3918,21 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 			dev_dbg(kctx->kbdev->dev,
 				"JIT allocation resize failed: va_pages 0x%llx, commit_pages 0x%llx\n",
 				info->va_pages, info->commit_pages);
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+			/* Undo the early change made to the recycled JIT
+			 * region's estimate of used_pages.
+			 */
+			if (!ignore_pressure_limit) {
+				kbase_jit_report_update_pressure(
+					kctx, reg, 0,
+					KBASE_JIT_REPORT_ON_ALLOC_OR_FREE);
+			}
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 			mutex_lock(&kctx->jit_evict_lock);
 			list_move(&reg->jit_node, &kctx->jit_pool_head);
 			mutex_unlock(&kctx->jit_evict_lock);
-			return NULL;
+			reg = NULL;
+			goto end;
 		}
 	} else {
 		/* No suitable JIT allocation was found so create a new one */
@@ -3783,12 +3942,25 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 				BASEP_MEM_NO_USER_FREE;
 		u64 gpu_addr;
 
-		mutex_unlock(&kctx->jit_evict_lock);
-
+#if !MALI_USE_CSF
 		if (info->flags & BASE_JIT_ALLOC_MEM_TILER_ALIGN_TOP)
 			flags |= BASE_MEM_TILER_ALIGN_TOP;
+#endif /* !MALI_USE_CSF */
 
 		flags |= base_mem_group_id_set(kctx->jit_group_id);
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+		if (!ignore_pressure_limit) {
+			flags |= BASEP_MEM_PERFORM_JIT_TRIM;
+			/* The corresponding call to 'done_phys_increase' would
+			 * be made inside the kbase_mem_alloc().
+			 */
+			kbase_jit_request_phys_increase_locked(
+				kctx, info->commit_pages);
+		}
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
+
+		mutex_unlock(&kctx->jit_evict_lock);
+		kbase_gpu_vm_unlock(kctx);
 
 		reg = kbase_mem_alloc(kctx, info->va_pages, info->commit_pages,
 				info->extent, &flags, &gpu_addr);
@@ -3799,12 +3971,22 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 			dev_dbg(kctx->kbdev->dev,
 				"Failed to allocate JIT memory: va_pages 0x%llx, commit_pages 0x%llx\n",
 				info->va_pages, info->commit_pages);
-			return NULL;
+			goto end;
 		}
 
-		mutex_lock(&kctx->jit_evict_lock);
-		list_add(&reg->jit_node, &kctx->jit_active_head);
-		mutex_unlock(&kctx->jit_evict_lock);
+		if (!ignore_pressure_limit) {
+			/* Due to enforcing of pressure limit, kbase_mem_alloc
+			 * was instructed to perform the trimming which in turn
+			 * would have ensured that the new JIT allocation is
+			 * already in the jit_active_head list, so nothing to
+			 * do here.
+			 */
+			WARN_ON(list_empty(&reg->jit_node));
+		} else {
+			mutex_lock(&kctx->jit_evict_lock);
+			list_add(&reg->jit_node, &kctx->jit_active_head);
+			mutex_unlock(&kctx->jit_evict_lock);
+		}
 	}
 
 	trace_mali_jit_alloc(reg, info->id);
@@ -3816,13 +3998,18 @@ struct kbase_va_region *kbase_jit_allocate(struct kbase_context *kctx,
 
 	reg->jit_usage_id = info->usage_id;
 	reg->jit_bin_id = info->bin_id;
-#if MALI_JIT_PRESSURE_LIMIT
+	reg->flags |= KBASE_REG_ACTIVE_JIT_ALLOC;
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	if (info->flags & BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE)
 		reg->flags = reg->flags | KBASE_REG_HEAP_INFO_IS_SIZE;
 	reg->heap_info_gpu_addr = info->heap_info_gpu_addr;
 	kbase_jit_report_update_pressure(kctx, reg, info->va_pages,
 			KBASE_JIT_REPORT_ON_ALLOC_OR_FREE);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
+
+end:
+	for (i = 0; i != ARRAY_SIZE(prealloc_sas); ++i)
+		kfree(prealloc_sas[i]);
 
 	return reg;
 }
@@ -3848,11 +4035,11 @@ void kbase_jit_free(struct kbase_context *kctx, struct kbase_va_region *reg)
 			kbase_mem_shrink(kctx, reg, old_pages - delta);
 	}
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	reg->heap_info_gpu_addr = 0;
 	kbase_jit_report_update_pressure(kctx, reg, 0,
 			KBASE_JIT_REPORT_ON_ALLOC_OR_FREE);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	kctx->jit_current_allocations--;
 	kctx->jit_current_allocations_per_bin[reg->jit_bin_id]--;
@@ -3863,6 +4050,7 @@ void kbase_jit_free(struct kbase_context *kctx, struct kbase_va_region *reg)
 
 	kbase_gpu_vm_lock(kctx);
 	reg->flags |= KBASE_REG_DONT_NEED;
+	reg->flags &= ~KBASE_REG_ACTIVE_JIT_ALLOC;
 	kbase_mem_shrink_cpu_mapping(kctx, reg, 0, reg->gpu_alloc->nents);
 	kbase_gpu_vm_unlock(kctx);
 
@@ -3962,6 +4150,9 @@ void kbase_jit_term(struct kbase_context *kctx)
 		kbase_mem_free_region(kctx, walker);
 		mutex_lock(&kctx->jit_evict_lock);
 	}
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	WARN_ON(kctx->jit_phys_pages_to_be_allocated);
+#endif
 	mutex_unlock(&kctx->jit_evict_lock);
 	kbase_gpu_vm_unlock(kctx);
 
@@ -3972,7 +4163,7 @@ void kbase_jit_term(struct kbase_context *kctx)
 	cancel_work_sync(&kctx->jit_work);
 }
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 void kbase_trace_jit_report_gpu_mem_trace_enabled(struct kbase_context *kctx,
 		struct kbase_va_region *reg, unsigned int flags)
 {
@@ -4015,16 +4206,18 @@ void kbase_trace_jit_report_gpu_mem_trace_enabled(struct kbase_context *kctx,
 out:
 	return;
 }
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 void kbase_jit_report_update_pressure(struct kbase_context *kctx,
 		struct kbase_va_region *reg, u64 new_used_pages,
 		unsigned int flags)
 {
 	u64 diff;
 
+#if !MALI_USE_CSF
 	lockdep_assert_held(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
 
 	trace_mali_jit_report_pressure(reg, new_used_pages,
 		kctx->jit_current_phys_pressure + new_used_pages -
@@ -4053,7 +4246,7 @@ void kbase_jit_report_update_pressure(struct kbase_context *kctx,
 	}
 
 }
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 bool kbase_has_exec_va_zone(struct kbase_context *kctx)
 {
@@ -4066,6 +4259,20 @@ bool kbase_has_exec_va_zone(struct kbase_context *kctx)
 	return has_exec_va_zone;
 }
 
+#if MALI_USE_CSF
+static void kbase_jd_user_buf_unpin_pages(struct kbase_mem_phy_alloc *alloc)
+{
+	if (alloc->nents) {
+		struct page **pages = alloc->imported.user_buf.pages;
+		long i;
+
+		WARN_ON(alloc->nents != alloc->imported.user_buf.nr_pages);
+
+		for (i = 0; i < alloc->nents; i++)
+			put_page(pages[i]);
+	}
+}
+#endif
 
 int kbase_jd_user_buf_pin_pages(struct kbase_context *kctx,
 		struct kbase_va_region *reg)
@@ -4232,12 +4439,16 @@ static void kbase_jd_user_buf_unmap(struct kbase_context *kctx,
 				DMA_BIDIRECTIONAL);
 		if (writeable)
 			set_page_dirty_lock(pages[i]);
+#if !MALI_USE_CSF
 		put_page(pages[i]);
 		pages[i] = NULL;
+#endif
 
 		size -= local_size;
 	}
+#if !MALI_USE_CSF
 	alloc->nents = 0;
+#endif
 }
 
 int kbase_mem_copy_to_pinned_user_pages(struct page **dest_pages,
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem.h b/drivers/gpu/arm/bifrost/mali_kbase_mem.h
index 6e921ece883f..2238fbfe9e99 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem.h
@@ -141,6 +141,7 @@ struct kbase_mem_phy_alloc {
 
 	union {
 		struct {
+			struct kbase_context *kctx;
 			struct dma_buf *dma_buf;
 			struct dma_buf_attachment *dma_attachment;
 			unsigned int current_mapping_usage_count;
@@ -328,14 +329,28 @@ struct kbase_va_region {
 /* Imported buffer is padded? */
 #define KBASE_REG_IMPORT_PAD        (1ul << 21)
 
+#if MALI_USE_CSF
+/* CSF event memory */
+#define KBASE_REG_CSF_EVENT         (1ul << 22)
+#else
 /* Bit 22 is reserved.
  *
- * Do not remove, use the next unreserved bit for new flags */
+ * Do not remove, use the next unreserved bit for new flags
+ */
 #define KBASE_REG_RESERVED_BIT_22   (1ul << 22)
+#endif
 
+#if !MALI_USE_CSF
 /* The top of the initial commit is aligned to extent pages.
  * Extent must be a power of 2 */
 #define KBASE_REG_TILER_ALIGN_TOP   (1ul << 23)
+#else
+/* Bit 23 is reserved.
+ *
+ * Do not remove, use the next unreserved bit for new flags
+ */
+#define KBASE_REG_RESERVED_BIT_23   (1ul << 23)
+#endif /* !MALI_USE_CSF */
 
 /* Whilst this flag is set the GPU allocation is not supposed to be freed by
  * user space. The flag will remain set for the lifetime of JIT allocations.
@@ -367,6 +382,9 @@ struct kbase_va_region {
  */
 #define KBASE_REG_HEAP_INFO_IS_SIZE (1ul << 27)
 
+/* Allocation is actively used for JIT memory */
+#define KBASE_REG_ACTIVE_JIT_ALLOC (1ul << 28)
+
 #define KBASE_REG_ZONE_SAME_VA      KBASE_REG_ZONE(0)
 
 /* only used with 32-bit clients */
@@ -390,6 +408,12 @@ struct kbase_va_region {
 #define KBASE_REG_ZONE_EXEC_VA           KBASE_REG_ZONE(2)
 #define KBASE_REG_ZONE_EXEC_VA_MAX_PAGES ((1ULL << 32) >> PAGE_SHIFT) /* 4 GB */
 
+#if MALI_USE_CSF
+#define KBASE_REG_ZONE_MCU_SHARED      KBASE_REG_ZONE(3)
+#define KBASE_REG_ZONE_MCU_SHARED_BASE (0x04000000ULL >> PAGE_SHIFT)
+#define KBASE_REG_ZONE_MCU_SHARED_SIZE (((0x08000000ULL) >> PAGE_SHIFT) - \
+		KBASE_REG_ZONE_MCU_SHARED_BASE)
+#endif
 
 	unsigned long flags;
 	size_t extent;
@@ -398,7 +422,7 @@ struct kbase_va_region {
 	struct list_head jit_node;
 	u16 jit_usage_id;
 	u8 jit_bin_id;
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	/* Pointer to an object in GPU memory defining an end of an allocated
 	 * region
 	 *
@@ -423,7 +447,7 @@ struct kbase_va_region {
 	 * gpu_alloc->nents)
 	 */
 	size_t used_pages;
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	int    va_refcnt;
 };
@@ -1136,20 +1160,23 @@ void kbase_mmu_disable_as(struct kbase_device *kbdev, int as_nr);
 
 void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat);
 
-/** Dump the MMU tables to a buffer
+/**
+ * kbase_mmu_dump() - Dump the MMU tables to a buffer.
  *
- * This function allocates a buffer (of @c nr_pages pages) to hold a dump of the MMU tables and fills it. If the
- * buffer is too small then the return value will be NULL.
+ * This function allocates a buffer (of @c nr_pages pages) to hold a dump
+ * of the MMU tables and fills it. If the buffer is too small
+ * then the return value will be NULL.
  *
  * The GPU vm lock must be held when calling this function.
  *
- * The buffer returned should be freed with @ref vfree when it is no longer required.
+ * The buffer returned should be freed with @ref vfree when it is no longer
+ * required.
  *
- * @param[in]   kctx        The kbase context to dump
- * @param[in]   nr_pages    The number of pages to allocate for the buffer.
+ * @kctx:        The kbase context to dump
+ * @nr_pages:    The number of pages to allocate for the buffer.
  *
- * @return The address of the buffer containing the MMU dump or NULL on error (including if the @c nr_pages is too
- * small)
+ * Return: The address of the buffer containing the MMU dump or NULL on error
+ * (including if the @c nr_pages is too small)
  */
 void *kbase_mmu_dump(struct kbase_context *kctx, int nr_pages);
 
@@ -1174,25 +1201,27 @@ void kbase_os_mem_map_lock(struct kbase_context *kctx);
 void kbase_os_mem_map_unlock(struct kbase_context *kctx);
 
 /**
- * @brief Update the memory allocation counters for the current process
+ * kbasep_os_process_page_usage_update() - Update the memory allocation
+ *                                         counters for the current process.
  *
- * OS specific call to updates the current memory allocation counters for the current process with
- * the supplied delta.
+ * OS specific call to updates the current memory allocation counters
+ * for the current process with the supplied delta.
  *
- * @param[in] kctx  The kbase context
- * @param[in] pages The desired delta to apply to the memory usage counters.
+ * @kctx:  The kbase context
+ * @pages: The desired delta to apply to the memory usage counters.
  */
 
 void kbasep_os_process_page_usage_update(struct kbase_context *kctx, int pages);
 
 /**
- * @brief Add to the memory allocation counters for the current process
+ * kbase_process_page_usage_inc() - Add to the memory allocation counters for
+ *                                  the current process
  *
- * OS specific call to add to the current memory allocation counters for the current process by
- * the supplied amount.
+ * OS specific call to add to the current memory allocation counters for
+ * the current process by the supplied amount.
  *
- * @param[in] kctx  The kernel base context used for the allocation.
- * @param[in] pages The desired delta to apply to the memory usage counters.
+ * @kctx:  The kernel base context used for the allocation.
+ * @pages: The desired delta to apply to the memory usage counters.
  */
 
 static inline void kbase_process_page_usage_inc(struct kbase_context *kctx, int pages)
@@ -1201,13 +1230,14 @@ static inline void kbase_process_page_usage_inc(struct kbase_context *kctx, int
 }
 
 /**
- * @brief Subtract from the memory allocation counters for the current process
+ * kbase_process_page_usage_dec() - Subtract from the memory allocation
+ *                                  counters for the current process.
  *
- * OS specific call to subtract from the current memory allocation counters for the current process by
- * the supplied amount.
+ * OS specific call to subtract from the current memory allocation counters
+ * for the current process by the supplied amount.
  *
- * @param[in] kctx  The kernel base context used for the allocation.
- * @param[in] pages The desired delta to apply to the memory usage counters.
+ * @kctx:  The kernel base context used for the allocation.
+ * @pages: The desired delta to apply to the memory usage counters.
  */
 
 static inline void kbase_process_page_usage_dec(struct kbase_context *kctx, int pages)
@@ -1332,15 +1362,15 @@ struct tagged_addr *kbase_alloc_phy_pages_helper_locked(
 		struct kbase_sub_alloc **prealloc_sa);
 
 /**
-* @brief Free physical pages.
-*
-* Frees \a nr_pages and updates the alloc object.
-*
-* @param[in] alloc allocation object to free pages from
-* @param[in] nr_pages_to_free number of physical pages to free
-*
-* Return: 0 on success, otherwise a negative error code
-*/
+ * kbase_free_phy_pages_helper() - Free physical pages.
+ *
+ * Frees \a nr_pages and updates the alloc object.
+ *
+ * @alloc:            allocation object to free pages from
+ * @nr_pages_to_free: number of physical pages to free
+ *
+ * Return: 0 on success, otherwise a negative error code
+ */
 int kbase_free_phy_pages_helper(struct kbase_mem_phy_alloc *alloc, size_t nr_pages_to_free);
 
 /**
@@ -1392,26 +1422,11 @@ static inline void kbase_clear_dma_addr(struct page *p)
 }
 
 /**
- * @brief Process a page fault.
- *
- * @param[in] data  work_struct passed by queue_work()
- */
-void page_fault_worker(struct work_struct *data);
-
-/**
- * @brief Process a bus fault.
- *
- * @param[in] data  work_struct passed by queue_work()
- */
-void bus_fault_worker(struct work_struct *data);
-
-/**
- * @brief Flush MMU workqueues.
+ * kbase_flush_mmu_wqs() - Flush MMU workqueues.
+ * @kbdev:   Device pointer.
  *
  * This function will cause any outstanding page or bus faults to be processed.
  * It should be called prior to powering off the GPU.
- *
- * @param[in] kbdev   Device pointer
  */
 void kbase_flush_mmu_wqs(struct kbase_device *kbdev);
 
@@ -1497,7 +1512,7 @@ bool kbase_jit_evict(struct kbase_context *kctx);
  */
 void kbase_jit_term(struct kbase_context *kctx);
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 /**
  * kbase_trace_jit_report_gpu_mem_trace_enabled - variant of
  * kbase_trace_jit_report_gpu_mem() that should only be called once the
@@ -1508,7 +1523,7 @@ void kbase_jit_term(struct kbase_context *kctx);
  */
 void kbase_trace_jit_report_gpu_mem_trace_enabled(struct kbase_context *kctx,
 		struct kbase_va_region *reg, unsigned int flags);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 /**
  * kbase_trace_jit_report_gpu_mem - Trace information about the GPU memory used
@@ -1530,7 +1545,7 @@ void kbase_trace_jit_report_gpu_mem_trace_enabled(struct kbase_context *kctx,
  * been included. Also gives no opportunity for the compiler to mess up
  * inlining it.
  */
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 #define kbase_trace_jit_report_gpu_mem(kctx, reg, flags) \
 	do { \
 		if (trace_mali_jit_report_gpu_mem_enabled()) \
@@ -1540,9 +1555,9 @@ void kbase_trace_jit_report_gpu_mem_trace_enabled(struct kbase_context *kctx,
 #else
 #define kbase_trace_jit_report_gpu_mem(kctx, reg, flags) \
 	CSTD_NOP(kctx, reg, flags)
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 /**
  * kbase_jit_report_update_pressure - safely update the JIT physical page
  * pressure and JIT region's estimate of used_pages
@@ -1562,7 +1577,127 @@ void kbase_trace_jit_report_gpu_mem_trace_enabled(struct kbase_context *kctx,
 void kbase_jit_report_update_pressure(struct kbase_context *kctx,
 		struct kbase_va_region *reg, u64 new_used_pages,
 		unsigned int flags);
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+
+/**
+ * jit_trim_necessary_pages() - calculate and trim the least pages possible to
+ * satisfy a new JIT allocation
+ *
+ * @kctx: Pointer to the kbase context
+ * @needed_pages: Number of JIT physical pages by which trimming is requested.
+ *                The actual number of pages trimmed could differ.
+ *
+ * Before allocating a new just-in-time memory region or reusing a previous
+ * one, ensure that the total JIT physical page usage also will not exceed the
+ * pressure limit.
+ *
+ * If there are no reported-on allocations, then we already guarantee this will
+ * be the case - because our current pressure then only comes from the va_pages
+ * of each JIT region, hence JIT physical page usage is guaranteed to be
+ * bounded by this.
+ *
+ * However as soon as JIT allocations become "reported on", the pressure is
+ * lowered to allow new JIT regions to be allocated. It is after such a point
+ * that the total JIT physical page usage could (either now or in the future on
+ * a grow-on-GPU-page-fault) exceed the pressure limit, but only on newly
+ * allocated JIT regions. Hence, trim any "reported on" regions.
+ *
+ * Any pages freed will go into the pool and be allocated from there in
+ * kbase_mem_alloc().
+ */
+void kbase_jit_trim_necessary_pages(struct kbase_context *kctx,
+				    size_t needed_pages);
+
+/*
+ * Same as kbase_jit_request_phys_increase(), except that Caller is supposed
+ * to take jit_evict_lock also on @kctx before calling this function.
+ */
+static inline void
+kbase_jit_request_phys_increase_locked(struct kbase_context *kctx,
+				       size_t needed_pages)
+{
+#if !MALI_USE_CSF
+	lockdep_assert_held(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
+	lockdep_assert_held(&kctx->reg_lock);
+	lockdep_assert_held(&kctx->jit_evict_lock);
+
+	kctx->jit_phys_pages_to_be_allocated += needed_pages;
+
+	kbase_jit_trim_necessary_pages(kctx,
+				       kctx->jit_phys_pages_to_be_allocated);
+}
+
+/**
+ * kbase_jit_request_phys_increase() - Increment the backing pages count and do
+ * the required trimming before allocating pages for a JIT allocation.
+ *
+ * @kctx: Pointer to the kbase context
+ * @needed_pages: Number of pages to be allocated for the JIT allocation.
+ *
+ * This function needs to be called before allocating backing pages for a
+ * just-in-time memory region. The backing pages are currently allocated when,
+ *
+ * - A new JIT region is created.
+ * - An old JIT region is reused from the cached pool.
+ * - GPU page fault occurs for the active JIT region.
+ * - Backing is grown for the JIT region through the commit ioctl.
+ *
+ * This function would ensure that the total JIT physical page usage does not
+ * exceed the pressure limit even when the backing pages get allocated
+ * simultaneously for multiple JIT allocations from different threads.
+ *
+ * There should be a matching call to kbase_jit_done_phys_increase(), after
+ * the pages have been allocated and accounted against the active JIT
+ * allocation.
+ *
+ * Caller is supposed to take reg_lock on @kctx before calling this function.
+ */
+static inline void kbase_jit_request_phys_increase(struct kbase_context *kctx,
+						   size_t needed_pages)
+{
+#if !MALI_USE_CSF
+	lockdep_assert_held(&kctx->jctx.lock);
+#endif /* !MALI_USE_CSF */
+	lockdep_assert_held(&kctx->reg_lock);
+
+	mutex_lock(&kctx->jit_evict_lock);
+	kbase_jit_request_phys_increase_locked(kctx, needed_pages);
+	mutex_unlock(&kctx->jit_evict_lock);
+}
+
+/**
+ * kbase_jit_done_phys_increase() - Decrement the backing pages count after the
+ * allocation of pages for a JIT allocation.
+ *
+ * @kctx: Pointer to the kbase context
+ * @needed_pages: Number of pages that were allocated for the JIT allocation.
+ *
+ * This function should be called after backing pages have been allocated and
+ * accounted against the active JIT allocation.
+ * The call should be made when the following have been satisfied:
+ *    when the allocation is on the jit_active_head.
+ *    when additional needed_pages have been allocated.
+ *    kctx->reg_lock was held during the above and has not yet been unlocked.
+ * Failure to call this function before unlocking the kctx->reg_lock when
+ * either the above have changed may result in over-accounting the memory.
+ * This ensures kbase_jit_trim_necessary_pages() gets a consistent count of
+ * the memory.
+ *
+ * A matching call to kbase_jit_request_phys_increase() should have been made,
+ * before the allocation of backing pages.
+ *
+ * Caller is supposed to take reg_lock on @kctx before calling this function.
+ */
+static inline void kbase_jit_done_phys_increase(struct kbase_context *kctx,
+						size_t needed_pages)
+{
+	lockdep_assert_held(&kctx->reg_lock);
+
+	WARN_ON(kctx->jit_phys_pages_to_be_allocated < needed_pages);
+
+	kctx->jit_phys_pages_to_be_allocated -= needed_pages;
+}
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 /**
  * kbase_has_exec_va_zone - EXEC_VA zone predicate
@@ -1693,6 +1828,63 @@ static inline void kbase_mem_pool_unlock(struct kbase_mem_pool *pool)
  */
 void kbase_mem_evictable_mark_reclaim(struct kbase_mem_phy_alloc *alloc);
 
+#if MALI_USE_CSF
+/**
+ * kbase_link_event_mem_page - Add the new event memory region to the per
+ *                             context list of event pages.
+ * @kctx: Pointer to kbase context
+ * @reg: Pointer to the region allocated for event memory.
+ *
+ * The region being linked shouldn't have been marked as free and should
+ * have KBASE_REG_CSF_EVENT flag set for it.
+ */
+static inline void kbase_link_event_mem_page(struct kbase_context *kctx,
+		struct kbase_va_region *reg)
+{
+	lockdep_assert_held(&kctx->reg_lock);
+
+	WARN_ON(reg->flags & KBASE_REG_FREE);
+	WARN_ON(!(reg->flags & KBASE_REG_CSF_EVENT));
+
+	list_add(&reg->link, &kctx->csf.event_pages_head);
+}
+
+/**
+ * kbase_unlink_event_mem_page - Remove the event memory region from the per
+ *                               context list of event pages.
+ * @kctx: Pointer to kbase context
+ * @reg: Pointer to the region allocated for event memory.
+ *
+ * The region being un-linked shouldn't have been marked as free and should
+ * have KBASE_REG_CSF_EVENT flag set for it.
+ */
+static inline void kbase_unlink_event_mem_page(struct kbase_context *kctx,
+		struct kbase_va_region *reg)
+{
+	lockdep_assert_held(&kctx->reg_lock);
+
+	WARN_ON(reg->flags & KBASE_REG_FREE);
+	WARN_ON(!(reg->flags & KBASE_REG_CSF_EVENT));
+
+	list_del(&reg->link);
+}
+
+/**
+ * kbase_mcu_shared_interface_region_tracker_init - Initialize the rb tree to
+ *         manage the shared interface segment of MCU firmware address space.
+ * @kbdev: Pointer to the kbase device
+ *
+ * Returns zero on success or negative error number on failure.
+ */
+int kbase_mcu_shared_interface_region_tracker_init(struct kbase_device *kbdev);
+
+/**
+ * kbase_mcu_shared_interface_region_tracker_term - Teardown the rb tree
+ *         managing the shared interface segment of MCU firmware address space.
+ * @kbdev: Pointer to the kbase device
+ */
+void kbase_mcu_shared_interface_region_tracker_term(struct kbase_device *kbdev);
+#endif
 
 /**
  * kbase_mem_umm_map - Map dma-buf
@@ -1742,7 +1934,6 @@ void kbase_mem_umm_unmap(struct kbase_context *kctx,
 int kbase_mem_do_sync_imported(struct kbase_context *kctx,
 		struct kbase_va_region *reg, enum kbase_sync_type sync_fn);
 
-
 /**
  * kbase_mem_copy_to_pinned_user_pages - Memcpy from source input page to
  * an unaligned address at a given offset from the start of a target page.
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c
index acaa82b9767b..99b5b852667e 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.c
@@ -49,6 +49,8 @@
 #include <tl/mali_kbase_tracepoints.h>
 #include <mali_kbase_ioctl.h>
 #include <mmu/mali_kbase_mmu.h>
+#include <mali_kbase_caps.h>
+#include <mali_kbase_trace_gpu_mem.h>
 
 #if ((KERNEL_VERSION(5, 3, 0) <= LINUX_VERSION_CODE) || \
 	(KERNEL_VERSION(5, 0, 0) > LINUX_VERSION_CODE))
@@ -87,6 +89,12 @@
 
 #define IR_THRESHOLD_STEPS (256u)
 
+#if MALI_USE_CSF
+static int kbase_csf_cpu_mmap_user_reg_page(struct kbase_context *kctx,
+			struct vm_area_struct *vma);
+static int kbase_csf_cpu_mmap_user_io_pages(struct kbase_context *kctx,
+			struct vm_area_struct *vma);
+#endif
 
 static int kbase_vmap_phy_pages(struct kbase_context *kctx,
 		struct kbase_va_region *reg, u64 offset_bytes, size_t size,
@@ -107,6 +115,25 @@ static int kbase_mem_shrink_gpu_mapping(struct kbase_context *kctx,
 static struct kbase_va_region *kbase_find_event_mem_region(
 			struct kbase_context *kctx, u64 gpu_addr)
 {
+#if MALI_USE_CSF
+	u64 gpu_pfn = gpu_addr >> PAGE_SHIFT;
+	struct kbase_va_region *reg;
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	list_for_each_entry(reg, &kctx->csf.event_pages_head, link) {
+		if ((reg->start_pfn <= gpu_pfn) &&
+		    (gpu_pfn < (reg->start_pfn + reg->nr_pages))) {
+			if (WARN_ON(reg->flags & KBASE_REG_FREE))
+				return NULL;
+
+			if (WARN_ON(!(reg->flags & KBASE_REG_CSF_EVENT)))
+				return NULL;
+
+			return reg;
+		}
+	}
+#endif
 
 	return NULL;
 }
@@ -285,12 +312,16 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 	dev_dbg(dev, "Allocating %lld va_pages, %lld commit_pages, %lld extent, 0x%llX flags\n",
 		va_pages, commit_pages, extent, *flags);
 
+#if MALI_USE_CSF
+	*gpu_va = 0; /* return 0 on failure */
+#else
 	if (!(*flags & BASE_MEM_FLAG_MAP_FIXED))
 		*gpu_va = 0; /* return 0 on failure */
 	else
 		dev_err(dev,
 			"Keeping requested GPU VA of 0x%llx\n",
 			(unsigned long long)*gpu_va);
+#endif
 
 	if (!kbase_check_alloc_flags(*flags)) {
 		dev_warn(dev,
@@ -363,6 +394,15 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 		goto prepare_failed;
 	}
 
+	if (unlikely(reg->cpu_alloc != reg->gpu_alloc))
+		*flags |= BASE_MEM_KERNEL_SYNC;
+
+	/* make sure base knows if the memory is actually cached or not */
+	if (reg->flags & KBASE_REG_CPU_CACHED)
+		*flags |= BASE_MEM_CACHED_CPU;
+	else
+		*flags &= ~BASE_MEM_CACHED_CPU;
+
 	if (*flags & BASE_MEM_GROW_ON_GPF) {
 		unsigned int const ir_threshold = atomic_read(
 			&kctx->kbdev->memdev.ir_threshold);
@@ -372,10 +412,14 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 	} else
 		reg->threshold_pages = 0;
 
-	if (*flags & (BASE_MEM_GROW_ON_GPF|BASE_MEM_TILER_ALIGN_TOP)) {
+	if (*flags & BASE_MEM_GROW_ON_GPF) {
 		/* kbase_check_alloc_sizes() already checks extent is valid for
 		 * assigning to reg->extent */
 		reg->extent = extent;
+#if !MALI_USE_CSF
+	} else if (*flags & BASE_MEM_TILER_ALIGN_TOP) {
+		reg->extent = extent;
+#endif /* !MALI_USE_CSF */
 	} else {
 		reg->extent = 0;
 	}
@@ -404,6 +448,13 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 		}
 	}
 
+#if MALI_USE_CSF
+	if (reg->flags & KBASE_REG_CSF_EVENT) {
+		WARN_ON(!(*flags & BASE_MEM_SAME_VA));
+
+		kbase_link_event_mem_page(kctx, reg);
+	}
+#endif
 
 	/* mmap needed to setup VA? */
 	if (*flags & BASE_MEM_SAME_VA) {
@@ -436,13 +487,38 @@ struct kbase_va_region *kbase_mem_alloc(struct kbase_context *kctx,
 		*gpu_va = reg->start_pfn << PAGE_SHIFT;
 	}
 
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	if (*flags & BASEP_MEM_PERFORM_JIT_TRIM) {
+		kbase_jit_done_phys_increase(kctx, commit_pages);
+
+		mutex_lock(&kctx->jit_evict_lock);
+		WARN_ON(!list_empty(&reg->jit_node));
+		list_add(&reg->jit_node, &kctx->jit_active_head);
+		mutex_unlock(&kctx->jit_evict_lock);
+	}
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
+
 	kbase_gpu_vm_unlock(kctx);
 	return reg;
 
 no_mmap:
 no_cookie:
+#if MALI_USE_CSF
+	if (reg->flags & KBASE_REG_CSF_EVENT) {
+		kbase_gpu_vm_lock(kctx);
+		kbase_unlink_event_mem_page(kctx, reg);
+		kbase_gpu_vm_unlock(kctx);
+	}
+#endif
 no_kern_mapping:
 no_mem:
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	if (*flags & BASEP_MEM_PERFORM_JIT_TRIM) {
+		kbase_gpu_vm_lock(kctx);
+		kbase_jit_done_phys_increase(kctx, commit_pages);
+		kbase_gpu_vm_unlock(kctx);
+	}
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 	kbase_mem_phy_alloc_put(reg->cpu_alloc);
 	kbase_mem_phy_alloc_put(reg->gpu_alloc);
 invalid_flags:
@@ -511,21 +587,36 @@ int kbase_mem_query(struct kbase_context *kctx,
 			*out |= BASE_MEM_COHERENT_SYSTEM;
 		if (KBASE_REG_SHARE_IN & reg->flags)
 			*out |= BASE_MEM_COHERENT_LOCAL;
-		if (kctx->api_version >= KBASE_API_VERSION(11, 2)) {
-			/* Prior to 11.2, these were known about by user-side
-			 * but we did not return them. Returning some of these
-			 * caused certain clients that were not expecting them
-			 * to fail, so we omit all of them as a special-case
-			 * for compatibility reasons */
+		if (mali_kbase_supports_mem_grow_on_gpf(kctx->api_version)) {
+			/* Prior to this version, this was known about by
+			 * user-side but we did not return them. Returning
+			 * it caused certain clients that were not expecting
+			 * it to fail, so we omit it as a special-case for
+			 * compatibility reasons
+			 */
 			if (KBASE_REG_PF_GROW & reg->flags)
 				*out |= BASE_MEM_GROW_ON_GPF;
+		}
+		if (mali_kbase_supports_mem_protected(kctx->api_version)) {
+			/* Prior to this version, this was known about by
+			 * user-side but we did not return them. Returning
+			 * it caused certain clients that were not expecting
+			 * it to fail, so we omit it as a special-case for
+			 * compatibility reasons
+			 */
 			if (KBASE_REG_PROTECTED & reg->flags)
 				*out |= BASE_MEM_PROTECTED;
 		}
+#if !MALI_USE_CSF
 		if (KBASE_REG_TILER_ALIGN_TOP & reg->flags)
 			*out |= BASE_MEM_TILER_ALIGN_TOP;
+#endif /* !MALI_USE_CSF */
 		if (!(KBASE_REG_GPU_CACHED & reg->flags))
 			*out |= BASE_MEM_UNCACHED_GPU;
+#if MALI_USE_CSF
+		if (KBASE_REG_CSF_EVENT & reg->flags)
+			*out |= BASE_MEM_CSF_EVENT;
+#endif
 		if (KBASE_REG_GPU_VA_SAME_4GB_PAGE & reg->flags)
 			*out |= BASE_MEM_GPU_VA_SAME_4GB_PAGE;
 
@@ -705,6 +796,7 @@ void kbase_mem_evictable_mark_reclaim(struct kbase_mem_phy_alloc *alloc)
 			kbdev,
 			kctx->id,
 			(u64)new_page_count);
+	kbase_trace_gpu_mem_usage_dec(kbdev, kctx, alloc->nents);
 }
 
 /**
@@ -731,6 +823,7 @@ void kbase_mem_evictable_unmark_reclaim(struct kbase_mem_phy_alloc *alloc)
 			kbdev,
 			kctx->id,
 			(u64)new_page_count);
+	kbase_trace_gpu_mem_usage_inc(kbdev, kctx, alloc->nents);
 }
 
 int kbase_mem_evictable_make(struct kbase_mem_phy_alloc *gpu_alloc)
@@ -840,7 +933,7 @@ int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, unsigned in
 		real_flags |= KBASE_REG_SHARE_IN;
 
 	/* now we can lock down the context, and find the region */
-	down_write(&current->mm->mmap_sem);
+	down_write(kbase_mem_get_process_mmap_lock());
 	kbase_gpu_vm_lock(kctx);
 
 	/* Validate the region */
@@ -938,7 +1031,7 @@ int kbase_mem_flags_change(struct kbase_context *kctx, u64 gpu_addr, unsigned in
 
 out_unlock:
 	kbase_gpu_vm_unlock(kctx);
-	up_write(&current->mm->mmap_sem);
+	up_write(kbase_mem_get_process_mmap_lock());
 out:
 	return ret;
 }
@@ -1056,6 +1149,8 @@ static void kbase_mem_umm_unmap_attachment(struct kbase_context *kctx,
 				 alloc->imported.umm.sgt, DMA_BIDIRECTIONAL);
 	alloc->imported.umm.sgt = NULL;
 
+	kbase_remove_dma_buf_usage(kctx, alloc);
+
 	memset(pa, 0xff, sizeof(*pa) * alloc->nents);
 	alloc->nents = 0;
 }
@@ -1126,6 +1221,7 @@ static int kbase_mem_umm_map_attachment(struct kbase_context *kctx,
 
 	/* Update nents as we now have pages to map */
 	alloc->nents = count;
+	kbase_add_dma_buf_usage(kctx, alloc);
 
 	return 0;
 
@@ -1386,6 +1482,7 @@ static struct kbase_va_region *kbase_mem_from_umm(struct kbase_context *kctx,
 	reg->gpu_alloc->imported.umm.dma_attachment = dma_attachment;
 	reg->gpu_alloc->imported.umm.current_mapping_usage_count = 0;
 	reg->gpu_alloc->imported.umm.need_sync = need_sync;
+	reg->gpu_alloc->imported.umm.kctx = kctx;
 	reg->extent = 0;
 
 	if (!IS_ENABLED(CONFIG_MALI_DMA_BUF_MAP_ON_DEMAND)) {
@@ -1550,7 +1647,7 @@ static struct kbase_va_region *kbase_mem_from_user_buffer(
 		*flags |= KBASE_MEM_IMPORT_HAVE_PAGES;
 	}
 
-	down_read(&current->mm->mmap_sem);
+	down_read(kbase_mem_get_process_mmap_lock());
 
 #if LINUX_VERSION_CODE < KERNEL_VERSION(4, 6, 0)
 	faulted_pages = get_user_pages(current, current->mm, address, *va_pages,
@@ -1570,7 +1667,7 @@ KERNEL_VERSION(4, 5, 0) > LINUX_VERSION_CODE
 			pages, NULL);
 #endif
 
-	up_read(&current->mm->mmap_sem);
+	up_read(kbase_mem_get_process_mmap_lock());
 
 	if (faulted_pages != *va_pages)
 		goto fault_mismatch;
@@ -2027,7 +2124,7 @@ static int kbase_mem_shrink_gpu_mapping(struct kbase_context *const kctx,
 int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 {
 	u64 old_pages;
-	u64 delta;
+	u64 delta = 0;
 	int res = -EINVAL;
 	struct kbase_va_region *reg;
 	bool read_locked = false;
@@ -2040,7 +2137,7 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 		return -EINVAL;
 	}
 
-	down_write(&current->mm->mmap_sem);
+	down_write(kbase_mem_get_process_mmap_lock());
 	kbase_gpu_vm_lock(kctx);
 
 	/* Validate the region */
@@ -2057,6 +2154,9 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 	if (0 == (reg->flags & KBASE_REG_GROWABLE))
 		goto out_unlock;
 
+	if (reg->flags & KBASE_REG_ACTIVE_JIT_ALLOC)
+		goto out_unlock;
+
 	/* Would overflow the VA region */
 	if (new_pages > reg->nr_pages)
 		goto out_unlock;
@@ -2088,7 +2188,7 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 		 * No update to the mm so downgrade the writer lock to a read
 		 * lock so other readers aren't blocked after this point.
 		 */
-		downgrade_write(&current->mm->mmap_sem);
+		downgrade_write(kbase_mem_get_process_mmap_lock());
 		read_locked = true;
 
 		/* Allocate some more pages */
@@ -2130,9 +2230,9 @@ int kbase_mem_commit(struct kbase_context *kctx, u64 gpu_addr, u64 new_pages)
 out_unlock:
 	kbase_gpu_vm_unlock(kctx);
 	if (read_locked)
-		up_read(&current->mm->mmap_sem);
+		up_read(kbase_mem_get_process_mmap_lock());
 	else
-		up_write(&current->mm->mmap_sem);
+		up_write(kbase_mem_get_process_mmap_lock());
 
 	return res;
 }
@@ -2513,16 +2613,14 @@ static int kbase_mmu_dump_mmap(struct kbase_context *kctx,
 
 void kbase_os_mem_map_lock(struct kbase_context *kctx)
 {
-	struct mm_struct *mm = current->mm;
 	(void)kctx;
-	down_read(&mm->mmap_sem);
+	down_read(kbase_mem_get_process_mmap_lock());
 }
 
 void kbase_os_mem_map_unlock(struct kbase_context *kctx)
 {
-	struct mm_struct *mm = current->mm;
 	(void)kctx;
-	up_read(&mm->mmap_sem);
+	up_read(kbase_mem_get_process_mmap_lock());
 }
 
 static int kbasep_reg_mmap(struct kbase_context *kctx,
@@ -2654,6 +2752,20 @@ int kbase_context_mmap(struct kbase_context *const kctx,
 		/* free the region on munmap */
 		free_on_close = 1;
 		break;
+#if MALI_USE_CSF
+	case PFN_DOWN(BASEP_MEM_CSF_USER_REG_PAGE_HANDLE):
+		kbase_gpu_vm_unlock(kctx);
+		err = kbase_csf_cpu_mmap_user_reg_page(kctx, vma);
+		goto out;
+	case PFN_DOWN(BASEP_MEM_CSF_USER_IO_PAGES_HANDLE) ...
+	     PFN_DOWN(BASE_MEM_COOKIE_BASE) - 1: {
+		kbase_gpu_vm_unlock(kctx);
+		mutex_lock(&kctx->csf.lock);
+		err = kbase_csf_cpu_mmap_user_io_pages(kctx, vma);
+		mutex_unlock(&kctx->csf.lock);
+		goto out;
+	}
+#endif
 	case PFN_DOWN(BASE_MEM_COOKIE_BASE) ...
 	     PFN_DOWN(BASE_MEM_FIRST_FREE_ADDRESS) - 1: {
 		err = kbasep_reg_mmap(kctx, vma, &reg, &nr_pages,
@@ -3031,3 +3143,283 @@ static int kbase_tracking_page_setup(struct kbase_context *kctx, struct vm_area_
 	return 0;
 }
 
+#if MALI_USE_CSF
+static unsigned long get_queue_doorbell_pfn(struct kbase_device *kbdev,
+				struct kbase_queue *queue)
+{
+	lockdep_assert_held(&kbdev->csf.reg_lock);
+
+	/* Return the real Hw doorbell page if queue has been
+	 * assigned one, otherwise a dummy page. Always return the
+	 * dummy page in no mali builds.
+	 */
+	if ((queue->doorbell_nr == KBASEP_USER_DB_NR_INVALID) ||
+			IS_ENABLED(CONFIG_MALI_BIFROST_NO_MALI))
+		return PFN_DOWN(as_phys_addr_t(kbdev->csf.dummy_db_page));
+
+	return (PFN_DOWN(kbdev->reg_start + CSF_HW_DOORBELL_PAGE_OFFSET +
+			 (u64)queue->doorbell_nr * CSF_HW_DOORBELL_PAGE_SIZE));
+}
+
+static void kbase_csf_user_io_pages_vm_open(struct vm_area_struct *vma)
+{
+	WARN(1, "Unexpected attempt to clone private vma\n");
+	vma->vm_private_data = NULL;
+}
+
+static void kbase_csf_user_io_pages_vm_close(struct vm_area_struct *vma)
+{
+	struct kbase_queue *queue = vma->vm_private_data;
+	struct kbase_context *kctx;
+
+	if (WARN_ON(!queue))
+		return;
+
+	kctx = queue->kctx;
+
+	mutex_lock(&kctx->csf.lock);
+	kbase_csf_queue_unbind(queue);
+	mutex_unlock(&kctx->csf.lock);
+
+	/* Now as the vma is closed, drop the reference on mali device file */
+	fput(kctx->filp);
+}
+
+#if (KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE)
+static vm_fault_t kbase_csf_user_io_pages_vm_fault(struct vm_area_struct *vma,
+			struct vm_fault *vmf)
+{
+#else
+static vm_fault_t kbase_csf_user_io_pages_vm_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+#endif
+	struct kbase_queue *queue = vma->vm_private_data;
+	unsigned long doorbell_cpu_addr, input_cpu_addr, output_cpu_addr;
+	unsigned long doorbell_page_pfn, input_page_pfn, output_page_pfn;
+	pgprot_t doorbell_pgprot, input_page_pgprot, output_page_pgprot;
+	size_t nr_pages = PFN_DOWN(vma->vm_end - vma->vm_start);
+	vm_fault_t ret;
+	struct kbase_device *kbdev;
+	struct memory_group_manager_device *mgm_dev;
+
+	/* Few sanity checks up front */
+	if ((nr_pages != BASEP_QUEUE_NR_MMAP_USER_PAGES) ||
+	    (vma->vm_pgoff != queue->db_file_offset))
+		return VM_FAULT_SIGBUS;
+
+	mutex_lock(&queue->kctx->csf.lock);
+	kbdev = queue->kctx->kbdev;
+	mgm_dev = kbdev->mgm_dev;
+
+	/* Always map the doorbell page as uncached */
+	doorbell_pgprot = pgprot_device(vma->vm_page_prot);
+
+#if ((KERNEL_VERSION(4, 4, 147) >= LINUX_VERSION_CODE) || \
+		((KERNEL_VERSION(4, 6, 0) > LINUX_VERSION_CODE) && \
+		 (KERNEL_VERSION(4, 5, 0) <= LINUX_VERSION_CODE)))
+	vma->vm_page_prot = doorbell_pgprot;
+	input_page_pgprot = doorbell_pgprot;
+	output_page_pgprot = doorbell_pgprot;
+#else
+	if (kbdev->system_coherency == COHERENCY_NONE) {
+		input_page_pgprot = pgprot_writecombine(vma->vm_page_prot);
+		output_page_pgprot = pgprot_writecombine(vma->vm_page_prot);
+	} else {
+		input_page_pgprot = vma->vm_page_prot;
+		output_page_pgprot = vma->vm_page_prot;
+	}
+#endif
+
+	doorbell_cpu_addr = vma->vm_start;
+
+#if KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE
+	if ((unsigned long)vmf->virtual_address == doorbell_cpu_addr) {
+#else
+	if (vmf->address == doorbell_cpu_addr) {
+#endif
+		mutex_lock(&kbdev->csf.reg_lock);
+		doorbell_page_pfn = get_queue_doorbell_pfn(kbdev, queue);
+		ret = mgm_dev->ops.mgm_vmf_insert_pfn_prot(mgm_dev,
+			KBASE_MEM_GROUP_CSF_IO, vma, doorbell_cpu_addr,
+			doorbell_page_pfn, doorbell_pgprot);
+		mutex_unlock(&kbdev->csf.reg_lock);
+	} else {
+		/* Map the Input page */
+		input_cpu_addr = doorbell_cpu_addr + PAGE_SIZE;
+		input_page_pfn = PFN_DOWN(as_phys_addr_t(queue->phys[0]));
+		ret = mgm_dev->ops.mgm_vmf_insert_pfn_prot(mgm_dev,
+			KBASE_MEM_GROUP_CSF_IO, vma, input_cpu_addr,
+			input_page_pfn,	input_page_pgprot);
+		if (ret != VM_FAULT_NOPAGE)
+			goto exit;
+
+		/* Map the Output page */
+		output_cpu_addr = input_cpu_addr + PAGE_SIZE;
+		output_page_pfn = PFN_DOWN(as_phys_addr_t(queue->phys[1]));
+		ret = mgm_dev->ops.mgm_vmf_insert_pfn_prot(mgm_dev,
+			KBASE_MEM_GROUP_CSF_IO, vma, output_cpu_addr,
+			output_page_pfn, output_page_pgprot);
+	}
+
+exit:
+	mutex_unlock(&queue->kctx->csf.lock);
+	return ret;
+}
+
+static const struct vm_operations_struct kbase_csf_user_io_pages_vm_ops = {
+	.open = kbase_csf_user_io_pages_vm_open,
+	.close = kbase_csf_user_io_pages_vm_close,
+	.fault = kbase_csf_user_io_pages_vm_fault
+};
+
+/* Program the client process's page table entries to map the pair of
+ * input/output pages & Hw doorbell page. The caller should have validated that
+ * vma->vm_pgoff maps to the range of csf cookies.
+ */
+static int kbase_csf_cpu_mmap_user_io_pages(struct kbase_context *kctx,
+					 struct vm_area_struct *vma)
+{
+	unsigned long cookie =
+		vma->vm_pgoff - PFN_DOWN(BASEP_MEM_CSF_USER_IO_PAGES_HANDLE);
+	size_t nr_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+	struct kbase_queue *queue;
+	int err = 0;
+
+	lockdep_assert_held(&kctx->csf.lock);
+
+	queue = kctx->csf.user_pages_info[cookie];
+
+	/* Looks like the bind has been aborted */
+	if (!queue)
+		return -EINVAL;
+
+	if (WARN_ON(test_bit(cookie, kctx->csf.cookies)))
+		return -EINVAL;
+
+	/* no need for the cookie anymore */
+	kctx->csf.user_pages_info[cookie] = NULL;
+	bitmap_set(kctx->csf.cookies, cookie, 1);
+
+	/* Reset the handle to avoid (re)freeing the cookie (which can
+	 * now get re-assigned) on unbind.
+	 */
+	queue->handle = BASEP_MEM_INVALID_HANDLE;
+
+	if (nr_pages != BASEP_QUEUE_NR_MMAP_USER_PAGES) {
+		err = -EINVAL;
+		goto map_failed;
+	}
+
+	err = kbase_csf_alloc_command_stream_user_pages(kctx, queue);
+	if (err)
+		goto map_failed;
+
+#if (KERNEL_VERSION(3, 7, 0) <= LINUX_VERSION_CODE)
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO;
+#else
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND | VM_RESERVED | VM_IO;
+#endif
+	/* TODO use VM_MIXEDMAP, since it is more appropriate as both types of
+	 * memory with and without "struct page" backing are being inserted here.
+	 * Hw Doorbell pages comes from the device register area so kernel does
+	 * not use "struct page" for them.
+	 */
+	vma->vm_flags |= VM_PFNMAP;
+
+	vma->vm_ops = &kbase_csf_user_io_pages_vm_ops;
+	vma->vm_private_data = queue;
+
+	/* Make vma point to the special internal file, but don't drop the
+	 * reference on mali device file (that would be done later when the
+	 * vma is closed).
+	 */
+	vma->vm_file = kctx->kbdev->csf.db_filp;
+	get_file(vma->vm_file);
+	/* Also adjust the vm_pgoff */
+	vma->vm_pgoff = queue->db_file_offset;
+
+	return 0;
+
+map_failed:
+	kbase_csf_queue_unbind(queue);
+
+	return err;
+}
+
+static void kbase_csf_user_reg_vm_close(struct vm_area_struct *vma)
+{
+	struct kbase_context *kctx = vma->vm_private_data;
+
+	WARN_ON(!kctx->csf.user_reg_vma);
+
+	kctx->csf.user_reg_vma = NULL;
+}
+
+#if (KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE)
+static vm_fault_t kbase_csf_user_reg_vm_fault(struct vm_area_struct *vma,
+			struct vm_fault *vmf)
+{
+#else
+static vm_fault_t kbase_csf_user_reg_vm_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+#endif
+	struct kbase_context *kctx = vma->vm_private_data;
+	struct kbase_device *kbdev = kctx->kbdev;
+	unsigned long pfn = PFN_DOWN(kbdev->reg_start + USER_BASE);
+	size_t nr_pages = PFN_DOWN(vma->vm_end - vma->vm_start);
+
+	/* Few sanity checks up front */
+	if (WARN_ON(nr_pages != 1) ||
+	    WARN_ON(vma != kctx->csf.user_reg_vma) ||
+	    WARN_ON(vma->vm_pgoff !=
+			PFN_DOWN(BASEP_MEM_CSF_USER_REG_PAGE_HANDLE)))
+		return VM_FAULT_SIGBUS;
+
+	/* TODO: check PM state here and don't map in the actual register page
+	 * if GPU is powered down or is about to be powered down.
+	 */
+
+	return vmf_insert_pfn_prot(vma, vma->vm_start, pfn, vma->vm_page_prot);
+}
+
+static const struct vm_operations_struct kbase_csf_user_reg_vm_ops = {
+	.close = kbase_csf_user_reg_vm_close,
+	.fault = kbase_csf_user_reg_vm_fault
+};
+
+static int kbase_csf_cpu_mmap_user_reg_page(struct kbase_context *kctx,
+				struct vm_area_struct *vma)
+{
+	size_t nr_pages = PFN_DOWN(vma->vm_end - vma->vm_start);
+
+	/* Few sanity checks */
+	if (kctx->csf.user_reg_vma)
+		return -EBUSY;
+
+	if (nr_pages != 1)
+		return -EINVAL;
+
+	if (vma->vm_flags & (VM_WRITE | VM_MAYWRITE))
+		return -EPERM;
+
+	/* Map uncached */
+	vma->vm_page_prot = pgprot_device(vma->vm_page_prot);
+
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO;
+
+	/* User register page comes from the device register area so
+	 * "struct page" isn't available for it.
+	 */
+	vma->vm_flags |= VM_PFNMAP;
+
+	kctx->csf.user_reg_vma = vma;
+
+	vma->vm_ops = &kbase_csf_user_reg_vm_ops;
+	vma->vm_private_data = kctx;
+
+	return 0;
+}
+
+#endif /* MALI_USE_CSF */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h
index cd094b3d10bf..85e030ab751a 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_linux.h
@@ -194,8 +194,8 @@ int kbase_mem_grow_gpu_mapping(struct kbase_context *kctx,
  * Take the provided region and make all the physical pages within it
  * reclaimable by the kernel, updating the per-process VM stats as well.
  * Remove any CPU mappings (as these can't be removed in the shrinker callback
- * as mmap_sem might already be taken) but leave the GPU mapping intact as
- * and until the shrinker reclaims the allocation.
+ * as mmap_sem/mmap_lock might already be taken) but leave the GPU mapping
+ * intact as and until the shrinker reclaims the allocation.
  *
  * Note: Must be called with the region lock of the containing context.
  */
@@ -461,4 +461,18 @@ static inline vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma,
 }
 #endif
 
+/**
+ * kbase_mem_get_process_mmap_lock - Return the mmap lock for the current process
+ *
+ * Return: the mmap lock for the current process
+ */
+static inline struct rw_semaphore *kbase_mem_get_process_mmap_lock(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0)
+	return &current->mm->mmap_sem;
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(5, 8, 0) */
+	return &current->mm->mmap_lock;
+#endif /* LINUX_VERSION_CODE < KERNEL_VERSION(5, 8, 0) */
+}
+
 #endif				/* _KBASE_MEM_LINUX_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.c b/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.c
index 5d38ed241211..85723f825054 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mem_profile_debugfs.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2017, 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2017, 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -68,6 +68,11 @@ static const struct file_operations kbasep_mem_profile_debugfs_fops = {
 int kbasep_mem_profile_debugfs_insert(struct kbase_context *kctx, char *data,
 					size_t size)
 {
+#if (KERNEL_VERSION(4, 7, 0) <= LINUX_VERSION_CODE)
+	const mode_t mode = 0444;
+#else
+	const mode_t mode = 0400;
+#endif
 	int err = 0;
 
 	mutex_lock(&kctx->mem_profile_lock);
@@ -78,7 +83,7 @@ int kbasep_mem_profile_debugfs_insert(struct kbase_context *kctx, char *data,
 	if (!kbase_ctx_flag(kctx, KCTX_MEM_PROFILE_INITIALIZED)) {
 		if (IS_ERR_OR_NULL(kctx->kctx_dentry)) {
 			err  = -ENOMEM;
-		} else if (!debugfs_create_file("mem_profile", 0444,
+		} else if (!debugfs_create_file("mem_profile", mode,
 					kctx->kctx_dentry, kctx,
 					&kbasep_mem_profile_debugfs_fops)) {
 			err = -EAGAIN;
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_mipe_gen_header.h b/drivers/gpu/arm/bifrost/mali_kbase_mipe_gen_header.h
index ec5212275751..72acadfae993 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_mipe_gen_header.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_mipe_gen_header.h
@@ -80,6 +80,7 @@
  * The list of tracepoints to process.
  *
  * It should be defined as follows:
+ *
  * #define MIPE_HEADER_TRACEPOINT_LIST \
  *     TRACEPOINT_DESC(FIRST_TRACEPOINT, "Some description", "@II", "first_arg,second_arg") \
  *     TRACEPOINT_DESC(SECOND_TRACEPOINT, "Some description", "@II", "first_arg,second_arg") \
@@ -105,6 +106,7 @@
  * The list of enums to process.
  *
  * It should be defined as follows:
+ *
  * #define MIPE_HEADER_ENUM_LIST \
  *     ENUM_DESC(enum_arg_name, enum_value) \
  *     ENUM_DESC(enum_arg_name, enum_value) \
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pm.c b/drivers/gpu/arm/bifrost/mali_kbase_pm.c
index b9ed8c31033d..630ab1550045 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pm.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pm.c
@@ -39,6 +39,8 @@
 #include <arbiter/mali_kbase_arbiter_pm.h>
 #endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
+#include <mali_kbase_clk_rate_trace_mgr.h>
+
 int kbase_pm_powerup(struct kbase_device *kbdev, unsigned int flags)
 {
 	return kbase_hwaccess_pm_powerup(kbdev, flags);
@@ -66,14 +68,14 @@ int kbase_pm_context_active_handle_suspend(struct kbase_device *kbdev,
 	kbase_pm_lock(kbdev);
 
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
-	if (kbase_arbiter_pm_ctx_active_handle_suspend(kbdev, suspend_handler))
+	if (kbase_arbiter_pm_ctx_active_handle_suspend(kbdev,
+			suspend_handler)) {
+		kbase_pm_unlock(kbdev);
 		return 1;
+	}
+#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 
-	if (kbase_pm_is_suspending(kbdev) ||
-		kbase_pm_is_gpu_lost(kbdev)) {
-#else
 	if (kbase_pm_is_suspending(kbdev)) {
-#endif /* CONFIG_MALI_ARBITER_SUPPORT */
 		switch (suspend_handler) {
 		case KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE:
 			if (kbdev->pm.active_count != 0)
@@ -101,6 +103,7 @@ int kbase_pm_context_active_handle_suspend(struct kbase_device *kbdev,
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
 		kbase_arbiter_pm_vm_event(kbdev, KBASE_VM_REF_EVENT);
 #endif /* CONFIG_MALI_ARBITER_SUPPORT */
+		kbase_clk_rate_trace_manager_gpu_active(kbdev);
 	}
 
 	kbase_pm_unlock(kbdev);
@@ -128,6 +131,7 @@ void kbase_pm_context_idle(struct kbase_device *kbdev)
 	if (c == 0) {
 		/* Last context has gone idle */
 		kbase_hwaccess_pm_gpu_idle(kbdev);
+		kbase_clk_rate_trace_manager_gpu_idle(kbdev);
 
 		/* Wake up anyone waiting for this to become 0 (e.g. suspend).
 		 * The waiters must synchronize with us by locking the pm.lock
@@ -171,6 +175,7 @@ void kbase_pm_driver_suspend(struct kbase_device *kbdev)
 		unsigned long flags;
 
 		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbdev->js_data.runpool_irq.submit_allowed = 0;
 		kbase_disjoint_state_up(kbdev);
 		for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
 			kbase_job_slot_softstop(kbdev, i, NULL);
@@ -184,9 +189,13 @@ void kbase_pm_driver_suspend(struct kbase_device *kbdev)
 	 * all pm references
 	 */
 
+#if !MALI_USE_CSF
 	/* Suspend job scheduler and associated components, so that it releases all
 	 * the PM active count references */
 	kbasep_js_suspend(kbdev);
+#else
+	kbase_csf_scheduler_pm_suspend(kbdev);
+#endif
 
 	/* Wait for the active count to reach zero. This is not the same as
 	 * waiting for a power down, since not all policies power down when this
@@ -221,14 +230,16 @@ void kbase_pm_driver_resume(struct kbase_device *kbdev, bool arb_gpu_start)
 
 	/* Initial active call, to power on the GPU/cores if needed */
 #ifdef CONFIG_MALI_ARBITER_SUPPORT
-	(void)kbase_pm_context_active_handle_suspend(kbdev,
-		(arb_gpu_start ?
-			KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED :
-			KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE));
+	if (kbase_pm_context_active_handle_suspend(kbdev,
+			(arb_gpu_start ?
+				KBASE_PM_SUSPEND_HANDLER_VM_GPU_GRANTED :
+				KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE)))
+		return;
 #else
 	kbase_pm_context_active(kbdev);
 #endif
 
+#if !MALI_USE_CSF
 	/* Resume any blocked atoms (which may cause contexts to be scheduled in
 	 * and dependent atoms to run)
 	 */
@@ -238,6 +249,9 @@ void kbase_pm_driver_resume(struct kbase_device *kbdev, bool arb_gpu_start)
 	 * atoms
 	 */
 	kbasep_js_resume(kbdev);
+#else
+	kbase_csf_scheduler_pm_resume(kbdev);
+#endif
 
 	/* Matching idle call, to power off the GPU/cores if we didn't actually
 	 * need it and the policy doesn't want it on
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_pm.h b/drivers/gpu/arm/bifrost/mali_kbase_pm.h
index 257f959cc5a4..13565186c11f 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_pm.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_pm.h
@@ -35,6 +35,13 @@
 #define PM_ENABLE_IRQS       0x01
 #define PM_HW_ISSUES_DETECT  0x02
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+/* In the case that the GPU was granted by the Arbiter, it will have
+ * already been reset. The following flag ensures it is not reset
+ * twice.
+ */
+#define PM_NO_RESET          0x04
+#endif
 
 /** Initialize the power management framework.
  *
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c b/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c
index 8a349040d714..7b86c58440db 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2016, 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014, 2016, 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -21,13 +21,127 @@
  */
 
 #include "mali_kbase.h"
-
 #include "mali_kbase_regs_history_debugfs.h"
 
 #if defined(CONFIG_DEBUG_FS) && !defined(CONFIG_MALI_BIFROST_NO_MALI)
 
 #include <linux/debugfs.h>
 
+/**
+ * kbase_io_history_resize - resize the register access history buffer.
+ *
+ * @h: Pointer to a valid register history to resize
+ * @new_size: Number of accesses the buffer could hold
+ *
+ * A successful resize will clear all recent register accesses.
+ * If resizing fails for any reason (e.g., could not allocate memory, invalid
+ * buffer size) then the original buffer will be kept intact.
+ *
+ * @return 0 if the buffer was resized, failure otherwise
+ */
+static int kbase_io_history_resize(struct kbase_io_history *h, u16 new_size)
+{
+	struct kbase_io_access *old_buf;
+	struct kbase_io_access *new_buf;
+	unsigned long flags;
+
+	if (!new_size)
+		goto out_err; /* The new size must not be 0 */
+
+	new_buf = vmalloc(new_size * sizeof(*h->buf));
+	if (!new_buf)
+		goto out_err;
+
+	spin_lock_irqsave(&h->lock, flags);
+
+	old_buf = h->buf;
+
+	/* Note: we won't bother with copying the old data over. The dumping
+	 * logic wouldn't work properly as it relies on 'count' both as a
+	 * counter and as an index to the buffer which would have changed with
+	 * the new array. This is a corner case that we don't need to support.
+	 */
+	h->count = 0;
+	h->size = new_size;
+	h->buf = new_buf;
+
+	spin_unlock_irqrestore(&h->lock, flags);
+
+	vfree(old_buf);
+
+	return 0;
+
+out_err:
+	return -1;
+}
+
+int kbase_io_history_init(struct kbase_io_history *h, u16 n)
+{
+	h->enabled = false;
+	spin_lock_init(&h->lock);
+	h->count = 0;
+	h->size = 0;
+	h->buf = NULL;
+	if (kbase_io_history_resize(h, n))
+		return -1;
+
+	return 0;
+}
+
+void kbase_io_history_term(struct kbase_io_history *h)
+{
+	vfree(h->buf);
+	h->buf = NULL;
+}
+
+void kbase_io_history_add(struct kbase_io_history *h,
+		void __iomem const *addr, u32 value, u8 write)
+{
+	struct kbase_io_access *io;
+	unsigned long flags;
+
+	spin_lock_irqsave(&h->lock, flags);
+
+	io = &h->buf[h->count % h->size];
+	io->addr = (uintptr_t)addr | write;
+	io->value = value;
+	++h->count;
+	/* If count overflows, move the index by the buffer size so the entire
+	 * buffer will still be dumped later
+	 */
+	if (unlikely(!h->count))
+		h->count = h->size;
+
+	spin_unlock_irqrestore(&h->lock, flags);
+}
+
+void kbase_io_history_dump(struct kbase_device *kbdev)
+{
+	struct kbase_io_history *const h = &kbdev->io_history;
+	u16 i;
+	size_t iters;
+	unsigned long flags;
+
+	if (!unlikely(h->enabled))
+		return;
+
+	spin_lock_irqsave(&h->lock, flags);
+
+	dev_err(kbdev->dev, "Register IO History:");
+	iters = (h->size > h->count) ? h->count : h->size;
+	dev_err(kbdev->dev, "Last %zu register accesses of %zu total:\n", iters,
+			h->count);
+	for (i = 0; i < iters; ++i) {
+		struct kbase_io_access *io =
+			&h->buf[(h->count - iters + i) % h->size];
+		char const access = (io->addr & 1) ? 'w' : 'r';
+
+		dev_err(kbdev->dev, "%6i: %c: reg 0x%016lx val %08x\n", i,
+			access, (unsigned long)(io->addr & ~0x1), io->value);
+	}
+
+	spin_unlock_irqrestore(&h->lock, flags);
+}
 
 static int regs_history_size_get(void *data, u64 *val)
 {
@@ -95,7 +209,6 @@ static int regs_history_show(struct seq_file *sfile, void *data)
 	return 0;
 }
 
-
 /**
  * regs_history_open - open operation for regs_history debugfs file
  *
@@ -109,7 +222,6 @@ static int regs_history_open(struct inode *in, struct file *file)
 	return single_open(file, &regs_history_show, in->i_private);
 }
 
-
 static const struct file_operations regs_history_fops = {
 	.owner = THIS_MODULE,
 	.open = &regs_history_open,
@@ -118,7 +230,6 @@ static const struct file_operations regs_history_fops = {
 	.release = single_release,
 };
 
-
 void kbasep_regs_history_debugfs_init(struct kbase_device *kbdev)
 {
 	debugfs_create_bool("regs_history_enabled", S_IRUGO | S_IWUSR,
@@ -131,6 +242,4 @@ void kbasep_regs_history_debugfs_init(struct kbase_device *kbdev)
 			kbdev->mali_debugfs_directory, &kbdev->io_history,
 			&regs_history_fops);
 }
-
-
-#endif /* CONFIG_DEBUG_FS */
+#endif /* defined(CONFIG_DEBUG_FS) && !defined(CONFIG_MALI_BIFROST_NO_MALI) */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.h b/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.h
index 8cc11010a92b..200c0c2d8de8 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_regs_history_debugfs.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2016 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014, 2016, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -39,6 +39,30 @@ struct kbase_device;
 
 #if defined(CONFIG_DEBUG_FS) && !defined(CONFIG_MALI_BIFROST_NO_MALI)
 
+/**
+ * kbase_io_history_init - initialize data struct for register access history
+ *
+ * @h: The register history to initialize
+ * @n: The number of register accesses that the buffer could hold
+ *
+ * @return 0 if successfully initialized, failure otherwise
+ */
+int kbase_io_history_init(struct kbase_io_history *h, u16 n);
+
+/**
+ * kbase_io_history_term - uninit all resources for the register access history
+ *
+ * @h: The register history to terminate
+ */
+void kbase_io_history_term(struct kbase_io_history *h);
+
+/**
+ * kbase_io_history_dump - print the register history to the kernel ring buffer
+ *
+ * @kbdev: Pointer to kbase_device containing the register history to dump
+ */
+void kbase_io_history_dump(struct kbase_device *kbdev);
+
 /**
  * kbasep_regs_history_debugfs_init - add debugfs entries for register history
  *
@@ -46,10 +70,16 @@ struct kbase_device;
  */
 void kbasep_regs_history_debugfs_init(struct kbase_device *kbdev);
 
-#else /* CONFIG_DEBUG_FS */
+#else /* defined(CONFIG_DEBUG_FS) && !defined(CONFIG_MALI_BIFROST_NO_MALI) */
+
+#define kbase_io_history_init(...) ((int)0)
+
+#define kbase_io_history_term CSTD_NOP
+
+#define kbase_io_history_dump CSTD_NOP
 
 #define kbasep_regs_history_debugfs_init CSTD_NOP
 
-#endif /* CONFIG_DEBUG_FS */
+#endif /* defined(CONFIG_DEBUG_FS) && !defined(CONFIG_MALI_BIFROST_NO_MALI) */
 
 #endif  /*_KBASE_REGS_HISTORY_DEBUGFS_H*/
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h b/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h
index df72eecc5e0f..61bbb0b48490 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_reset_gpu.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -126,14 +126,4 @@ int kbase_reset_gpu_init(struct kbase_device *kbdev);
  */
 void kbase_reset_gpu_term(struct kbase_device *kbdev);
 
-/**
- * kbase_reset_gpu_register_complete_cb - Register the callback function to be
- *                                        invoked on completion of GPU reset.
- *
- * @kbdev: Device pointer
- * @complete_callback: Pointer to the callback function
- */
-void kbase_reset_gpu_register_complete_cb(struct kbase_device *kbdev,
-			int (*complete_callback)(struct kbase_device *kbdev));
-
 #endif
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c b/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c
index 40e80ae656c1..c164719b3d7b 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_softjobs.c
@@ -32,6 +32,7 @@
 #include <linux/dma-mapping.h>
 #include <mali_base_kernel.h>
 #include <mali_kbase_hwaccess_time.h>
+#include <mali_kbase_kinstr_jm.h>
 #include <mali_kbase_mem_linux.h>
 #include <tl/mali_kbase_tracepoints.h>
 #include <mali_linux_trace.h>
@@ -42,6 +43,7 @@
 #include <linux/kernel.h>
 #include <linux/cache.h>
 
+#if !MALI_USE_CSF
 /**
  * @file mali_kbase_softjobs.c
  *
@@ -719,6 +721,7 @@ static int kbase_debug_copy_prepare(struct kbase_jd_atom *katom)
 
 	return ret;
 }
+#endif /* !MALI_USE_CSF */
 
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 6, 0)
 static void *dma_buf_kmap_page(struct kbase_mem_phy_alloc *gpu_alloc,
@@ -846,6 +849,7 @@ int kbase_mem_copy_from_extres(struct kbase_context *kctx,
 	return ret;
 }
 
+#if !MALI_USE_CSF
 static int kbase_debug_copy(struct kbase_jd_atom *katom)
 {
 	struct kbase_debug_copy_buffer *buffers = katom->softjob_data;
@@ -863,6 +867,7 @@ static int kbase_debug_copy(struct kbase_jd_atom *katom)
 
 	return 0;
 }
+#endif /* !MALI_USE_CSF */
 
 #define KBASEP_JIT_ALLOC_GPU_ADDR_ALIGNMENT ((u32)0x7)
 
@@ -899,7 +904,7 @@ int kbasep_jit_alloc_validate(struct kbase_context *kctx,
 	if (info->flags & ~(BASE_JIT_ALLOC_VALID_FLAGS))
 		return -EINVAL;
 
-#if !MALI_JIT_PRESSURE_LIMIT
+#if !MALI_JIT_PRESSURE_LIMIT_BASE
 	/* If just-in-time memory allocation pressure limit feature is disabled,
 	 * heap_info_gpu_addr must be zeroed-out
 	 */
@@ -907,16 +912,19 @@ int kbasep_jit_alloc_validate(struct kbase_context *kctx,
 		return -EINVAL;
 #endif
 
+#if !MALI_USE_CSF
 	/* If BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE is set, heap_info_gpu_addr
 	 * cannot be 0
 	 */
 	if ((info->flags & BASE_JIT_ALLOC_HEAP_INFO_IS_SIZE) &&
 			!info->heap_info_gpu_addr)
 		return -EINVAL;
+#endif /* !MALI_USE_CSF */
 
 	return 0;
 }
 
+#if !MALI_USE_CSF
 
 #if (KERNEL_VERSION(3, 18, 63) > LINUX_VERSION_CODE)
 #define offsetofend(TYPE, MEMBER) \
@@ -1091,14 +1099,19 @@ static int kbase_jit_allocate_process(struct kbase_jd_atom *katom)
 		}
 	}
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 	/**
-	 * If this is the only JIT_ALLOC atom in-flight then allow it to exceed
-	 * the defined pressure limit.
+	 * If this is the only JIT_ALLOC atom in-flight or if JIT pressure limit
+	 * is disabled at the context scope, then bypass JIT pressure limit
+	 * logic in kbase_jit_allocate().
 	 */
-	if (kctx->jit_current_allocations == 0)
+	if (!kbase_ctx_flag(kctx, KCTX_JPL_ENABLED)
+		|| (kctx->jit_current_allocations == 0)) {
 		ignore_pressure_limit = true;
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+	}
+#else
+	ignore_pressure_limit = true;
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 	for (i = 0, info = katom->softjob_data; i < count; i++, info++) {
 		if (kctx->jit_alloc[info->id]) {
@@ -1358,12 +1371,16 @@ void kbase_jit_retry_pending_alloc(struct kbase_context *kctx)
 	list_for_each_safe(i, tmp, &jit_pending_alloc_list) {
 		struct kbase_jd_atom *pending_atom = list_entry(i,
 				struct kbase_jd_atom, queue);
+		KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTJOB_START(kctx->kbdev, pending_atom);
+		kbase_kinstr_jm_atom_sw_start(pending_atom);
 		if (kbase_jit_allocate_process(pending_atom) == 0) {
 			/* Atom has completed */
 			INIT_WORK(&pending_atom->work,
 					kbasep_jit_finish_worker);
 			queue_work(kctx->jctx.job_done_wq, &pending_atom->work);
 		}
+		KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTJOB_END(kctx->kbdev, pending_atom);
+		kbase_kinstr_jm_atom_sw_stop(pending_atom);
 	}
 }
 
@@ -1538,6 +1555,7 @@ int kbase_process_soft_job(struct kbase_jd_atom *katom)
 	struct kbase_device *kbdev = kctx->kbdev;
 
 	KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTJOB_START(kbdev, katom);
+	kbase_kinstr_jm_atom_sw_start(katom);
 
 	trace_sysgraph(SGR_SUBMIT, kctx->id,
 			kbase_jd_atom_id(kctx, katom));
@@ -1600,6 +1618,7 @@ int kbase_process_soft_job(struct kbase_jd_atom *katom)
 
 	/* Atom is complete */
 	KBASE_TLSTREAM_TL_EVENT_ATOM_SOFTJOB_END(kbdev, katom);
+	kbase_kinstr_jm_atom_sw_stop(katom);
 	return ret;
 }
 
@@ -1783,3 +1802,4 @@ void kbase_resume_suspended_soft_jobs(struct kbase_device *kbdev)
 	if (resched)
 		kbase_js_sched_all(kbdev);
 }
+#endif /* !MALI_USE_CSF */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_sync.h b/drivers/gpu/arm/bifrost/mali_kbase_sync.h
index 37990c25cd91..4e5ab3ca557a 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_sync.h
+++ b/drivers/gpu/arm/bifrost/mali_kbase_sync.h
@@ -72,6 +72,7 @@ struct kbase_sync_fence_info {
  */
 int kbase_sync_fence_stream_create(const char *name, int *const out_fd);
 
+#if !MALI_USE_CSF
 /**
  * kbase_sync_fence_out_create Create an explicit output fence to specified atom
  * @katom: Atom to assign the new explicit fence to
@@ -92,6 +93,7 @@ int kbase_sync_fence_out_create(struct kbase_jd_atom *katom, int stream_fd);
  * return: 0 on success, < 0 on error
  */
 int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd);
+#endif /* !MALI_USE_CSF */
 
 /**
  * kbase_sync_fence_validate() - Validate a fd to be a valid fence
@@ -104,6 +106,7 @@ int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd);
  */
 int kbase_sync_fence_validate(int fd);
 
+#if !MALI_USE_CSF
 /**
  * kbase_sync_fence_out_trigger - Signal explicit output fence attached on katom
  * @katom: Atom with an explicit fence to signal
@@ -154,6 +157,7 @@ void kbase_sync_fence_in_remove(struct kbase_jd_atom *katom);
  * This will also release the corresponding reference.
  */
 void kbase_sync_fence_out_remove(struct kbase_jd_atom *katom);
+#endif /* !MALI_USE_CSF */
 
 /**
  * kbase_sync_fence_close_fd() - Close a file descriptor representing a fence
@@ -168,6 +172,7 @@ static inline void kbase_sync_fence_close_fd(int fd)
 #endif
 }
 
+#if !MALI_USE_CSF
 /**
  * kbase_sync_fence_in_info_get() - Retrieves information about input fence
  * @katom: Atom to get fence information from
@@ -187,6 +192,7 @@ int kbase_sync_fence_in_info_get(struct kbase_jd_atom *katom,
  */
 int kbase_sync_fence_out_info_get(struct kbase_jd_atom *katom,
 				  struct kbase_sync_fence_info *info);
+#endif /* !MALI_USE_CSF */
 
 #if defined(CONFIG_SYNC_FILE)
 #if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
@@ -207,6 +213,7 @@ void kbase_sync_fence_info_get(struct dma_fence *fence,
 const char *kbase_sync_status_string(int status);
 
 
+#if !MALI_USE_CSF
 /*
  * Internal worker used to continue processing of atom.
  */
@@ -219,5 +226,6 @@ void kbase_sync_fence_wait_worker(struct work_struct *data);
  */
 void kbase_sync_fence_in_dump(struct kbase_jd_atom *katom);
 #endif
+#endif /* !MALI_USE_CSF */
 
 #endif /* MALI_KBASE_SYNC_H */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_sync_common.c b/drivers/gpu/arm/bifrost/mali_kbase_sync_common.c
index 2e1ede5bdb70..866894bd0f94 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_sync_common.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_sync_common.c
@@ -30,6 +30,7 @@
 #include "mali_kbase.h"
 #include "mali_kbase_sync.h"
 
+#if !MALI_USE_CSF
 void kbase_sync_fence_wait_worker(struct work_struct *data)
 {
 	struct kbase_jd_atom *katom;
@@ -37,6 +38,7 @@ void kbase_sync_fence_wait_worker(struct work_struct *data)
 	katom = container_of(data, struct kbase_jd_atom, work);
 	kbase_soft_event_wait_callback(katom);
 }
+#endif /* !MALI_USE_CSF */
 
 const char *kbase_sync_status_string(int status)
 {
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c b/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c
index 19d627d42e08..271873b9fe29 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_sync_file.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2012-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2012-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -59,6 +59,7 @@ int kbase_sync_fence_stream_create(const char *name, int *const out_fd)
 	return 0;
 }
 
+#if !MALI_USE_CSF
 int kbase_sync_fence_out_create(struct kbase_jd_atom *katom, int stream_fd)
 {
 #if (LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0))
@@ -119,6 +120,7 @@ int kbase_sync_fence_in_from_fd(struct kbase_jd_atom *katom, int fd)
 
 	return 0;
 }
+#endif /* !MALI_USE_CSF */
 
 int kbase_sync_fence_validate(int fd)
 {
@@ -136,6 +138,7 @@ int kbase_sync_fence_validate(int fd)
 	return 0; /* valid */
 }
 
+#if !MALI_USE_CSF
 enum base_jd_event_code
 kbase_sync_fence_out_trigger(struct kbase_jd_atom *katom, int result)
 {
@@ -175,7 +178,7 @@ static void kbase_fence_wait_callback(struct dma_fence *fence,
 #if (KERNEL_VERSION(4, 11, 0) <= LINUX_VERSION_CODE || \
 	 (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE && \
 	  KERNEL_VERSION(4, 9, 68) <= LINUX_VERSION_CODE))
-	if (dma_fence_is_signaled(kcb->fence) && kcb->fence->error)
+	if (dma_fence_is_signaled(kcb->fence) && kcb->fence->error < 0)
 #else
 	if (dma_fence_is_signaled(kcb->fence) && kcb->fence->status < 0)
 #endif
@@ -273,6 +276,7 @@ void kbase_sync_fence_in_remove(struct kbase_jd_atom *katom)
 	kbase_fence_free_callbacks(katom);
 	kbase_fence_in_remove(katom);
 }
+#endif /* !MALI_USE_CSF */
 
 #if (KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE)
 void kbase_sync_fence_info_get(struct fence *fence,
@@ -317,6 +321,7 @@ void kbase_sync_fence_info_get(struct dma_fence *fence,
 #endif
 }
 
+#if !MALI_USE_CSF
 int kbase_sync_fence_in_info_get(struct kbase_jd_atom *katom,
 				 struct kbase_sync_fence_info *info)
 {
@@ -364,3 +369,4 @@ void kbase_sync_fence_in_dump(struct kbase_jd_atom *katom)
 	/* Not implemented */
 }
 #endif
+#endif /* !MALI_USE_CSF*/
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.c b/drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.c
new file mode 100644
index 000000000000..7669895b3c5d
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.c
@@ -0,0 +1,227 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_mem_linux.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_trace_gpu_mem.h>
+
+/**
+ * struct kbase_dma_buf - Object instantiated when a dma-buf imported allocation
+ *                        is mapped to GPU for the first time within a process.
+ *                        Another instantiation is done for the case when that
+ *                        allocation is mapped for the first time to GPU.
+ *
+ * @dma_buf:              Reference to dma_buf been imported.
+ * @dma_buf_node:         Link node to maintain a rb_tree of kbase_dma_buf.
+ * @import_count:         The number of times the dma_buf was imported.
+ */
+struct kbase_dma_buf {
+	struct dma_buf *dma_buf;
+	struct rb_node dma_buf_node;
+	u32 import_count;
+};
+
+/**
+ * kbase_delete_dma_buf_mapping - Delete a dma buffer mapping.
+ *
+ * @kctx: Pointer to kbase context.
+ * @dma_buf: Pointer to a dma buffer mapping.
+ * @tree: Pointer to root of rb_tree containing the dma_buf's mapped.
+ *
+ * when we un-map any dma mapping we need to remove them from rb_tree,
+ * rb_tree is maintained at kbase_device level and kbase_process level
+ * by passing the root of kbase_device or kbase_process we can remove
+ * the node from the tree.
+ */
+static bool kbase_delete_dma_buf_mapping(struct kbase_context *kctx,
+					 struct dma_buf *dma_buf,
+					 struct rb_root *tree)
+{
+	struct kbase_dma_buf *buf_node = NULL;
+	struct rb_node *node = tree->rb_node;
+	bool mapping_removed = false;
+
+	lockdep_assert_held(&kctx->kbdev->dma_buf_lock);
+
+	while (node) {
+		buf_node = rb_entry(node, struct kbase_dma_buf, dma_buf_node);
+
+		if (dma_buf == buf_node->dma_buf) {
+			WARN_ON(!buf_node->import_count);
+
+			buf_node->import_count--;
+
+			if (!buf_node->import_count) {
+				rb_erase(&buf_node->dma_buf_node, tree);
+				kfree(buf_node);
+				mapping_removed = true;
+			}
+
+			break;
+		}
+
+		if (dma_buf < buf_node->dma_buf)
+			node = node->rb_left;
+		else
+			node = node->rb_right;
+	}
+
+	WARN_ON(!buf_node);
+	return mapping_removed;
+}
+
+/**
+ * kbase_capture_dma_buf_mapping - capture a dma buffer mapping.
+ *
+ * @kctx: Pointer to kbase context.
+ * @dma_buf: Pointer to a dma buffer mapping.
+ * @root: Pointer to root of rb_tree containing the dma_buf's.
+ *
+ * We maintain a kbase_device level and kbase_process level rb_tree
+ * of all unique dma_buf's mapped to gpu memory. So when attach any
+ * dma_buf add it the rb_tree's. To add the unique mapping we need
+ * check if the mapping is not a duplicate and then add them.
+ */
+static bool kbase_capture_dma_buf_mapping(struct kbase_context *kctx,
+					  struct dma_buf *dma_buf,
+					  struct rb_root *root)
+{
+	struct kbase_dma_buf *buf_node = NULL;
+	struct rb_node *node = root->rb_node;
+	bool unique_buf_imported = true;
+
+	lockdep_assert_held(&kctx->kbdev->dma_buf_lock);
+
+	while (node) {
+		buf_node = rb_entry(node, struct kbase_dma_buf, dma_buf_node);
+
+		if (dma_buf == buf_node->dma_buf) {
+			unique_buf_imported = false;
+			break;
+		}
+
+		if (dma_buf < buf_node->dma_buf)
+			node = node->rb_left;
+		else
+			node = node->rb_right;
+	}
+
+	if (unique_buf_imported) {
+		struct kbase_dma_buf *buf_node =
+			kzalloc(sizeof(*buf_node), GFP_KERNEL);
+
+		if (buf_node == NULL) {
+			dev_err(kctx->kbdev->dev, "Error allocating memory for kbase_dma_buf\n");
+			/* Dont account for it if we fail to allocate memory */
+			unique_buf_imported = false;
+		} else {
+			struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+			buf_node->dma_buf = dma_buf;
+			buf_node->import_count = 1;
+			while (*new) {
+				struct kbase_dma_buf *node;
+
+				parent = *new;
+				node = rb_entry(parent, struct kbase_dma_buf,
+						dma_buf_node);
+				if (dma_buf < node->dma_buf)
+					new = &(*new)->rb_left;
+				else
+					new = &(*new)->rb_right;
+			}
+			rb_link_node(&buf_node->dma_buf_node, parent, new);
+			rb_insert_color(&buf_node->dma_buf_node, root);
+		}
+	} else if (!WARN_ON(!buf_node)) {
+		buf_node->import_count++;
+	}
+
+	return unique_buf_imported;
+}
+
+void kbase_remove_dma_buf_usage(struct kbase_context *kctx,
+				struct kbase_mem_phy_alloc *alloc)
+{
+	struct kbase_device *kbdev = kctx->kbdev;
+	bool dev_mapping_removed, prcs_mapping_removed;
+
+	mutex_lock(&kbdev->dma_buf_lock);
+
+	dev_mapping_removed = kbase_delete_dma_buf_mapping(
+		kctx, alloc->imported.umm.dma_buf, &kbdev->dma_buf_root);
+
+	prcs_mapping_removed = kbase_delete_dma_buf_mapping(
+		kctx, alloc->imported.umm.dma_buf, &kctx->kprcs->dma_buf_root);
+
+	WARN_ON(dev_mapping_removed && !prcs_mapping_removed);
+
+	spin_lock(&kbdev->gpu_mem_usage_lock);
+	if (dev_mapping_removed)
+		kbdev->total_gpu_pages -= alloc->nents;
+
+	if (prcs_mapping_removed)
+		kctx->kprcs->total_gpu_pages -= alloc->nents;
+
+	if (dev_mapping_removed || prcs_mapping_removed)
+		kbase_trace_gpu_mem_usage(kbdev, kctx);
+	spin_unlock(&kbdev->gpu_mem_usage_lock);
+
+	mutex_unlock(&kbdev->dma_buf_lock);
+}
+
+void kbase_add_dma_buf_usage(struct kbase_context *kctx,
+				    struct kbase_mem_phy_alloc *alloc)
+{
+	struct kbase_device *kbdev = kctx->kbdev;
+	bool unique_dev_dmabuf, unique_prcs_dmabuf;
+
+	mutex_lock(&kbdev->dma_buf_lock);
+
+	/* add dma_buf to device and process. */
+	unique_dev_dmabuf = kbase_capture_dma_buf_mapping(
+		kctx, alloc->imported.umm.dma_buf, &kbdev->dma_buf_root);
+
+	unique_prcs_dmabuf = kbase_capture_dma_buf_mapping(
+		kctx, alloc->imported.umm.dma_buf, &kctx->kprcs->dma_buf_root);
+
+	WARN_ON(unique_dev_dmabuf && !unique_prcs_dmabuf);
+
+	spin_lock(&kbdev->gpu_mem_usage_lock);
+	if (unique_dev_dmabuf)
+		kbdev->total_gpu_pages += alloc->nents;
+
+	if (unique_prcs_dmabuf)
+		kctx->kprcs->total_gpu_pages += alloc->nents;
+
+	if (unique_prcs_dmabuf || unique_dev_dmabuf)
+		kbase_trace_gpu_mem_usage(kbdev, kctx);
+	spin_unlock(&kbdev->gpu_mem_usage_lock);
+
+	mutex_unlock(&kbdev->dma_buf_lock);
+}
+
+#if !defined(CONFIG_TRACE_GPU_MEM) && !MALI_CUSTOMER_RELEASE
+#define CREATE_TRACE_POINTS
+#include "mali_gpu_mem_trace.h"
+#endif
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.h b/drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.h
new file mode 100644
index 000000000000..7e95956f3132
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_kbase_trace_gpu_mem.h
@@ -0,0 +1,103 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KBASE_TRACE_GPU_MEM_H_
+#define _KBASE_TRACE_GPU_MEM_H_
+
+#ifdef CONFIG_TRACE_GPU_MEM
+#include <trace/events/gpu_mem.h>
+#elif !MALI_CUSTOMER_RELEASE
+#include "mali_gpu_mem_trace.h"
+#endif
+
+#define DEVICE_TGID ((u32) 0U)
+
+static void kbase_trace_gpu_mem_usage(struct kbase_device *kbdev,
+				      struct kbase_context *kctx)
+{
+	lockdep_assert_held(&kbdev->gpu_mem_usage_lock);
+
+#if defined(CONFIG_TRACE_GPU_MEM) || !MALI_CUSTOMER_RELEASE
+	trace_gpu_mem_total(kbdev->id, DEVICE_TGID,
+			    kbdev->total_gpu_pages << PAGE_SHIFT);
+
+	if (likely(kctx))
+		trace_gpu_mem_total(kbdev->id, kctx->kprcs->tgid,
+				kctx->kprcs->total_gpu_pages << PAGE_SHIFT);
+#endif
+}
+
+static inline void kbase_trace_gpu_mem_usage_dec(struct kbase_device *kbdev,
+				struct kbase_context *kctx, size_t pages)
+{
+	spin_lock(&kbdev->gpu_mem_usage_lock);
+
+	if (likely(kctx))
+		kctx->kprcs->total_gpu_pages -= pages;
+
+	kbdev->total_gpu_pages -= pages;
+
+	kbase_trace_gpu_mem_usage(kbdev, kctx);
+
+	spin_unlock(&kbdev->gpu_mem_usage_lock);
+}
+
+static inline void kbase_trace_gpu_mem_usage_inc(struct kbase_device *kbdev,
+				struct kbase_context *kctx, size_t pages)
+{
+	spin_lock(&kbdev->gpu_mem_usage_lock);
+
+	if (likely(kctx))
+		kctx->kprcs->total_gpu_pages += pages;
+
+	kbdev->total_gpu_pages += pages;
+
+	kbase_trace_gpu_mem_usage(kbdev, kctx);
+
+	spin_unlock(&kbdev->gpu_mem_usage_lock);
+}
+
+/**
+ * kbase_remove_dma_buf_usage - Remove a dma-buf entry captured.
+ *
+ * @kctx: Pointer to the kbase context
+ * @alloc: Pointer to the alloc to unmap
+ *
+ * Remove reference to dma buf been unmapped from kbase_device level
+ * rb_tree and Kbase_process level dma buf rb_tree.
+ */
+void kbase_remove_dma_buf_usage(struct kbase_context *kctx,
+				struct kbase_mem_phy_alloc *alloc);
+
+/**
+ * kbase_add_dma_buf_usage - Add a dma-buf entry captured.
+ *
+ * @kctx: Pointer to the kbase context
+ * @alloc: Pointer to the alloc to map in
+ *
+ * Add reference to dma buf been mapped to kbase_device level
+ * rb_tree and Kbase_process level dma buf rb_tree.
+ */
+void kbase_add_dma_buf_usage(struct kbase_context *kctx,
+				    struct kbase_mem_phy_alloc *alloc);
+
+#endif /* _KBASE_TRACE_GPU_MEM_H_ */
diff --git a/drivers/gpu/arm/bifrost/mali_kbase_vinstr.c b/drivers/gpu/arm/bifrost/mali_kbase_vinstr.c
index d96b565a966e..3b0e2d6855ce 100644
--- a/drivers/gpu/arm/bifrost/mali_kbase_vinstr.c
+++ b/drivers/gpu/arm/bifrost/mali_kbase_vinstr.c
@@ -83,6 +83,7 @@ struct kbase_vinstr_context {
  * @next_dump_time_ns: Time in ns when this client's next periodic dump must
  *                     occur. If 0, not a periodic client.
  * @enable_map:        Counters enable map.
+ * @tmp_buf:           Temporary buffer to use before handing dump to client.
  * @dump_bufs:         Array of dump buffers allocated by this client.
  * @dump_bufs_meta:    Metadata of dump buffers.
  * @meta_idx:          Index of metadata being accessed by userspace.
@@ -97,6 +98,7 @@ struct kbase_vinstr_client {
 	u64 next_dump_time_ns;
 	u32 dump_interval_ns;
 	struct kbase_hwcnt_enable_map enable_map;
+	struct kbase_hwcnt_dump_buffer tmp_buf;
 	struct kbase_hwcnt_dump_buffer_array dump_bufs;
 	struct kbase_hwcnt_reader_metadata *dump_bufs_meta;
 	atomic_t meta_idx;
@@ -182,8 +184,10 @@ static int kbasep_vinstr_client_dump(
 	u64 ts_end_ns;
 	unsigned int write_idx;
 	unsigned int read_idx;
+	struct kbase_hwcnt_dump_buffer *tmp_buf;
 	struct kbase_hwcnt_dump_buffer *dump_buf;
 	struct kbase_hwcnt_reader_metadata *meta;
+	u8 clk_cnt;
 
 	WARN_ON(!vcli);
 	lockdep_assert_held(&vcli->vctx->lock);
@@ -198,23 +202,33 @@ static int kbasep_vinstr_client_dump(
 
 	dump_buf = &vcli->dump_bufs.bufs[write_idx];
 	meta = &vcli->dump_bufs_meta[write_idx];
+	tmp_buf = &vcli->tmp_buf;
 
 	errcode = kbase_hwcnt_virtualizer_client_dump(
-		vcli->hvcli, &ts_start_ns, &ts_end_ns, dump_buf);
+		vcli->hvcli, &ts_start_ns, &ts_end_ns, tmp_buf);
 	if (errcode)
 		return errcode;
 
 	/* Patch the dump buf headers, to hide the counters that other hwcnt
 	 * clients are using.
 	 */
-	kbase_hwcnt_gpu_patch_dump_headers(dump_buf, &vcli->enable_map);
+	kbase_hwcnt_gpu_patch_dump_headers(tmp_buf, &vcli->enable_map);
 
-	/* Zero all non-enabled counters (current values are undefined) */
-	kbase_hwcnt_dump_buffer_zero_non_enabled(dump_buf, &vcli->enable_map);
+	/* Copy the temp buffer to the userspace visible buffer. The strict
+	 * variant will explicitly zero any non-enabled counters to ensure
+	 * nothing except exactly what the user asked for is made visible.
+	 */
+	kbase_hwcnt_dump_buffer_copy_strict(
+		dump_buf, tmp_buf, &vcli->enable_map);
+
+	clk_cnt = vcli->vctx->metadata->clk_cnt;
 
 	meta->timestamp = ts_end_ns;
 	meta->event_id = event_id;
 	meta->buffer_idx = write_idx;
+	meta->cycles.top = (clk_cnt > 0) ? dump_buf->clk_cnt_buf[0] : 0;
+	meta->cycles.shader_cores =
+	    (clk_cnt > 1) ? dump_buf->clk_cnt_buf[1] : 0;
 
 	/* Notify client. Make sure all changes to memory are visible. */
 	wmb();
@@ -365,6 +379,7 @@ static void kbasep_vinstr_client_destroy(struct kbase_vinstr_client *vcli)
 	kbase_hwcnt_virtualizer_client_destroy(vcli->hvcli);
 	kfree(vcli->dump_bufs_meta);
 	kbase_hwcnt_dump_buffer_array_free(&vcli->dump_bufs);
+	kbase_hwcnt_dump_buffer_free(&vcli->tmp_buf);
 	kbase_hwcnt_enable_map_free(&vcli->enable_map);
 	kfree(vcli);
 }
@@ -404,12 +419,19 @@ static int kbasep_vinstr_client_create(
 	if (errcode)
 		goto error;
 
-	phys_em.jm_bm = setup->jm_bm;
+	phys_em.fe_bm = setup->fe_bm;
 	phys_em.shader_bm = setup->shader_bm;
 	phys_em.tiler_bm = setup->tiler_bm;
 	phys_em.mmu_l2_bm = setup->mmu_l2_bm;
 	kbase_hwcnt_gpu_enable_map_from_physical(&vcli->enable_map, &phys_em);
 
+	errcode = kbase_hwcnt_dump_buffer_alloc(vctx->metadata, &vcli->tmp_buf);
+	if (errcode)
+		goto error;
+
+	/* Enable all the available clk_enable_map. */
+	vcli->enable_map.clk_enable_map = (1ull << vctx->metadata->clk_cnt) - 1;
+
 	errcode = kbase_hwcnt_dump_buffer_array_alloc(
 		vctx->metadata, setup->buffer_count, &vcli->dump_bufs);
 	if (errcode)
@@ -675,23 +697,26 @@ static long kbasep_vinstr_hwcnt_reader_ioctl_get_buffer(
 	unsigned int idx = meta_idx % cli->dump_bufs.buf_cnt;
 
 	struct kbase_hwcnt_reader_metadata *meta = &cli->dump_bufs_meta[idx];
+	const size_t meta_size = sizeof(struct kbase_hwcnt_reader_metadata);
+	const size_t min_size = min(size, meta_size);
 
 	/* Metadata sanity check. */
 	WARN_ON(idx != meta->buffer_idx);
 
-	if (sizeof(struct kbase_hwcnt_reader_metadata) != size)
-		return -EINVAL;
-
 	/* Check if there is any buffer available. */
-	if (atomic_read(&cli->write_idx) == meta_idx)
+	if (unlikely(atomic_read(&cli->write_idx) == meta_idx))
 		return -EAGAIN;
 
 	/* Check if previously taken buffer was put back. */
-	if (atomic_read(&cli->read_idx) != meta_idx)
+	if (unlikely(atomic_read(&cli->read_idx) != meta_idx))
 		return -EBUSY;
 
+	/* Clear user buffer to zero. */
+	if (unlikely(meta_size < size && clear_user(buffer, size)))
+		return -EFAULT;
+
 	/* Copy next available buffer's metadata to user. */
-	if (copy_to_user(buffer, meta, size))
+	if (unlikely(copy_to_user(buffer, meta, min_size)))
 		return -EFAULT;
 
 	atomic_inc(&cli->meta_idx);
@@ -715,24 +740,62 @@ static long kbasep_vinstr_hwcnt_reader_ioctl_put_buffer(
 	unsigned int read_idx = atomic_read(&cli->read_idx);
 	unsigned int idx = read_idx % cli->dump_bufs.buf_cnt;
 
-	struct kbase_hwcnt_reader_metadata meta;
-
-	if (sizeof(struct kbase_hwcnt_reader_metadata) != size)
-		return -EINVAL;
+	struct kbase_hwcnt_reader_metadata *meta;
+	const size_t meta_size = sizeof(struct kbase_hwcnt_reader_metadata);
+	const size_t max_size = max(size, meta_size);
+	int ret = 0;
+	u8 stack_kbuf[64];
+	u8 *kbuf = NULL;
+	size_t i;
 
 	/* Check if any buffer was taken. */
-	if (atomic_read(&cli->meta_idx) == read_idx)
+	if (unlikely(atomic_read(&cli->meta_idx) == read_idx))
 		return -EPERM;
 
+	if (likely(max_size <= sizeof(stack_kbuf))) {
+		/* Use stack buffer when the size is small enough. */
+		if (unlikely(meta_size > size))
+			memset(stack_kbuf, 0, sizeof(stack_kbuf));
+		kbuf = stack_kbuf;
+	} else {
+		kbuf = kzalloc(max_size, GFP_KERNEL);
+		if (unlikely(!kbuf))
+			return -ENOMEM;
+	}
+
+	/*
+	 * Copy user buffer to zero cleared kernel buffer which has enough
+	 * space for both user buffer and kernel metadata.
+	 */
+	if (unlikely(copy_from_user(kbuf, buffer, size))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	/*
+	 * Make sure any "extra" data passed from userspace is zero.
+	 * It's meaningful only in case meta_size < size.
+	 */
+	for (i = meta_size; i < size; i++) {
+		/* Check if user data beyond meta size is zero. */
+		if (unlikely(kbuf[i] != 0)) {
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
 	/* Check if correct buffer is put back. */
-	if (copy_from_user(&meta, buffer, size))
-		return -EFAULT;
-	if (idx != meta.buffer_idx)
-		return -EINVAL;
+	meta = (struct kbase_hwcnt_reader_metadata *)kbuf;
+	if (unlikely(idx != meta->buffer_idx)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
 	atomic_inc(&cli->read_idx);
-
-	return 0;
+out:
+	if (unlikely(kbuf != stack_kbuf))
+		kfree(kbuf);
+	return ret;
 }
 
 /**
@@ -813,26 +876,49 @@ static long kbasep_vinstr_hwcnt_reader_ioctl_get_hwver(
 	struct kbase_vinstr_client *cli,
 	u32 __user *hwver)
 {
-	u32 ver = 0;
+	u32 ver = 5;
 	const enum kbase_hwcnt_gpu_group_type type =
 		kbase_hwcnt_metadata_group_type(cli->vctx->metadata, 0);
 
-	switch (type) {
-	case KBASE_HWCNT_GPU_GROUP_TYPE_V4:
-		ver = 4;
-		break;
-	case KBASE_HWCNT_GPU_GROUP_TYPE_V5:
-		ver = 5;
-		break;
-	default:
-		WARN_ON(true);
-	}
-
-	if (ver != 0) {
-		return put_user(ver, hwver);
-	} else {
+	if (WARN_ON(type != KBASE_HWCNT_GPU_GROUP_TYPE_V5))
 		return -EINVAL;
+
+	return put_user(ver, hwver);
+}
+
+/**
+ * The hwcnt reader's ioctl command - get API version.
+ * @cli:    The non-NULL pointer to the client
+ * @arg:    Command's argument.
+ * @size:   Size of arg.
+ *
+ * @return 0 on success, else error code.
+ */
+static long kbasep_vinstr_hwcnt_reader_ioctl_get_api_version(
+	struct kbase_vinstr_client *cli, unsigned long arg, size_t size)
+{
+	long ret = -EINVAL;
+	u8 clk_cnt = cli->vctx->metadata->clk_cnt;
+
+	if (size == sizeof(u32)) {
+		ret = put_user(HWCNT_READER_API, (u32 __user *)arg);
+	} else if (size == sizeof(struct kbase_hwcnt_reader_api_version)) {
+		struct kbase_hwcnt_reader_api_version api_version = {
+			.version = HWCNT_READER_API,
+			.features = KBASE_HWCNT_READER_API_VERSION_NO_FEATURE,
+		};
+
+		if (clk_cnt > 0)
+			api_version.features |=
+			    KBASE_HWCNT_READER_API_VERSION_FEATURE_CYCLES_TOP;
+		if (clk_cnt > 1)
+			api_version.features |=
+			    KBASE_HWCNT_READER_API_VERSION_FEATURE_CYCLES_SHADER_CORES;
+
+		ret = copy_to_user(
+			(void __user *)arg, &api_version, sizeof(api_version));
 	}
+	return ret;
 }
 
 /**
@@ -858,42 +944,43 @@ static long kbasep_vinstr_hwcnt_reader_ioctl(
 	if (!cli)
 		return -EINVAL;
 
-	switch (cmd) {
-	case KBASE_HWCNT_READER_GET_API_VERSION:
-		rcode = put_user(HWCNT_READER_API, (u32 __user *)arg);
+	switch (_IOC_NR(cmd)) {
+	case _IOC_NR(KBASE_HWCNT_READER_GET_API_VERSION):
+		rcode = kbasep_vinstr_hwcnt_reader_ioctl_get_api_version(
+				cli, arg, _IOC_SIZE(cmd));
 		break;
-	case KBASE_HWCNT_READER_GET_HWVER:
+	case _IOC_NR(KBASE_HWCNT_READER_GET_HWVER):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_get_hwver(
 			cli, (u32 __user *)arg);
 		break;
-	case KBASE_HWCNT_READER_GET_BUFFER_SIZE:
+	case _IOC_NR(KBASE_HWCNT_READER_GET_BUFFER_SIZE):
 		rcode = put_user(
 			(u32)cli->vctx->metadata->dump_buf_bytes,
 			(u32 __user *)arg);
 		break;
-	case KBASE_HWCNT_READER_DUMP:
+	case _IOC_NR(KBASE_HWCNT_READER_DUMP):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_dump(cli);
 		break;
-	case KBASE_HWCNT_READER_CLEAR:
+	case _IOC_NR(KBASE_HWCNT_READER_CLEAR):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_clear(cli);
 		break;
-	case KBASE_HWCNT_READER_GET_BUFFER:
+	case _IOC_NR(KBASE_HWCNT_READER_GET_BUFFER):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_get_buffer(
 			cli, (void __user *)arg, _IOC_SIZE(cmd));
 		break;
-	case KBASE_HWCNT_READER_PUT_BUFFER:
+	case _IOC_NR(KBASE_HWCNT_READER_PUT_BUFFER):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_put_buffer(
 			cli, (void __user *)arg, _IOC_SIZE(cmd));
 		break;
-	case KBASE_HWCNT_READER_SET_INTERVAL:
+	case _IOC_NR(KBASE_HWCNT_READER_SET_INTERVAL):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_set_interval(
 			cli, (u32)arg);
 		break;
-	case KBASE_HWCNT_READER_ENABLE_EVENT:
+	case _IOC_NR(KBASE_HWCNT_READER_ENABLE_EVENT):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_enable_event(
 			cli, (enum base_hwcnt_reader_event)arg);
 		break;
-	case KBASE_HWCNT_READER_DISABLE_EVENT:
+	case _IOC_NR(KBASE_HWCNT_READER_DISABLE_EVENT):
 		rcode = kbasep_vinstr_hwcnt_reader_ioctl_disable_event(
 			cli, (enum base_hwcnt_reader_event)arg);
 		break;
diff --git a/drivers/gpu/arm/bifrost/mali_linux_trace.h b/drivers/gpu/arm/bifrost/mali_linux_trace.h
index 783fd2ede1a4..be812f62c862 100644
--- a/drivers/gpu/arm/bifrost/mali_linux_trace.h
+++ b/drivers/gpu/arm/bifrost/mali_linux_trace.h
@@ -181,6 +181,23 @@ TRACE_EVENT(mali_total_alloc_pages_change,
 		__print_symbolic(KBASE_MMU_FAULT_STATUS_ACCESS(status), \
 				KBASE_MMU_FAULT_ACCESS_SYMBOLIC_STRINGS)
 
+#if MALI_USE_CSF
+#define KBASE_MMU_FAULT_CODE_VALID(code) \
+		((code >= 0xC0 && code <= 0xEB) && \
+		(!(code >= 0xC5 && code <= 0xC7)) && \
+		(!(code >= 0xCC && code <= 0xD8)) && \
+		(!(code >= 0xDC && code <= 0xDF)) && \
+		(!(code >= 0xE1 && code <= 0xE3)))
+#define KBASE_MMU_FAULT_CODE_SYMBOLIC_STRINGS _ENSURE_PARENTHESIS(\
+		{0xC0, "TRANSLATION_FAULT_" }, \
+		{0xC4, "TRANSLATION_FAULT_" }, \
+		{0xC8, "PERMISSION_FAULT_" }, \
+		{0xD0, "TRANSTAB_BUS_FAULT_" }, \
+		{0xD8, "ACCESS_FLAG_" }, \
+		{0xE0, "ADDRESS_SIZE_FAULT_IN" }, \
+		{0xE4, "ADDRESS_SIZE_FAULT_OUT" }, \
+		{0xE8, "MEMORY_ATTRIBUTES_FAULT_" })
+#else /* MALI_USE_CSF */
 #define KBASE_MMU_FAULT_CODE_VALID(code) \
 	((code >= 0xC0 && code <= 0xEF) && \
 		(!(code >= 0xC5 && code <= 0xC6)) && \
@@ -197,6 +214,7 @@ TRACE_EVENT(mali_total_alloc_pages_change,
 		{0xE4, "ADDRESS_SIZE_FAULT_OUT" }, \
 		{0xE8, "MEMORY_ATTRIBUTES_FAULT_" }, \
 		{0xEC, "MEMORY_ATTRIBUTES_NONCACHEABLE_" })
+#endif /* MALI_USE_CSF */
 #endif /* __TRACE_MALI_MMU_HELPERS */
 
 /* trace_mali_mmu_page_fault_grow
@@ -288,7 +306,8 @@ DEFINE_EVENT_PRINT(mali_jit_softjob_template, mali_jit_free,
 	TP_printk("start=0x%llx va_pages=0x%zx backed_size=0x%zx",
 		__entry->start_addr, __entry->nr_pages, __entry->backed_pages));
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if !MALI_USE_CSF
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 /* trace_mali_jit_report
  *
  * Tracepoint about the GPU data structure read to form a just-in-time memory
@@ -326,13 +345,14 @@ TRACE_EVENT(mali_jit_report,
 		),
 		__entry->read_val, __entry->used_pages)
 );
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
+#endif /* !MALI_USE_CSF */
 
 #if (KERNEL_VERSION(4, 1, 0) <= LINUX_VERSION_CODE)
 TRACE_DEFINE_ENUM(KBASE_JIT_REPORT_ON_ALLOC_OR_FREE);
 #endif
 
-#if MALI_JIT_PRESSURE_LIMIT
+#if MALI_JIT_PRESSURE_LIMIT_BASE
 /* trace_mali_jit_report_pressure
  *
  * Tracepoint about change in physical memory pressure, due to the information
@@ -366,7 +386,7 @@ TRACE_EVENT(mali_jit_report_pressure,
 			{ KBASE_JIT_REPORT_ON_ALLOC_OR_FREE,
 				"HAPPENED_ON_ALLOC_OR_FREE" }))
 );
-#endif /* MALI_JIT_PRESSURE_LIMIT */
+#endif /* MALI_JIT_PRESSURE_LIMIT_BASE */
 
 #ifndef __TRACE_SYSGRAPH_ENUM
 #define __TRACE_SYSGRAPH_ENUM
diff --git a/drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.c b/drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.c
new file mode 100644
index 000000000000..b6fb5a094fab
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.c
@@ -0,0 +1,27 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/* Create the trace point if not configured in kernel */
+#ifndef CONFIG_TRACE_POWER_GPU_FREQUENCY
+#define CREATE_TRACE_POINTS
+#include "mali_power_gpu_frequency_trace.h"
+#endif
diff --git a/drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.h b/drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.h
new file mode 100644
index 000000000000..3b90ae437db9
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mali_power_gpu_frequency_trace.h
@@ -0,0 +1,69 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _TRACE_POWER_GPU_FREQUENCY_MALI
+#define _TRACE_POWER_GPU_FREQUENCY_MALI
+#endif
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM power
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE mali_power_gpu_frequency_trace
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+
+#if !defined(_TRACE_POWER_GPU_FREQUENCY_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_POWER_GPU_FREQUENCY_H
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(gpu,
+
+	TP_PROTO(unsigned int state, unsigned int gpu_id),
+
+	TP_ARGS(state, gpu_id),
+
+	TP_STRUCT__entry(
+		__field(	u32,		state		)
+		__field(	u32,		gpu_id		)
+	),
+
+	TP_fast_assign(
+		__entry->state = state;
+		__entry->gpu_id = gpu_id;
+	),
+
+	TP_printk("state=%lu gpu_id=%lu", (unsigned long)__entry->state,
+		  (unsigned long)__entry->gpu_id)
+);
+
+DEFINE_EVENT(gpu, gpu_frequency,
+
+	TP_PROTO(unsigned int frequency, unsigned int gpu_id),
+
+	TP_ARGS(frequency, gpu_id)
+);
+
+#endif /* _TRACE_POWER_GPU_FREQUENCY_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c
new file mode 100644
index 000000000000..1d106999228a
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_csf.c
@@ -0,0 +1,532 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+/**
+ * Base kernel MMU management specific for CSF GPU.
+ */
+
+#include <mali_kbase.h>
+#include <gpu/mali_kbase_gpu_fault.h>
+#include <mali_kbase_ctx_sched.h>
+#include <mali_kbase_hwaccess_jm.h>
+#include <mali_kbase_reset_gpu.h>
+#include <mali_kbase_as_fault_debugfs.h>
+#include "../mali_kbase_mmu_internal.h"
+
+void kbase_mmu_get_as_setup(struct kbase_mmu_table *mmut,
+		struct kbase_mmu_setup * const setup)
+{
+	/* Set up the required caching policies at the correct indices
+	 * in the memattr register.
+	 */
+	setup->memattr =
+		(AS_MEMATTR_IMPL_DEF_CACHE_POLICY <<
+			(AS_MEMATTR_INDEX_IMPL_DEF_CACHE_POLICY * 8)) |
+		(AS_MEMATTR_FORCE_TO_CACHE_ALL <<
+			(AS_MEMATTR_INDEX_FORCE_TO_CACHE_ALL * 8)) |
+		(AS_MEMATTR_WRITE_ALLOC <<
+			(AS_MEMATTR_INDEX_WRITE_ALLOC * 8)) |
+		(AS_MEMATTR_AARCH64_OUTER_IMPL_DEF   <<
+			(AS_MEMATTR_INDEX_OUTER_IMPL_DEF * 8)) |
+		(AS_MEMATTR_AARCH64_OUTER_WA <<
+			(AS_MEMATTR_INDEX_OUTER_WA * 8)) |
+		(AS_MEMATTR_AARCH64_NON_CACHEABLE <<
+			(AS_MEMATTR_INDEX_NON_CACHEABLE * 8)) |
+		(AS_MEMATTR_AARCH64_SHARED <<
+			(AS_MEMATTR_INDEX_SHARED * 8));
+
+	setup->transtab = (u64)mmut->pgd & AS_TRANSTAB_BASE_MASK;
+	setup->transcfg = AS_TRANSCFG_ADRMODE_AARCH64_4K;
+}
+
+/**
+ * submit_work_pagefault() - Submit a work for MMU page fault.
+ *
+ * @kbdev:    Kbase device pointer
+ * @as_nr:    Faulty address space
+ * @fault:    Data relating to the fault
+ *
+ * This function submits a work for reporting the details of MMU fault.
+ */
+static void submit_work_pagefault(struct kbase_device *kbdev, u32 as_nr,
+		struct kbase_fault *fault)
+{
+	struct kbase_as *const as = &kbdev->as[as_nr];
+
+	as->pf_data = (struct kbase_fault) {
+		.status = fault->status,
+		.addr = fault->addr,
+	};
+
+	if (kbase_ctx_sched_as_to_ctx_refcount(kbdev, as_nr)) {
+		WARN_ON(!queue_work(as->pf_wq, &as->work_pagefault));
+		atomic_inc(&kbdev->faults_pending);
+	}
+}
+
+void kbase_mmu_report_mcu_as_fault_and_reset(struct kbase_device *kbdev,
+		struct kbase_fault *fault)
+{
+	/* decode the fault status */
+	u32 exception_type = fault->status & 0xFF;
+	u32 access_type = (fault->status >> 8) & 0x3;
+	u32 source_id = (fault->status >> 16);
+	int as_no;
+
+	/* terminal fault, print info about the fault */
+	dev_err(kbdev->dev,
+		"Unexpected Page fault in firmware address space at VA 0x%016llX\n"
+		"raw fault status: 0x%X\n"
+		"exception type 0x%X: %s\n"
+		"access type 0x%X: %s\n"
+		"source id 0x%X\n",
+		fault->addr,
+		fault->status,
+		exception_type, kbase_gpu_exception_name(exception_type),
+		access_type, kbase_gpu_access_type_name(fault->status),
+		source_id);
+
+	/* Report MMU fault for all address spaces (except MCU_AS_NR) */
+	for (as_no = 1; as_no < kbdev->nr_hw_address_spaces; as_no++)
+		if (kbase_ctx_sched_as_to_ctx(kbdev, as_no))
+			submit_work_pagefault(kbdev, as_no, fault);
+
+	/* GPU reset is required to recover */
+	if (kbase_prepare_to_reset_gpu(kbdev))
+		kbase_reset_gpu(kbdev);
+}
+KBASE_EXPORT_TEST_API(kbase_mmu_report_mcu_as_fault_and_reset);
+
+void kbase_gpu_report_bus_fault_and_kill(struct kbase_context *kctx,
+		struct kbase_as *as, struct kbase_fault *fault)
+{
+	struct kbase_device *kbdev = kctx->kbdev;
+	u32 const status = fault->status;
+	int exception_type = (status & GPU_FAULTSTATUS_EXCEPTION_TYPE_MASK) >>
+				GPU_FAULTSTATUS_EXCEPTION_TYPE_SHIFT;
+	int access_type = (status & GPU_FAULTSTATUS_ACCESS_TYPE_MASK) >>
+				GPU_FAULTSTATUS_ACCESS_TYPE_SHIFT;
+	int source_id = (status & GPU_FAULTSTATUS_SOURCE_ID_MASK) >>
+				GPU_FAULTSTATUS_SOURCE_ID_SHIFT;
+	const char *addr_valid = (status & GPU_FAULTSTATUS_ADDR_VALID_FLAG) ?
+					"true" : "false";
+	int as_no = as->number;
+	unsigned long flags;
+
+	/* terminal fault, print info about the fault */
+	dev_err(kbdev->dev,
+		"GPU bus fault in AS%d at VA 0x%016llX\n"
+		"VA_VALID: %s\n"
+		"raw fault status: 0x%X\n"
+		"exception type 0x%X: %s\n"
+		"access type 0x%X: %s\n"
+		"source id 0x%X\n"
+		"pid: %d\n",
+		as_no, fault->addr,
+		addr_valid,
+		status,
+		exception_type, kbase_gpu_exception_name(exception_type),
+		access_type, kbase_gpu_access_type_name(access_type),
+		source_id,
+		kctx->pid);
+
+	/* AS transaction begin */
+	mutex_lock(&kbdev->mmu_hw_mutex);
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_mmu_disable(kctx);
+	kbase_ctx_flag_set(kctx, KCTX_AS_DISABLED_ON_FAULT);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	mutex_unlock(&kbdev->mmu_hw_mutex);
+
+	/* Switching to UNMAPPED mode above would have enabled the firmware to
+	 * recover from the fault (if the memory access was made by firmware)
+	 * and it can then respond to CSG termination requests to be sent now.
+	 * All GPU command queue groups associated with the context would be
+	 * affected as they use the same GPU address space.
+	 */
+	kbase_csf_ctx_handle_fault(kctx, fault);
+
+	/* Now clear the GPU fault */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
+			GPU_COMMAND_CLEAR_FAULT);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+}
+
+/**
+ * The caller must ensure it's retained the ctx to prevent it from being
+ * scheduled out whilst it's being worked on.
+ */
+void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx,
+		struct kbase_as *as, const char *reason_str,
+		struct kbase_fault *fault)
+{
+	unsigned long flags;
+	unsigned int exception_type;
+	unsigned int access_type;
+	unsigned int source_id;
+	int as_no;
+	struct kbase_device *kbdev;
+	const u32 status = fault->status;
+
+	as_no = as->number;
+	kbdev = kctx->kbdev;
+
+	/* Make sure the context was active */
+	if (WARN_ON(atomic_read(&kctx->refcount) <= 0))
+		return;
+
+	/* decode the fault status */
+	exception_type = AS_FAULTSTATUS_EXCEPTION_TYPE_GET(status);
+	access_type = AS_FAULTSTATUS_ACCESS_TYPE_GET(status);
+	source_id = AS_FAULTSTATUS_SOURCE_ID_GET(status);
+
+	/* terminal fault, print info about the fault */
+	dev_err(kbdev->dev,
+		"Unhandled Page fault in AS%d at VA 0x%016llX\n"
+		"Reason: %s\n"
+		"raw fault status: 0x%X\n"
+		"exception type 0x%X: %s\n"
+		"access type 0x%X: %s\n"
+		"source id 0x%X\n"
+		"pid: %d\n",
+		as_no, fault->addr,
+		reason_str,
+		status,
+		exception_type, kbase_gpu_exception_name(exception_type),
+		access_type, kbase_gpu_access_type_name(status),
+		source_id,
+		kctx->pid);
+
+	/* AS transaction begin */
+	mutex_lock(&kbdev->mmu_hw_mutex);
+
+	/* switch to UNMAPPED mode,
+	 * will abort all jobs and stop any hw counter dumping
+	 */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_mmu_disable(kctx);
+	kbase_ctx_flag_set(kctx, KCTX_AS_DISABLED_ON_FAULT);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	mutex_unlock(&kbdev->mmu_hw_mutex);
+	/* AS transaction end */
+
+	/* Switching to UNMAPPED mode above would have enabled the firmware to
+	 * recover from the fault (if the memory access was made by firmware)
+	 * and it can then respond to CSG termination requests to be sent now.
+	 * All GPU command queue groups associated with the context would be
+	 * affected as they use the same GPU address space.
+	 */
+	kbase_csf_ctx_handle_fault(kctx, fault);
+
+	/* Clear down the fault */
+	kbase_mmu_hw_clear_fault(kbdev, as,
+			KBASE_MMU_FAULT_TYPE_PAGE_UNEXPECTED);
+	kbase_mmu_hw_enable_fault(kbdev, as,
+			KBASE_MMU_FAULT_TYPE_PAGE_UNEXPECTED);
+}
+
+/**
+ * kbase_mmu_interrupt_process() - Process a bus or page fault.
+ * @kbdev:	The kbase_device the fault happened on
+ * @kctx:	The kbase_context for the faulting address space if one was
+ *		found.
+ * @as:		The address space that has the fault
+ * @fault:	Data relating to the fault
+ *
+ * This function will process a fault on a specific address space
+ */
+static void kbase_mmu_interrupt_process(struct kbase_device *kbdev,
+		struct kbase_context *kctx, struct kbase_as *as,
+		struct kbase_fault *fault)
+{
+	lockdep_assert_held(&kbdev->hwaccess_lock);
+
+	if (!kctx) {
+		dev_warn(kbdev->dev, "%s in AS%d at 0x%016llx with no context present! Spurious IRQ or SW Design Error?\n",
+				kbase_as_has_bus_fault(as, fault) ?
+						"Bus error" : "Page fault",
+				as->number, fault->addr);
+
+		/* Since no ctx was found, the MMU must be disabled. */
+		WARN_ON(as->current_setup.transtab);
+
+		if (kbase_as_has_bus_fault(as, fault))
+			kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND),
+				GPU_COMMAND_CLEAR_FAULT);
+		else if (kbase_as_has_page_fault(as, fault)) {
+			kbase_mmu_hw_clear_fault(kbdev, as,
+					KBASE_MMU_FAULT_TYPE_PAGE_UNEXPECTED);
+			kbase_mmu_hw_enable_fault(kbdev, as,
+					KBASE_MMU_FAULT_TYPE_PAGE_UNEXPECTED);
+		}
+
+		return;
+	}
+
+	if (kbase_as_has_bus_fault(as, fault)) {
+		/*
+		 * We need to switch to UNMAPPED mode - but we do this in a
+		 * worker so that we can sleep
+		 */
+		WARN_ON(!queue_work(as->pf_wq, &as->work_busfault));
+		atomic_inc(&kbdev->faults_pending);
+	} else {
+		WARN_ON(!queue_work(as->pf_wq, &as->work_pagefault));
+		atomic_inc(&kbdev->faults_pending);
+	}
+}
+
+int kbase_mmu_bus_fault_interrupt(struct kbase_device *kbdev,
+		u32 status, u32 as_nr)
+{
+	struct kbase_context *kctx;
+	unsigned long flags;
+	struct kbase_as *as;
+	struct kbase_fault *fault;
+
+	if (WARN_ON(as_nr == MCU_AS_NR))
+		return -EINVAL;
+
+	if (WARN_ON(as_nr >= BASE_MAX_NR_AS))
+		return -EINVAL;
+
+	as = &kbdev->as[as_nr];
+	fault = &as->bf_data;
+	fault->status = status;
+	fault->addr = (u64) kbase_reg_read(kbdev,
+		GPU_CONTROL_REG(GPU_FAULTADDRESS_HI)) << 32;
+	fault->addr |= kbase_reg_read(kbdev,
+		GPU_CONTROL_REG(GPU_FAULTADDRESS_LO));
+	fault->protected_mode = false;
+
+	/* report the fault to debugfs */
+	kbase_as_fault_debugfs_new(kbdev, as_nr);
+
+	kctx = kbase_ctx_sched_as_to_ctx_refcount(kbdev, as_nr);
+
+	/* Process the bus fault interrupt for this address space */
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	kbase_mmu_interrupt_process(kbdev, kctx, as, fault);
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	return 0;
+}
+
+void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
+{
+	const int num_as = 16;
+	const int pf_shift = 0;
+	const unsigned long as_bit_mask = (1UL << num_as) - 1;
+	unsigned long flags;
+	u32 new_mask;
+	u32 tmp;
+	u32 pf_bits = ((irq_stat >> pf_shift) & as_bit_mask);
+
+	/* remember current mask */
+	spin_lock_irqsave(&kbdev->mmu_mask_change, flags);
+	new_mask = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK));
+	/* mask interrupts for now */
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), 0);
+	spin_unlock_irqrestore(&kbdev->mmu_mask_change, flags);
+
+	while (pf_bits) {
+		struct kbase_context *kctx;
+		int as_no = ffs(pf_bits) - 1;
+		struct kbase_as *as = &kbdev->as[as_no];
+		struct kbase_fault *fault = &as->pf_data;
+
+		/* find faulting address */
+		fault->addr = kbase_reg_read(kbdev, MMU_AS_REG(as_no,
+				AS_FAULTADDRESS_HI));
+		fault->addr <<= 32;
+		fault->addr |= kbase_reg_read(kbdev, MMU_AS_REG(as_no,
+				AS_FAULTADDRESS_LO));
+
+		/* Mark the fault protected or not */
+		fault->protected_mode = false;
+
+		/* report the fault to debugfs */
+		kbase_as_fault_debugfs_new(kbdev, as_no);
+
+		/* record the fault status */
+		fault->status = kbase_reg_read(kbdev, MMU_AS_REG(as_no,
+				AS_FAULTSTATUS));
+
+		fault->extra_addr = kbase_reg_read(kbdev,
+					MMU_AS_REG(as_no, AS_FAULTEXTRA_HI));
+		fault->extra_addr <<= 32;
+		fault->extra_addr |= kbase_reg_read(kbdev,
+					MMU_AS_REG(as_no, AS_FAULTEXTRA_LO));
+
+		/* Mark page fault as handled */
+		pf_bits &= ~(1UL << as_no);
+
+		/* remove the queued PF from the mask */
+		new_mask &= ~MMU_PAGE_FAULT(as_no);
+
+		if (as_no == MCU_AS_NR) {
+			kbase_mmu_report_mcu_as_fault_and_reset(kbdev, fault);
+			/* Pointless to handle remaining faults */
+			break;
+		}
+
+		/*
+		 * Refcount the kctx - it shouldn't disappear anyway, since
+		 * Page faults _should_ only occur whilst GPU commands are
+		 * executing, and a command causing the Page fault shouldn't
+		 * complete until the MMU is updated.
+		 * Reference is released at the end of bottom half of page
+		 * fault handling.
+		 */
+		kctx = kbase_ctx_sched_as_to_ctx_refcount(kbdev, as_no);
+
+		/* Process the interrupt for this address space */
+		spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+		kbase_mmu_interrupt_process(kbdev, kctx, as, fault);
+		spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+	}
+
+	/* reenable interrupts */
+	spin_lock_irqsave(&kbdev->mmu_mask_change, flags);
+	tmp = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK));
+	new_mask |= tmp;
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), new_mask);
+	spin_unlock_irqrestore(&kbdev->mmu_mask_change, flags);
+}
+
+int kbase_mmu_switch_to_ir(struct kbase_context *const kctx,
+	struct kbase_va_region *const reg)
+{
+	/* Can't soft-stop the provoking job */
+	return -EPERM;
+}
+
+/**
+ * kbase_mmu_gpu_fault_worker() - Process a GPU fault for the device.
+ *
+ * @data:  work_struct passed by queue_work()
+ *
+ * Report a GPU fatal error for all GPU command queue groups that are
+ * using the address space and terminate them.
+ */
+static void kbase_mmu_gpu_fault_worker(struct work_struct *data)
+{
+	struct kbase_as *const faulting_as = container_of(data, struct kbase_as,
+			work_gpufault);
+	const u32 as_nr = faulting_as->number;
+	struct kbase_device *const kbdev = container_of(faulting_as, struct
+			kbase_device, as[as_nr]);
+	struct kbase_fault *fault;
+	struct kbase_context *kctx;
+	u32 status;
+	u64 address;
+	u32 as_valid;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	fault = &faulting_as->gf_data;
+	status = fault->status;
+	as_valid = status & GPU_FAULTSTATUS_JASID_VALID_FLAG;
+	address = fault->addr;
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	dev_warn(kbdev->dev,
+		 "GPU Fault 0x%08x (%s) in AS%u at 0x%016llx\n"
+		 "ASID_VALID: %s,  ADDRESS_VALID: %s\n",
+		 status,
+		 kbase_gpu_exception_name(
+			GPU_FAULTSTATUS_EXCEPTION_TYPE_GET(status)),
+		 as_nr, address,
+		 as_valid ? "true" : "false",
+		 status & GPU_FAULTSTATUS_ADDR_VALID_FLAG ? "true" : "false");
+
+	kctx = kbase_ctx_sched_as_to_ctx(kbdev, as_nr);
+	kbase_csf_ctx_handle_fault(kctx, fault);
+	kbase_ctx_sched_release_ctx_lock(kctx);
+
+	atomic_dec(&kbdev->faults_pending);
+}
+
+/**
+ * submit_work_gpufault() - Submit a work for GPU fault.
+ *
+ * @kbdev:    Kbase device pointer
+ * @status:   GPU fault status
+ * @as_nr:    Faulty address space
+ * @address:  GPU fault address
+ *
+ * This function submits a work for reporting the details of GPU fault.
+ */
+static void submit_work_gpufault(struct kbase_device *kbdev, u32 status,
+		u32 as_nr, u64 address)
+{
+	unsigned long flags;
+	struct kbase_as *const as = &kbdev->as[as_nr];
+
+	spin_lock_irqsave(&kbdev->hwaccess_lock, flags);
+	as->gf_data = (struct kbase_fault) {
+			.status = status,
+			.addr = address,
+	};
+	spin_unlock_irqrestore(&kbdev->hwaccess_lock, flags);
+
+	if (kbase_ctx_sched_as_to_ctx_refcount(kbdev, as_nr)) {
+		WARN_ON(!queue_work(as->pf_wq, &as->work_gpufault));
+		atomic_inc(&kbdev->faults_pending);
+	}
+}
+
+void kbase_mmu_gpu_fault_interrupt(struct kbase_device *kbdev, u32 status,
+		u32 as_nr, u64 address, bool as_valid)
+{
+	if (!as_valid || (as_nr == MCU_AS_NR)) {
+		int as;
+
+		/* Report GPU fault for all contexts (except MCU_AS_NR) in case either
+		 * the address space is invalid or it's MCU address space.
+		 */
+		for (as = 1; as < kbdev->nr_hw_address_spaces; as++)
+			submit_work_gpufault(kbdev, status, as, address);
+	} else
+		submit_work_gpufault(kbdev, status, as_nr, address);
+}
+KBASE_EXPORT_TEST_API(kbase_mmu_gpu_fault_interrupt);
+
+int kbase_mmu_as_init(struct kbase_device *kbdev, int i)
+{
+	kbdev->as[i].number = i;
+	kbdev->as[i].bf_data.addr = 0ULL;
+	kbdev->as[i].pf_data.addr = 0ULL;
+	kbdev->as[i].gf_data.addr = 0ULL;
+
+	kbdev->as[i].pf_wq = alloc_workqueue("mali_mmu%d", 0, 1, i);
+	if (!kbdev->as[i].pf_wq)
+		return -ENOMEM;
+
+	INIT_WORK(&kbdev->as[i].work_pagefault, kbase_mmu_page_fault_worker);
+	INIT_WORK(&kbdev->as[i].work_busfault, kbase_mmu_bus_fault_worker);
+	INIT_WORK(&kbdev->as[i].work_gpufault, kbase_mmu_gpu_fault_worker);
+
+	return 0;
+}
diff --git a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c
index 2d8fb51d11ac..b0187a46b733 100644
--- a/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c
+++ b/drivers/gpu/arm/bifrost/mmu/backend/mali_kbase_mmu_jm.c
@@ -27,10 +27,9 @@
 #include <mali_kbase.h>
 #include <gpu/mali_kbase_gpu_fault.h>
 #include <mali_kbase_hwaccess_jm.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <mali_kbase_as_fault_debugfs.h>
 #include "../mali_kbase_mmu_internal.h"
-#include "mali_kbase_device_internal.h"
 
 void kbase_mmu_get_as_setup(struct kbase_mmu_table *mmut,
 		struct kbase_mmu_setup * const setup)
@@ -187,7 +186,17 @@ void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx,
 			KBASE_MMU_FAULT_TYPE_PAGE_UNEXPECTED);
 }
 
-void kbase_mmu_interrupt_process(struct kbase_device *kbdev,
+/**
+ * kbase_mmu_interrupt_process() - Process a bus or page fault.
+ * @kbdev:	The kbase_device the fault happened on
+ * @kctx:	The kbase_context for the faulting address space if one was
+ *		found.
+ * @as:		The address space that has the fault
+ * @fault:	Data relating to the fault
+ *
+ * This function will process a fault on a specific address space
+ */
+static void kbase_mmu_interrupt_process(struct kbase_device *kbdev,
 		struct kbase_context *kctx, struct kbase_as *as,
 		struct kbase_fault *fault)
 {
@@ -296,7 +305,6 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
 	unsigned long flags;
 	u32 new_mask;
 	u32 tmp, bf_bits, pf_bits;
-	bool gpu_lost = false;
 
 	dev_dbg(kbdev->dev, "Entering %s irq_stat %u\n",
 		__func__, irq_stat);
@@ -371,14 +379,6 @@ void kbase_mmu_interrupt(struct kbase_device *kbdev, u32 irq_stat)
 					MMU_AS_REG(as_no, AS_FAULTEXTRA_LO));
 		}
 
-		/* check if we still have GPU */
-		gpu_lost = kbase_is_gpu_lost(kbdev);
-		if (gpu_lost) {
-			if (kctx)
-				kbasep_js_runpool_release_ctx(kbdev, kctx);
-			return;
-		}
-
 		if (kbase_as_has_bus_fault(as, fault)) {
 			/* Mark bus fault as handled.
 			 * Note that a bus fault is processed first in case
@@ -422,3 +422,19 @@ int kbase_mmu_switch_to_ir(struct kbase_context *const kctx,
 		(void *)reg);
 	return kbase_job_slot_softstop_start_rp(kctx, reg);
 }
+
+int kbase_mmu_as_init(struct kbase_device *kbdev, int i)
+{
+	kbdev->as[i].number = i;
+	kbdev->as[i].bf_data.addr = 0ULL;
+	kbdev->as[i].pf_data.addr = 0ULL;
+
+	kbdev->as[i].pf_wq = alloc_workqueue("mali_mmu%d", 0, 1, i);
+	if (!kbdev->as[i].pf_wq)
+		return -ENOMEM;
+
+	INIT_WORK(&kbdev->as[i].work_pagefault, kbase_mmu_page_fault_worker);
+	INIT_WORK(&kbdev->as[i].work_busfault, kbase_mmu_bus_fault_worker);
+
+	return 0;
+}
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c
index db27832bf251..a5cda009426d 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.c
@@ -44,7 +44,9 @@
 #include <mmu/mali_kbase_mmu.h>
 #include <mmu/mali_kbase_mmu_internal.h>
 #include <mali_kbase_cs_experimental.h>
+#include <device/mali_kbase_device.h>
 
+#include <mali_kbase_trace_gpu_mem.h>
 #define KBASE_MMU_PAGE_ENTRIES 512
 
 /**
@@ -150,6 +152,14 @@ static size_t reg_grow_calc_extra_pages(struct kbase_device *kbdev,
 	 * Depending on reg's flags, the base used for calculating multiples is
 	 * different
 	 */
+
+	/* multiple is based from the current backed size, even if the
+	 * current backed size/pfn for end of committed memory are not
+	 * themselves aligned to multiple
+	 */
+	remainder = minimum_extra % multiple;
+
+#if !MALI_USE_CSF
 	if (reg->flags & KBASE_REG_TILER_ALIGN_TOP) {
 		/* multiple is based from the top of the initial commit, which
 		 * has been allocated in such a way that (start_pfn +
@@ -175,13 +185,8 @@ static size_t reg_grow_calc_extra_pages(struct kbase_device *kbdev,
 
 			remainder = pages_after_initial % multiple;
 		}
-	} else {
-		/* multiple is based from the current backed size, even if the
-		 * current backed size/pfn for end of committed memory are not
-		 * themselves aligned to multiple
-		 */
-		remainder = minimum_extra % multiple;
 	}
+#endif /* !MALI_USE_CSF */
 
 	if (remainder == 0)
 		return minimum_extra;
@@ -522,10 +527,15 @@ static bool page_fault_try_alloc(struct kbase_context *kctx,
 static void release_ctx(struct kbase_device *kbdev,
 		struct kbase_context *kctx)
 {
+#if MALI_USE_CSF
+	CSTD_UNUSED(kbdev);
+	kbase_ctx_sched_release_ctx_lock(kctx);
+#else /* MALI_USE_CSF */
 	kbasep_js_runpool_release_ctx(kbdev, kctx);
+#endif /* MALI_USE_CSF */
 }
 
-void page_fault_worker(struct work_struct *data)
+void kbase_mmu_page_fault_worker(struct work_struct *data)
 {
 	u64 fault_pfn;
 	u32 fault_status;
@@ -544,7 +554,9 @@ void page_fault_worker(struct work_struct *data)
 	struct kbase_sub_alloc *prealloc_sas[2] = { NULL, NULL };
 	int i;
 	size_t current_backed_size;
-
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	size_t pages_trimmed = 0;
+#endif
 
 	faulting_as = container_of(data, struct kbase_as, work_pagefault);
 	fault = &faulting_as->pf_data;
@@ -568,6 +580,21 @@ void page_fault_worker(struct work_struct *data)
 
 	KBASE_DEBUG_ASSERT(kctx->kbdev == kbdev);
 
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+#if !MALI_USE_CSF
+	mutex_lock(&kctx->jctx.lock);
+#endif
+#endif
+
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	/* check if we still have GPU */
+	if (unlikely(kbase_is_gpu_removed(kbdev))) {
+		dev_dbg(kbdev->dev,
+				"%s: GPU has been removed\n", __func__);
+		goto fault_done;
+	}
+#endif
+
 	if (unlikely(fault->protected_mode)) {
 		kbase_mmu_report_fault_and_kill(kctx, faulting_as,
 				"Protected mode fault", fault);
@@ -758,6 +785,13 @@ void page_fault_worker(struct work_struct *data)
 
 	pages_to_grow = 0;
 
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	if ((region->flags & KBASE_REG_ACTIVE_JIT_ALLOC) && !pages_trimmed) {
+		kbase_jit_request_phys_increase(kctx, new_pages);
+		pages_trimmed = new_pages;
+	}
+#endif
+
 	spin_lock(&kctx->mem_partials_lock);
 	grown = page_fault_try_alloc(kctx, region, new_pages, &pages_to_grow,
 			&grow_2mb_pool, prealloc_sas);
@@ -872,6 +906,13 @@ void page_fault_worker(struct work_struct *data)
 			}
 		}
 #endif
+
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+		if (pages_trimmed) {
+			kbase_jit_done_phys_increase(kctx, pages_trimmed);
+			pages_trimmed = 0;
+		}
+#endif
 		kbase_gpu_vm_unlock(kctx);
 	} else {
 		int ret = -ENOMEM;
@@ -918,6 +959,17 @@ void page_fault_worker(struct work_struct *data)
 	}
 
 fault_done:
+#if MALI_JIT_PRESSURE_LIMIT_BASE
+	if (pages_trimmed) {
+		kbase_gpu_vm_lock(kctx);
+		kbase_jit_done_phys_increase(kctx, pages_trimmed);
+		kbase_gpu_vm_unlock(kctx);
+	}
+#if !MALI_USE_CSF
+	mutex_unlock(&kctx->jctx.lock);
+#endif
+#endif
+
 	for (i = 0; i != ARRAY_SIZE(prealloc_sas); ++i)
 		kfree(prealloc_sas[i]);
 
@@ -964,6 +1016,8 @@ static phys_addr_t kbase_mmu_alloc_pgd(struct kbase_device *kbdev,
 
 	atomic_add(1, &kbdev->memdev.used_pages);
 
+	kbase_trace_gpu_mem_usage_inc(kbdev, mmut->kctx, 1);
+
 	for (i = 0; i < KBASE_MMU_PAGE_ENTRIES; i++)
 		kbdev->mmu_mode->entry_invalidate(&page[i]);
 
@@ -1290,6 +1344,8 @@ static inline void cleanup_empty_pte(struct kbase_device *kbdev,
 		atomic_sub(1, &mmut->kctx->used_pages);
 	}
 	atomic_sub(1, &kbdev->memdev.used_pages);
+
+	kbase_trace_gpu_mem_usage_dec(kbdev, mmut->kctx, 1);
 }
 
 u64 kbase_mmu_create_ate(struct kbase_device *const kbdev,
@@ -1569,9 +1625,13 @@ static void kbase_mmu_flush_invalidate(struct kbase_context *kctx,
 		return;
 
 	kbdev = kctx->kbdev;
+#if !MALI_USE_CSF
 	mutex_lock(&kbdev->js_data.queue_mutex);
+#endif /* !MALI_USE_CSF */
 	ctx_is_in_runpool = kbase_ctx_sched_inc_refcount(kctx);
+#if !MALI_USE_CSF
 	mutex_unlock(&kbdev->js_data.queue_mutex);
+#endif /* !MALI_USE_CSF */
 
 	if (ctx_is_in_runpool) {
 		KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
@@ -1932,6 +1992,8 @@ static void mmu_teardown_level(struct kbase_device *kbdev,
 		kbase_process_page_usage_dec(mmut->kctx, 1);
 		atomic_sub(1, &mmut->kctx->used_pages);
 	}
+
+	kbase_trace_gpu_mem_usage_dec(kbdev, mmut->kctx, 1);
 }
 
 int kbase_mmu_init(struct kbase_device *const kbdev,
@@ -1992,6 +2054,11 @@ void kbase_mmu_term(struct kbase_device *kbdev, struct kbase_mmu_table *mmut)
 	mutex_destroy(&mmut->mmu_lock);
 }
 
+void kbase_mmu_as_term(struct kbase_device *kbdev, int i)
+{
+	destroy_workqueue(kbdev->as[i].pf_wq);
+}
+
 static size_t kbasep_mmu_dump_level(struct kbase_context *kctx, phys_addr_t pgd,
 		int level, char ** const buffer, size_t *size_left)
 {
@@ -2132,7 +2199,7 @@ void *kbase_mmu_dump(struct kbase_context *kctx, int nr_pages)
 }
 KBASE_EXPORT_TEST_API(kbase_mmu_dump);
 
-void bus_fault_worker(struct work_struct *data)
+void kbase_mmu_bus_fault_worker(struct work_struct *data)
 {
 	struct kbase_as *faulting_as;
 	int as_no;
@@ -2160,6 +2227,17 @@ void bus_fault_worker(struct work_struct *data)
 		return;
 	}
 
+#ifdef CONFIG_MALI_ARBITER_SUPPORT
+	/* check if we still have GPU */
+	if (unlikely(kbase_is_gpu_removed(kbdev))) {
+		dev_dbg(kbdev->dev,
+				"%s: GPU has been removed\n", __func__);
+		release_ctx(kbdev, kctx);
+		atomic_dec(&kbdev->faults_pending);
+		return;
+	}
+#endif
+
 	if (unlikely(fault->protected_mode)) {
 		kbase_mmu_report_fault_and_kill(kctx, faulting_as,
 				"Permission failure", fault);
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h
index c9e27b1255c5..f2613e881dac 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -23,6 +23,30 @@
 #ifndef _KBASE_MMU_H_
 #define _KBASE_MMU_H_
 
+/**
+ * kbase_mmu_as_init() - Initialising GPU address space object.
+ *
+ * This is called from device probe to initialise an address space object
+ * of the device.
+ *
+ * @kbdev: The kbase device structure for the device (must be a valid pointer).
+ * @i:     Array index of address space object.
+ *
+ * Return: 0 on success and non-zero value on failure.
+ */
+int kbase_mmu_as_init(struct kbase_device *kbdev, int i);
+
+/**
+ * kbase_mmu_as_term() - Terminate address space object.
+ *
+ * This is called upon device termination to destroy
+ * the address space object of the device.
+ *
+ * @kbdev: The kbase device structure for the device (must be a valid pointer).
+ * @i:     Array index of address space object.
+ */
+void kbase_mmu_as_term(struct kbase_device *kbdev, int i);
+
 /**
  * kbase_mmu_init - Initialise an object representing GPU page tables
  *
@@ -115,4 +139,18 @@ int kbase_mmu_update_pages(struct kbase_context *kctx, u64 vpfn,
 int kbase_mmu_bus_fault_interrupt(struct kbase_device *kbdev, u32 status,
 		u32 as_nr);
 
+/**
+ * kbase_mmu_gpu_fault_interrupt() - Report a GPU fault.
+ * @kbdev:    Kbase device pointer
+ * @status:   GPU fault status
+ * @as_nr:    Faulty address space
+ * @address:  GPU fault address
+ * @as_valid: true if address space is valid
+ *
+ * This function builds GPU fault information to submit a work
+ * for reporting the details of the fault.
+ */
+void kbase_mmu_gpu_fault_interrupt(struct kbase_device *kbdev, u32 status,
+		u32 as_nr, u64 address, bool as_valid);
+
 #endif /* _KBASE_MMU_H_ */
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c
index f22e73e07398..a820ab24ac05 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_hw_direct.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2019 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -25,7 +25,7 @@
 #include <mali_kbase_mem.h>
 #include <mmu/mali_kbase_mmu_hw.h>
 #include <tl/mali_kbase_tracepoints.h>
-#include <backend/gpu/mali_kbase_device_internal.h>
+#include <device/mali_kbase_device.h>
 #include <mali_kbase_as_fault_debugfs.h>
 
 /**
@@ -230,10 +230,11 @@ void kbase_mmu_hw_clear_fault(struct kbase_device *kbdev, struct kbase_as *as,
 
 	/* Clear the page (and bus fault IRQ as well in case one occurred) */
 	pf_bf_mask = MMU_PAGE_FAULT(as->number);
+#if !MALI_USE_CSF
 	if (type == KBASE_MMU_FAULT_TYPE_BUS ||
 			type == KBASE_MMU_FAULT_TYPE_BUS_UNEXPECTED)
 		pf_bf_mask |= MMU_BUS_ERROR(as->number);
-
+#endif
 	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_CLEAR), pf_bf_mask);
 
 unlock:
@@ -261,10 +262,11 @@ void kbase_mmu_hw_enable_fault(struct kbase_device *kbdev, struct kbase_as *as,
 	irq_mask = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK)) |
 			MMU_PAGE_FAULT(as->number);
 
+#if !MALI_USE_CSF
 	if (type == KBASE_MMU_FAULT_TYPE_BUS ||
 			type == KBASE_MMU_FAULT_TYPE_BUS_UNEXPECTED)
 		irq_mask |= MMU_BUS_ERROR(as->number);
-
+#endif
 	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), irq_mask);
 
 unlock:
diff --git a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h
index 28bd341bf082..8ecb14d72327 100644
--- a/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h
+++ b/drivers/gpu/arm/bifrost/mmu/mali_kbase_mmu_internal.h
@@ -26,6 +26,15 @@
 void kbase_mmu_get_as_setup(struct kbase_mmu_table *mmut,
 		struct kbase_mmu_setup * const setup);
 
+/**
+ * kbase_mmu_report_mcu_as_fault_and_reset - Report page fault for all
+ *                                           address spaces and reset the GPU.
+ * @kbdev:   The kbase_device the fault happened on
+ * @fault:   Data relating to the fault
+ */
+void kbase_mmu_report_mcu_as_fault_and_reset(struct kbase_device *kbdev,
+		struct kbase_fault *fault);
+
 void kbase_gpu_report_bus_fault_and_kill(struct kbase_context *kctx,
 		struct kbase_as *as, struct kbase_fault *fault);
 
@@ -33,24 +42,11 @@ void kbase_mmu_report_fault_and_kill(struct kbase_context *kctx,
 		struct kbase_as *as, const char *reason_str,
 		struct kbase_fault *fault);
 
-/**
- * kbase_mmu_interrupt_process - Process a bus or page fault.
- * @kbdev   The kbase_device the fault happened on
- * @kctx    The kbase_context for the faulting address space if one was found.
- * @as      The address space that has the fault
- * @fault   Data relating to the fault
- *
- * This function will process a fault on a specific address space
- */
-void kbase_mmu_interrupt_process(struct kbase_device *kbdev,
-		struct kbase_context *kctx, struct kbase_as *as,
-		struct kbase_fault *fault);
-
 /**
  * kbase_mmu_switch_to_ir() - Switch to incremental rendering if possible
- * @kctx    The kbase_context for the faulting address space.
- * @reg     Reference of a growable GPU memory region in the same context.
- *          Takes ownership of the reference if successful.
+ * @kctx:	kbase_context for the faulting address space.
+ * @reg:	of a growable GPU memory region in the same context.
+ *		Takes ownership of the reference if successful.
  *
  * Used to switch to incremental rendering if we have nearly run out of
  * virtual address space in a growable memory region.
@@ -60,4 +56,18 @@ void kbase_mmu_interrupt_process(struct kbase_device *kbdev,
 int kbase_mmu_switch_to_ir(struct kbase_context *kctx,
 	struct kbase_va_region *reg);
 
+/**
+ * kbase_mmu_page_fault_worker() - Process a page fault.
+ *
+ * @data:  work_struct passed by queue_work()
+ */
+void kbase_mmu_page_fault_worker(struct work_struct *data);
+
+/**
+ * kbase_mmu_bus_fault_worker() - Process a bus fault.
+ *
+ * @data:  work_struct passed by queue_work()
+ */
+void kbase_mmu_bus_fault_worker(struct work_struct *data);
+
 #endif /* _KBASE_MMU_INTERNAL_H_ */
diff --git a/drivers/gpu/arm/bifrost/platform/devicetree/Kbuild b/drivers/gpu/arm/bifrost/platform/devicetree/Kbuild
index ce637fbb5ef7..78343c0570d1 100644
--- a/drivers/gpu/arm/bifrost/platform/devicetree/Kbuild
+++ b/drivers/gpu/arm/bifrost/platform/devicetree/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2012-2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2012-2017, 2020 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -21,4 +21,5 @@
 
 mali_kbase-y += \
 	$(MALI_PLATFORM_DIR)/mali_kbase_config_devicetree.o \
-	$(MALI_PLATFORM_DIR)/mali_kbase_runtime_pm.o
+	$(MALI_PLATFORM_DIR)/mali_kbase_runtime_pm.o \
+	$(MALI_PLATFORM_DIR)/mali_kbase_clk_rate_trace.o
diff --git a/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_clk_rate_trace.c b/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_clk_rate_trace.c
new file mode 100644
index 000000000000..11a8b77dca06
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_clk_rate_trace.c
@@ -0,0 +1,68 @@
+/*
+ *
+ * (C) COPYRIGHT 2015, 2017-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <linux/clk.h>
+#include "mali_kbase_config_platform.h"
+
+static void *enumerate_gpu_clk(struct kbase_device *kbdev,
+		unsigned int index)
+{
+	if (index >= kbdev->nr_clocks)
+		return NULL;
+
+	return kbdev->clocks[index];
+}
+
+static unsigned long get_gpu_clk_rate(struct kbase_device *kbdev,
+		void *gpu_clk_handle)
+{
+	return clk_get_rate((struct clk *)gpu_clk_handle);
+}
+
+static int gpu_clk_notifier_register(struct kbase_device *kbdev,
+		void *gpu_clk_handle, struct notifier_block *nb)
+{
+	compiletime_assert(offsetof(struct clk_notifier_data, clk) ==
+		offsetof(struct kbase_gpu_clk_notifier_data, gpu_clk_handle),
+		"mismatch in the offset of clk member");
+
+	compiletime_assert(sizeof(((struct clk_notifier_data *)0)->clk) ==
+	     sizeof(((struct kbase_gpu_clk_notifier_data *)0)->gpu_clk_handle),
+	     "mismatch in the size of clk member");
+
+	return clk_notifier_register((struct clk *)gpu_clk_handle, nb);
+}
+
+static void gpu_clk_notifier_unregister(struct kbase_device *kbdev,
+		void *gpu_clk_handle, struct notifier_block *nb)
+{
+	clk_notifier_unregister((struct clk *)gpu_clk_handle, nb);
+}
+
+struct kbase_clk_rate_trace_op_conf clk_rate_trace_ops = {
+	.get_gpu_clk_rate = get_gpu_clk_rate,
+	.enumerate_gpu_clk = enumerate_gpu_clk,
+	.gpu_clk_notifier_register = gpu_clk_notifier_register,
+	.gpu_clk_notifier_unregister = gpu_clk_notifier_unregister,
+};
diff --git a/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_config_platform.h b/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_config_platform.h
index 5990313308fa..2137b425c1ab 100644
--- a/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_config_platform.h
+++ b/drivers/gpu/arm/bifrost/platform/devicetree/mali_kbase_config_platform.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2014-2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2014-2017, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -36,7 +36,10 @@
  */
 #define PLATFORM_FUNCS (NULL)
 
+#define CLK_RATE_TRACE_OPS (&clk_rate_trace_ops)
+
 extern struct kbase_pm_callback_conf pm_callbacks;
+extern struct kbase_clk_rate_trace_op_conf clk_rate_trace_ops;
 
 /**
  * Autosuspend delay
diff --git a/drivers/gpu/arm/bifrost/tests/Kbuild b/drivers/gpu/arm/bifrost/tests/Kbuild
index df16a77a7f66..c26bef780781 100644
--- a/drivers/gpu/arm/bifrost/tests/Kbuild
+++ b/drivers/gpu/arm/bifrost/tests/Kbuild
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2017, 2020 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -21,3 +21,4 @@
 
 obj-$(CONFIG_MALI_KUTF) += kutf/
 obj-$(CONFIG_MALI_IRQ_LATENCY) += mali_kutf_irq_test/
+obj-$(CONFIG_MALI_CLK_RATE_TRACE_PORTAL) += mali_kutf_clk_rate_trace/kernel/
diff --git a/drivers/gpu/arm/bifrost/tests/Kconfig b/drivers/gpu/arm/bifrost/tests/Kconfig
index fa91aea4ac5c..83a4d7764a50 100644
--- a/drivers/gpu/arm/bifrost/tests/Kconfig
+++ b/drivers/gpu/arm/bifrost/tests/Kconfig
@@ -1,5 +1,5 @@
 #
-# (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
+# (C) COPYRIGHT 2017, 2020 ARM Limited. All rights reserved.
 #
 # This program is free software and is provided to you under the terms of the
 # GNU General Public License version 2 as published by the Free Software
@@ -21,3 +21,4 @@
 
 source "drivers/gpu/arm/midgard/tests/kutf/Kconfig"
 source "drivers/gpu/arm/midgard/tests/mali_kutf_irq_test/Kconfig"
+source "drivers/gpu/arm/midgard/tests/mali_kutf_clk_rate_trace/kernel/Kconfig"
diff --git a/drivers/gpu/arm/bifrost/tests/include/kutf/kutf_helpers.h b/drivers/gpu/arm/bifrost/tests/include/kutf/kutf_helpers.h
index 15e168c2385b..858b9c38b49a 100644
--- a/drivers/gpu/arm/bifrost/tests/include/kutf/kutf_helpers.h
+++ b/drivers/gpu/arm/bifrost/tests/include/kutf/kutf_helpers.h
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2017, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -33,6 +33,14 @@
 
 #include <kutf/kutf_suite.h>
 
+/**
+ * kutf_helper_pending_input() - Check any pending lines sent by user space
+ * @context:    KUTF context
+ *
+ * Return: true if there are pending lines, otherwise false
+ */
+bool kutf_helper_pending_input(struct kutf_context *context);
+
 /**
  * kutf_helper_input_dequeue() - Dequeue a line sent by user space
  * @context:    KUTF context
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers.c b/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers.c
index cab5add6d93c..4463b04792f5 100644
--- a/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers.c
+++ b/drivers/gpu/arm/bifrost/tests/kutf/kutf_helpers.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2017 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2017, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -29,10 +29,11 @@
 #include <linux/preempt.h>
 #include <linux/wait.h>
 #include <linux/uaccess.h>
+#include <linux/export.h>
 
 static DEFINE_SPINLOCK(kutf_input_lock);
 
-static bool pending_input(struct kutf_context *context)
+bool kutf_helper_pending_input(struct kutf_context *context)
 {
 	bool input_pending;
 
@@ -44,6 +45,7 @@ static bool pending_input(struct kutf_context *context)
 
 	return input_pending;
 }
+EXPORT_SYMBOL(kutf_helper_pending_input);
 
 char *kutf_helper_input_dequeue(struct kutf_context *context, size_t *str_size)
 {
@@ -59,7 +61,7 @@ char *kutf_helper_input_dequeue(struct kutf_context *context, size_t *str_size)
 		spin_unlock(&kutf_input_lock);
 
 		err = wait_event_interruptible(context->userdata.input_waitq,
-				pending_input(context));
+				kutf_helper_pending_input(context));
 
 		if (err)
 			return ERR_PTR(-EINTR);
diff --git a/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c b/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c
index 3f15669a2a0a..9dc6e2b4bad4 100644
--- a/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c
+++ b/drivers/gpu/arm/bifrost/tests/kutf/kutf_suite.c
@@ -692,12 +692,17 @@ void kutf_add_test_with_filters_and_data(
 	}
 
 	test_func->test_id = id;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 5, 0)
+	debugfs_create_u32("test_id", S_IROTH, test_func->dir,
+                       &test_func->test_id);
+#else
 	tmp = debugfs_create_u32("test_id", S_IROTH, test_func->dir,
 				 &test_func->test_id);
 	if (!tmp) {
 		pr_err("Failed to create debugfs file \"test_id\" when adding test %s\n", name);
 		goto fail_file;
 	}
+#endif
 
 	for (i = 0; i < suite->fixture_variants; i++) {
 		if (create_fixture_variant(test_func, i)) {
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kbuild b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kbuild
new file mode 100644
index 000000000000..f5565d30f9cf
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kbuild
@@ -0,0 +1,26 @@
+#
+# (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, you can access it online at
+# http://www.gnu.org/licenses/gpl-2.0.html.
+#
+# SPDX-License-Identifier: GPL-2.0
+#
+#
+
+ccflags-y += -I$(src)/../include -I$(src)/../../../ -I$(src)/../../ -I$(src)/../../backend/gpu -I$(srctree)/drivers/staging/android
+
+obj-$(CONFIG_MALI_CLK_RATE_TRACE_PORTAL) += mali_kutf_clk_rate_trace_test_portal.o
+
+mali_kutf_clk_rate_trace_test_portal-y := mali_kutf_clk_rate_trace_test.o
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kconfig b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kconfig
new file mode 100644
index 000000000000..8196e4cc6b37
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Kconfig
@@ -0,0 +1,30 @@
+#
+# (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, you can access it online at
+# http://www.gnu.org/licenses/gpl-2.0.html.
+#
+# SPDX-License-Identifier: GPL-2.0
+#
+#
+
+config CONFIG_MALI_CLK_RATE_TRACE_PORTAL
+ tristate "Mali GPU Clock Trace Test portal"
+ depends on MALI_BIFROST && MALI_BIFROST_DEBUG && MALI_KUTF
+ default m
+ help
+   This option will build a test module mali_kutf_clk_rate_trace_test_portal
+   that can test the clocks integration into the platform and exercise some
+   basic trace test in the system. Choosing M here will generate a single
+   module called mali_kutf_clk_rate_trace_test_portal.
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Makefile b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Makefile
new file mode 100644
index 000000000000..71c78b84830c
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/Makefile
@@ -0,0 +1,57 @@
+#
+# (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, you can access it online at
+# http://www.gnu.org/licenses/gpl-2.0.html.
+#
+# SPDX-License-Identifier: GPL-2.0
+#
+#
+
+ifneq ($(KERNELRELEASE),)
+
+ccflags-y := \
+	-DMALI_UNIT_TEST=$(MALI_UNIT_TEST) \
+	-DMALI_CUSTOMER_RELEASE=$(MALI_CUSTOMER_RELEASE) \
+	-I$(src)/../../include \
+	-I$(src)/../../../../../../../include \
+	-I$(src)/../../../../ \
+	-I$(src)/../../../ \
+	-I$(src)/../../../backend/gpu \
+	-I$(src)/../../../debug \
+	-I$(src)/../../../debug/backend \
+	-I$(src)/ \
+	-I$(srctree)/drivers/staging/android \
+	-I$(srctree)/include/linux
+
+obj-m := mali_kutf_clk_rate_trace_test_portal.o
+mali_kutf_clk_rate_trace_test_portal-y := mali_kutf_clk_rate_trace_test.o
+
+else
+# linux build system bootstrap for out-of-tree module
+
+# default to building for the host
+ARCH ?= $(shell uname -m)
+
+ifeq ($(KDIR),)
+$(error Must specify KDIR to point to the kernel to target))
+endif
+
+all:
+	$(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(CURDIR) KBUILD_EXTRA_SYMBOLS="$(CURDIR)/../../kutf/Module.symvers $(CURDIR)/../../../Module.symvers" modules
+
+clean:
+	$(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(CURDIR) clean
+
+endif
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/build.bp b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/build.bp
new file mode 100644
index 000000000000..0cc2904db542
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/build.bp
@@ -0,0 +1,34 @@
+/*
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ */
+
+bob_kernel_module {
+    name: "mali_kutf_clk_rate_trace_test_portal",
+    defaults: [
+        "mali_kbase_shared_config_defaults",
+        "kernel_test_includes",
+    ],
+    srcs: [
+        "../mali_kutf_clk_rate_trace_test.h",
+        "Makefile",
+        "mali_kutf_clk_rate_trace_test.c",
+    ],
+    extra_symbols: [
+        "mali_kbase",
+        "kutf",
+    ],
+    enabled: false,
+    base_build_kutf: {
+        enabled: true,
+        kbuild_options: ["CONFIG_MALI_CLK_RATE_TRACE_PORTAL=m"],
+    },
+}
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c
new file mode 100644
index 000000000000..d74a278bffa7
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/kernel/mali_kutf_clk_rate_trace_test.c
@@ -0,0 +1,890 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include <linux/fdtable.h>
+#include <linux/module.h>
+
+#include <linux/delay.h>
+#include <linux/mutex.h>
+#include <linux/ktime.h>
+#include <linux/version.h>
+#if (KERNEL_VERSION(4, 11, 0) <= LINUX_VERSION_CODE)
+#include <linux/sched/task.h>
+#else
+#include <linux/sched.h>
+#endif
+#include "mali_kbase.h"
+#include "mali_kbase_irq_internal.h"
+#include "mali_kbase_pm_internal.h"
+#include "mali_kbase_clk_rate_trace_mgr.h"
+
+#include <kutf/kutf_suite.h>
+#include <kutf/kutf_utils.h>
+#include <kutf/kutf_helpers.h>
+#include <kutf/kutf_helpers_user.h>
+
+#include "../mali_kutf_clk_rate_trace_test.h"
+
+#define MINOR_FOR_FIRST_KBASE_DEV	(-1)
+
+/* KUTF test application pointer for this test */
+struct kutf_application *kutf_app;
+
+enum portal_server_state {
+	PORTAL_STATE_NO_CLK,
+	PORTAL_STATE_LIVE,
+	PORTAL_STATE_CLOSING,
+};
+
+/**
+ * struct clk_trace_snapshot - Trace info data on a clock.
+ * @previous_rate:   Snapshot start point clock rate.
+ * @current_rate:    End point clock rate. It becomes the start rate of the
+ *                   next trace snapshot.
+ * @rate_up_cnt:     Count in the snapshot duration when the clock trace
+ *                   write is a rate of higher value than the last.
+ * @rate_down_cnt:   Count in the snapshot duration when the clock trace write
+ *                   is a rate of lower value than the last.
+ */
+struct clk_trace_snapshot {
+	unsigned long previous_rate;
+	unsigned long current_rate;
+	u32 rate_up_cnt;
+	u32 rate_down_cnt;
+};
+
+/**
+ * struct kutf_clk_rate_trace_fixture_data - Fixture data for the test.
+ * @kbdev:            kbase device for the GPU.
+ * @listener:         Clock rate change listener structure.
+ * @invoke_notify:    When true, invoke notify command is being executed.
+ * @snapshot:         Clock trace update snapshot data array. A snapshot
+ *                    for each clock contains info accumulated beteen two
+ *                    GET_TRACE_SNAPSHOT requests.
+ * @nclks:            Number of clocks visible to the trace portal.
+ * @pm_ctx_cnt:       Net count of PM (Power Management) context INC/DEC
+ *                    PM_CTX_CNT requests made to the portal. On change from
+ *                    0 to 1 (INC), or, 1 to 0 (DEC), a PM context action is
+ *                    triggered.
+ * @total_update_cnt: Total number of received trace write callbacks.
+ * @server_state:     Portal server operational state.
+ * @result_msg:       Message for the test result.
+ * @test_status:      Portal test reslt status.
+ */
+struct kutf_clk_rate_trace_fixture_data {
+	struct kbase_device *kbdev;
+	struct kbase_clk_rate_listener listener;
+	bool invoke_notify;
+	struct clk_trace_snapshot snapshot[BASE_MAX_NR_CLOCKS_REGULATORS];
+	unsigned int nclks;
+	unsigned int pm_ctx_cnt;
+	unsigned int total_update_cnt;
+	enum portal_server_state server_state;
+	char const *result_msg;
+	enum kutf_result_status test_status;
+};
+
+struct clk_trace_portal_input {
+	struct kutf_helper_named_val cmd_input;
+	enum kbasep_clk_rate_trace_req portal_cmd;
+	int named_val_err;
+};
+
+struct kbasep_cmd_name_pair {
+	enum kbasep_clk_rate_trace_req cmd;
+	const char *name;
+};
+
+struct kbasep_cmd_name_pair kbasep_portal_cmd_name_map[] = {
+			{PORTAL_CMD_GET_CLK_RATE_MGR, GET_CLK_RATE_MGR},
+			{PORTAL_CMD_GET_CLK_RATE_TRACE, GET_CLK_RATE_TRACE},
+			{PORTAL_CMD_GET_TRACE_SNAPSHOT, GET_TRACE_SNAPSHOT},
+			{PORTAL_CMD_INC_PM_CTX_CNT, INC_PM_CTX_CNT},
+			{PORTAL_CMD_DEC_PM_CTX_CNT, DEC_PM_CTX_CNT},
+			{PORTAL_CMD_CLOSE_PORTAL, CLOSE_PORTAL},
+			{PORTAL_CMD_INVOKE_NOTIFY_42KHZ, INVOKE_NOTIFY_42KHZ},
+		};
+
+/* Global pointer for the kutf_portal_trace_write() to use. When
+ * this pointer is engaged, new requests for create fixture will fail
+ * hence limiting the use of the portal at any time to a singleton.
+ */
+struct kutf_clk_rate_trace_fixture_data *g_ptr_portal_data;
+
+#define PORTAL_MSG_LEN (KUTF_MAX_LINE_LENGTH - MAX_REPLY_NAME_LEN)
+static char portal_msg_buf[PORTAL_MSG_LEN];
+
+static void kutf_portal_trace_write(
+	struct kbase_clk_rate_listener *listener,
+	u32 index, u32 new_rate)
+{
+	struct clk_trace_snapshot *snapshot;
+	struct kutf_clk_rate_trace_fixture_data *data = container_of(
+		listener, struct kutf_clk_rate_trace_fixture_data, listener);
+
+	lockdep_assert_held(&data->kbdev->pm.clk_rtm.lock);
+
+	if (WARN_ON(g_ptr_portal_data == NULL))
+		return;
+	if (WARN_ON(index >= g_ptr_portal_data->nclks))
+		return;
+
+	/* This callback is triggered by invoke notify command, skipping */
+	if (data->invoke_notify)
+		return;
+
+	snapshot = &g_ptr_portal_data->snapshot[index];
+	if (new_rate > snapshot->current_rate)
+		snapshot->rate_up_cnt++;
+	else
+		snapshot->rate_down_cnt++;
+	snapshot->current_rate = new_rate;
+	g_ptr_portal_data->total_update_cnt++;
+}
+
+static void kutf_set_pm_ctx_active(struct kutf_context *context)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+
+	if (WARN_ON(data->pm_ctx_cnt != 1))
+		return;
+
+	kbase_pm_context_active(data->kbdev);
+	kbase_pm_wait_for_desired_state(data->kbdev);
+#if !MALI_USE_CSF
+	kbase_pm_request_gpu_cycle_counter(data->kbdev);
+#endif
+}
+
+static void kutf_set_pm_ctx_idle(struct kutf_context *context)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+
+	if (WARN_ON(data->pm_ctx_cnt > 0))
+		return;
+
+	kbase_pm_context_idle(data->kbdev);
+#if !MALI_USE_CSF
+	kbase_pm_release_gpu_cycle_counter(data->kbdev);
+#endif
+}
+
+static char const *kutf_clk_trace_do_change_pm_ctx(struct kutf_context *context,
+				struct clk_trace_portal_input *cmd)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	int seq = cmd->cmd_input.u.val_u64 & 0xFF;
+	const unsigned int cnt = data->pm_ctx_cnt;
+	const enum kbasep_clk_rate_trace_req req = cmd->portal_cmd;
+	char const *errmsg = NULL;
+
+	WARN_ON(req != PORTAL_CMD_INC_PM_CTX_CNT &&
+		req != PORTAL_CMD_DEC_PM_CTX_CNT);
+
+	if (req == PORTAL_CMD_INC_PM_CTX_CNT && cnt < UINT_MAX) {
+		data->pm_ctx_cnt++;
+		if (data->pm_ctx_cnt == 1)
+			kutf_set_pm_ctx_active(context);
+	}
+
+	if (req == PORTAL_CMD_DEC_PM_CTX_CNT && cnt > 0) {
+		data->pm_ctx_cnt--;
+		if (data->pm_ctx_cnt == 0)
+			kutf_set_pm_ctx_idle(context);
+	}
+
+	/* Skip the length check, no chance of overflow for two ints */
+	snprintf(portal_msg_buf, PORTAL_MSG_LEN,
+			"{SEQ:%d, PM_CTX_CNT:%u}", seq, data->pm_ctx_cnt);
+
+	if (kutf_helper_send_named_str(context, "ACK", portal_msg_buf)) {
+		pr_warn("Error in sending ack for adjusting pm_ctx_cnt\n");
+		errmsg = kutf_dsprintf(&context->fixture_pool,
+				"Error in sending ack for adjusting pm_ctx_cnt");
+	}
+
+	return errmsg;
+}
+
+static char const *kutf_clk_trace_do_get_rate(struct kutf_context *context,
+				struct clk_trace_portal_input *cmd)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	struct kbase_device *kbdev = data->kbdev;
+	int seq = cmd->cmd_input.u.val_u64 & 0xFF;
+	unsigned long rate;
+	bool idle;
+	int ret;
+	int i;
+	char const *errmsg = NULL;
+
+	WARN_ON((cmd->portal_cmd != PORTAL_CMD_GET_CLK_RATE_MGR) &&
+		(cmd->portal_cmd != PORTAL_CMD_GET_CLK_RATE_TRACE));
+
+	ret = snprintf(portal_msg_buf, PORTAL_MSG_LEN,
+			"{SEQ:%d, RATE:[", seq);
+
+	for (i = 0; i < data->nclks; i++) {
+		spin_lock(&kbdev->pm.clk_rtm.lock);
+		if (cmd->portal_cmd == PORTAL_CMD_GET_CLK_RATE_MGR)
+			rate = kbdev->pm.clk_rtm.clks[i]->clock_val;
+		else
+			rate = data->snapshot[i].current_rate;
+		idle = kbdev->pm.clk_rtm.gpu_idle;
+		spin_unlock(&kbdev->pm.clk_rtm.lock);
+
+		if ((i + 1) == data->nclks)
+			ret += snprintf(portal_msg_buf + ret,
+				PORTAL_MSG_LEN - ret, "0x%lx], GPU_IDLE:%d}",
+				rate, idle);
+		else
+			ret += snprintf(portal_msg_buf + ret,
+				PORTAL_MSG_LEN - ret, "0x%lx, ", rate);
+
+		if (ret >= PORTAL_MSG_LEN) {
+			pr_warn("Message buf overflow with rate array data\n");
+			return kutf_dsprintf(&context->fixture_pool,
+						"Message buf overflow with rate array data");
+		}
+	}
+
+	if (kutf_helper_send_named_str(context, "ACK", portal_msg_buf)) {
+		pr_warn("Error in sending back rate array\n");
+		errmsg = kutf_dsprintf(&context->fixture_pool,
+				"Error in sending rate array");
+	}
+
+	return errmsg;
+}
+
+/**
+ * kutf_clk_trace_do_get_snapshot() - Send back the current snapshot
+ * @context:  KUTF context
+ * @cmd:      The decoded portal input request
+ *
+ * The accumulated clock rate trace information is kept inside as an snapshot
+ * record. A user request of getting the snapshot marks the closure of the
+ * current snapshot record, and the start of the next one. The response
+ * message contains the current snapshot record, with each clock's
+ * data sequentially placed inside (array marker) [ ].
+ */
+static char const *kutf_clk_trace_do_get_snapshot(struct kutf_context *context,
+				struct clk_trace_portal_input *cmd)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	struct clk_trace_snapshot snapshot;
+	int seq = cmd->cmd_input.u.val_u64 & 0xFF;
+	int ret;
+	int i;
+	char const *fmt;
+	char const *errmsg = NULL;
+
+	WARN_ON(cmd->portal_cmd != PORTAL_CMD_GET_TRACE_SNAPSHOT);
+
+	ret = snprintf(portal_msg_buf, PORTAL_MSG_LEN,
+			"{SEQ:%d, SNAPSHOT_ARRAY:[", seq);
+
+	for (i = 0; i < data->nclks; i++) {
+		spin_lock(&data->kbdev->pm.clk_rtm.lock);
+		/* copy out the snapshot of the clock */
+		snapshot = data->snapshot[i];
+		/* Set the next snapshot start condition */
+		data->snapshot[i].previous_rate = snapshot.current_rate;
+		data->snapshot[i].rate_up_cnt = 0;
+		data->snapshot[i].rate_down_cnt = 0;
+		spin_unlock(&data->kbdev->pm.clk_rtm.lock);
+
+		/* Check i corresponding to the last clock */
+		if ((i + 1) == data->nclks)
+			fmt = "(0x%lx, 0x%lx, %u, %u)]}";
+		else
+			fmt = "(0x%lx, 0x%lx, %u, %u), ";
+		ret += snprintf(portal_msg_buf + ret, PORTAL_MSG_LEN - ret,
+			    fmt, snapshot.previous_rate, snapshot.current_rate,
+			    snapshot.rate_up_cnt, snapshot.rate_down_cnt);
+		if (ret >= PORTAL_MSG_LEN) {
+			pr_warn("Message buf overflow with snapshot data\n");
+			return kutf_dsprintf(&context->fixture_pool,
+					"Message buf overflow with snapshot data");
+		}
+	}
+
+	if (kutf_helper_send_named_str(context, "ACK", portal_msg_buf)) {
+		pr_warn("Error in sending back snapshot array\n");
+		errmsg = kutf_dsprintf(&context->fixture_pool,
+				"Error in sending snapshot array");
+	}
+
+	return errmsg;
+}
+
+/**
+ * kutf_clk_trace_do_invoke_notify_42k() - Invokes the stored notification callback
+ * @context:  KUTF context
+ * @cmd:      The decoded portal input request
+ *
+ * Invokes frequency change notification callbacks with a fake
+ * GPU frequency 42 kHz for the top clock domain.
+ */
+static char const *kutf_clk_trace_do_invoke_notify_42k(
+	struct kutf_context *context,
+	struct clk_trace_portal_input *cmd)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	int seq = cmd->cmd_input.u.val_u64 & 0xFF;
+	const unsigned long new_rate_hz = 42000;
+	int ret;
+	char const *errmsg = NULL;
+	struct kbase_clk_rate_trace_manager *clk_rtm = &data->kbdev->pm.clk_rtm;
+
+	WARN_ON(cmd->portal_cmd != PORTAL_CMD_INVOKE_NOTIFY_42KHZ);
+
+	spin_lock(&clk_rtm->lock);
+
+	data->invoke_notify = true;
+	kbase_clk_rate_trace_manager_notify_all(
+		clk_rtm, 0, new_rate_hz);
+	data->invoke_notify = false;
+
+	spin_unlock(&clk_rtm->lock);
+
+	ret = snprintf(portal_msg_buf, PORTAL_MSG_LEN,
+		       "{SEQ:%d, HZ:%lu}", seq, new_rate_hz);
+
+	if (ret >= PORTAL_MSG_LEN) {
+		pr_warn("Message buf overflow with invoked data\n");
+		return kutf_dsprintf(&context->fixture_pool,
+				"Message buf overflow with invoked data");
+	}
+
+	if (kutf_helper_send_named_str(context, "ACK", portal_msg_buf)) {
+		pr_warn("Error in sending ack for " INVOKE_NOTIFY_42KHZ "request\n");
+		errmsg = kutf_dsprintf(&context->fixture_pool,
+			"Error in sending ack for " INVOKE_NOTIFY_42KHZ "request");
+	}
+
+	return errmsg;
+}
+
+static char const *kutf_clk_trace_do_close_portal(struct kutf_context *context,
+				struct clk_trace_portal_input *cmd)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	int seq = cmd->cmd_input.u.val_u64 & 0xFF;
+	char const *errmsg = NULL;
+
+	WARN_ON(cmd->portal_cmd != PORTAL_CMD_CLOSE_PORTAL);
+
+	data->server_state = PORTAL_STATE_CLOSING;
+
+	/* Skip the length check, no chance of overflow for two ints */
+	snprintf(portal_msg_buf, PORTAL_MSG_LEN,
+			"{SEQ:%d, PM_CTX_CNT:%u}", seq, data->pm_ctx_cnt);
+
+	if (kutf_helper_send_named_str(context, "ACK", portal_msg_buf)) {
+		pr_warn("Error in sending ack for " CLOSE_PORTAL "reuquest\n");
+		errmsg = kutf_dsprintf(&context->fixture_pool,
+			"Error in sending ack for " CLOSE_PORTAL "reuquest");
+	}
+
+	return errmsg;
+}
+
+static bool kutf_clk_trace_dequeue_portal_cmd(struct kutf_context *context,
+				struct clk_trace_portal_input *cmd)
+{
+	int i;
+	int err = kutf_helper_receive_named_val(context, &cmd->cmd_input);
+
+	cmd->named_val_err = err;
+	if (err == KUTF_HELPER_ERR_NONE &&
+		cmd->cmd_input.type == KUTF_HELPER_VALTYPE_U64) {
+		/* All portal request commands are of format (named u64):
+		 *   CMD_NAME=1234
+		 * where, 1234 is a (variable) sequence number tag.
+		 */
+		for (i = 0; i < PORTAL_TOTAL_CMDS; i++) {
+			if (strcmp(cmd->cmd_input.val_name,
+				kbasep_portal_cmd_name_map[i].name))
+				continue;
+
+			cmd->portal_cmd = kbasep_portal_cmd_name_map[i].cmd;
+			return true;
+		}
+	}
+
+	cmd->portal_cmd = PORTAL_CMD_INVALID;
+	return false;
+}
+
+static void kutf_clk_trace_flag_result(struct kutf_context *context,
+			enum kutf_result_status result, char const *msg)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+
+	if (result > data->test_status) {
+		data->test_status = result;
+		if (msg)
+			data->result_msg = msg;
+		if (data->server_state == PORTAL_STATE_LIVE &&
+			result > KUTF_RESULT_WARN) {
+			data->server_state = PORTAL_STATE_CLOSING;
+		}
+	}
+}
+
+static bool kutf_clk_trace_process_portal_cmd(struct kutf_context *context,
+				struct clk_trace_portal_input *cmd)
+{
+	char const *errmsg = NULL;
+
+	BUILD_BUG_ON(ARRAY_SIZE(kbasep_portal_cmd_name_map) !=
+				PORTAL_TOTAL_CMDS);
+	WARN_ON(cmd->portal_cmd == PORTAL_CMD_INVALID);
+
+	switch (cmd->portal_cmd) {
+	case PORTAL_CMD_GET_CLK_RATE_MGR:
+		/* Fall through */
+	case PORTAL_CMD_GET_CLK_RATE_TRACE:
+		errmsg = kutf_clk_trace_do_get_rate(context, cmd);
+		break;
+	case PORTAL_CMD_GET_TRACE_SNAPSHOT:
+		errmsg = kutf_clk_trace_do_get_snapshot(context, cmd);
+		break;
+	case PORTAL_CMD_INC_PM_CTX_CNT:
+		/* Fall through */
+	case PORTAL_CMD_DEC_PM_CTX_CNT:
+		errmsg = kutf_clk_trace_do_change_pm_ctx(context, cmd);
+		break;
+	case PORTAL_CMD_CLOSE_PORTAL:
+		errmsg = kutf_clk_trace_do_close_portal(context, cmd);
+		break;
+	case PORTAL_CMD_INVOKE_NOTIFY_42KHZ:
+		errmsg = kutf_clk_trace_do_invoke_notify_42k(context, cmd);
+		break;
+	default:
+		pr_warn("Don't know how to handle portal_cmd: %d, abort session.\n",
+				cmd->portal_cmd);
+		errmsg = kutf_dsprintf(&context->fixture_pool,
+				"Don't know how to handle portal_cmd: %d",
+				cmd->portal_cmd);
+		break;
+	}
+
+	if (errmsg)
+		kutf_clk_trace_flag_result(context, KUTF_RESULT_FAIL, errmsg);
+
+	return (errmsg == NULL);
+}
+
+/**
+ * kutf_clk_trace_do_nack_response() - respond a NACK to erroneous input
+ * @context:  KUTF context
+ * @cmd:      The erroneous input request
+ *
+ * This function deal with an erroneous input request, and respond with
+ * a proper 'NACK' message.
+ */
+static int kutf_clk_trace_do_nack_response(struct kutf_context *context,
+				struct clk_trace_portal_input *cmd)
+{
+	int seq;
+	int err;
+	char const *errmsg = NULL;
+
+	WARN_ON(cmd->portal_cmd != PORTAL_CMD_INVALID);
+
+	if (cmd->named_val_err == KUTF_HELPER_ERR_NONE &&
+			  cmd->cmd_input.type == KUTF_HELPER_VALTYPE_U64) {
+		/* Keep seq number as % 256 */
+		seq = cmd->cmd_input.u.val_u64 & 255;
+		snprintf(portal_msg_buf, PORTAL_MSG_LEN,
+				 "{SEQ:%d, MSG: Unknown command '%s'.}", seq,
+				 cmd->cmd_input.val_name);
+		err = kutf_helper_send_named_str(context, "NACK",
+						portal_msg_buf);
+	} else
+		err = kutf_helper_send_named_str(context, "NACK",
+			"Wrong portal cmd format (Ref example: CMD_NAME=0X16)");
+
+	if (err) {
+		errmsg = kutf_dsprintf(&context->fixture_pool,
+						"Failed to send portal NACK response");
+		kutf_clk_trace_flag_result(context, KUTF_RESULT_FAIL, errmsg);
+	}
+
+	return err;
+}
+
+/**
+ * kutf_clk_trace_barebone_check() - Sanity test on the clock tracing
+ * @context:	KUTF context
+ *
+ * This function carries out some basic test on the tracing operation:
+ *     1). GPU idle on test start, trace rate should be 0 (low power state)
+ *     2). Make sure GPU is powered up, the trace rate should match
+ *         that from the clcok manager's internal recorded rate
+ *     3). If the GPU active transition occurs following 2), there
+ *         must be rate change event from tracing.
+ */
+void kutf_clk_trace_barebone_check(struct kutf_context *context)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	struct kbase_device *kbdev = data->kbdev;
+	bool fail = false;
+	bool idle[2] = { false };
+	char const *msg = NULL;
+	int i;
+
+	/* Check consistency if gpu happens to be idle */
+	spin_lock(&kbdev->pm.clk_rtm.lock);
+	idle[0] = kbdev->pm.clk_rtm.gpu_idle;
+	if (kbdev->pm.clk_rtm.gpu_idle) {
+		for (i = 0; i < data->nclks; i++) {
+			if (data->snapshot[i].current_rate) {
+				/* Idle should have a rate 0 */
+				fail = true;
+				break;
+			}
+		}
+	}
+	spin_unlock(&kbdev->pm.clk_rtm.lock);
+	if (fail) {
+		msg = kutf_dsprintf(&context->fixture_pool,
+				"GPU Idle not yielding 0-rate");
+		pr_err("Trace did not see idle rate\n");
+	} else {
+		/* Make local PM active if not done so yet */
+		if (data->pm_ctx_cnt == 0) {
+			/* Ensure the GPU is powered */
+			data->pm_ctx_cnt++;
+			kutf_set_pm_ctx_active(context);
+		}
+		/* Checking the rate is consistent */
+		spin_lock(&kbdev->pm.clk_rtm.lock);
+		idle[1] = kbdev->pm.clk_rtm.gpu_idle;
+		for (i = 0; i < data->nclks; i++) {
+			/* Rate match between the manager and the trace */
+			if (kbdev->pm.clk_rtm.clks[i]->clock_val !=
+				data->snapshot[i].current_rate) {
+				fail = true;
+				break;
+			}
+		}
+		spin_unlock(&kbdev->pm.clk_rtm.lock);
+
+		if (idle[1]) {
+			msg = kutf_dsprintf(&context->fixture_pool,
+				"GPU still idle after set_pm_ctx_active");
+			pr_err("GPU still idle after set_pm_ctx_active\n");
+		}
+
+		if (!msg && fail) {
+			msg = kutf_dsprintf(&context->fixture_pool,
+				"Trace rate not matching Clk manager's read");
+			pr_err("Trace rate not matching Clk manager's read\n");
+		}
+	}
+
+	if (!msg && idle[0] && !idle[1] && !data->total_update_cnt) {
+		msg = kutf_dsprintf(&context->fixture_pool,
+				"Trace update did not occur");
+		pr_err("Trace update did not occur\n");
+	}
+	if (msg)
+		kutf_clk_trace_flag_result(context, KUTF_RESULT_FAIL, msg);
+	else if (!data->total_update_cnt) {
+		msg = kutf_dsprintf(&context->fixture_pool,
+				"No trace update seen during the test!");
+		kutf_clk_trace_flag_result(context, KUTF_RESULT_WARN, msg);
+	}
+}
+
+static bool kutf_clk_trace_end_of_stream(struct clk_trace_portal_input *cmd)
+{
+	return (cmd->named_val_err == -EBUSY);
+}
+
+void kutf_clk_trace_no_clks_dummy(struct kutf_context *context)
+{
+	struct clk_trace_portal_input cmd;
+	unsigned long timeout = jiffies + HZ * 2;
+	bool has_cmd;
+
+	while (time_before(jiffies, timeout)) {
+		if (kutf_helper_pending_input(context)) {
+			has_cmd = kutf_clk_trace_dequeue_portal_cmd(context,
+									&cmd);
+			if (!has_cmd && kutf_clk_trace_end_of_stream(&cmd))
+				break;
+
+			kutf_helper_send_named_str(context, "NACK",
+				"Fatal! No clocks visible, aborting");
+		}
+		msleep(20);
+	}
+
+	kutf_clk_trace_flag_result(context, KUTF_RESULT_FATAL,
+				"No clocks visble to the portal");
+}
+
+/**
+ * mali_kutf_clk_rate_trace_test_portal() - Service portal input
+ * @context:	KUTF context
+ *
+ * The test portal operates on input requests. If the input request is one
+ * of the recognized portal commands, it handles it accordingly. Otherwise
+ * a negative response 'NACK' is returned. The portal service terminates
+ * when a 'CLOSE_PORTAL' request is received, or due to an internal error.
+ * Both case would result in the server_state transitioned to CLOSING.
+ *
+ * If the portal is closed on request, a sanity test on the clock rate
+ * trace operation is undertaken via function:
+ *    kutf_clk_trace_barebone_check();
+ */
+static void mali_kutf_clk_rate_trace_test_portal(struct kutf_context *context)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	struct clk_trace_portal_input new_cmd;
+
+	pr_debug("Test portal service start\n");
+
+	while (data->server_state == PORTAL_STATE_LIVE) {
+		if (kutf_clk_trace_dequeue_portal_cmd(context, &new_cmd))
+			kutf_clk_trace_process_portal_cmd(context, &new_cmd);
+		else if (kutf_clk_trace_end_of_stream(&new_cmd))
+			/* Dequeue on portal input, end of stream */
+			data->server_state = PORTAL_STATE_CLOSING;
+		else
+			kutf_clk_trace_do_nack_response(context, &new_cmd);
+	}
+
+	/* Closing, exhausting all the pending inputs with NACKs. */
+	if (data->server_state == PORTAL_STATE_CLOSING) {
+		while (kutf_helper_pending_input(context) &&
+		       (kutf_clk_trace_dequeue_portal_cmd(context, &new_cmd) ||
+				!kutf_clk_trace_end_of_stream(&new_cmd))) {
+			kutf_helper_send_named_str(context, "NACK",
+					"Portal closing down");
+		}
+	}
+
+	/* If no portal error, do a barebone test here irrespective
+	 * whatever the portal live session has been testing, which
+	 * is entirely driven by the user-side via portal requests.
+	 */
+	if (data->test_status <= KUTF_RESULT_WARN) {
+		if (data->server_state != PORTAL_STATE_NO_CLK)
+			kutf_clk_trace_barebone_check(context);
+		else {
+			/* No clocks case, NACK 2-sec for the fatal situation */
+			kutf_clk_trace_no_clks_dummy(context);
+		}
+	}
+
+	/* If we have changed pm_ctx count, drop it back */
+	if (data->pm_ctx_cnt) {
+		/* Although we count on portal requests, it only has material
+		 * impact when from 0 -> 1. So the reverse is a simple one off.
+		 */
+		data->pm_ctx_cnt = 0;
+		kutf_set_pm_ctx_idle(context);
+	}
+
+	/* Finally log the test result line */
+	if (data->test_status < KUTF_RESULT_WARN)
+		kutf_test_pass(context, data->result_msg);
+	else if (data->test_status == KUTF_RESULT_WARN)
+		kutf_test_warn(context, data->result_msg);
+	else if (data->test_status == KUTF_RESULT_FATAL)
+		kutf_test_fatal(context, data->result_msg);
+	else
+		kutf_test_fail(context, data->result_msg);
+
+	pr_debug("Test end\n");
+}
+
+/**
+ * mali_kutf_clk_rate_trace_create_fixture() - Creates the fixture data
+ *                           required for mali_kutf_clk_rate_trace_test_portal.
+ * @context:	KUTF context.
+ *
+ * Return: Fixture data created on success or NULL on failure
+ */
+static void *mali_kutf_clk_rate_trace_create_fixture(
+		struct kutf_context *context)
+{
+	struct kutf_clk_rate_trace_fixture_data *data;
+	struct kbase_device *kbdev;
+	unsigned long rate;
+	int i;
+
+	/* Acquire the kbase device */
+	pr_debug("Finding device\n");
+	kbdev = kbase_find_device(MINOR_FOR_FIRST_KBASE_DEV);
+	if (kbdev == NULL) {
+		kutf_test_fail(context, "Failed to find kbase device");
+		return NULL;
+	}
+
+	pr_debug("Creating fixture\n");
+	data = kutf_mempool_alloc(&context->fixture_pool,
+			sizeof(struct kutf_clk_rate_trace_fixture_data));
+	if (!data)
+		return NULL;
+
+	*data = (const struct kutf_clk_rate_trace_fixture_data) { 0 };
+	pr_debug("Hooking up the test portal to kbdev clk rate trace\n");
+	spin_lock(&kbdev->pm.clk_rtm.lock);
+
+	if (g_ptr_portal_data != NULL) {
+		pr_warn("Test portal is already in use, run aborted\n");
+		kutf_test_fail(context, "Portal allows single session only");
+		spin_unlock(&kbdev->pm.clk_rtm.lock);
+		return NULL;
+	}
+
+	for (i = 0; i < BASE_MAX_NR_CLOCKS_REGULATORS; i++) {
+		if (kbdev->pm.clk_rtm.clks[i]) {
+			data->nclks++;
+			if (kbdev->pm.clk_rtm.gpu_idle)
+				rate = 0;
+			else
+				rate = kbdev->pm.clk_rtm.clks[i]->clock_val;
+			data->snapshot[i].previous_rate = rate;
+			data->snapshot[i].current_rate = rate;
+		}
+	}
+
+	spin_unlock(&kbdev->pm.clk_rtm.lock);
+
+	if (data->nclks) {
+		/* Subscribe this test server portal */
+		data->listener.notify = kutf_portal_trace_write;
+		data->invoke_notify = false;
+
+		kbase_clk_rate_trace_manager_subscribe(
+			&kbdev->pm.clk_rtm, &data->listener);
+		/* Update the kutf_server_portal fixture_data pointer */
+		g_ptr_portal_data = data;
+	}
+
+	data->kbdev = kbdev;
+	data->result_msg = NULL;
+	data->test_status = KUTF_RESULT_PASS;
+
+	if (data->nclks == 0) {
+		data->server_state = PORTAL_STATE_NO_CLK;
+		pr_debug("Kbdev has no clocks for rate trace");
+	} else
+		data->server_state = PORTAL_STATE_LIVE;
+
+	pr_debug("Created fixture\n");
+
+	return data;
+}
+
+/**
+ * Destroy fixture data previously created by
+ * mali_kutf_clk_rate_trace_create_fixture.
+ *
+ * @context:             KUTF context.
+ */
+static void mali_kutf_clk_rate_trace_remove_fixture(
+		struct kutf_context *context)
+{
+	struct kutf_clk_rate_trace_fixture_data *data = context->fixture;
+	struct kbase_device *kbdev = data->kbdev;
+
+	if (data->nclks) {
+		/* Clean up the portal trace write arrangement */
+		g_ptr_portal_data = NULL;
+
+		kbase_clk_rate_trace_manager_unsubscribe(
+			&kbdev->pm.clk_rtm, &data->listener);
+	}
+	pr_debug("Destroying fixture\n");
+	kbase_release_device(kbdev);
+	pr_debug("Destroyed fixture\n");
+}
+
+/**
+ * mali_kutf_clk_rate_trace_test_module_init() - Entry point for test mdoule.
+ */
+int mali_kutf_clk_rate_trace_test_module_init(void)
+{
+	struct kutf_suite *suite;
+	unsigned int filters;
+	union kutf_callback_data suite_data = { 0 };
+
+	pr_debug("Creating app\n");
+
+	g_ptr_portal_data = NULL;
+	kutf_app = kutf_create_application(CLK_RATE_TRACE_APP_NAME);
+
+	if (!kutf_app) {
+		pr_warn("Creation of app " CLK_RATE_TRACE_APP_NAME
+				" failed!\n");
+		return -ENOMEM;
+	}
+
+	pr_debug("Create suite %s\n", CLK_RATE_TRACE_SUITE_NAME);
+	suite = kutf_create_suite_with_filters_and_data(
+			kutf_app, CLK_RATE_TRACE_SUITE_NAME, 1,
+			mali_kutf_clk_rate_trace_create_fixture,
+			mali_kutf_clk_rate_trace_remove_fixture,
+			KUTF_F_TEST_GENERIC,
+			suite_data);
+
+	if (!suite) {
+		pr_warn("Creation of suite %s failed!\n",
+				CLK_RATE_TRACE_SUITE_NAME);
+		kutf_destroy_application(kutf_app);
+		return -ENOMEM;
+	}
+
+	filters = suite->suite_default_flags;
+	kutf_add_test_with_filters(
+			suite, 0x0, CLK_RATE_TRACE_PORTAL,
+			mali_kutf_clk_rate_trace_test_portal,
+			filters);
+
+	pr_debug("Init complete\n");
+	return 0;
+}
+
+/**
+ * mali_kutf_clk_rate_trace_test_module_exit() - Module exit point for this
+ *                                               test.
+ */
+void mali_kutf_clk_rate_trace_test_module_exit(void)
+{
+	pr_debug("Exit start\n");
+	kutf_destroy_application(kutf_app);
+	pr_debug("Exit complete\n");
+}
+
+
+module_init(mali_kutf_clk_rate_trace_test_module_init);
+module_exit(mali_kutf_clk_rate_trace_test_module_exit);
+
+MODULE_LICENSE("GPL");
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/mali_kutf_clk_rate_trace_test.h b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/mali_kutf_clk_rate_trace_test.h
new file mode 100644
index 000000000000..f46afd5086bd
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_clk_rate_trace/mali_kutf_clk_rate_trace_test.h
@@ -0,0 +1,148 @@
+/*
+ *
+ * (C) COPYRIGHT 2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#ifndef _KUTF_CLK_RATE_TRACE_TEST_H_
+#define _KUTF_CLK_RATE_TRACE_TEST_H_
+
+#define CLK_RATE_TRACE_APP_NAME "clk_rate_trace"
+#define CLK_RATE_TRACE_SUITE_NAME "rate_trace"
+#define CLK_RATE_TRACE_PORTAL "portal"
+
+/**
+ * enum kbasep_clk_rate_trace_req - request command to the clock rate trace
+ *                                  service portal.
+ *
+ * @PORTAL_CMD_GET_CLK_RATE_MGR:   Request the clock trace manager internal
+ *                                 data record. On a positive acknowledgement
+ *                                 the prevailing clock rates and the GPU idle
+ *                                 condition flag are returned.
+ * @PORTAL_CMD_GET_CLK_RATE_TRACE: Request the clock trace portal to return its
+ *                                 data record. On a positive acknowledgement
+ *                                 the last trace recorded clock rates and the
+ *                                 GPU idle condition flag are returned.
+ * @PORTAL_CMD_GET_TRACE_SNAPSHOT: Request the clock trace portal to return its
+ *                                 current snapshot data record. On a positive
+ *                                 acknowledgement the snapshot array matching
+ *                                 the number of clocks are returned. It also
+ *                                 starts a fresh snapshot inside the clock
+ *                                 trace portal.
+ * @PORTAL_CMD_INC_PM_CTX_CNT:     Request the clock trace portal to increase
+ *                                 its internal PM_CTX_COUNT. If this increase
+ *                                 yielded a count of 0 -> 1 change, the portal
+ *                                 will initiate a PM_CTX_ACTIVE call to the
+ *                                 Kbase power management. Futher increase
+ *                                 requests will limit to only affect the
+ *                                 portal internal count value.
+ * @PORTAL_CMD_DEC_PM_CTX_CNT:     Request the clock trace portal to decrease
+ *                                 its internal PM_CTX_COUNT. If this decrease
+ *                                 yielded a count of 1 -> 0 change, the portal
+ *                                 will initiate a PM_CTX_IDLE call to the
+ *                                 Kbase power management.
+ * @PORTAL_CMD_CLOSE_PORTAL:       Inform the clock trace portal service the
+ *                                 client has completed its session. The portal
+ *                                 will start the close down action. If no
+ *                                 error has occurred during the dynamic
+ *                                 interactive session, an inherent basic test
+ *                                 carrying out some sanity check on the clock
+ *                                 trace is undertaken.
+ * @PORTAL_CMD_INVOKE_NOTIFY_42KHZ: Invokes all clock rate trace manager callbacks
+ *                                 for the top clock domain with a new GPU frequency
+ *                                 set to 42 kHZ.
+ * @PORTAL_CMD_INVALID:            Valid commands termination marker. Must be
+ *                                 the highest enumeration value, as it
+ *                                 represents valid command array size.
+ * @PORTAL_TOTAL_CMDS:             Alias of PORTAL_CMD_INVALID.
+ */
+/* PORTAL_CMD_INVALID must be the last one, serving the size */
+enum kbasep_clk_rate_trace_req {
+	PORTAL_CMD_GET_CLK_RATE_MGR,
+	PORTAL_CMD_GET_CLK_RATE_TRACE,
+	PORTAL_CMD_GET_TRACE_SNAPSHOT,
+	PORTAL_CMD_INC_PM_CTX_CNT,
+	PORTAL_CMD_DEC_PM_CTX_CNT,
+	PORTAL_CMD_CLOSE_PORTAL,
+	PORTAL_CMD_INVOKE_NOTIFY_42KHZ,
+	PORTAL_CMD_INVALID,
+	PORTAL_TOTAL_CMDS = PORTAL_CMD_INVALID,
+};
+
+/**
+ * Portal service request command names. The portal request consists of a kutf
+ * named u64-value. For those above enumerated PORTAL_CMD, the names defined
+ * here are used to mark the name and then followed with a sequence number
+ * value. Example (manual script here for illustration):
+ *   exec 5<>run                   # open the portal kutf run as fd-5
+ *   echo GET_CLK_RATE_MGR=1 >&5   # send the cmd and sequence number 1
+ *   head -n 1 <&5                 # read back the 1-line server reseponse
+ *     ACK="{SEQ:1, RATE:[0x1ad27480], GPU_IDLE:1}"   # response string
+ *   echo GET_TRACE_SNAPSHOT=1 >&5 # send the cmd and sequence number 1
+ *   head -n 1 <&5                 # read back the 1-line server reseponse
+ *     ACK="{SEQ:1, SNAPSHOT_ARRAY:[(0x0, 0x1ad27480, 1, 0)]}"
+ *   echo CLOSE_PORTAL=1 >&5       # close the portal
+ *   cat <&5                       # read back all the response lines
+ *     ACK="{SEQ:1, PM_CTX_CNT:0}"      # response to close command
+ *     KUTF_RESULT_PASS:(explicit pass) # internal sanity test passed.
+ *   exec 5>&-                     # close the service portal fd.
+ *
+ * Expected request command return format:
+ *  GET_CLK_RATE_MGR:   ACK="{SEQ:12, RATE:[1080, 1280], GPU_IDLE:1}"
+ *    Note, the above contains 2-clock with rates in [], GPU idle
+ *  GET_CLK_RATE_TRACE: ACK="{SEQ:6, RATE:[0x1ad27480], GPU_IDLE:0}"
+ *    Note, 1-clock with rate in [], GPU not idle
+ *  GET_TRACE_SNAPSHOT: ACK="{SEQ:8, SNAPSHOT_ARRAY:[(0x0, 0x1ad27480, 1, 0)]}"
+ *    Note, 1-clock, (start_rate : 0,  last_rate : 0x1ad27480,
+ *                    trace_rate_up_count: 1, trace_rate_down_count : 0)
+ *    For the specific sample case here, there is a single rate_trace event
+ *    that yielded a rate increase change. No rate drop event recorded in the
+ *    reporting snapshot duration.
+ *  INC_PM_CTX_CNT:     ACK="{SEQ:1, PM_CTX_CNT:1}"
+ *    Note, after the increment, M_CTX_CNT is 1. (i.e. 0 -> 1)
+ *  DEC_PM_CTX_CNT:     ACK="{SEQ:3, PM_CTX_CNT:0}"
+ *    Note, after the decrement, PM_CTX_CNT is 0. (i.e. 1 -> 0)
+ *  CLOSE_PORTAL:       ACK="{SEQ:1, PM_CTX_CNT:1}"
+ *    Note, at the close, PM_CTX_CNT is 1. The PM_CTX_CNT will internally be
+ *    dropped down to 0 as part of the portal close clean up.
+ */
+#define GET_CLK_RATE_MGR     "GET_CLK_RATE_MGR"
+#define GET_CLK_RATE_TRACE   "GET_CLK_RATE_TRACE"
+#define GET_TRACE_SNAPSHOT   "GET_TRACE_SNAPSHOT"
+#define INC_PM_CTX_CNT       "INC_PM_CTX_CNT"
+#define DEC_PM_CTX_CNT       "DEC_PM_CTX_CNT"
+#define CLOSE_PORTAL         "CLOSE_PORTAL"
+#define INVOKE_NOTIFY_42KHZ  "INVOKE_NOTIFY_42KHZ"
+
+/**
+ * Portal service response tag names. The response consists of a kutf
+ * named string-value. In case of a 'NACK' (negative acknowledgement), it
+ * can be one of the two formats:
+ *   1. NACK="{SEQ:2, MSG:xyzed}"     # NACK on command with sequence tag-2.
+ *      Note, the portal has received a valid name and valid sequence number
+ *            but can't carry-out the request, reason in the MSG field.
+ *   2. NACK="Failing-message"
+ *      Note, unable to parse a valid name or valid sequence number,
+ *            or some internal error condition. Reason in the quoted string.
+ */
+#define ACK "ACK"
+#define NACK "NACK"
+#define MAX_REPLY_NAME_LEN 32
+
+#endif /* _KUTF_CLK_RATE_TRACE_TEST_H_ */
diff --git a/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c b/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
index 26b442a82fdd..5f27c3a7e9b2 100644
--- a/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
+++ b/drivers/gpu/arm/bifrost/tests/mali_kutf_irq_test/mali_kutf_irq_test_main.c
@@ -1,6 +1,6 @@
 /*
  *
- * (C) COPYRIGHT 2016-2020 ARM Limited. All rights reserved.
+ * (C) COPYRIGHT 2016-2018, 2020 ARM Limited. All rights reserved.
  *
  * This program is free software and is provided to you under the terms of the
  * GNU General Public License version 2 as published by the Free Software
@@ -25,7 +25,7 @@
 #include <linux/interrupt.h>
 
 #include "mali_kbase.h"
-#include <midgard/backend/gpu/mali_kbase_device_internal.h>
+#include <midgard/device/mali_kbase_device.h>
 #include <midgard/backend/gpu/mali_kbase_pm_internal.h>
 
 #include <kutf/kutf_suite.h>
diff --git a/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c b/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c
index f266d8e6f5de..cd90ea0ec285 100644
--- a/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c
+++ b/drivers/gpu/arm/bifrost/thirdparty/mali_kbase_mmap.c
@@ -317,6 +317,7 @@ unsigned long kbase_context_get_unmapped_area(struct kbase_context *const kctx,
 					align_mask = align_offset - 1;
 					is_shader_code = true;
 				}
+#if !MALI_USE_CSF
 			} else if (reg->flags & KBASE_REG_TILER_ALIGN_TOP) {
 				unsigned long extent_bytes =
 				     (unsigned long)(reg->extent << PAGE_SHIFT);
@@ -330,6 +331,7 @@ unsigned long kbase_context_get_unmapped_area(struct kbase_context *const kctx,
 				align_mask = extent_bytes - 1;
 				align_offset =
 				      extent_bytes - (reg->initial_commit << PAGE_SHIFT);
+#endif /* !MALI_USE_CSF */
 			} else if (reg->flags & KBASE_REG_GPU_VA_SAME_4GB_PAGE) {
 				is_same_4gb_page = true;
 			}
diff --git a/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c b/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c
new file mode 100644
index 000000000000..abaa6bb12b9d
--- /dev/null
+++ b/drivers/gpu/arm/bifrost/tl/backend/mali_kbase_timeline_csf.c
@@ -0,0 +1,172 @@
+/*
+ *
+ * (C) COPYRIGHT 2019-2020 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
+ *
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ */
+
+#include "../mali_kbase_tracepoints.h"
+#include "../mali_kbase_timeline.h"
+#include "../mali_kbase_timeline_priv.h"
+
+#include <mali_kbase.h>
+
+void kbase_create_timeline_objects(struct kbase_device *kbdev)
+{
+	unsigned int as_nr;
+	unsigned int slot_i;
+	struct kbase_context *kctx;
+	struct kbase_timeline *timeline = kbdev->timeline;
+	struct kbase_tlstream *summary =
+		&kbdev->timeline->streams[TL_STREAM_TYPE_OBJ_SUMMARY];
+
+	/* Summarize the Address Space objects. */
+	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
+		__kbase_tlstream_tl_new_as(summary, &kbdev->as[as_nr], as_nr);
+
+	/* Create Legacy GPU object to track in AOM for dumping */
+	__kbase_tlstream_tl_new_gpu(summary,
+			kbdev,
+			kbdev->gpu_props.props.raw_props.gpu_id,
+			kbdev->gpu_props.num_cores);
+
+
+	for (as_nr = 0; as_nr < kbdev->nr_hw_address_spaces; as_nr++)
+		__kbase_tlstream_tl_lifelink_as_gpu(summary,
+				&kbdev->as[as_nr],
+				kbdev);
+
+	/* Trace the creation of a new kbase device and set its properties. */
+	__kbase_tlstream_tl_kbase_new_device(summary,
+		kbdev->gpu_props.props.raw_props.gpu_id,
+		kbdev->gpu_props.num_cores, kbdev->csf.global_iface.group_num,
+		kbdev->nr_hw_address_spaces);
+
+	/* Lock the context list, to ensure no changes to the list are made
+	 * while we're summarizing the contexts and their contents.
+	 */
+	mutex_lock(&kbdev->kctx_list_lock);
+
+	/* Hold the scheduler lock while we emit the current state
+	 * We also need to continue holding the lock until after the first body
+	 * stream tracepoints are emitted to ensure we don't change the
+	 * scheduler until after then
+	 */
+	mutex_lock(&kbdev->csf.scheduler.lock);
+
+	for (slot_i = 0; slot_i < kbdev->csf.global_iface.group_num; slot_i++) {
+
+		struct kbase_queue_group *group =
+			kbdev->csf.scheduler.csg_slots[slot_i].resident_group;
+
+		if (group)
+			__kbase_tlstream_tl_kbase_device_program_csg(summary,
+				kbdev->gpu_props.props.raw_props.gpu_id,
+				group->handle, slot_i);
+	}
+
+	/* Reset body stream buffers while holding the kctx lock.
+	 * As we are holding the lock, we can guarantee that no kctx creation or
+	 * deletion tracepoints can be fired from outside of this function by
+	 * some other thread.
+	 */
+	kbase_timeline_streams_body_reset(timeline);
+
+	mutex_unlock(&kbdev->csf.scheduler.lock);
+
+	/* For each context in the device... */
+	list_for_each_entry(kctx, &kbdev->kctx_list, kctx_list_link) {
+		size_t i;
+		struct kbase_tlstream *body =
+			&timeline->streams[TL_STREAM_TYPE_OBJ];
+
+		/* Lock the context's KCPU queues, to ensure no KCPU-queue
+		 * related actions can occur in this context from now on.
+		 */
+		mutex_lock(&kctx->csf.kcpu_queues.lock);
+
+		/* Acquire the MMU lock, to ensure we don't get a concurrent
+		 * address space assignment while summarizing this context's
+		 * address space.
+		 */
+		mutex_lock(&kbdev->mmu_hw_mutex);
+
+		/* Trace the context itself into the body stream, not the
+		 * summary stream.
+		 * We place this in the body to ensure it is ordered after any
+		 * other tracepoints related to the contents of the context that
+		 * might have been fired before acquiring all of the per-context
+		 * locks.
+		 * This ensures that those tracepoints will not actually affect
+		 * the object model state, as they reference a context that
+		 * hasn't been traced yet. They may, however, cause benign
+		 * errors to be emitted.
+		 */
+		__kbase_tlstream_tl_kbase_new_ctx(body, kctx->id,
+				kbdev->gpu_props.props.raw_props.gpu_id);
+
+		/* Also trace with the legacy AOM tracepoint for dumping */
+		__kbase_tlstream_tl_new_ctx(body,
+				kctx,
+				kctx->id,
+				(u32)(kctx->tgid));
+
+		/* Trace the currently assigned address space */
+		if (kctx->as_nr != KBASEP_AS_NR_INVALID)
+			__kbase_tlstream_tl_kbase_ctx_assign_as(body, kctx->id,
+				kctx->as_nr);
+
+
+		/* Trace all KCPU queues in the context into the body stream.
+		 * As we acquired the KCPU lock after resetting the body stream,
+		 * it's possible that some KCPU-related events for this context
+		 * occurred between that reset and now.
+		 * These will cause errors to be emitted when parsing the
+		 * timeline, but they will not affect the correctness of the
+		 * object model.
+		 */
+		for (i = 0; i < KBASEP_MAX_KCPU_QUEUES; i++) {
+			const struct kbase_kcpu_command_queue *kcpu_queue =
+				kctx->csf.kcpu_queues.array[i];
+
+			if (kcpu_queue)
+				__kbase_tlstream_tl_kbase_new_kcpuqueue(
+					body, kcpu_queue, kcpu_queue->kctx->id,
+					kcpu_queue->num_pending_cmds);
+		}
+
+		mutex_unlock(&kbdev->mmu_hw_mutex);
+		mutex_unlock(&kctx->csf.kcpu_queues.lock);
+
+		/* Now that all per-context locks for this context have been
+		 * released, any per-context tracepoints that are fired from
+		 * any other threads will go into the body stream after
+		 * everything that was just summarised into the body stream in
+		 * this iteration of the loop, so will start to correctly update
+		 * the object model state.
+		 */
+	};
+
+	mutex_unlock(&kbdev->kctx_list_lock);
+
+	/* Static object are placed into summary packet that needs to be
+	 * transmitted first. Flush all streams to make it available to
+	 * user space.
+	 */
+	kbase_timeline_streams_flush(timeline);
+}
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c
index 8c465a4f4425..8d8834fdcda6 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline.c
@@ -109,6 +109,9 @@ int kbase_timeline_init(struct kbase_timeline **timeline,
 {
 	enum tl_stream_type i;
 	struct kbase_timeline *result;
+#if MALI_USE_CSF
+	struct kbase_tlstream *csffw_stream;
+#endif
 
 	if (!timeline || !timeline_flags)
 		return -EINVAL;
@@ -131,6 +134,10 @@ int kbase_timeline_init(struct kbase_timeline **timeline,
 			  kbasep_timeline_autoflush_timer_callback);
 	result->timeline_flags = timeline_flags;
 
+#if MALI_USE_CSF
+	csffw_stream = &result->streams[TL_STREAM_TYPE_CSFFW];
+	kbase_csf_tl_reader_init(&result->csf_tl_reader, csffw_stream);
+#endif
 
 	*timeline = result;
 	return 0;
@@ -143,6 +150,9 @@ void kbase_timeline_term(struct kbase_timeline *timeline)
 	if (!timeline)
 		return;
 
+#if MALI_USE_CSF
+	kbase_csf_tl_reader_term(&timeline->csf_tl_reader);
+#endif
 
 	for (i = (enum tl_stream_type)0; i < TL_STREAM_TYPE_COUNT; i++)
 		kbase_tlstream_term(&timeline->streams[i]);
@@ -182,6 +192,17 @@ int kbase_timeline_io_acquire(struct kbase_device *kbdev, u32 flags)
 	if (!atomic_cmpxchg(timeline->timeline_flags, 0, timeline_flags)) {
 		int rcode;
 
+#if MALI_USE_CSF
+		if (flags & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS) {
+			ret = kbase_csf_tl_reader_start(
+				&timeline->csf_tl_reader, kbdev);
+			if (ret)
+			{
+				atomic_set(timeline->timeline_flags, 0);
+				return ret;
+			}
+		}
+#endif
 		ret = anon_inode_getfd(
 				"[mali_tlstream]",
 				&kbasep_tlstream_fops,
@@ -189,6 +210,9 @@ int kbase_timeline_io_acquire(struct kbase_device *kbdev, u32 flags)
 				O_RDONLY | O_CLOEXEC);
 		if (ret < 0) {
 			atomic_set(timeline->timeline_flags, 0);
+#if MALI_USE_CSF
+			kbase_csf_tl_reader_stop(&timeline->csf_tl_reader);
+#endif
 			return ret;
 		}
 
@@ -206,6 +230,7 @@ int kbase_timeline_io_acquire(struct kbase_device *kbdev, u32 flags)
 				jiffies + msecs_to_jiffies(AUTOFLUSH_INTERVAL));
 		CSTD_UNUSED(rcode);
 
+#if !MALI_USE_CSF
 		/* If job dumping is enabled, readjust the software event's
 		 * timeout as the default value of 3 seconds is often
 		 * insufficient.
@@ -216,6 +241,7 @@ int kbase_timeline_io_acquire(struct kbase_device *kbdev, u32 flags)
 			atomic_set(&kbdev->js_data.soft_job_timeout_ms,
 					1800000);
 		}
+#endif /* !MALI_USE_CSF */
 
 		/* Summary stream was cleared during acquire.
 		 * Create static timeline objects that will be
@@ -242,6 +268,10 @@ void kbase_timeline_streams_flush(struct kbase_timeline *timeline)
 {
 	enum tl_stream_type stype;
 
+#if MALI_USE_CSF
+	kbase_csf_tl_reader_flush_buffer(&timeline->csf_tl_reader);
+#endif
+
 	for (stype = 0; stype < TL_STREAM_TYPE_COUNT; stype++)
 		kbase_tlstream_flush_stream(&timeline->streams[stype]);
 }
@@ -252,6 +282,10 @@ void kbase_timeline_streams_body_reset(struct kbase_timeline *timeline)
 			&timeline->streams[TL_STREAM_TYPE_OBJ]);
 	kbase_tlstream_reset(
 			&timeline->streams[TL_STREAM_TYPE_AUX]);
+#if MALI_USE_CSF
+	kbase_tlstream_reset(
+			&timeline->streams[TL_STREAM_TYPE_CSFFW]);
+#endif
 }
 
 #if MALI_UNIT_TEST
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c
index cdde928bbab9..724f5fa23725 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_io.c
@@ -84,6 +84,25 @@ static int kbasep_timeline_io_packet_pending(
 	return 0;
 }
 
+/**
+ * kbasep_timeline_has_header_data() -
+ *	check timeline headers for pending packets
+ *
+ * @timeline:      Timeline instance
+ *
+ * Return: non-zero if any of timeline headers has at last one packet ready.
+ */
+static int kbasep_timeline_has_header_data(
+	struct kbase_timeline *timeline)
+{
+	return timeline->obj_header_btc
+		|| timeline->aux_header_btc
+#if MALI_USE_CSF
+		|| timeline->csf_tl_reader.tl_header.btc
+#endif
+		;
+}
+
 /**
  * copy_stream_header() - copy timeline stream header.
  *
@@ -152,6 +171,13 @@ static inline int kbasep_timeline_copy_headers(
 			aux_desc_header_size,
 			&timeline->aux_header_btc))
 		return -1;
+#if MALI_USE_CSF
+	if (copy_stream_header(buffer, size, copy_len,
+			timeline->csf_tl_reader.tl_header.data,
+			timeline->csf_tl_reader.tl_header.size,
+			&timeline->csf_tl_reader.tl_header.btc))
+		return -1;
+#endif
 	return 0;
 }
 
@@ -294,6 +320,10 @@ static unsigned int kbasep_timeline_io_poll(struct file *filp, poll_table *wait)
 
 	timeline = (struct kbase_timeline *) filp->private_data;
 
+	/* If there are header bytes to copy, read will not block */
+	if (kbasep_timeline_has_header_data(timeline))
+		return POLLIN;
+
 	poll_wait(filp, &timeline->event_queue, wait);
 	if (kbasep_timeline_io_packet_pending(timeline, &stream, &rb_idx))
 		return POLLIN;
@@ -319,6 +349,9 @@ static int kbasep_timeline_io_release(struct inode *inode, struct file *filp)
 
 	timeline = (struct kbase_timeline *) filp->private_data;
 
+#if MALI_USE_CSF
+	kbase_csf_tl_reader_stop(&timeline->csf_tl_reader);
+#endif
 
 	/* Stop autoflush timer before releasing access to streams. */
 	atomic_set(&timeline->autoflush_timer_active, 0);
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_priv.h b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_priv.h
index 3596584d85c6..35eec467af90 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_priv.h
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_timeline_priv.h
@@ -26,6 +26,10 @@
 #include <mali_kbase.h>
 #include "mali_kbase_tlstream.h"
 
+#if MALI_USE_CSF
+#include "csf/mali_kbase_csf_tl_reader.h"
+#include "csf/mali_kbase_csf_trace_buffer.h"
+#endif
 
 #include <linux/timer.h>
 #include <linux/atomic.h>
@@ -57,6 +61,9 @@ struct kbase_timeline {
 	atomic_t         *timeline_flags;
 	size_t            obj_header_btc;
 	size_t            aux_header_btc;
+#if MALI_USE_CSF
+	struct kbase_csf_tl_reader csf_tl_reader;
+#endif
 };
 
 extern const struct file_operations kbasep_tlstream_fops;
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c
index bec4be71570e..f4239cfafb9d 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.c
@@ -131,6 +131,14 @@ static const struct {
 		TL_PACKET_TYPE_BODY,
 		TL_STREAM_ID_KERNEL,
 	},
+#if MALI_USE_CSF
+	{
+		TL_PACKET_FAMILY_TL,
+		TL_PACKET_CLASS_OBJ,
+		TL_PACKET_TYPE_BODY,
+		TL_STREAM_ID_CSFFW,
+	},
+#endif
 };
 
 void kbase_tlstream_init(
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.h b/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.h
index 427bb0969540..faf88d676b5d 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.h
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_tlstream.h
@@ -97,6 +97,9 @@ enum tl_stream_type {
 	TL_STREAM_TYPE_OBJ_SUMMARY = TL_STREAM_TYPE_FIRST,
 	TL_STREAM_TYPE_OBJ,
 	TL_STREAM_TYPE_AUX,
+#if MALI_USE_CSF
+	TL_STREAM_TYPE_CSFFW,
+#endif
 	TL_STREAM_TYPE_COUNT
 };
 
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c
index e445a3a3d683..de76fa57051e 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.c
@@ -66,10 +66,10 @@ enum tl_msg_id_obj {
 	KBASE_TL_EVENT_ATOM_SOFTSTOP_ISSUE,
 	KBASE_TL_EVENT_ATOM_SOFTJOB_START,
 	KBASE_TL_EVENT_ATOM_SOFTJOB_END,
-	KBASE_TL_EVENT_ARB_GRANTED,
-	KBASE_TL_EVENT_ARB_STARTED,
-	KBASE_TL_EVENT_ARB_STOP_REQUESTED,
-	KBASE_TL_EVENT_ARB_STOPPED,
+	KBASE_TL_ARBITER_GRANTED,
+	KBASE_TL_ARBITER_STARTED,
+	KBASE_TL_ARBITER_STOP_REQUESTED,
+	KBASE_TL_ARBITER_STOPPED,
 	KBASE_JD_GPU_SOFT_RESET,
 	KBASE_TL_KBASE_NEW_DEVICE,
 	KBASE_TL_KBASE_DEVICE_PROGRAM_CSG,
@@ -82,15 +82,8 @@ enum tl_msg_id_obj {
 	KBASE_TL_KBASE_DEL_KCPUQUEUE,
 	KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_SIGNAL,
 	KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_WAIT,
-	KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_WAIT,
-	KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_WAIT,
-	KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_WAIT,
-	KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_SET,
-	KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_SET,
-	KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_SET,
-	KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_DEBUGCOPY,
-	KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_DEBUGCOPY,
-	KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_DEBUGCOPY,
+	KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_WAIT,
+	KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_SET,
 	KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_MAP_IMPORT,
 	KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT,
 	KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT_FORCE,
@@ -107,8 +100,6 @@ enum tl_msg_id_obj {
 	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_START,
 	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_END,
 	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_SET,
-	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_START,
-	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_END,
 	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START,
 	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_END,
 	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_START,
@@ -125,6 +116,7 @@ enum tl_msg_id_obj {
 	KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_FREE_END,
 	KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_ERRORBARRIER,
 	KBASE_TL_KBASE_CSFFW_TLSTREAM_OVERFLOW,
+	KBASE_TL_KBASE_CSFFW_RESET,
 	KBASE_OBJ_MSG_COUNT,
 };
 
@@ -276,19 +268,19 @@ enum tl_msg_id_aux {
 		"atom soft job has completed", \
 		"@p", \
 		"atom") \
-	TRACEPOINT_DESC(KBASE_TL_EVENT_ARB_GRANTED, \
+	TRACEPOINT_DESC(KBASE_TL_ARBITER_GRANTED, \
 		"Arbiter has granted gpu access", \
 		"@p", \
 		"gpu") \
-	TRACEPOINT_DESC(KBASE_TL_EVENT_ARB_STARTED, \
+	TRACEPOINT_DESC(KBASE_TL_ARBITER_STARTED, \
 		"Driver is running again and able to process jobs", \
 		"@p", \
 		"gpu") \
-	TRACEPOINT_DESC(KBASE_TL_EVENT_ARB_STOP_REQUESTED, \
+	TRACEPOINT_DESC(KBASE_TL_ARBITER_STOP_REQUESTED, \
 		"Arbiter has requested driver to stop using gpu", \
 		"@p", \
 		"gpu") \
-	TRACEPOINT_DESC(KBASE_TL_EVENT_ARB_STOPPED, \
+	TRACEPOINT_DESC(KBASE_TL_ARBITER_STOPPED, \
 		"Driver has stopped using gpu", \
 		"@p", \
 		"gpu") \
@@ -340,42 +332,14 @@ enum tl_msg_id_aux {
 		"KCPU Queue enqueues Wait on Fence", \
 		"@pp", \
 		"kcpu_queue,fence") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_WAIT, \
-		"Begin array of KCPU Queue enqueues Wait on Cross Queue Sync Object", \
-		"@p", \
-		"kcpu_queue") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_WAIT, \
-		"Array item of KCPU Queue enqueues Wait on Cross Queue Sync Object", \
+	TRACEPOINT_DESC(KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_WAIT, \
+		"KCPU Queue enqueues Wait on Cross Queue Sync Object", \
 		"@pLI", \
 		"kcpu_queue,cqs_obj_gpu_addr,cqs_obj_compare_value") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_WAIT, \
-		"End array of KCPU Queue enqueues Wait on Cross Queue Sync Object", \
-		"@p", \
-		"kcpu_queue") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_SET, \
-		"Begin array of KCPU Queue enqueues Set on Cross Queue Sync Object", \
-		"@p", \
-		"kcpu_queue") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_SET, \
-		"Array item of KCPU Queue enqueues Set on Cross Queue Sync Object", \
+	TRACEPOINT_DESC(KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_SET, \
+		"KCPU Queue enqueues Set on Cross Queue Sync Object", \
 		"@pL", \
 		"kcpu_queue,cqs_obj_gpu_addr") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_SET, \
-		"End array of KCPU Queue enqueues Set on Cross Queue Sync Object", \
-		"@p", \
-		"kcpu_queue") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_DEBUGCOPY, \
-		"Begin array of KCPU Queue enqueues Debug Copy", \
-		"@p", \
-		"kcpu_queue") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_DEBUGCOPY, \
-		"Array item of KCPU Queue enqueues Debug Copy", \
-		"@pL", \
-		"kcpu_queue,debugcopy_dst_size") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_DEBUGCOPY, \
-		"End array of KCPU Queue enqueues Debug Copy", \
-		"@p", \
-		"kcpu_queue") \
 	TRACEPOINT_DESC(KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_MAP_IMPORT, \
 		"KCPU Queue enqueues Map Import", \
 		"@pL", \
@@ -440,14 +404,6 @@ enum tl_msg_id_aux {
 		"KCPU Queue executes a Set on an array of Cross Queue Sync Objects", \
 		"@p", \
 		"kcpu_queue") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_START, \
-		"KCPU Queue starts an array of Debug Copys", \
-		"@p", \
-		"kcpu_queue") \
-	TRACEPOINT_DESC(KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_END, \
-		"KCPU Queue ends an array of Debug Copys", \
-		"@p", \
-		"kcpu_queue") \
 	TRACEPOINT_DESC(KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START, \
 		"KCPU Queue starts a Map Import", \
 		"@p", \
@@ -512,6 +468,10 @@ enum tl_msg_id_aux {
 		"An overflow has happened with the CSFFW Timeline stream", \
 		"@LL", \
 		"csffw_timestamp,csffw_cycle") \
+	TRACEPOINT_DESC(KBASE_TL_KBASE_CSFFW_RESET, \
+		"A reset has happened with the CSFFW", \
+		"@L", \
+		"csffw_cycle") \
 
 #define MIPE_HEADER_BLOB_VAR_NAME		__obj_desc_header
 #define MIPE_HEADER_STREAM_ID			TL_STREAM_ID_KERNEL
@@ -1493,11 +1453,11 @@ void __kbase_tlstream_tl_event_atom_softjob_end(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_event_arb_granted(
+void __kbase_tlstream_tl_arbiter_granted(
 	struct kbase_tlstream *stream,
 	const void *gpu)
 {
-	const u32 msg_id = KBASE_TL_EVENT_ARB_GRANTED;
+	const u32 msg_id = KBASE_TL_ARBITER_GRANTED;
 	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
 		+ sizeof(gpu)
 		;
@@ -1515,11 +1475,11 @@ void __kbase_tlstream_tl_event_arb_granted(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_event_arb_started(
+void __kbase_tlstream_tl_arbiter_started(
 	struct kbase_tlstream *stream,
 	const void *gpu)
 {
-	const u32 msg_id = KBASE_TL_EVENT_ARB_STARTED;
+	const u32 msg_id = KBASE_TL_ARBITER_STARTED;
 	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
 		+ sizeof(gpu)
 		;
@@ -1537,11 +1497,11 @@ void __kbase_tlstream_tl_event_arb_started(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_event_arb_stop_requested(
+void __kbase_tlstream_tl_arbiter_stop_requested(
 	struct kbase_tlstream *stream,
 	const void *gpu)
 {
-	const u32 msg_id = KBASE_TL_EVENT_ARB_STOP_REQUESTED;
+	const u32 msg_id = KBASE_TL_ARBITER_STOP_REQUESTED;
 	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
 		+ sizeof(gpu)
 		;
@@ -1559,11 +1519,11 @@ void __kbase_tlstream_tl_event_arb_stop_requested(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_event_arb_stopped(
+void __kbase_tlstream_tl_arbiter_stopped(
 	struct kbase_tlstream *stream,
 	const void *gpu)
 {
-	const u32 msg_id = KBASE_TL_EVENT_ARB_STOPPED;
+	const u32 msg_id = KBASE_TL_ARBITER_STOPPED;
 	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
 		+ sizeof(gpu)
 		;
@@ -2161,35 +2121,13 @@ void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_cqs_wait(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_WAIT;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
-void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_cqs_wait(
+void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_wait(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue,
 	u64 cqs_obj_gpu_addr,
 	u32 cqs_obj_compare_value)
 {
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_WAIT;
+	const u32 msg_id = KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_WAIT;
 	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
 		+ sizeof(kcpu_queue)
 		+ sizeof(cqs_obj_gpu_addr)
@@ -2213,56 +2151,12 @@ void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_cqs_wait(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_cqs_wait(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_WAIT;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
-void __kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_cqs_set(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_SET;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
-void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_cqs_set(
+void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_set(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue,
 	u64 cqs_obj_gpu_addr)
 {
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_SET;
+	const u32 msg_id = KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_SET;
 	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
 		+ sizeof(kcpu_queue)
 		+ sizeof(cqs_obj_gpu_addr)
@@ -2283,98 +2177,6 @@ void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_cqs_set(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_cqs_set(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_SET;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
-void __kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_debugcopy(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_DEBUGCOPY;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
-void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_debugcopy(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue,
-	u64 debugcopy_dst_size)
-{
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_DEBUGCOPY;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		+ sizeof(debugcopy_dst_size)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &debugcopy_dst_size, sizeof(debugcopy_dst_size));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
-void __kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_debugcopy(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_DEBUGCOPY;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
 void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_map_import(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue,
@@ -2779,50 +2581,6 @@ void __kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_set(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
-void __kbase_tlstream_tl_kbase_kcpuqueue_execute_debugcopy_start(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_START;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
-void __kbase_tlstream_tl_kbase_kcpuqueue_execute_debugcopy_end(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue)
-{
-	const u32 msg_id = KBASE_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_END;
-	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
-		+ sizeof(kcpu_queue)
-		;
-	char *buffer;
-	unsigned long acq_flags;
-	size_t pos = 0;
-
-	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
-
-	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
-	pos = kbasep_serialize_timestamp(buffer, pos);
-	pos = kbasep_serialize_bytes(buffer,
-		pos, &kcpu_queue, sizeof(kcpu_queue));
-
-	kbase_tlstream_msgbuf_release(stream, acq_flags);
-}
-
 void __kbase_tlstream_tl_kbase_kcpuqueue_execute_map_import_start(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue)
@@ -3191,4 +2949,26 @@ void __kbase_tlstream_tl_kbase_csffw_tlstream_overflow(
 	kbase_tlstream_msgbuf_release(stream, acq_flags);
 }
 
+void __kbase_tlstream_tl_kbase_csffw_reset(
+	struct kbase_tlstream *stream,
+	u64 csffw_cycle)
+{
+	const u32 msg_id = KBASE_TL_KBASE_CSFFW_RESET;
+	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
+		+ sizeof(csffw_cycle)
+		;
+	char *buffer;
+	unsigned long acq_flags;
+	size_t pos = 0;
+
+	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);
+
+	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
+	pos = kbasep_serialize_timestamp(buffer, pos);
+	pos = kbasep_serialize_bytes(buffer,
+		pos, &csffw_cycle, sizeof(csffw_cycle));
+
+	kbase_tlstream_msgbuf_release(stream, acq_flags);
+}
+
 /* clang-format on */
diff --git a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h
index ef0454386799..5651f0a0fc57 100644
--- a/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h
+++ b/drivers/gpu/arm/bifrost/tl/mali_kbase_tracepoints.h
@@ -226,16 +226,16 @@ void __kbase_tlstream_tl_event_atom_softjob_start(
 void __kbase_tlstream_tl_event_atom_softjob_end(
 	struct kbase_tlstream *stream,
 	const void *atom);
-void __kbase_tlstream_tl_event_arb_granted(
+void __kbase_tlstream_tl_arbiter_granted(
 	struct kbase_tlstream *stream,
 	const void *gpu);
-void __kbase_tlstream_tl_event_arb_started(
+void __kbase_tlstream_tl_arbiter_started(
 	struct kbase_tlstream *stream,
 	const void *gpu);
-void __kbase_tlstream_tl_event_arb_stop_requested(
+void __kbase_tlstream_tl_arbiter_stop_requested(
 	struct kbase_tlstream *stream,
 	const void *gpu);
-void __kbase_tlstream_tl_event_arb_stopped(
+void __kbase_tlstream_tl_arbiter_stopped(
 	struct kbase_tlstream *stream,
 	const void *gpu);
 void __kbase_tlstream_jd_gpu_soft_reset(
@@ -328,37 +328,15 @@ void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue,
 	const void *fence);
-void __kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_cqs_wait(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
-void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_cqs_wait(
+void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_wait(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue,
 	u64 cqs_obj_gpu_addr,
 	u32 cqs_obj_compare_value);
-void __kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_cqs_wait(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
-void __kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_cqs_set(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
-void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_cqs_set(
+void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_set(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue,
 	u64 cqs_obj_gpu_addr);
-void __kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_cqs_set(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
-void __kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_debugcopy(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
-void __kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_debugcopy(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue,
-	u64 debugcopy_dst_size);
-void __kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_debugcopy(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
 void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_map_import(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue,
@@ -420,12 +398,6 @@ void __kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_wait_end(
 void __kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_set(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue);
-void __kbase_tlstream_tl_kbase_kcpuqueue_execute_debugcopy_start(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
-void __kbase_tlstream_tl_kbase_kcpuqueue_execute_debugcopy_end(
-	struct kbase_tlstream *stream,
-	const void *kcpu_queue);
 void __kbase_tlstream_tl_kbase_kcpuqueue_execute_map_import_start(
 	struct kbase_tlstream *stream,
 	const void *kcpu_queue);
@@ -478,6 +450,9 @@ void __kbase_tlstream_tl_kbase_csffw_tlstream_overflow(
 	struct kbase_tlstream *stream,
 	u64 csffw_timestamp,
 	u64 csffw_cycle);
+void __kbase_tlstream_tl_kbase_csffw_reset(
+	struct kbase_tlstream *stream,
+	u64 csffw_cycle);
 
 struct kbase_tlstream;
 
@@ -1215,77 +1190,77 @@ struct kbase_tlstream;
 	} while (0)
 
 /**
- * KBASE_TLSTREAM_TL_EVENT_ARB_GRANTED -
+ * KBASE_TLSTREAM_TL_ARBITER_GRANTED -
  *   Arbiter has granted gpu access
  *
  * @kbdev: Kbase device
  * @gpu: Name of the GPU object
  */
-#define KBASE_TLSTREAM_TL_EVENT_ARB_GRANTED(	\
+#define KBASE_TLSTREAM_TL_ARBITER_GRANTED(	\
 	kbdev,	\
 	gpu	\
 	)	\
 	do {	\
 		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
-			__kbase_tlstream_tl_event_arb_granted(	\
+			__kbase_tlstream_tl_arbiter_granted(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
 				gpu);	\
 	} while (0)
 
 /**
- * KBASE_TLSTREAM_TL_EVENT_ARB_STARTED -
+ * KBASE_TLSTREAM_TL_ARBITER_STARTED -
  *   Driver is running again and able to process jobs
  *
  * @kbdev: Kbase device
  * @gpu: Name of the GPU object
  */
-#define KBASE_TLSTREAM_TL_EVENT_ARB_STARTED(	\
+#define KBASE_TLSTREAM_TL_ARBITER_STARTED(	\
 	kbdev,	\
 	gpu	\
 	)	\
 	do {	\
 		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
-			__kbase_tlstream_tl_event_arb_started(	\
+			__kbase_tlstream_tl_arbiter_started(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
 				gpu);	\
 	} while (0)
 
 /**
- * KBASE_TLSTREAM_TL_EVENT_ARB_STOP_REQUESTED -
+ * KBASE_TLSTREAM_TL_ARBITER_STOP_REQUESTED -
  *   Arbiter has requested driver to stop using gpu
  *
  * @kbdev: Kbase device
  * @gpu: Name of the GPU object
  */
-#define KBASE_TLSTREAM_TL_EVENT_ARB_STOP_REQUESTED(	\
+#define KBASE_TLSTREAM_TL_ARBITER_STOP_REQUESTED(	\
 	kbdev,	\
 	gpu	\
 	)	\
 	do {	\
 		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
-			__kbase_tlstream_tl_event_arb_stop_requested(	\
+			__kbase_tlstream_tl_arbiter_stop_requested(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
 				gpu);	\
 	} while (0)
 
 /**
- * KBASE_TLSTREAM_TL_EVENT_ARB_STOPPED -
+ * KBASE_TLSTREAM_TL_ARBITER_STOPPED -
  *   Driver has stopped using gpu
  *
  * @kbdev: Kbase device
  * @gpu: Name of the GPU object
  */
-#define KBASE_TLSTREAM_TL_EVENT_ARB_STOPPED(	\
+#define KBASE_TLSTREAM_TL_ARBITER_STOPPED(	\
 	kbdev,	\
 	gpu	\
 	)	\
 	do {	\
 		int enabled = atomic_read(&kbdev->timeline_flags);	\
 		if (enabled & TLSTREAM_ENABLED)	\
-			__kbase_tlstream_tl_event_arb_stopped(	\
+			__kbase_tlstream_tl_arbiter_stopped(	\
 				__TL_DISPATCH_STREAM(kbdev, obj),	\
 				gpu);	\
 	} while (0)
@@ -1534,6 +1509,22 @@ struct kbase_tlstream;
  * @kbase_device_max_num_csgs: The max number of CSGs the physical hardware supports
  * @kbase_device_as_count: The number of address spaces the physical hardware has available
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_NEW_DEVICE(	\
+	kbdev,	\
+	kbase_device_id,	\
+	kbase_device_gpu_core_count,	\
+	kbase_device_max_num_csgs,	\
+	kbase_device_as_count	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_new_device(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kbase_device_id, kbase_device_gpu_core_count, kbase_device_max_num_csgs, kbase_device_as_count);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_NEW_DEVICE(	\
 	kbdev,	\
 	kbase_device_id,	\
@@ -1542,6 +1533,7 @@ struct kbase_tlstream;
 	kbase_device_as_count	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_DEVICE_PROGRAM_CSG -
@@ -1552,6 +1544,21 @@ struct kbase_tlstream;
  * @gpu_cmdq_grp_handle: GPU Command Queue Group handle which will match userspace
  * @kbase_device_csg_slot_index: The index of the slot in the scheduler being programmed
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_DEVICE_PROGRAM_CSG(	\
+	kbdev,	\
+	kbase_device_id,	\
+	gpu_cmdq_grp_handle,	\
+	kbase_device_csg_slot_index	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_device_program_csg(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kbase_device_id, gpu_cmdq_grp_handle, kbase_device_csg_slot_index);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_DEVICE_PROGRAM_CSG(	\
 	kbdev,	\
 	kbase_device_id,	\
@@ -1559,6 +1566,7 @@ struct kbase_tlstream;
 	kbase_device_csg_slot_index	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_DEVICE_DEPROGRAM_CSG -
@@ -1568,12 +1576,27 @@ struct kbase_tlstream;
  * @kbase_device_id: The id of the physical hardware
  * @kbase_device_csg_slot_index: The index of the slot in the scheduler being programmed
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_DEVICE_DEPROGRAM_CSG(	\
+	kbdev,	\
+	kbase_device_id,	\
+	kbase_device_csg_slot_index	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_device_deprogram_csg(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kbase_device_id, kbase_device_csg_slot_index);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_DEVICE_DEPROGRAM_CSG(	\
 	kbdev,	\
 	kbase_device_id,	\
 	kbase_device_csg_slot_index	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_NEW_CTX -
@@ -1583,12 +1606,27 @@ struct kbase_tlstream;
  * @kernel_ctx_id: Unique ID for the KBase Context
  * @kbase_device_id: The id of the physical hardware
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_NEW_CTX(	\
+	kbdev,	\
+	kernel_ctx_id,	\
+	kbase_device_id	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_new_ctx(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kernel_ctx_id, kbase_device_id);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_NEW_CTX(	\
 	kbdev,	\
 	kernel_ctx_id,	\
 	kbase_device_id	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_DEL_CTX -
@@ -1597,11 +1635,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kernel_ctx_id: Unique ID for the KBase Context
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_DEL_CTX(	\
+	kbdev,	\
+	kernel_ctx_id	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_del_ctx(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kernel_ctx_id);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_DEL_CTX(	\
 	kbdev,	\
 	kernel_ctx_id	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_CTX_ASSIGN_AS -
@@ -1611,12 +1663,27 @@ struct kbase_tlstream;
  * @kernel_ctx_id: Unique ID for the KBase Context
  * @kbase_device_as_index: The index of the device address space being assigned
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_CTX_ASSIGN_AS(	\
+	kbdev,	\
+	kernel_ctx_id,	\
+	kbase_device_as_index	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_ctx_assign_as(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kernel_ctx_id, kbase_device_as_index);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_CTX_ASSIGN_AS(	\
 	kbdev,	\
 	kernel_ctx_id,	\
 	kbase_device_as_index	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_CTX_UNASSIGN_AS -
@@ -1625,11 +1692,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kernel_ctx_id: Unique ID for the KBase Context
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_CTX_UNASSIGN_AS(	\
+	kbdev,	\
+	kernel_ctx_id	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_ctx_unassign_as(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kernel_ctx_id);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_CTX_UNASSIGN_AS(	\
 	kbdev,	\
 	kernel_ctx_id	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_NEW_KCPUQUEUE -
@@ -1641,6 +1722,21 @@ struct kbase_tlstream;
  * @kcpuq_num_pending_cmds: Number of commands already enqueued
  * in the KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_NEW_KCPUQUEUE(	\
+	kbdev,	\
+	kcpu_queue,	\
+	kernel_ctx_id,	\
+	kcpuq_num_pending_cmds	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_new_kcpuqueue(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, kernel_ctx_id, kcpuq_num_pending_cmds);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_NEW_KCPUQUEUE(	\
 	kbdev,	\
 	kcpu_queue,	\
@@ -1648,6 +1744,7 @@ struct kbase_tlstream;
 	kcpuq_num_pending_cmds	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_DEL_KCPUQUEUE -
@@ -1656,11 +1753,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_DEL_KCPUQUEUE(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_del_kcpuqueue(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_DEL_KCPUQUEUE(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_SIGNAL -
@@ -1670,12 +1781,27 @@ struct kbase_tlstream;
  * @kcpu_queue: KCPU queue
  * @fence: Fence object handle
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_SIGNAL(	\
+	kbdev,	\
+	kcpu_queue,	\
+	fence	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_signal(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, fence);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_SIGNAL(	\
 	kbdev,	\
 	kcpu_queue,	\
 	fence	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_WAIT -
@@ -1685,29 +1811,31 @@ struct kbase_tlstream;
  * @kcpu_queue: KCPU queue
  * @fence: Fence object handle
  */
+#if MALI_USE_CSF
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_WAIT(	\
 	kbdev,	\
 	kcpu_queue,	\
 	fence	\
 	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_WAIT -
- *   Begin array of KCPU Queue enqueues Wait on Cross Queue Sync Object
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_WAIT(	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, fence);	\
+	} while (0)
+#else
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_WAIT(	\
 	kbdev,	\
-	kcpu_queue	\
+	kcpu_queue,	\
+	fence	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_WAIT -
- *   Array item of KCPU Queue enqueues Wait on Cross Queue Sync Object
+ * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_WAIT -
+ *   KCPU Queue enqueues Wait on Cross Queue Sync Object
  *
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
@@ -1715,108 +1843,59 @@ struct kbase_tlstream;
  * @cqs_obj_compare_value: Semaphore value that should be exceeded
  * for the WAIT to pass
  */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_WAIT(	\
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_WAIT(	\
 	kbdev,	\
 	kcpu_queue,	\
 	cqs_obj_gpu_addr,	\
 	cqs_obj_compare_value	\
 	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_WAIT -
- *   End array of KCPU Queue enqueues Wait on Cross Queue Sync Object
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_WAIT(	\
-	kbdev,	\
-	kcpu_queue	\
-	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_SET -
- *   Begin array of KCPU Queue enqueues Set on Cross Queue Sync Object
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_CQS_SET(	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_wait(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, cqs_obj_gpu_addr, cqs_obj_compare_value);	\
+	} while (0)
+#else
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_WAIT(	\
 	kbdev,	\
-	kcpu_queue	\
+	kcpu_queue,	\
+	cqs_obj_gpu_addr,	\
+	cqs_obj_compare_value	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_SET -
- *   Array item of KCPU Queue enqueues Set on Cross Queue Sync Object
+ * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_SET -
+ *   KCPU Queue enqueues Set on Cross Queue Sync Object
  *
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  * @cqs_obj_gpu_addr: CQS Object GPU ptr
  */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_CQS_SET(	\
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_SET(	\
 	kbdev,	\
 	kcpu_queue,	\
 	cqs_obj_gpu_addr	\
 	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_SET -
- *   End array of KCPU Queue enqueues Set on Cross Queue Sync Object
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_CQS_SET(	\
-	kbdev,	\
-	kcpu_queue	\
-	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_DEBUGCOPY -
- *   Begin array of KCPU Queue enqueues Debug Copy
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_DEBUGCOPY(	\
-	kbdev,	\
-	kcpu_queue	\
-	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_DEBUGCOPY -
- *   Array item of KCPU Queue enqueues Debug Copy
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- * @debugcopy_dst_size: Debug Copy destination size
- */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_DEBUGCOPY(	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_cqs_set(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, cqs_obj_gpu_addr);	\
+	} while (0)
+#else
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_CQS_SET(	\
 	kbdev,	\
 	kcpu_queue,	\
-	debugcopy_dst_size	\
-	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_DEBUGCOPY -
- *   End array of KCPU Queue enqueues Debug Copy
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
-#define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_DEBUGCOPY(	\
-	kbdev,	\
-	kcpu_queue	\
+	cqs_obj_gpu_addr	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_MAP_IMPORT -
@@ -1826,12 +1905,27 @@ struct kbase_tlstream;
  * @kcpu_queue: KCPU queue
  * @map_import_buf_gpu_addr: Map import buffer GPU ptr
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_MAP_IMPORT(	\
+	kbdev,	\
+	kcpu_queue,	\
+	map_import_buf_gpu_addr	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_map_import(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, map_import_buf_gpu_addr);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_MAP_IMPORT(	\
 	kbdev,	\
 	kcpu_queue,	\
 	map_import_buf_gpu_addr	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT -
@@ -1841,12 +1935,27 @@ struct kbase_tlstream;
  * @kcpu_queue: KCPU queue
  * @map_import_buf_gpu_addr: Map import buffer GPU ptr
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT(	\
+	kbdev,	\
+	kcpu_queue,	\
+	map_import_buf_gpu_addr	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_unmap_import(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, map_import_buf_gpu_addr);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT(	\
 	kbdev,	\
 	kcpu_queue,	\
 	map_import_buf_gpu_addr	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT_FORCE -
@@ -1856,12 +1965,27 @@ struct kbase_tlstream;
  * @kcpu_queue: KCPU queue
  * @map_import_buf_gpu_addr: Map import buffer GPU ptr
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT_FORCE(	\
+	kbdev,	\
+	kcpu_queue,	\
+	map_import_buf_gpu_addr	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_unmap_import_force(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, map_import_buf_gpu_addr);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_ENQUEUE_UNMAP_IMPORT_FORCE(	\
 	kbdev,	\
 	kcpu_queue,	\
 	map_import_buf_gpu_addr	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_ALLOC -
@@ -1870,11 +1994,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_ALLOC(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_jit_alloc(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_ALLOC(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_ALLOC -
@@ -1902,6 +2040,28 @@ struct kbase_tlstream;
  * reused. The kernel should attempt to use a previous allocation with the same
  * usage_id
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_ALLOC(	\
+	kbdev,	\
+	kcpu_queue,	\
+	jit_alloc_gpu_alloc_addr_dest,	\
+	jit_alloc_va_pages,	\
+	jit_alloc_commit_pages,	\
+	jit_alloc_extent,	\
+	jit_alloc_jit_id,	\
+	jit_alloc_bin_id,	\
+	jit_alloc_max_allocations,	\
+	jit_alloc_flags,	\
+	jit_alloc_usage_id	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_jit_alloc(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, jit_alloc_gpu_alloc_addr_dest, jit_alloc_va_pages, jit_alloc_commit_pages, jit_alloc_extent, jit_alloc_jit_id, jit_alloc_bin_id, jit_alloc_max_allocations, jit_alloc_flags, jit_alloc_usage_id);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_ALLOC(	\
 	kbdev,	\
 	kcpu_queue,	\
@@ -1916,6 +2076,7 @@ struct kbase_tlstream;
 	jit_alloc_usage_id	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_ALLOC -
@@ -1924,11 +2085,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_ALLOC(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_jit_alloc(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_ALLOC(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_FREE -
@@ -1937,11 +2112,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_FREE(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_enqueue_jit_free(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_ENQUEUE_JIT_FREE(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_FREE -
@@ -1952,12 +2141,27 @@ struct kbase_tlstream;
  * @jit_alloc_jit_id: Unique ID provided by the caller, this is used
  * to pair allocation and free requests. Zero is not a valid value
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_FREE(	\
+	kbdev,	\
+	kcpu_queue,	\
+	jit_alloc_jit_id	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_enqueue_jit_free(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, jit_alloc_jit_id);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_ENQUEUE_JIT_FREE(	\
 	kbdev,	\
 	kcpu_queue,	\
 	jit_alloc_jit_id	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_FREE -
@@ -1966,11 +2170,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_FREE(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_enqueue_jit_free(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_ENQUEUE_JIT_FREE(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_START -
@@ -1979,11 +2197,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_START(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_signal_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_END -
@@ -1992,11 +2224,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_signal_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_SIGNAL_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_START -
@@ -2005,11 +2251,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_START(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_wait_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_END -
@@ -2018,11 +2278,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_fence_wait_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_FENCE_WAIT_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_START -
@@ -2031,11 +2305,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_START(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_wait_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_END -
@@ -2044,11 +2332,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_wait_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_WAIT_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_SET -
@@ -2057,50 +2359,52 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_SET(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_START -
- *   KCPU Queue starts an array of Debug Copys
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
-#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_START(	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_cqs_set(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_CQS_SET(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
- * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_END -
- *   KCPU Queue ends an array of Debug Copys
+ * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START -
+ *   KCPU Queue starts a Map Import
  *
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
-#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_DEBUGCOPY_END(	\
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
-	do { } while (0)
-
-/**
- * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START -
- *   KCPU Queue starts a Map Import
- *
- * @kbdev: Kbase device
- * @kcpu_queue: KCPU queue
- */
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_map_import_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_END -
@@ -2109,11 +2413,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_map_import_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_MAP_IMPORT_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_START -
@@ -2122,11 +2440,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_START(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_END -
@@ -2135,11 +2467,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_START -
@@ -2148,11 +2494,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_START(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_force_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_END -
@@ -2161,11 +2521,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_unmap_import_force_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_UNMAP_IMPORT_FORCE_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_START -
@@ -2174,11 +2548,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_START(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_jit_alloc_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_ALLOC_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_ALLOC_END -
@@ -2187,11 +2575,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_execute_jit_alloc_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_ALLOC_END -
@@ -2202,6 +2604,21 @@ struct kbase_tlstream;
  * @jit_alloc_gpu_alloc_addr: The JIT allocated GPU virtual address
  * @jit_alloc_mmu_flags: The MMU flags for the JIT allocation
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(	\
+	kbdev,	\
+	kcpu_queue,	\
+	jit_alloc_gpu_alloc_addr,	\
+	jit_alloc_mmu_flags	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_execute_jit_alloc_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, jit_alloc_gpu_alloc_addr, jit_alloc_mmu_flags);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(	\
 	kbdev,	\
 	kcpu_queue,	\
@@ -2209,6 +2626,7 @@ struct kbase_tlstream;
 	jit_alloc_mmu_flags	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_ALLOC_END -
@@ -2217,11 +2635,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_execute_jit_alloc_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_ALLOC_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_START -
@@ -2230,11 +2662,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_START(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_jit_free_start(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_JIT_FREE_START(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_FREE_END -
@@ -2243,11 +2689,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_FREE_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_begin_kcpuqueue_execute_jit_free_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_BEGIN_KCPUQUEUE_EXECUTE_JIT_FREE_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_FREE_END -
@@ -2258,12 +2718,27 @@ struct kbase_tlstream;
  * @jit_free_pages_used: The actual number of pages used by the JIT
  * allocation
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_FREE_END(	\
+	kbdev,	\
+	kcpu_queue,	\
+	jit_free_pages_used	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_item_kcpuqueue_execute_jit_free_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue, jit_free_pages_used);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_ITEM_KCPUQUEUE_EXECUTE_JIT_FREE_END(	\
 	kbdev,	\
 	kcpu_queue,	\
 	jit_free_pages_used	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_FREE_END -
@@ -2272,11 +2747,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_FREE_END(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_array_end_kcpuqueue_execute_jit_free_end(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_ARRAY_END_KCPUQUEUE_EXECUTE_JIT_FREE_END(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_ERRORBARRIER -
@@ -2285,11 +2774,25 @@ struct kbase_tlstream;
  * @kbdev: Kbase device
  * @kcpu_queue: KCPU queue
  */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_ERRORBARRIER(	\
+	kbdev,	\
+	kcpu_queue	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_kcpuqueue_execute_errorbarrier(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				kcpu_queue);	\
+	} while (0)
+#else
 #define KBASE_TLSTREAM_TL_KBASE_KCPUQUEUE_EXECUTE_ERRORBARRIER(	\
 	kbdev,	\
 	kcpu_queue	\
 	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 /**
  * KBASE_TLSTREAM_TL_KBASE_CSFFW_TLSTREAM_OVERFLOW -
@@ -2299,12 +2802,54 @@ struct kbase_tlstream;
  * @csffw_timestamp: Timestamp of a CSFFW event
  * @csffw_cycle: Cycle number of a CSFFW event
  */
+#if MALI_USE_CSF
 #define KBASE_TLSTREAM_TL_KBASE_CSFFW_TLSTREAM_OVERFLOW(	\
 	kbdev,	\
 	csffw_timestamp,	\
 	csffw_cycle	\
 	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_csffw_tlstream_overflow(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				csffw_timestamp, csffw_cycle);	\
+	} while (0)
+#else
+#define KBASE_TLSTREAM_TL_KBASE_CSFFW_TLSTREAM_OVERFLOW(	\
+	kbdev,	\
+	csffw_timestamp,	\
+	csffw_cycle	\
+	)	\
+	do { } while (0)
+#endif /* MALI_USE_CSF */
+
+/**
+ * KBASE_TLSTREAM_TL_KBASE_CSFFW_RESET -
+ *   A reset has happened with the CSFFW
+ *
+ * @kbdev: Kbase device
+ * @csffw_cycle: Cycle number of a CSFFW event
+ */
+#if MALI_USE_CSF
+#define KBASE_TLSTREAM_TL_KBASE_CSFFW_RESET(	\
+	kbdev,	\
+	csffw_cycle	\
+	)	\
+	do {	\
+		int enabled = atomic_read(&kbdev->timeline_flags);	\
+		if (enabled & BASE_TLSTREAM_ENABLE_CSFFW_TRACEPOINTS)	\
+			__kbase_tlstream_tl_kbase_csffw_reset(	\
+				__TL_DISPATCH_STREAM(kbdev, obj),	\
+				csffw_cycle);	\
+	} while (0)
+#else
+#define KBASE_TLSTREAM_TL_KBASE_CSFFW_RESET(	\
+	kbdev,	\
+	csffw_cycle	\
+	)	\
 	do { } while (0)
+#endif /* MALI_USE_CSF */
 
 
 /* Gator tracepoints are hooked into TLSTREAM interface.
-- 
2.35.3

